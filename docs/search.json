[
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n- 전북대학교 통계학과 학사(부전공: 컴퓨터공학) 졸업 | 2015. 03 ~ 2021. 02\n- 전북대학교 통계학과 석사 졸업 | 2021. 03 ~ 2023. 02"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n- 국민연금공단 빅데이터부 현장실습 | 2020. 03 ~ 2020. 06\n- 지역 문화산업 융복합 데이터 전문가 과정 | 과학기술정보통신부, 한국데이터산업진흥원 | 2021. 06 ~ 2021. 08\n- 에너지AI융합대학원 빅데이터 분석 특강 조교 | 전북대학교 AI에너지 융합대학원 |2021. 06 ~ 2021. 10\n- SPSS를 이용한 통계자료분석 특강 조교 | 전북대학교 통계학과 | 2022. 01 ~ 2022. 02\n- 데이터 준전문가 ADsP 특강 조교 | 전북대학교 통계학과 | 2022. 01 ~ 2022. 02"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About Me",
    "section": "Publications",
    "text": "Publications\n- 데이터 분석을 통한 지역별 고령친화도 시각화\n`-` 김영선, 강민구, 이강철 등  | 문화융복합아카이빙연구소 | 2021. 10 | 기록관리/보존 \n- 핵심어 추출 및 데이터 증강기법을 이용한 텍스트 분류 모델 성능 개선\n`-` 이강철, 안정용 | 한국자료분석학회 | 한국자료분석학회 | 2022. 10 | 통계학"
  },
  {
    "objectID": "about.html#certificate",
    "href": "about.html#certificate",
    "title": "About Me",
    "section": "Certificate",
    "text": "Certificate\n- 워드프로세서 | 대한상공회의소 | 19-19-017981 | 2019. 08. 30\n- 데이터분석준전문가(ADsP) | 한국데이터진흥원 | ADsP-0223898 | 2019. 10. 01\n- 사회조사분석사 2급 | 한국산업인력공단 | 19201142418N | 2019. 10. 01"
  },
  {
    "objectID": "about.html#conctact",
    "href": "about.html#conctact",
    "title": "About Me",
    "section": "Conctact",
    "text": "Conctact\n- rkdcjf8232@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gang Cheol Portfolio",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 7, 2023\n\n\n05. data visualization\n\n\nGANGCHEOL LEE\n\n\n\n\nJul 6, 2023\n\n\n04. data processing\n\n\nGANGCHEOL LEE\n\n\n\n\nJul 5, 2023\n\n\n03. data frame\n\n\nGANGCHEOL LEE\n\n\n\n\nJul 4, 2023\n\n\n02. Python basic\n\n\nGANGCHEOL LEE\n\n\n\n\nJul 3, 2023\n\n\n01. Intro\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 30, 2023\n\n\nsummary\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 23, 2023\n\n\npart 5\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 22, 2023\n\n\npart 4\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 21, 2023\n\n\npart 3\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 20, 2023\n\n\npart 2\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 19, 2023\n\n\npart 1\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 23, 2023\n\n\n02. 리스트\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 17, 2023\n\n\n11. Choropleth Map & Plotly\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 17, 2023\n\n\n12. final term\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 16, 2023\n\n\n01. 자료형\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 16, 2023\n\n\n10. Choropleth Map\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 15, 2023\n\n\n09. folium\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 14, 2023\n\n\n08. mid term\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 13, 2023\n\n\n07. tidydata\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 6, 2023\n\n\n06. Partial Correlation & Pandas Handling\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 3, 2023\n\n\n16. Extra-2: 생성모형 (GAN)\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 29, 2023\n\n\n05. Plotnine & Pandas\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 26, 2023\n\n\n15. Extra-1: 추천시스템\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 22, 2023\n\n\n04. qqplot, 찰스미나도의 도표\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 19, 2023\n\n\n14. 순환신경망 (5)\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 15, 2023\n\n\n03. 그래프를 그리는 여러 방법\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 12, 2023\n\n\n13. 순환신경망 (4)\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 8, 2023\n\n\n02. Histogram equalization\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 5, 2023\n\n\n12. 순환신경망 (3)\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 1, 2023\n\n\n01. intro\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 29, 2023\n\n\n11. 순환신경망 (2)\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 22, 2023\n\n\n10. 순환신경망 (1)\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 8, 2023\n\n\n09. 이미지분석 (2)\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 7, 2023\n\n\n08. 이미지분석 (1)\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 1, 2023\n\n\n07. 딥러닝의 기초 (5)\n\n\nGANGCHEOL LEE\n\n\n\n\nFeb 25, 2023\n\n\n06. 딥러닝의 기초 (4)\n\n\nGANGCHEOL LEE\n\n\n\n\nFeb 18, 2023\n\n\n05. 딥러닝의 기초 (3)\n\n\nGANGCHEOL LEE\n\n\n\n\nFeb 11, 2023\n\n\n04. 딥러닝의 기초 (2)\n\n\nGANGCHEOL LEE\n\n\n\n\nFeb 4, 2023\n\n\n03. 딥러닝의 기초 (1)\n\n\nGANGCHEOL LEE\n\n\n\n\nFeb 1, 2023\n\n\n02. Overview\n\n\nGANGCHEOL LEE\n\n\n\n\nJan 27, 2023\n\n\n01. Intro\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 13, 2022\n\n\n14. Final term\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 30, 2022\n\n\n13. model fitting\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 23, 2022\n\n\n13. CNN-2\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 11, 2022\n\n\n12. Max pooling, CNN\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 8, 2022\n\n\n11. softmax\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 2, 2022\n\n\n10. 확률적 경사하강법\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 1, 2022\n\n\n09. MLE\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 27, 2022\n\n\n08. Mid term\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 18, 2022\n\n\n07. Adam\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 11, 2022\n\n\n06. layer\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 11, 2022\n\n\n06. layer 과제\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 30, 2022\n\n\n05. 경사하강법-2\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 28, 2022\n\n\n04. 경사하강법-1\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 21, 2022\n\n\n03. Tensorflow-2\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 14, 2022\n\n\n02. Tensorflow-1\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 8, 2022\n\n\n01. 단순선형회귀\n\n\nGANGCHEOL LEE\n\n\n\n\nDec 5, 2021\n\n\n01. 결측치 처리\n\n\nGANGCHEOL LEE\n\n\n\n\nDec 5, 2021\n\n\n02. 표준화와 정규화\n\n\nGANGCHEOL LEE\n\n\n\n\nDec 5, 2021\n\n\n03. 날짜 데이터\n\n\nGANGCHEOL LEE\n\n\n\n\nJul 24, 2021\n\n\n03. Text Mining\n\n\nGANGCHEOL LEE\n\n\n\n\nJul 11, 2021\n\n\n02. unsupervised learning\n\n\nGANGCHEOL LEE\n\n\n\n\nJul 10, 2021\n\n\n01. Supervised Learning\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 28, 2021\n\n\n11. Time-seris analysis\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 24, 2021\n\n\n10. Multidimensional scaling\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 21, 2021\n\n\n09. Factor Analysis\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 17, 2021\n\n\n08. PCA\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 16, 2021\n\n\n07. regression analysis-2\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 15, 2021\n\n\n06. regression analysis\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 9, 2021\n\n\n05. cor-test\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 7, 2021\n\n\n04. chi-square test\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 5, 2021\n\n\n03. ANOVA\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 3, 2021\n\n\n02. T-test\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 1, 2021\n\n\n01. sampling\n\n\nGANGCHEOL LEE\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/etc/ai study/2023-07-03-01.Intro.html",
    "href": "post/etc/ai study/2023-07-03-01.Intro.html",
    "title": "01. Intro",
    "section": "",
    "text": "- 해당 자료는 KT AICE 강의 자료를 바탕으로 만들었습니다. 문제시 삭제하겠습니다."
  },
  {
    "objectID": "post/etc/ai study/2023-07-03-01.Intro.html#a.-인공지능",
    "href": "post/etc/ai study/2023-07-03-01.Intro.html#a.-인공지능",
    "title": "01. Intro",
    "section": "a. 인공지능",
    "text": "a. 인공지능\n- 개념 : 인간의 지적능력(추론, 인지)를 구현하는 모든 기술 \\(\\to\\) 고양이인지, 강아지인지…."
  },
  {
    "objectID": "post/etc/ai study/2023-07-03-01.Intro.html#b.-머신러닝",
    "href": "post/etc/ai study/2023-07-03-01.Intro.html#b.-머신러닝",
    "title": "01. Intro",
    "section": "b. 머신러닝",
    "text": "b. 머신러닝\n- 알고리즘으로 데이터를 분석, 학습하여 판단이나 예측하는 기술"
  },
  {
    "objectID": "post/etc/ai study/2023-07-03-01.Intro.html#c.-딥러닝",
    "href": "post/etc/ai study/2023-07-03-01.Intro.html#c.-딥러닝",
    "title": "01. Intro",
    "section": "c. 딥러닝",
    "text": "c. 딥러닝\n- 인공신경망 알고리즘을 활용하는 머신러닝 기술\n- 포함관계 : 딥러닝 \\(\\subset\\) 머신러닝 \\(\\subset\\) 인공지능"
  },
  {
    "objectID": "post/etc/ai study/2023-07-04-02.python basic.html",
    "href": "post/etc/ai study/2023-07-04-02.python basic.html",
    "title": "02. Python basic",
    "section": "",
    "text": "x = \"Rome is not built in a day!\"\n\n\nx\n\n'Rome is not built in a day!'\n\n\n\nx[-1]\n\n'!'\n\n\n\nx[0]\n\n'R'\n\n\n\nx[-2]\n\n'y'\n\n\n\nx[-6]\n\n'a'"
  },
  {
    "objectID": "post/etc/ai study/2023-07-04-02.python basic.html#summary",
    "href": "post/etc/ai study/2023-07-04-02.python basic.html#summary",
    "title": "02. Python basic",
    "section": "Summary",
    "text": "Summary\n- 내 생각 : 이런식으로 숫자값을 이용하면 먼가 코드의 확정성이 없는 것 같다…\n\nmap, lambda, list comprehension이 중요한 것 같다."
  },
  {
    "objectID": "post/etc/ai study/2023-07-04-02.python basic.html#리스트",
    "href": "post/etc/ai study/2023-07-04-02.python basic.html#리스트",
    "title": "02. Python basic",
    "section": "리스트",
    "text": "리스트\n\na = [1,2,3]\n\n\na\n\n[1, 2, 3]\n\n\n\nb= list(range(1,10,2))\nb\n\n[1, 3, 5, 7, 9]"
  },
  {
    "objectID": "post/etc/ai study/2023-07-04-02.python basic.html#리스트-관련함수",
    "href": "post/etc/ai study/2023-07-04-02.python basic.html#리스트-관련함수",
    "title": "02. Python basic",
    "section": "리스트 관련함수",
    "text": "리스트 관련함수\n- append\n\na = [\"Red\",\"Green\",\"blue\"]\na.append(\"yellow\")\na\n\n['Red', 'Green', 'blue', 'yellow']\n\n\n- insert\n\na.insert(1,\"black\")\na\n\n['Red', 'black', 'Green', 'blue', 'yellow']\n\n\n\nb = [\"purple\",\"white\"]\na.extend(b)\n\n\na\n\n['Red', 'black', 'Green', 'blue', 'yellow', 'purple', 'white']\n\n\n\nc = a+b\n\n\nprint(c)\n\n['Red', 'black', 'Green', 'blue', 'yellow', 'purple', 'white', 'purple', 'white']\n\n\n- remove\n\nd = list(range(10,101,10))\nd\n\n[10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n\n\n\nd.remove(90)\nd\n\n[10, 20, 30, 40, 50, 60, 70, 80, 100]\n\n\n- count\n\nlist1 = [\"a\",\"aaa\",\"aaa\"]\n\nlist1.count(\"aaa\")\n\n2\n\n\n\nlist2 = [1,-2,3,0,4,-7]\n\n- sort\n\nlist2.sort(reverse = True)\n\n\nlist2\n\n[4, 3, 1, 0, -2, -7]\n\n\n- pop\n\nd.pop(0)\nd\n\n[20, 30, 40, 50, 60, 70, 80, 100]"
  },
  {
    "objectID": "post/etc/ai study/2023-07-04-02.python basic.html#튜플",
    "href": "post/etc/ai study/2023-07-04-02.python basic.html#튜플",
    "title": "02. Python basic",
    "section": "튜플",
    "text": "튜플\n\n요소의 추가, 수정이 불가\n\n\nt1 = (\"b\",\"a\",\"c\")\n\n\nt1\n\n('b', 'a', 'c')\n\n\n\nt2 = \"b\",\"a\",\"c\"\n\n\nt2\n\n('b', 'a', 'c')\n\n\n- t1과 t2는 사실상 동일한 튜플임\n- 아래처럼 요소 값을 변환할 경우 에러 발생\n\nt2[0] = \"w\"\n\nTypeError: 'tuple' object does not support item assignment"
  },
  {
    "objectID": "post/etc/ai study/2023-07-04-02.python basic.html#딕셔너리",
    "href": "post/etc/ai study/2023-07-04-02.python basic.html#딕셔너리",
    "title": "02. Python basic",
    "section": "딕셔너리",
    "text": "딕셔너리\n\nm = {\"name\": \"인지명\",\"age\": 30, \"email\": \"dd@gam.com\"}\nm\n\n{'name': '인지명', 'age': 30, 'email': 'dd@gam.com'}\n\n\n- keys\n\nm.keys()\n\ndict_keys(['name', 'age', 'email'])\n\n\n\nm.values()\n\ndict_values(['인지명', 30, 'dd@gam.com'])\n\n\n\nm.items()\n\ndict_items([('name', '인지명'), ('age', 30), ('email', 'dd@gam.com')])\n\n\n\nm.get(\"name\") # 키값으로 value를 추적\n\n'인지명'\n\n\n\n\"name\" in members\n\nTrue\n\n\n\n\"birth\" in members\n\nFalse\n\n\n\nm.clear()\nm\n\n{}"
  },
  {
    "objectID": "post/etc/ai study/2023-07-04-02.python basic.html#리스트-튜플-딕셔너리-차이",
    "href": "post/etc/ai study/2023-07-04-02.python basic.html#리스트-튜플-딕셔너리-차이",
    "title": "02. Python basic",
    "section": "리스트, 튜플, 딕셔너리 차이",
    "text": "리스트, 튜플, 딕셔너리 차이\n\n\n\n구분\n설명\n\n\n\n\n리스트\n순서대로 정리된 항목들을 담는 자료구조\n\n\n튜플\n리스트와 비슷하지만 요소 추가, 수정 불가\n\n\n딕셔너리\nkey와 value로 구성되는 자료구조"
  },
  {
    "objectID": "post/etc/ai study/2023-07-05-03.python 이해 및 활용.html",
    "href": "post/etc/ai study/2023-07-05-03.python 이해 및 활용.html",
    "title": "03. data frame",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n# !pip install Ipython\nfrom IPython.display import Image\n\n\n\n\na1 = pd.DataFrame({\"a\" : [1,2,3], \"b\" : [4,5,6], \"c\" : [7,8,9]})\na1\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      4\n      7\n    \n    \n      1\n      2\n      5\n      8\n    \n    \n      2\n      3\n      6\n      9\n    \n  \n\n\n\n\n\nList 형태로 데이터 프레임 생성\n\n\na2 = pd.DataFrame([[1,2,3], [4,5,6], [7,8,9]], [\"a\",\"b\",\"c\"])\na2\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      a\n      1\n      2\n      3\n    \n    \n      b\n      4\n      5\n      6\n    \n    \n      c\n      7\n      8\n      9\n    \n  \n\n\n\n\n\n\n\n\n# kt 데이터 파일을 활용\n# 파일을 수정하고 저장 자체를 MS Office에서 하여서 encoding을 cp949로 해주어야 함\ncust = pd.read_csv('./sc_cust_info_txn_v1.5.csv', encoding = \"cp949\")\ncust\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      0\n      202006\n      20200630\n      10001\n      C\n      F\n      28\n      0\n      N\n      N\n      2640.0000\n      792.000000\n      1584.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      1\n      202006\n      20200630\n      10001\n      _\n      _\n      _\n      1\n      N\n      N\n      300.0000\n      90.000000\n      180.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      2\n      202006\n      20200630\n      10001\n      E\n      F\n      24\n      1\n      N\n      N\n      16840.0000\n      2526.000000\n      6983.0000\n      0.0\n      6981.0000\n      N\n    \n    \n      3\n      202006\n      20200630\n      10001\n      F\n      F\n      32\n      1\n      N\n      N\n      15544.7334\n      2331.710010\n      6750.4666\n      0.0\n      6508.8000\n      N\n    \n    \n      4\n      202006\n      20200630\n      10001\n      D\n      M\n      18\n      1\n      N\n      N\n      4700.0000\n      0.000000\n      4502.0000\n      0.0\n      4507.7000\n      N\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9925\n      202006\n      20200630\n      10001\n      C\n      F\n      15\n      1\n      N\n      Y\n      1296.0999\n      194.414985\n      643.1001\n      0.0\n      852.5499\n      N\n    \n    \n      9926\n      202006\n      20200630\n      10001\n      G\n      M\n      12\n      1\n      N\n      N\n      13799.6666\n      2069.949990\n      10605.9266\n      0.0\n      10603.9266\n      N\n    \n    \n      9927\n      202006\n      20200630\n      10005\n      C\n      _\n      _\n      1\n      N\n      N\n      1396.2000\n      1206.000000\n      0.0000\n      1212.0\n      0.0000\n      N\n    \n    \n      9928\n      202006\n      20200630\n      10001\n      C\n      F\n      40\n      0\n      N\n      N\n      3140.0000\n      942.000000\n      1884.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      9929\n      202006\n      20200630\n      10001\n      C\n      F\n      59\n      1\n      N\n      N\n      2436.9000\n      365.535000\n      1839.9000\n      0.0\n      1919.7999\n      N\n    \n  \n\n9930 rows × 15 columns\n\n\n\n\nDataFrame 데이터 살펴보기 >DataFrame의 구조 (인덱스와 컬럼)\n인덱스(Index) : 행의 레이블에 대한 정보를 보유하고 있음\n컬럼(Columns) : 열의 레이블에 대한 정보를 보유하고 있음\n인덱스와 컬럼 자체는 중복값일 수 없음\n\n\ncust\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      0\n      202006\n      20200630\n      10001\n      C\n      F\n      28\n      0\n      N\n      N\n      2640.0000\n      792.000000\n      1584.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      1\n      202006\n      20200630\n      10001\n      _\n      _\n      _\n      1\n      N\n      N\n      300.0000\n      90.000000\n      180.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      2\n      202006\n      20200630\n      10001\n      E\n      F\n      24\n      1\n      N\n      N\n      16840.0000\n      2526.000000\n      6983.0000\n      0.0\n      6981.0000\n      N\n    \n    \n      3\n      202006\n      20200630\n      10001\n      F\n      F\n      32\n      1\n      N\n      N\n      15544.7334\n      2331.710010\n      6750.4666\n      0.0\n      6508.8000\n      N\n    \n    \n      4\n      202006\n      20200630\n      10001\n      D\n      M\n      18\n      1\n      N\n      N\n      4700.0000\n      0.000000\n      4502.0000\n      0.0\n      4507.7000\n      N\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9925\n      202006\n      20200630\n      10001\n      C\n      F\n      15\n      1\n      N\n      Y\n      1296.0999\n      194.414985\n      643.1001\n      0.0\n      852.5499\n      N\n    \n    \n      9926\n      202006\n      20200630\n      10001\n      G\n      M\n      12\n      1\n      N\n      N\n      13799.6666\n      2069.949990\n      10605.9266\n      0.0\n      10603.9266\n      N\n    \n    \n      9927\n      202006\n      20200630\n      10005\n      C\n      _\n      _\n      1\n      N\n      N\n      1396.2000\n      1206.000000\n      0.0000\n      1212.0\n      0.0000\n      N\n    \n    \n      9928\n      202006\n      20200630\n      10001\n      C\n      F\n      40\n      0\n      N\n      N\n      3140.0000\n      942.000000\n      1884.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      9929\n      202006\n      20200630\n      10001\n      C\n      F\n      59\n      1\n      N\n      N\n      2436.9000\n      365.535000\n      1839.9000\n      0.0\n      1919.7999\n      N\n    \n  \n\n9930 rows × 15 columns\n\n\n\n\n\n\n# 상위 3개\ncust.head(n=3)\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      0\n      202006\n      20200630\n      10001\n      C\n      F\n      28\n      0\n      N\n      N\n      2640.0\n      792.0\n      1584.0\n      0.0\n      0.0\n      Y\n    \n    \n      1\n      202006\n      20200630\n      10001\n      _\n      _\n      _\n      1\n      N\n      N\n      300.0\n      90.0\n      180.0\n      0.0\n      0.0\n      Y\n    \n    \n      2\n      202006\n      20200630\n      10001\n      E\n      F\n      24\n      1\n      N\n      N\n      16840.0\n      2526.0\n      6983.0\n      0.0\n      6981.0\n      N\n    \n  \n\n\n\n\n\n#하위 10개 \ncust.tail(n=10)\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      9920\n      202006\n      20200630\n      10001\n      D\n      M\n      46\n      0\n      N\n      N\n      6920.0000\n      2076.000000\n      4152.0000\n      0.0000\n      0.0000\n      Y\n    \n    \n      9921\n      202006\n      20200630\n      10001\n      D\n      M\n      54\n      1\n      N\n      Y\n      5198.0666\n      0.000000\n      4760.8666\n      0.0000\n      4749.3000\n      N\n    \n    \n      9922\n      202006\n      20200630\n      10001\n      E\n      M\n      65\n      4\n      N\n      N\n      9115.1334\n      3209.600000\n      0.0000\n      3523.7334\n      0.0000\n      N\n    \n    \n      9923\n      202006\n      20200630\n      10001\n      C\n      M\n      76\n      1\n      N\n      N\n      1860.0000\n      1716.000000\n      0.0000\n      1722.0000\n      0.0000\n      N\n    \n    \n      9924\n      202006\n      20200630\n      10005\n      C\n      _\n      _\n      0\n      N\n      N\n      17038.2000\n      5111.460000\n      10222.9200\n      0.0000\n      0.0000\n      Y\n    \n    \n      9925\n      202006\n      20200630\n      10001\n      C\n      F\n      15\n      1\n      N\n      Y\n      1296.0999\n      194.414985\n      643.1001\n      0.0000\n      852.5499\n      N\n    \n    \n      9926\n      202006\n      20200630\n      10001\n      G\n      M\n      12\n      1\n      N\n      N\n      13799.6666\n      2069.949990\n      10605.9266\n      0.0000\n      10603.9266\n      N\n    \n    \n      9927\n      202006\n      20200630\n      10005\n      C\n      _\n      _\n      1\n      N\n      N\n      1396.2000\n      1206.000000\n      0.0000\n      1212.0000\n      0.0000\n      N\n    \n    \n      9928\n      202006\n      20200630\n      10001\n      C\n      F\n      40\n      0\n      N\n      N\n      3140.0000\n      942.000000\n      1884.0000\n      0.0000\n      0.0000\n      Y\n    \n    \n      9929\n      202006\n      20200630\n      10001\n      C\n      F\n      59\n      1\n      N\n      N\n      2436.9000\n      365.535000\n      1839.9000\n      0.0000\n      1919.7999\n      N\n    \n  \n\n\n\n\n- DataFrame 기본 함수로 살펴보기 - shape : 속성 반환값은 튜플로 존재하며 row과 col의 개수를 튜플로 반환함(row,col순) - columns : 해당 DataFrame을 구성하는 컬럼명을 확인할 수 있음 - info : 데이터 타입, 각 아이템의 개수 등 출력 - describe : 데이터 컬럼별 요약 통계량을 나타냄, 숫자형 데이터의 통계치 계산 (count:데이터의 갯수 / mean:평균값 / std:표준편차 / min:최소값 / 4분위 수 / max:최대값) - dtypes : 데이터 형태의 종류(Data Types)\n\n# shape : 데이터를 파악하는데 중요함\ncust.shape\n\n(9930, 15)\n\n\n\n# DataFrame의 columns들을 보여줌\ncust.columns\n\nIndex(['base_ym', 'dpro_tgt_perd_val', 'cust_ctg_type', 'cust_class',\n       'sex_type', 'age', 'efct_svc_count', 'dt_stop_yn', 'npay_yn',\n       'r3m_avg_bill_amt', 'r3m_A_avg_arpu_amt', 'r3m_B_avg_arpu_amt',\n       'r6m_A_avg_arpu_amt', 'r6m_B_avg_arpu_amt', 'termination_yn'],\n      dtype='object')\n\n\n\n# 데이터 타입 및 각 아이템등의 정보를 보여줌\ncust.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 9930 entries, 0 to 9929\nData columns (total 15 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   base_ym             9930 non-null   int64  \n 1   dpro_tgt_perd_val   9930 non-null   int64  \n 2   cust_ctg_type       9930 non-null   int64  \n 3   cust_class          9930 non-null   object \n 4   sex_type            9930 non-null   object \n 5   age                 9930 non-null   object \n 6   efct_svc_count      9930 non-null   int64  \n 7   dt_stop_yn          9930 non-null   object \n 8   npay_yn             9930 non-null   object \n 9   r3m_avg_bill_amt    9930 non-null   float64\n 10  r3m_A_avg_arpu_amt  9930 non-null   float64\n 11  r3m_B_avg_arpu_amt  9930 non-null   float64\n 12  r6m_A_avg_arpu_amt  9930 non-null   float64\n 13  r6m_B_avg_arpu_amt  9930 non-null   float64\n 14  termination_yn      9930 non-null   object \ndtypes: float64(5), int64(4), object(6)\nmemory usage: 1.1+ MB\n\n\n\n# DataFrame의 기본적인 통계정보를 보여줌\ncust.describe()\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      efct_svc_count\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n    \n  \n  \n    \n      count\n      9930.0\n      9930.0\n      9930.000000\n      9930.000000\n      9.930000e+03\n      9.930000e+03\n      9.930000e+03\n      9.930000e+03\n      9930.000000\n    \n    \n      mean\n      202006.0\n      20200630.0\n      10001.372810\n      1.520040\n      1.181774e+04\n      1.897536e+03\n      6.395259e+03\n      8.496206e+02\n      4624.897630\n    \n    \n      std\n      0.0\n      0.0\n      1.605016\n      15.404037\n      1.397822e+05\n      1.235342e+04\n      8.346138e+04\n      1.235124e+04\n      4561.049131\n    \n    \n      min\n      202006.0\n      20200630.0\n      10001.000000\n      0.000000\n      0.000000e+00\n      0.000000e+00\n      0.000000e+00\n      0.000000e+00\n      0.000000\n    \n    \n      25%\n      202006.0\n      20200630.0\n      10001.000000\n      1.000000\n      3.624503e+03\n      3.240000e+02\n      1.260000e+03\n      0.000000e+00\n      0.000000\n    \n    \n      50%\n      202006.0\n      20200630.0\n      10001.000000\n      1.000000\n      8.284467e+03\n      1.593307e+03\n      4.768617e+03\n      0.000000e+00\n      3959.316700\n    \n    \n      75%\n      202006.0\n      20200630.0\n      10001.000000\n      1.000000\n      1.372000e+04\n      2.308360e+03\n      7.982000e+03\n      1.006125e+03\n      7741.006900\n    \n    \n      max\n      202006.0\n      20200630.0\n      10010.000000\n      905.000000\n      1.281568e+07\n      1.188998e+06\n      7.689409e+06\n      1.208498e+06\n      64947.092000\n    \n  \n\n\n\n\n\n# DataFrame의 데이터 종류\ncust.dtypes\n\nbase_ym                 int64\ndpro_tgt_perd_val       int64\ncust_ctg_type           int64\ncust_class             object\nsex_type               object\nage                    object\nefct_svc_count          int64\ndt_stop_yn             object\nnpay_yn                object\nr3m_avg_bill_amt      float64\nr3m_A_avg_arpu_amt    float64\nr3m_B_avg_arpu_amt    float64\nr6m_A_avg_arpu_amt    float64\nr6m_B_avg_arpu_amt    float64\ntermination_yn         object\ndtype: object\n\n\n\n\n\n\n\n함수에 커서를 가져다 두고 shift+tab을 누르면 해당 함수의 parameter 볼 수 있음\nsep - 각 데이터 값을 구별하기 위한 구분자(separator) 설정\nindex_col : index로 사용할 column 설정\nusecols : 실제로 dataframe에 로딩할 columns만 설정\nusecols은 index_col을 포함하여야 함\n\n\n\ncust2 = pd.read_csv('./sc_cust_info_txn_v1.5.csv', index_col='cust_class', usecols=['cust_class', 'r3m_avg_bill_amt', 'r3m_B_avg_arpu_amt', 'r6m_B_avg_arpu_amt'])\ncust2\n\n\n\n\n\n  \n    \n      \n      r3m_avg_bill_amt\n      r3m_B_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n    \n    \n      cust_class\n      \n      \n      \n    \n  \n  \n    \n      C\n      2640.0000\n      1584.0000\n      0.0000\n    \n    \n      _\n      300.0000\n      180.0000\n      0.0000\n    \n    \n      E\n      16840.0000\n      6983.0000\n      6981.0000\n    \n    \n      F\n      15544.7334\n      6750.4666\n      6508.8000\n    \n    \n      D\n      4700.0000\n      4502.0000\n      4507.7000\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      C\n      1296.0999\n      643.1001\n      852.5499\n    \n    \n      G\n      13799.6666\n      10605.9266\n      10603.9266\n    \n    \n      C\n      1396.2000\n      0.0000\n      0.0000\n    \n    \n      C\n      3140.0000\n      1884.0000\n      0.0000\n    \n    \n      C\n      2436.9000\n      1839.9000\n      1919.7999\n    \n  \n\n9930 rows × 3 columns\n\n\n\n\n\n\n\n\n\n\n\n기본적으로 [ ]는 column을 추출 : 특정한 col을기준으로 모델링을 하고자 하는 경우\n컬럼 인덱스일 경우 인덱스의 리스트 사용 가능\n\n리스트를 전달할 경우 결과는 Dataframe\n하나의 컬럼명을 전달할 경우 결과는 Series\n\n\n\n\n\n\n\n\nSeries 형태로 가지고 올 수도, DataFrame형태로 가지고 올 수 있음\n\n\n\ncust.cust_class = cust['cust_class']\n\n\ncust.cust_class\n\n0       C\n1       _\n2       E\n3       F\n4       D\n       ..\n9925    C\n9926    G\n9927    C\n9928    C\n9929    C\nName: cust_class, Length: 9930, dtype: object\n\n\n\n# cf : series 형태로 가지고 오기(hcust.cust_class = cust['cust_class'])\ncust['cust_class']\n\n0       C\n1       _\n2       E\n3       F\n4       D\n       ..\n9925    C\n9926    G\n9927    C\n9928    C\n9929    C\nName: cust_class, Length: 9930, dtype: object\n\n\n\n# cf : Dataframe형태로 가지고 오기\ncust[['cust_class']]\n\n\n\n\n\n  \n    \n      \n      cust_class\n    \n  \n  \n    \n      0\n      C\n    \n    \n      1\n      _\n    \n    \n      2\n      E\n    \n    \n      3\n      F\n    \n    \n      4\n      D\n    \n    \n      ...\n      ...\n    \n    \n      9925\n      C\n    \n    \n      9926\n      G\n    \n    \n      9927\n      C\n    \n    \n      9928\n      C\n    \n    \n      9929\n      C\n    \n  \n\n9930 rows × 1 columns\n\n\n\n\n\n\n\n# 'cust_class' , 'age' 'r3m_avg_bill_amt'등 3개의 col 선택하기\ncust[['cust_class', 'age', 'r3m_avg_bill_amt']]\n\n\n\n\n\n  \n    \n      \n      cust_class\n      age\n      r3m_avg_bill_amt\n    \n  \n  \n    \n      0\n      C\n      28\n      2640.0000\n    \n    \n      1\n      _\n      _\n      300.0000\n    \n    \n      2\n      E\n      24\n      16840.0000\n    \n    \n      3\n      F\n      32\n      15544.7334\n    \n    \n      4\n      D\n      18\n      4700.0000\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      9925\n      C\n      15\n      1296.0999\n    \n    \n      9926\n      G\n      12\n      13799.6666\n    \n    \n      9927\n      C\n      _\n      1396.2000\n    \n    \n      9928\n      C\n      40\n      3140.0000\n    \n    \n      9929\n      C\n      59\n      2436.9000\n    \n  \n\n9930 rows × 3 columns\n\n\n\n\n\n\n\n\n\n특정 행 범위를 가지고 오고 싶다면 [ ]를 사용\nDataFrame의 경우 기본적으로 [ ] 연산자가 column 선택에 사용되지만 slicing은 row 레벨로 지원\n\n\n\n# 7,8,9행을 가지고 옴 (인덱스 기준)\ncust[7:10]\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      7\n      202006\n      20200630\n      10001\n      D\n      F\n      65\n      1\n      N\n      N\n      4953.9334\n      987.0000\n      2700.3999\n      0.0000\n      2689.1\n      N\n    \n    \n      8\n      202006\n      20200630\n      10001\n      D\n      M\n      60\n      1\n      N\n      N\n      5503.0000\n      2093.0001\n      0.0000\n      1981.8999\n      0.0\n      N\n    \n    \n      9\n      202006\n      20200630\n      10001\n      C\n      F\n      67\n      1\n      N\n      N\n      1349.7000\n      1227.0000\n      0.0000\n      985.5000\n      0.0\n      N\n    \n  \n\n\n\n\n\n\n\nDataFrame에서는 기본적으로 [ ]을 사용하여 column을 선택 > row 선택(두가지 방법이 존재) > - loc : Dataframe에 존재하는 인덱스를 그대로 사용 (인덱스 기준으로 행 데이터 읽기) > - iloc : Datafrmae에 존재하는 인덱스 상관없이 0 based index로 사용 (행 번호 기준으로 행 데이터 읽기) > - 이 두 함수는 ,를 사용하여 column 선택도 가능\n\n\ncust.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 9930 entries, 0 to 9929\nData columns (total 15 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   base_ym             9930 non-null   int64  \n 1   dpro_tgt_perd_val   9930 non-null   int64  \n 2   cust_ctg_type       9930 non-null   int64  \n 3   cust_class          9930 non-null   object \n 4   sex_type            9930 non-null   object \n 5   age                 9930 non-null   object \n 6   efct_svc_count      9930 non-null   int64  \n 7   dt_stop_yn          9930 non-null   object \n 8   npay_yn             9930 non-null   object \n 9   r3m_avg_bill_amt    9930 non-null   float64\n 10  r3m_A_avg_arpu_amt  9930 non-null   float64\n 11  r3m_B_avg_arpu_amt  9930 non-null   float64\n 12  r6m_A_avg_arpu_amt  9930 non-null   float64\n 13  r6m_B_avg_arpu_amt  9930 non-null   float64\n 14  termination_yn      9930 non-null   object \ndtypes: float64(5), int64(4), object(6)\nmemory usage: 1.1+ MB\n\n\n\n# arange함수는 10부터 19에서 끝나도록 간격을 1로 반환한다.\ncp=np.arange(10,20)\ncp\n\narray([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n\n\n\n#index를 100부터 달아주기 \ncust.index = np.arange(100, 10030)\ncust\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      100\n      202006\n      20200630\n      10001\n      C\n      F\n      28\n      0\n      N\n      N\n      2640.0000\n      792.000000\n      1584.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      101\n      202006\n      20200630\n      10001\n      _\n      _\n      _\n      1\n      N\n      N\n      300.0000\n      90.000000\n      180.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      102\n      202006\n      20200630\n      10001\n      E\n      F\n      24\n      1\n      N\n      N\n      16840.0000\n      2526.000000\n      6983.0000\n      0.0\n      6981.0000\n      N\n    \n    \n      103\n      202006\n      20200630\n      10001\n      F\n      F\n      32\n      1\n      N\n      N\n      15544.7334\n      2331.710010\n      6750.4666\n      0.0\n      6508.8000\n      N\n    \n    \n      104\n      202006\n      20200630\n      10001\n      D\n      M\n      18\n      1\n      N\n      N\n      4700.0000\n      0.000000\n      4502.0000\n      0.0\n      4507.7000\n      N\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      10025\n      202006\n      20200630\n      10001\n      C\n      F\n      15\n      1\n      N\n      Y\n      1296.0999\n      194.414985\n      643.1001\n      0.0\n      852.5499\n      N\n    \n    \n      10026\n      202006\n      20200630\n      10001\n      G\n      M\n      12\n      1\n      N\n      N\n      13799.6666\n      2069.949990\n      10605.9266\n      0.0\n      10603.9266\n      N\n    \n    \n      10027\n      202006\n      20200630\n      10005\n      C\n      _\n      _\n      1\n      N\n      N\n      1396.2000\n      1206.000000\n      0.0000\n      1212.0\n      0.0000\n      N\n    \n    \n      10028\n      202006\n      20200630\n      10001\n      C\n      F\n      40\n      0\n      N\n      N\n      3140.0000\n      942.000000\n      1884.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      10029\n      202006\n      20200630\n      10001\n      C\n      F\n      59\n      1\n      N\n      N\n      2436.9000\n      365.535000\n      1839.9000\n      0.0\n      1919.7999\n      N\n    \n  \n\n9930 rows × 15 columns\n\n\n\n\ncust.tail()\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      10025\n      202006\n      20200630\n      10001\n      C\n      F\n      15\n      1\n      N\n      Y\n      1296.0999\n      194.414985\n      643.1001\n      0.0\n      852.5499\n      N\n    \n    \n      10026\n      202006\n      20200630\n      10001\n      G\n      M\n      12\n      1\n      N\n      N\n      13799.6666\n      2069.949990\n      10605.9266\n      0.0\n      10603.9266\n      N\n    \n    \n      10027\n      202006\n      20200630\n      10005\n      C\n      _\n      _\n      1\n      N\n      N\n      1396.2000\n      1206.000000\n      0.0000\n      1212.0\n      0.0000\n      N\n    \n    \n      10028\n      202006\n      20200630\n      10001\n      C\n      F\n      40\n      0\n      N\n      N\n      3140.0000\n      942.000000\n      1884.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      10029\n      202006\n      20200630\n      10001\n      C\n      F\n      59\n      1\n      N\n      N\n      2436.9000\n      365.535000\n      1839.9000\n      0.0\n      1919.7999\n      N\n    \n  \n\n\n\n\n\n#한개의 row만 가지고 오기\ncust.loc[[289]]\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      289\n      202006\n      20200630\n      10001\n      G\n      M\n      28\n      1\n      N\n      N\n      14340.91989\n      331.5999\n      12705.6\n      0.0\n      12703.6\n      N\n    \n  \n\n\n\n\n\n#여러개의 row 가지고 오기\ncust.loc[[102, 202, 302]]\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      102\n      202006\n      20200630\n      10001\n      E\n      F\n      24\n      1\n      N\n      N\n      16840.00000\n      2526.0000\n      6983.0\n      0.0\n      6981.00\n      N\n    \n    \n      202\n      202006\n      20200630\n      10010\n      _\n      _\n      _\n      1\n      N\n      N\n      22362.93330\n      2082.0000\n      9410.0\n      0.0\n      9408.00\n      N\n    \n    \n      302\n      202006\n      20200630\n      10001\n      G\n      M\n      52\n      1\n      N\n      Y\n      17769.50989\n      1506.9999\n      14647.1\n      0.0\n      14744.95\n      N\n    \n  \n\n\n\n\n\n#iloc과비교(위와 같은 값을 가지고 오려면...)    (직접 타이핑 해보세요)\ncust.iloc[[2, 102, 202]]\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      102\n      202006\n      20200630\n      10001\n      E\n      F\n      24\n      1\n      N\n      N\n      16840.00000\n      2526.0000\n      6983.0\n      0.0\n      6981.00\n      N\n    \n    \n      202\n      202006\n      20200630\n      10010\n      _\n      _\n      _\n      1\n      N\n      N\n      22362.93330\n      2082.0000\n      9410.0\n      0.0\n      9408.00\n      N\n    \n    \n      302\n      202006\n      20200630\n      10001\n      G\n      M\n      52\n      1\n      N\n      Y\n      17769.50989\n      1506.9999\n      14647.1\n      0.0\n      14744.95\n      N\n    \n  \n\n\n\n\n\nrow, column 동시에 선택하기 > loc, iloc 속성을 이용할 때, 콤마를 이용하여 row와 col 다 명시 가능\n\n\n# 100, 200, 300 대상으로 cust_class, sex_type, age, r3m_avg_bill_amt, r3m_A_avg_arpu_amt  col 가지고 오기(loc사용)\ncust.loc[[100, 200, 300], ['cust_class', 'sex_type', 'age', 'r3m_avg_bill_amt', 'r3m_A_avg_arpu_amt']]   # row, col\n\n\n\n\n\n  \n    \n      \n      cust_class\n      sex_type\n      age\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n    \n  \n  \n    \n      100\n      C\n      F\n      28\n      2640.00000\n      792.0000\n    \n    \n      200\n      E\n      M\n      61\n      9526.77000\n      1878.9000\n    \n    \n      300\n      D\n      M\n      84\n      11622.37472\n      2716.7952\n    \n  \n\n\n\n\n\n# 같은 형태로 iloc사용하기 (index를 level로 가지고 오기)\n# 100, 200, 300 대상으로 cust_class, sex_type, age, r3m_avg_bill_amt, r3m_A_avg_arpu_amt  col 가지고 오기(iloc사용)\ncust.iloc[[0, 100, 200], [3, 4, 5, 9, 10]]\n\n\n\n\n\n  \n    \n      \n      cust_class\n      sex_type\n      age\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n    \n  \n  \n    \n      100\n      C\n      F\n      28\n      2640.00000\n      792.0000\n    \n    \n      200\n      E\n      M\n      61\n      9526.77000\n      1878.9000\n    \n    \n      300\n      D\n      M\n      84\n      11622.37472\n      2716.7952\n    \n  \n\n\n\n\n- 100, 200, 300 대상으로 cust_class, sex_type, age, r3m_avg_bill_amt, r3m_A_avg_arpu_amt col 가지고 오기(loc사용 : 에러발생 함)\n\ncust.loc[[100, 200, 300], [3, 4, 5, 9, 10]]   # row, col\n\nKeyError: \"None of [Int64Index([3, 4, 5, 9, 10], dtype='int64')] are in the [columns]\"\n\n\n\n\n\n\n해당 조건에 맞는 row만 선택\n조건을 명시하고 조건을 명시한 형태로 inedxing 하여 가지고 옴\nex: 남자이면서 3개월 평균 청구 금액이 50000 이상이면서 100000 미만인 사람만 가지고오기\n\n- 조건을 전부다 [ ]안에 넣어 주면 됨\n\nextract = cust[(cust['sex_type']=='M') & (cust['r3m_avg_bill_amt']>=50000) & (cust['r3m_avg_bill_amt']< 100000)]\nextract.head()\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      472\n      202006\n      20200630\n      10001\n      F\n      M\n      28\n      1\n      N\n      N\n      65113.66670\n      1310.4000\n      20083.5033\n      0.0\n      19426.1983\n      N\n    \n    \n      1149\n      202006\n      20200630\n      10001\n      D\n      M\n      20\n      2\n      N\n      N\n      80335.67685\n      69136.1001\n      3896.3334\n      0.0\n      3727.6666\n      N\n    \n    \n      1464\n      202006\n      20200630\n      10001\n      _\n      M\n      45\n      1\n      N\n      Y\n      54865.70000\n      2321.8666\n      8744.9382\n      0.0\n      8774.7162\n      N\n    \n    \n      1893\n      202006\n      20200630\n      10001\n      G\n      M\n      48\n      1\n      N\n      N\n      64804.34037\n      47599.4501\n      11313.5866\n      0.0\n      11351.4984\n      N\n    \n    \n      2127\n      202006\n      20200630\n      10001\n      G\n      M\n      47\n      1\n      N\n      Y\n      55368.98422\n      37432.2666\n      12903.1736\n      0.0\n      12901.1736\n      N\n    \n  \n\n\n\n\n-조건문이 너무 길어지거나 복잡해지면…아래와 같은 방식으로 해도 무방함\n\n# 조건문이 너무 길어지거나 복잡해지면...아래와 같은 방식으로 해도 무방함\n# 남자이면서 \nsex = cust['sex_type']=='M'\n# 3개월 평균 청구 금액이 50000 이상이면서 100000 미만\nbill = (cust['r3m_avg_bill_amt']>=50000) & (cust['r3m_avg_bill_amt']< 100000)\n\ncust[sex & bill].head()\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      472\n      202006\n      20200630\n      10001\n      F\n      M\n      28\n      1\n      N\n      N\n      65113.66670\n      1310.4000\n      20083.5033\n      0.0\n      19426.1983\n      N\n    \n    \n      1149\n      202006\n      20200630\n      10001\n      D\n      M\n      20\n      2\n      N\n      N\n      80335.67685\n      69136.1001\n      3896.3334\n      0.0\n      3727.6666\n      N\n    \n    \n      1464\n      202006\n      20200630\n      10001\n      _\n      M\n      45\n      1\n      N\n      Y\n      54865.70000\n      2321.8666\n      8744.9382\n      0.0\n      8774.7162\n      N\n    \n    \n      1893\n      202006\n      20200630\n      10001\n      G\n      M\n      48\n      1\n      N\n      N\n      64804.34037\n      47599.4501\n      11313.5866\n      0.0\n      11351.4984\n      N\n    \n    \n      2127\n      202006\n      20200630\n      10001\n      G\n      M\n      47\n      1\n      N\n      Y\n      55368.98422\n      37432.2666\n      12903.1736\n      0.0\n      12901.1736\n      N\n    \n  \n\n\n\n\n\n\n\n\n기본적인 대괄호는 col을 가지고 오는 경우 사용, 하지만 slicing은 row를 가지고 온다.\nrow를 가지고 오는 경우는 loc과 iloc을 사용하는데, loc과 iloc은 컬럼과 row를 동시에 가지고 올 수 있다.\n\n\nimport matplotlib.pyplot as plt      #matplotlib.pyplot import   \n\n\n\n\n\n\n\n\n\n데이터 전처리 과정에서 빈번하게 발생하는 것\ninsert 함수 사용하여 원하는 위치에 추가하기\n\n\n- r3m_avg_bill_amt 두배로 새로운 col만들기\n\ncust['r3m_avg_bill_amt2'] = cust['r3m_avg_bill_amt'] * 2\n\n\ncust[['r3m_avg_bill_amt','r3m_avg_bill_amt2']].head()\n\n\n\n\n\n  \n    \n      \n      r3m_avg_bill_amt\n      r3m_avg_bill_amt2\n    \n  \n  \n    \n      100\n      2640.0000\n      5280.0000\n    \n    \n      101\n      300.0000\n      600.0000\n    \n    \n      102\n      16840.0000\n      33680.0000\n    \n    \n      103\n      15544.7334\n      31089.4668\n    \n    \n      104\n      4700.0000\n      9400.0000\n    \n  \n\n\n\n\n- 기존에 col을 연산하여 새로운 데이터 생성\n\ncust['r3m_avg_bill_amt3'] = cust['r3m_avg_bill_amt2'] + cust['r3m_avg_bill_amt']\ncust[['r3m_avg_bill_amt','r3m_avg_bill_amt2','r3m_avg_bill_amt3']].head()\n\n\n\n\n\n  \n    \n      \n      r3m_avg_bill_amt\n      r3m_avg_bill_amt2\n      r3m_avg_bill_amt3\n    \n  \n  \n    \n      100\n      2640.0000\n      5280.0000\n      7920.0000\n    \n    \n      101\n      300.0000\n      600.0000\n      900.0000\n    \n    \n      102\n      16840.0000\n      33680.0000\n      50520.0000\n    \n    \n      103\n      15544.7334\n      31089.4668\n      46634.2002\n    \n    \n      104\n      4700.0000\n      9400.0000\n      14100.0000\n    \n  \n\n\n\n\n- 새로은 col들은 항상맨뒤에 존재 원하는 위치에 col을 추가하고자 하는 경우\n\n위치를 조절 하고 싶다면(insert함수 사용)\n\n\n#cust.insert(10, 'r3m_avg_bill_amt10', cust['r3m_avg_bill_amt'] *10)  # 0부터 시작하여 10번째 col에 insert\ncust[['r3m_avg_bill_amt','r3m_avg_bill_amt10']].head()\n\n\n\n\n\n  \n    \n      \n      r3m_avg_bill_amt\n      r3m_avg_bill_amt10\n    \n  \n  \n    \n      100\n      2640.0000\n      26400.000\n    \n    \n      101\n      300.0000\n      3000.000\n    \n    \n      102\n      16840.0000\n      168400.000\n    \n    \n      103\n      15544.7334\n      155447.334\n    \n    \n      104\n      4700.0000\n      47000.000\n    \n  \n\n\n\n\n\n\n\n\n\ndrop 함수 사용하여 삭제\naxis는 삭제를 가로(행)기준으로 할 것인지, 세로(열)기준으로 할 것인지 명시하는 ’drop()’메소드의 파라미터임\n리스트를 사용하면 멀티플 col 삭제 가능\n\n\n\n# axis : dataframe은 차원이 존재 함으로 항상 0과 1이 존재 \n# (0은 행레벨, 1을 열 레벨)\ncust.drop('r3m_avg_bill_amt10', axis=1)\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n      r3m_avg_bill_amt2\n      r3m_avg_bill_amt3\n    \n  \n  \n    \n      100\n      202006\n      20200630\n      10001\n      C\n      F\n      28\n      0\n      N\n      N\n      2640.0000\n      792.000000\n      1584.0000\n      0.0\n      0.0000\n      Y\n      5280.0000\n      7920.0000\n    \n    \n      101\n      202006\n      20200630\n      10001\n      _\n      _\n      _\n      1\n      N\n      N\n      300.0000\n      90.000000\n      180.0000\n      0.0\n      0.0000\n      Y\n      600.0000\n      900.0000\n    \n    \n      102\n      202006\n      20200630\n      10001\n      E\n      F\n      24\n      1\n      N\n      N\n      16840.0000\n      2526.000000\n      6983.0000\n      0.0\n      6981.0000\n      N\n      33680.0000\n      50520.0000\n    \n    \n      103\n      202006\n      20200630\n      10001\n      F\n      F\n      32\n      1\n      N\n      N\n      15544.7334\n      2331.710010\n      6750.4666\n      0.0\n      6508.8000\n      N\n      31089.4668\n      46634.2002\n    \n    \n      104\n      202006\n      20200630\n      10001\n      D\n      M\n      18\n      1\n      N\n      N\n      4700.0000\n      0.000000\n      4502.0000\n      0.0\n      4507.7000\n      N\n      9400.0000\n      14100.0000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      10025\n      202006\n      20200630\n      10001\n      C\n      F\n      15\n      1\n      N\n      Y\n      1296.0999\n      194.414985\n      643.1001\n      0.0\n      852.5499\n      N\n      2592.1998\n      3888.2997\n    \n    \n      10026\n      202006\n      20200630\n      10001\n      G\n      M\n      12\n      1\n      N\n      N\n      13799.6666\n      2069.949990\n      10605.9266\n      0.0\n      10603.9266\n      N\n      27599.3332\n      41398.9998\n    \n    \n      10027\n      202006\n      20200630\n      10005\n      C\n      _\n      _\n      1\n      N\n      N\n      1396.2000\n      1206.000000\n      0.0000\n      1212.0\n      0.0000\n      N\n      2792.4000\n      4188.6000\n    \n    \n      10028\n      202006\n      20200630\n      10001\n      C\n      F\n      40\n      0\n      N\n      N\n      3140.0000\n      942.000000\n      1884.0000\n      0.0\n      0.0000\n      Y\n      6280.0000\n      9420.0000\n    \n    \n      10029\n      202006\n      20200630\n      10001\n      C\n      F\n      59\n      1\n      N\n      N\n      2436.9000\n      365.535000\n      1839.9000\n      0.0\n      1919.7999\n      N\n      4873.8000\n      7310.7000\n    \n  \n\n9930 rows × 17 columns\n\n\n\n- 원본 데이터를 열어 보면 원본 데이터는 안 지워진 상태\n\n#원본 데이터를 열어 보면 원본 데이터는 안 지워진 상태\ncust.loc[:,[\"r3m_avg_bill_amt\",\"r3m_avg_bill_amt10\"]].head()\n\n\n\n\n\n  \n    \n      \n      r3m_avg_bill_amt\n      r3m_avg_bill_amt10\n    \n  \n  \n    \n      100\n      2640.0000\n      26400.000\n    \n    \n      101\n      300.0000\n      3000.000\n    \n    \n      102\n      16840.0000\n      168400.000\n    \n    \n      103\n      15544.7334\n      155447.334\n    \n    \n      104\n      4700.0000\n      47000.000\n    \n  \n\n\n\n\n- 방법1 : 데이터를 지우고 다른 데이터 프레임에 저장\n\ncust1=cust.drop('r3m_avg_bill_amt10', axis=1)\ncust1.iloc[:,10:-1].head()\n\n\n\n\n\n  \n    \n      \n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n      r3m_avg_bill_amt2\n    \n  \n  \n    \n      100\n      792.00000\n      1584.0000\n      0.0\n      0.0\n      Y\n      5280.0000\n    \n    \n      101\n      90.00000\n      180.0000\n      0.0\n      0.0\n      Y\n      600.0000\n    \n    \n      102\n      2526.00000\n      6983.0000\n      0.0\n      6981.0\n      N\n      33680.0000\n    \n    \n      103\n      2331.71001\n      6750.4666\n      0.0\n      6508.8\n      N\n      31089.4668\n    \n    \n      104\n      0.00000\n      4502.0000\n      0.0\n      4507.7\n      N\n      9400.0000\n    \n  \n\n\n\n\n- 방법 2 : inplace 파라미터를 할용 True인 경우 원본데이터에 수행\n\ncust.drop('r3m_avg_bill_amt10', axis=1, inplace=True)\n\n\n# 원본확인\ncust.iloc[:,10:-1].head()\n\n\n\n\n\n  \n    \n      \n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n      r3m_avg_bill_amt2\n    \n  \n  \n    \n      100\n      792.00000\n      1584.0000\n      0.0\n      0.0\n      Y\n      5280.0000\n    \n    \n      101\n      90.00000\n      180.0000\n      0.0\n      0.0\n      Y\n      600.0000\n    \n    \n      102\n      2526.00000\n      6983.0000\n      0.0\n      6981.0\n      N\n      33680.0000\n    \n    \n      103\n      2331.71001\n      6750.4666\n      0.0\n      6508.8\n      N\n      31089.4668\n    \n    \n      104\n      0.00000\n      4502.0000\n      0.0\n      4507.7\n      N\n      9400.0000\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\n\n\n\n같은 값을 하나로 묶어 통계 또는 집계 결과를얻기위해 사용하는 것\n아래의 세 단계를 적용하여 데이터를 그룹화(groupping) / 특정한 col을 기준으로 데이터를 그룹핑 하여 통계에 활용하는 것\n\n데이터 분할(split) : 어떠한 기준을 바탕으로 데이터를 나누는 일\noperation 적용(applying) : 각 그룹에 어떤 함수를 독립적으로 적용시키는 일\n데이터 병합(cobine) : 적용되어 나온 결과들을 통합하는 일\n\n데이터 분석에 있어 사용빈도가 높음\ngroupby의 결과는 dictionary형태임\n\n\n\ncust = pd.read_csv('sc_cust_info_txn_v1.5.csv', encoding = \"cp949\")\ncust\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      0\n      202006\n      20200630\n      10001\n      C\n      F\n      28\n      0\n      N\n      N\n      2640.0000\n      792.000000\n      1584.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      1\n      202006\n      20200630\n      10001\n      _\n      _\n      _\n      1\n      N\n      N\n      300.0000\n      90.000000\n      180.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      2\n      202006\n      20200630\n      10001\n      E\n      F\n      24\n      1\n      N\n      N\n      16840.0000\n      2526.000000\n      6983.0000\n      0.0\n      6981.0000\n      N\n    \n    \n      3\n      202006\n      20200630\n      10001\n      F\n      F\n      32\n      1\n      N\n      N\n      15544.7334\n      2331.710010\n      6750.4666\n      0.0\n      6508.8000\n      N\n    \n    \n      4\n      202006\n      20200630\n      10001\n      D\n      M\n      18\n      1\n      N\n      N\n      4700.0000\n      0.000000\n      4502.0000\n      0.0\n      4507.7000\n      N\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9925\n      202006\n      20200630\n      10001\n      C\n      F\n      15\n      1\n      N\n      Y\n      1296.0999\n      194.414985\n      643.1001\n      0.0\n      852.5499\n      N\n    \n    \n      9926\n      202006\n      20200630\n      10001\n      G\n      M\n      12\n      1\n      N\n      N\n      13799.6666\n      2069.949990\n      10605.9266\n      0.0\n      10603.9266\n      N\n    \n    \n      9927\n      202006\n      20200630\n      10005\n      C\n      _\n      _\n      1\n      N\n      N\n      1396.2000\n      1206.000000\n      0.0000\n      1212.0\n      0.0000\n      N\n    \n    \n      9928\n      202006\n      20200630\n      10001\n      C\n      F\n      40\n      0\n      N\n      N\n      3140.0000\n      942.000000\n      1884.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      9929\n      202006\n      20200630\n      10001\n      C\n      F\n      59\n      1\n      N\n      N\n      2436.9000\n      365.535000\n      1839.9000\n      0.0\n      1919.7999\n      N\n    \n  \n\n9930 rows × 15 columns\n\n\n\n\n\n\n\n각 그룹과 그룹에 속한 index를 dict 형태로 표현\n\n\n- 파라미터 값으로 col의 리스트나 col을 전달\n- 출력은 우선 dataframe이라고 하는 객체임(그룹을 생성까지 한 상태)\n\ngender_group = cust.groupby('sex_type')\ngender_group\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000026C7BFB5310>\n\n\n\n# groups를 활용하여 그룹의 속성을 살펴보기\ngender_group.groups.get(\"M\")\n\nInt64Index([   4,    6,    8,   11,   15,   18,   22,   23,   27,   28,\n            ...\n            9912, 9915, 9916, 9918, 9919, 9920, 9921, 9922, 9923, 9926],\n           dtype='int64', length=4998)\n\n\n\ngender_group.groups.get(\"F\")\n\nInt64Index([   0,    2,    3,    5,    7,    9,   12,   13,   14,   16,\n            ...\n            9903, 9904, 9906, 9908, 9913, 9914, 9917, 9925, 9928, 9929],\n           dtype='int64', length=4251)\n\n\n\n\n\n\n그룹 데이터에 적용 가능한 통계 함수(NaN은 제외하여 연산)\ncount : 데이터 개수\nsize : 집단별 크기\nsum : 데이터의 합\nmean, std, var : 평균, 표준편차, 분산\nmin, max : 최소, 최대값\n\n\n# count 함수 확인\ngender_group.count()\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n    \n      sex_type\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      F\n      4251\n      4251\n      4251\n      4251\n      4251\n      4251\n      4251\n      4251\n      4251\n      4251\n      4251\n      4251\n      4251\n      4251\n    \n    \n      M\n      4998\n      4998\n      4998\n      4998\n      4998\n      4998\n      4998\n      4998\n      4998\n      4998\n      4998\n      4998\n      4998\n      4998\n    \n    \n      _\n      681\n      681\n      681\n      681\n      681\n      681\n      681\n      681\n      681\n      681\n      681\n      681\n      681\n      681\n    \n  \n\n\n\n\n\n# mean 함수 확인\ngender_group.mean()\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      efct_svc_count\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n    \n    \n      sex_type\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      F\n      202006.0\n      20200630.0\n      10001.001882\n      1.098800\n      9580.926307\n      1570.372299\n      5283.876950\n      452.780488\n      4961.455810\n    \n    \n      M\n      202006.0\n      20200630.0\n      10001.004802\n      1.204882\n      9839.707339\n      1778.988878\n      5342.748791\n      641.426866\n      4851.027884\n    \n    \n      _\n      202006.0\n      20200630.0\n      10006.389134\n      6.462555\n      40297.790077\n      4809.827390\n      21057.412985\n      4854.788244\n      864.386867\n    \n  \n\n\n\n\n\n# max값 확인하기\ngender_group.max()\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n    \n      sex_type\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      F\n      202006\n      20200630\n      10005\n      _\n      _\n      9\n      Y\n      Y\n      7.979563e+04\n      5.500240e+04\n      5.710153e+04\n      2.302815e+04\n      48787.2333\n      Y\n    \n    \n      M\n      202006\n      20200630\n      10005\n      _\n      _\n      14\n      Y\n      Y\n      1.447397e+05\n      1.315815e+05\n      6.583656e+04\n      2.749282e+04\n      64947.0920\n      Y\n    \n    \n      _\n      202006\n      20200630\n      10010\n      _\n      _\n      905\n      Y\n      Y\n      1.281568e+07\n      1.188998e+06\n      7.689409e+06\n      1.208498e+06\n      18796.6266\n      Y\n    \n  \n\n\n\n\n\n# 특정 col만 보는 경우 : gender별 r3m_avg_bill_amt의 평균\ngender_group.mean()[['r3m_avg_bill_amt']]\n\n\n\n\n\n  \n    \n      \n      r3m_avg_bill_amt\n    \n    \n      sex_type\n      \n    \n  \n  \n    \n      F\n      9580.926307\n    \n    \n      M\n      9839.707339\n    \n    \n      _\n      40297.790077\n    \n  \n\n\n\n\n\n\n\n\n성별 r3m_avg_bill_amt의 평균\n\n\n# groupby한 상태에서 가지고 오는 경우 \ngender_group.mean()[['r3m_avg_bill_amt']]\n\n\n\n\n\n  \n    \n      \n      r3m_avg_bill_amt\n    \n    \n      sex_type\n      \n    \n  \n  \n    \n      F\n      9580.926307\n    \n    \n      M\n      9839.707339\n    \n    \n      _\n      40297.790077\n    \n  \n\n\n\n\n\n# groupby하기 전 원 DataFrame에서 가지고 오는 경우(같은 의미)\ncust.groupby('sex_type').mean()[['r3m_avg_bill_amt']]\n\n\n\n\n\n  \n    \n      \n      r3m_avg_bill_amt\n    \n    \n      sex_type\n      \n    \n  \n  \n    \n      F\n      9580.926307\n    \n    \n      M\n      9839.707339\n    \n    \n      _\n      40297.790077\n    \n  \n\n\n\n\n\n\n\n\n\ngroupby에 column 리스트를 전달할 수 있고 복수개의 전달도 가능함\n통계함수를 적용한 결과는 multiindex를 갖는 DataFrame\n\n\n- cust_class 와 sex_type으로 index를 정하고 이에따른 r3m_avg_bill_amt의 평균을 구하기\n\ncust.groupby(['cust_class', 'sex_type']).mean()[['r3m_avg_bill_amt']]\n\n\n\n\n\n  \n    \n      \n      \n      r3m_avg_bill_amt\n    \n    \n      cust_class\n      sex_type\n      \n    \n  \n  \n    \n      C\n      F\n      3804.342244\n    \n    \n      M\n      3155.385796\n    \n    \n      _\n      4719.075980\n    \n    \n      D\n      F\n      7848.842709\n    \n    \n      M\n      7774.098954\n    \n    \n      _\n      8419.764574\n    \n    \n      E\n      F\n      11257.301485\n    \n    \n      M\n      11158.736812\n    \n    \n      _\n      12208.980550\n    \n    \n      F\n      F\n      14913.105272\n    \n    \n      M\n      15013.379957\n    \n    \n      _\n      16231.437767\n    \n    \n      G\n      F\n      16538.595017\n    \n    \n      M\n      16847.160637\n    \n    \n      _\n      20180.014811\n    \n    \n      H\n      F\n      20154.938768\n    \n    \n      M\n      21052.154088\n    \n    \n      _\n      F\n      4304.582332\n    \n    \n      M\n      4728.388010\n    \n    \n      _\n      65268.733884\n    \n  \n\n\n\n\n- 위와 동일하게 groupby한 이후에 평균 구하기\n\nmulti_group=cust.groupby(['cust_class', 'sex_type'])\nmulti_group.mean()[['r3m_avg_bill_amt']]\n\n\n\n\n\n  \n    \n      \n      \n      r3m_avg_bill_amt\n    \n    \n      cust_class\n      sex_type\n      \n    \n  \n  \n    \n      C\n      F\n      3804.342244\n    \n    \n      M\n      3155.385796\n    \n    \n      _\n      4719.075980\n    \n    \n      D\n      F\n      7848.842709\n    \n    \n      M\n      7774.098954\n    \n    \n      _\n      8419.764574\n    \n    \n      E\n      F\n      11257.301485\n    \n    \n      M\n      11158.736812\n    \n    \n      _\n      12208.980550\n    \n    \n      F\n      F\n      14913.105272\n    \n    \n      M\n      15013.379957\n    \n    \n      _\n      16231.437767\n    \n    \n      G\n      F\n      16538.595017\n    \n    \n      M\n      16847.160637\n    \n    \n      _\n      20180.014811\n    \n    \n      H\n      F\n      20154.938768\n    \n    \n      M\n      21052.154088\n    \n    \n      _\n      F\n      4304.582332\n    \n    \n      M\n      4728.388010\n    \n    \n      _\n      65268.733884\n    \n  \n\n\n\n\n\n# INDEX는 DEPTH가 존재함 (loc을 사용하여 원하는 것만 가지고 옴)\ncust.groupby(['cust_class', 'sex_type']).mean().loc[[(\"D\",\"M\")]]\n\n\n\n\n\n  \n    \n      \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      efct_svc_count\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n    \n    \n      cust_class\n      sex_type\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      D\n      M\n      202006.0\n      20200630.0\n      10001.00736\n      1.045078\n      7774.098954\n      1742.249493\n      4098.230598\n      550.449533\n      3054.969393\n    \n  \n\n\n\n\n\n\n\n\n\nindex가 있는 경우, groupby 함수에 level 사용 가능\nlevel은 index의 depth를 의미하며, 가장 왼쪽부터 0부터 증가\nset_index 함수\ncolumn 데이터를 index 레벨로 변경하는 경우 사용\n기존의 행 인덱스를 제거하고 데이터 열 중 하나를 인덱스로 설정\nreset_index 함수\n인덱스 초기화\n기존의 행 인덱스를 제거하고 인덱스를 데이터 열로 추가\n\n\n\n\n\n\n# set_index로 index셋팅(멀티도 가능)\ncust.set_index(['cust_class','sex_type'])\n\n\n\n\n\n  \n    \n      \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n    \n      cust_class\n      sex_type\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      C\n      F\n      202006\n      20200630\n      10001\n      28\n      0\n      N\n      N\n      2640.0000\n      792.000000\n      1584.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      _\n      _\n      202006\n      20200630\n      10001\n      _\n      1\n      N\n      N\n      300.0000\n      90.000000\n      180.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      E\n      F\n      202006\n      20200630\n      10001\n      24\n      1\n      N\n      N\n      16840.0000\n      2526.000000\n      6983.0000\n      0.0\n      6981.0000\n      N\n    \n    \n      F\n      F\n      202006\n      20200630\n      10001\n      32\n      1\n      N\n      N\n      15544.7334\n      2331.710010\n      6750.4666\n      0.0\n      6508.8000\n      N\n    \n    \n      D\n      M\n      202006\n      20200630\n      10001\n      18\n      1\n      N\n      N\n      4700.0000\n      0.000000\n      4502.0000\n      0.0\n      4507.7000\n      N\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      C\n      F\n      202006\n      20200630\n      10001\n      15\n      1\n      N\n      Y\n      1296.0999\n      194.414985\n      643.1001\n      0.0\n      852.5499\n      N\n    \n    \n      G\n      M\n      202006\n      20200630\n      10001\n      12\n      1\n      N\n      N\n      13799.6666\n      2069.949990\n      10605.9266\n      0.0\n      10603.9266\n      N\n    \n    \n      C\n      _\n      202006\n      20200630\n      10005\n      _\n      1\n      N\n      N\n      1396.2000\n      1206.000000\n      0.0000\n      1212.0\n      0.0000\n      N\n    \n    \n      F\n      202006\n      20200630\n      10001\n      40\n      0\n      N\n      N\n      3140.0000\n      942.000000\n      1884.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      F\n      202006\n      20200630\n      10001\n      59\n      1\n      N\n      N\n      2436.9000\n      365.535000\n      1839.9000\n      0.0\n      1919.7999\n      N\n    \n  \n\n9930 rows × 13 columns\n\n\n\n- reset_index활용하여 기존 DataFrame으로 변환 (set_index <-> reset_index)\n\ncust.set_index(['cust_class','sex_type']).reset_index()\n\n\n\n\n\n  \n    \n      \n      cust_class\n      sex_type\n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      0\n      C\n      F\n      202006\n      20200630\n      10001\n      28\n      0\n      N\n      N\n      2640.0000\n      792.000000\n      1584.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      1\n      _\n      _\n      202006\n      20200630\n      10001\n      _\n      1\n      N\n      N\n      300.0000\n      90.000000\n      180.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      2\n      E\n      F\n      202006\n      20200630\n      10001\n      24\n      1\n      N\n      N\n      16840.0000\n      2526.000000\n      6983.0000\n      0.0\n      6981.0000\n      N\n    \n    \n      3\n      F\n      F\n      202006\n      20200630\n      10001\n      32\n      1\n      N\n      N\n      15544.7334\n      2331.710010\n      6750.4666\n      0.0\n      6508.8000\n      N\n    \n    \n      4\n      D\n      M\n      202006\n      20200630\n      10001\n      18\n      1\n      N\n      N\n      4700.0000\n      0.000000\n      4502.0000\n      0.0\n      4507.7000\n      N\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9925\n      C\n      F\n      202006\n      20200630\n      10001\n      15\n      1\n      N\n      Y\n      1296.0999\n      194.414985\n      643.1001\n      0.0\n      852.5499\n      N\n    \n    \n      9926\n      G\n      M\n      202006\n      20200630\n      10001\n      12\n      1\n      N\n      N\n      13799.6666\n      2069.949990\n      10605.9266\n      0.0\n      10603.9266\n      N\n    \n    \n      9927\n      C\n      _\n      202006\n      20200630\n      10005\n      _\n      1\n      N\n      N\n      1396.2000\n      1206.000000\n      0.0000\n      1212.0\n      0.0000\n      N\n    \n    \n      9928\n      C\n      F\n      202006\n      20200630\n      10001\n      40\n      0\n      N\n      N\n      3140.0000\n      942.000000\n      1884.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      9929\n      C\n      F\n      202006\n      20200630\n      10001\n      59\n      1\n      N\n      N\n      2436.9000\n      365.535000\n      1839.9000\n      0.0\n      1919.7999\n      N\n    \n  \n\n9930 rows × 15 columns\n\n\n\n- 멀티 인덱스 셋팅 후 인덱스 기준으로 groupby하기\n\n’sex’와 ’cp’를 기준으로 index를셋팅하고 index를 기준으로 groupby하고자 하는경우\ngroupby의 level은 index가 있는 경우에 사용\n\n\ncust.set_index(['cust_class','sex_type']).groupby(level=[0]).mean()\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      efct_svc_count\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n    \n    \n      cust_class\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      C\n      202006.0\n      20200630.0\n      10001.347992\n      1.019598\n      3552.548591\n      1689.963677\n      1139.409155\n      1236.575918\n      623.090664\n    \n    \n      D\n      202006.0\n      20200630.0\n      10001.122667\n      1.047111\n      7829.480212\n      1590.890907\n      4176.241962\n      489.304579\n      3309.849733\n    \n    \n      E\n      202006.0\n      20200630.0\n      10001.075650\n      1.189913\n      11228.231434\n      1795.634627\n      6302.510076\n      489.232177\n      5371.352846\n    \n    \n      F\n      202006.0\n      20200630.0\n      10001.077135\n      1.433884\n      14994.348233\n      2047.064710\n      8166.923501\n      742.959676\n      7389.011610\n    \n    \n      G\n      202006.0\n      20200630.0\n      10001.013652\n      1.523159\n      16735.260101\n      2005.347119\n      9735.625736\n      386.361783\n      9617.168424\n    \n    \n      H\n      202006.0\n      20200630.0\n      10001.000000\n      2.113475\n      20714.902939\n      2282.967852\n      10859.251396\n      504.888362\n      10836.495186\n    \n    \n      _\n      202006.0\n      20200630.0\n      10002.797288\n      3.306210\n      21358.005450\n      2518.150052\n      11633.643589\n      1945.066645\n      2670.334370\n    \n  \n\n\n\n\n\ncust.set_index(['cust_class','sex_type']).groupby(level=[0,1]).mean()\n\n\n\n\n\n  \n    \n      \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      efct_svc_count\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n    \n    \n      cust_class\n      sex_type\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      C\n      F\n      202006.0\n      20200630.0\n      10001.004711\n      0.978799\n      3804.342244\n      1583.492691\n      1326.369780\n      1005.475117\n      828.849523\n    \n    \n      M\n      202006.0\n      20200630.0\n      10001.011278\n      1.026316\n      3155.385796\n      1648.323176\n      968.955465\n      1237.865008\n      556.026278\n    \n    \n      _\n      202006.0\n      20200630.0\n      10004.977654\n      1.173184\n      4719.075980\n      2442.474066\n      1265.851368\n      2325.028369\n      45.812648\n    \n    \n      D\n      F\n      202006.0\n      20200630.0\n      10001.003646\n      1.040109\n      7848.842709\n      1377.127105\n      4297.720359\n      315.530984\n      3761.540720\n    \n    \n      M\n      202006.0\n      20200630.0\n      10001.007360\n      1.045078\n      7774.098954\n      1742.249493\n      4098.230598\n      550.449533\n      3054.969393\n    \n    \n      _\n      202006.0\n      20200630.0\n      10005.000000\n      1.196970\n      8419.764574\n      2651.074376\n      3441.947276\n      2370.593506\n      0.000000\n    \n    \n      E\n      F\n      202006.0\n      20200630.0\n      10001.000000\n      1.148670\n      11257.301485\n      1623.670941\n      6531.544954\n      304.847684\n      5998.054678\n    \n    \n      M\n      202006.0\n      20200630.0\n      10001.000000\n      1.222772\n      11158.736812\n      1905.210132\n      6114.544589\n      602.976972\n      4923.250531\n    \n    \n      _\n      202006.0\n      20200630.0\n      10005.000000\n      1.458333\n      12208.980550\n      3607.386253\n      4950.584995\n      2526.413200\n      0.000000\n    \n    \n      F\n      F\n      202006.0\n      20200630.0\n      10001.000000\n      1.275000\n      14913.105272\n      1935.092224\n      8166.414828\n      519.668193\n      7656.897493\n    \n    \n      M\n      202006.0\n      20200630.0\n      10001.000000\n      1.560102\n      15013.379957\n      2081.920917\n      8126.197332\n      846.922216\n      7423.372289\n    \n    \n      _\n      202006.0\n      20200630.0\n      10004.733333\n      1.533333\n      16231.437767\n      3527.225972\n      9239.370660\n      2796.554453\n      778.444440\n    \n    \n      G\n      F\n      202006.0\n      20200630.0\n      10001.000000\n      1.435196\n      16538.595017\n      1941.946105\n      9520.483640\n      325.034607\n      9461.716215\n    \n    \n      M\n      202006.0\n      20200630.0\n      10001.000000\n      1.583680\n      16847.160637\n      2032.943013\n      9882.218237\n      408.043516\n      9777.235315\n    \n    \n      _\n      202006.0\n      20200630.0\n      10004.111111\n      1.666667\n      20180.014811\n      4247.300999\n      10277.504440\n      3223.738911\n      2783.276289\n    \n    \n      H\n      F\n      202006.0\n      20200630.0\n      10001.000000\n      1.962264\n      20154.938768\n      2250.525648\n      9926.724864\n      289.192455\n      9969.301691\n    \n    \n      M\n      202006.0\n      20200630.0\n      10001.000000\n      2.204545\n      21052.154088\n      2302.506907\n      11420.886693\n      634.796124\n      11358.782177\n    \n    \n      _\n      F\n      202006.0\n      20200630.0\n      10001.000000\n      0.544248\n      4304.582332\n      910.064466\n      2879.027895\n      166.396450\n      3301.780467\n    \n    \n      M\n      202006.0\n      20200630.0\n      10001.007130\n      0.618538\n      4728.388010\n      1124.726857\n      2604.231501\n      85.475068\n      3039.238578\n    \n    \n      _\n      202006.0\n      20200630.0\n      10007.479381\n      10.409794\n      65268.733884\n      6406.204426\n      34887.732443\n      6705.865106\n      1401.342370\n    \n  \n\n\n\n\n\n\n\n\ngroupby 결과에 집계함수를 적용하여 그룹별(mean, max등) 데이터 확인 가능\n\n- 그룹별로 한번에 데이터를 한번에 보는 경우\n\ncust.set_index(['cust_class','sex_type']).groupby(level=[0,1]).aggregate([np.mean, np.max])\n\nC:\\Users\\rkdcj\\AppData\\Local\\Temp\\ipykernel_17392\\4135122951.py:1: FutureWarning: ['age', 'dt_stop_yn', 'npay_yn', 'termination_yn'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n  cust.set_index(['cust_class','sex_type']).groupby(level=[0,1]).aggregate([np.mean, np.max])\n\n\n\n\n\n\n  \n    \n      \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      efct_svc_count\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n    \n    \n      \n      \n      mean\n      amax\n      mean\n      amax\n      mean\n      amax\n      mean\n      amax\n      mean\n      amax\n      mean\n      amax\n      mean\n      amax\n      mean\n      amax\n      mean\n      amax\n    \n    \n      cust_class\n      sex_type\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      C\n      F\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10001.004711\n      10005\n      0.978799\n      3\n      3804.342244\n      6.815017e+04\n      1583.492691\n      4.810162e+04\n      1326.369780\n      1.443402e+04\n      1005.475117\n      1.031603e+04\n      828.849523\n      14432.0160\n    \n    \n      M\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10001.011278\n      10005\n      1.026316\n      5\n      3155.385796\n      2.306917e+04\n      1648.323176\n      1.477473e+04\n      968.955465\n      1.390940e+04\n      1237.865008\n      9.689333e+03\n      556.026278\n      14787.1166\n    \n    \n      _\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10004.977654\n      10005\n      1.173184\n      5\n      4719.075980\n      2.293453e+04\n      2442.474066\n      1.238933e+04\n      1265.851368\n      1.376072e+04\n      2325.028369\n      1.964850e+04\n      45.812648\n      8200.4640\n    \n    \n      D\n      F\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10001.003646\n      10005\n      1.040109\n      6\n      7848.842709\n      5.407650e+04\n      1377.127105\n      1.665367e+04\n      4297.720359\n      1.438200e+04\n      315.530984\n      8.964522e+03\n      3761.540720\n      14467.3334\n    \n    \n      M\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10001.007360\n      10005\n      1.045078\n      5\n      7774.098954\n      1.447397e+05\n      1742.249493\n      1.315815e+05\n      4098.230598\n      1.925570e+04\n      550.449533\n      2.749282e+04\n      3054.969393\n      18133.6304\n    \n    \n      _\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10005.000000\n      10005\n      1.196970\n      5\n      8419.764574\n      3.232967e+04\n      2651.074376\n      1.868697e+04\n      3441.947276\n      1.939780e+04\n      2370.593506\n      1.024273e+04\n      0.000000\n      0.0000\n    \n    \n      E\n      F\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10001.000000\n      10001\n      1.148670\n      6\n      11257.301485\n      5.226307e+04\n      1623.670941\n      1.637333e+04\n      6531.544954\n      1.753289e+04\n      304.847684\n      1.306933e+04\n      5998.054678\n      17306.4000\n    \n    \n      M\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10001.000000\n      10001\n      1.222772\n      7\n      11158.736812\n      1.055625e+05\n      1905.210132\n      3.797280e+04\n      6114.544589\n      4.347433e+04\n      602.976972\n      1.310073e+04\n      4923.250531\n      39802.3921\n    \n    \n      _\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10005.000000\n      10005\n      1.458333\n      5\n      12208.980550\n      1.644177e+04\n      3607.386253\n      8.696067e+03\n      4950.584995\n      9.865060e+03\n      2526.413200\n      8.704500e+03\n      0.000000\n      0.0000\n    \n    \n      F\n      F\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10001.000000\n      10001\n      1.275000\n      9\n      14913.105272\n      5.993527e+04\n      1935.092224\n      1.211230e+04\n      8166.414828\n      2.822429e+04\n      519.668193\n      1.295988e+04\n      7656.897493\n      29511.7824\n    \n    \n      M\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10001.000000\n      10001\n      1.560102\n      11\n      15013.379957\n      7.300037e+04\n      2081.920917\n      1.589153e+04\n      8126.197332\n      3.605447e+04\n      846.922216\n      1.706433e+04\n      7423.372289\n      33871.3333\n    \n    \n      _\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10004.733333\n      10005\n      1.533333\n      4\n      16231.437767\n      2.956300e+04\n      3527.225972\n      1.405020e+04\n      9239.370660\n      1.773780e+04\n      2796.554453\n      1.321063e+04\n      778.444440\n      11676.6666\n    \n    \n      G\n      F\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10001.000000\n      10001\n      1.435196\n      6\n      16538.595017\n      7.979563e+04\n      1941.946105\n      5.500240e+04\n      9520.483640\n      5.710153e+04\n      325.034607\n      1.559547e+04\n      9461.716215\n      48787.2333\n    \n    \n      M\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10001.000000\n      10001\n      1.583680\n      14\n      16847.160637\n      1.129005e+05\n      2032.943013\n      4.759945e+04\n      9882.218237\n      6.583656e+04\n      408.043516\n      1.954363e+04\n      9777.235315\n      64947.0920\n    \n    \n      _\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10004.111111\n      10005\n      1.666667\n      6\n      20180.014811\n      3.589550e+04\n      4247.300999\n      1.218147e+04\n      10277.504440\n      2.153730e+04\n      3223.738911\n      1.268773e+04\n      2783.276289\n      15642.2866\n    \n    \n      H\n      F\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10001.000000\n      10001\n      1.962264\n      6\n      20154.938768\n      7.471997e+04\n      2250.525648\n      1.120800e+04\n      9926.724864\n      1.826570e+04\n      289.192455\n      2.960000e+03\n      9969.301691\n      18233.2224\n    \n    \n      M\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10001.000000\n      10001\n      2.204545\n      9\n      21052.154088\n      9.613610e+04\n      2302.506907\n      1.442042e+04\n      11420.886693\n      3.036068e+04\n      634.796124\n      1.236263e+04\n      11358.782177\n      30402.5750\n    \n    \n      _\n      F\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10001.000000\n      10001\n      0.544248\n      4\n      4304.582332\n      3.024210e+04\n      910.064466\n      2.749282e+04\n      2879.027895\n      1.659055e+04\n      166.396450\n      2.302815e+04\n      3301.780467\n      16505.8102\n    \n    \n      M\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10001.007130\n      10005\n      0.618538\n      5\n      4728.388010\n      1.320952e+05\n      1124.726857\n      9.946849e+04\n      2604.231501\n      1.704006e+04\n      85.475068\n      4.409887e+03\n      3039.238578\n      16932.6252\n    \n    \n      _\n      202006.0\n      202006\n      20200630.0\n      20200630\n      10007.479381\n      10010\n      10.409794\n      905\n      65268.733884\n      1.281568e+07\n      6406.204426\n      1.188998e+06\n      34887.732443\n      7.689409e+06\n      6705.865106\n      1.208498e+06\n      1401.342370\n      18796.6266\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\ndataframe의 형태를 변경\n여러 분류로 섞인 행 데이터를 열 데이터로 회전 시키는 것\n\npivot의 사전적의미 : (축을 중심으로)회전하다, 회전시키다.\n\npivot형태 : pandas.pivot(index, columns, values) 로 사용할 컴럼을 명시\n\n\n\nimport numpy as np\nimport pandas as pd\n\n\ndata = pd.DataFrame({'cust_id': ['cust_1', 'cust_1', 'cust_1', 'cust_2', 'cust_2', 'cust_2', 'cust_3', 'cust_3', 'cust_3'],\n                  'prod_cd': ['p1', 'p2', 'p3', 'p1', 'p2', 'p3', 'p1', 'p2', 'p3'],\n                  'grade' : ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B'],\n                  'pch_amt': [30, 10, 0, 40, 15, 30, 0, 0, 10]})\ndata\n\n\n\n\n\n  \n    \n      \n      cust_id\n      prod_cd\n      grade\n      pch_amt\n    \n  \n  \n    \n      0\n      cust_1\n      p1\n      A\n      30\n    \n    \n      1\n      cust_1\n      p2\n      A\n      10\n    \n    \n      2\n      cust_1\n      p3\n      A\n      0\n    \n    \n      3\n      cust_2\n      p1\n      A\n      40\n    \n    \n      4\n      cust_2\n      p2\n      A\n      15\n    \n    \n      5\n      cust_2\n      p3\n      A\n      30\n    \n    \n      6\n      cust_3\n      p1\n      B\n      0\n    \n    \n      7\n      cust_3\n      p2\n      B\n      0\n    \n    \n      8\n      cust_3\n      p3\n      B\n      10\n    \n  \n\n\n\n\n- 행(row)는 고객ID(cust_id), 열(col)은 상품코드(prod_cd), 값은 구매금액(pch_amt)을 pivot릏 활용하여 만들어보기\n\ndata.pivot(index = 'cust_id', columns ='prod_cd', values ='pch_amt')\n\n\n\n\n\n  \n    \n      prod_cd\n      p1\n      p2\n      p3\n    \n    \n      cust_id\n      \n      \n      \n    \n  \n  \n    \n      cust_1\n      30\n      10\n      0\n    \n    \n      cust_2\n      40\n      15\n      30\n    \n    \n      cust_3\n      0\n      0\n      10\n    \n  \n\n\n\n\n\ndata.pivot('cust_id', 'prod_cd', 'pch_amt')\n\n\n\n\n\n  \n    \n      prod_cd\n      p1\n      p2\n      p3\n    \n    \n      cust_id\n      \n      \n      \n    \n  \n  \n    \n      cust_1\n      30\n      10\n      0\n    \n    \n      cust_2\n      40\n      15\n      30\n    \n    \n      cust_3\n      0\n      0\n      10\n    \n  \n\n\n\n\n\n\n\n\npivot_table형태 : pandas.pivot_table(data, index, columns, aggfunc)\n\n- pivot_table을 활용하여 위와 동일하게 만들기\n\ndata.pivot_table(index = 'cust_id', columns ='prod_cd', values ='pch_amt')\n\n\n\n\n\n  \n    \n      prod_cd\n      p1\n      p2\n      p3\n    \n    \n      cust_id\n      \n      \n      \n    \n  \n  \n    \n      cust_1\n      30\n      10\n      0\n    \n    \n      cust_2\n      40\n      15\n      30\n    \n    \n      cust_3\n      0\n      0\n      10\n    \n  \n\n\n\n\n\n\n\n\npivot은 안되고 pivot_table만을 사용해야 하는 경우가 있음\n\n\n\n\ndata.pivot(index = ['cust_id','grade'], columns ='prod_cd', values ='pch_amt')\n\n\n\n\n\n  \n    \n      \n      prod_cd\n      p1\n      p2\n      p3\n    \n    \n      cust_id\n      grade\n      \n      \n      \n    \n  \n  \n    \n      cust_1\n      A\n      30\n      10\n      0\n    \n    \n      cust_2\n      A\n      40\n      15\n      30\n    \n    \n      cust_3\n      B\n      0\n      0\n      10\n    \n  \n\n\n\n\n\ndata.pivot_table(index = ['cust_id','grade'], columns ='prod_cd', values ='pch_amt')\n\n\n\n\n\n  \n    \n      \n      prod_cd\n      p1\n      p2\n      p3\n    \n    \n      cust_id\n      grade\n      \n      \n      \n    \n  \n  \n    \n      cust_1\n      A\n      30\n      10\n      0\n    \n    \n      cust_2\n      A\n      40\n      15\n      30\n    \n    \n      cust_3\n      B\n      0\n      0\n      10\n    \n  \n\n\n\n\n\n\n\n\ndata.pivot(index = 'cust_id', columns =['grade','prod_cd'], values ='pch_amt')\n\n\n\n\n\n  \n    \n      grade\n      A\n      B\n    \n    \n      prod_cd\n      p1\n      p2\n      p3\n      p1\n      p2\n      p3\n    \n    \n      cust_id\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      cust_1\n      30.0\n      10.0\n      0.0\n      NaN\n      NaN\n      NaN\n    \n    \n      cust_2\n      40.0\n      15.0\n      30.0\n      NaN\n      NaN\n      NaN\n    \n    \n      cust_3\n      NaN\n      NaN\n      NaN\n      0.0\n      0.0\n      10.0\n    \n  \n\n\n\n\n\ndata.pivot_table(index = 'cust_id', columns =['grade','prod_cd'], values ='pch_amt')\n\n\n\n\n\n  \n    \n      grade\n      A\n      B\n    \n    \n      prod_cd\n      p1\n      p2\n      p3\n      p1\n      p2\n      p3\n    \n    \n      cust_id\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      cust_1\n      30.0\n      10.0\n      0.0\n      NaN\n      NaN\n      NaN\n    \n    \n      cust_2\n      40.0\n      15.0\n      30.0\n      NaN\n      NaN\n      NaN\n    \n    \n      cust_3\n      NaN\n      NaN\n      NaN\n      0.0\n      0.0\n      10.0\n    \n  \n\n\n\n\n\n\n\n\n\npivot은 중복 값이 있는 경우 valueError를 반환함\npivot_table은 aggregation 함수를 활용하여 처리\n\n\n- index로 쓰인 grade가 중복이 있음\n\ndata\n\n\n\n\n\n  \n    \n      \n      cust_id\n      prod_cd\n      grade\n      pch_amt\n    \n  \n  \n    \n      0\n      cust_1\n      p1\n      A\n      30\n    \n    \n      1\n      cust_1\n      p2\n      A\n      10\n    \n    \n      2\n      cust_1\n      p3\n      A\n      0\n    \n    \n      3\n      cust_2\n      p1\n      A\n      40\n    \n    \n      4\n      cust_2\n      p2\n      A\n      15\n    \n    \n      5\n      cust_2\n      p3\n      A\n      30\n    \n    \n      6\n      cust_3\n      p1\n      B\n      0\n    \n    \n      7\n      cust_3\n      p2\n      B\n      0\n    \n    \n      8\n      cust_3\n      p3\n      B\n      10\n    \n  \n\n\n\n\n\ndata.pivot(index='grade', columns='prod_cd', values='pch_amt')  \n\nValueError: Index contains duplicate entries, cannot reshape\n\n\n\ndata.pivot_table(index='grade', columns='prod_cd', values='pch_amt')  \n\n\n\n\n\n  \n    \n      prod_cd\n      p1\n      p2\n      p3\n    \n    \n      grade\n      \n      \n      \n    \n  \n  \n    \n      A\n      35.0\n      12.5\n      15.0\n    \n    \n      B\n      0.0\n      0.0\n      10.0\n    \n  \n\n\n\n\n\n\n\n\npivot_table은 aggregation 함수를 활용하여 처리\n\n- aggfunc를 sum으로 구하기\n\ndata.pivot_table(index='grade', columns='prod_cd', values='pch_amt', aggfunc=np.sum)\n\n\n\n\n\n  \n    \n      prod_cd\n      p1\n      p2\n      p3\n    \n    \n      grade\n      \n      \n      \n    \n  \n  \n    \n      A\n      70\n      25\n      30\n    \n    \n      B\n      0\n      0\n      10\n    \n  \n\n\n\n\n- 위와 같은결과(참고)\n\npd.pivot_table(data, index='grade', columns='prod_cd', values='pch_amt', aggfunc=np.sum)\n\n\n\n\n\n  \n    \n      prod_cd\n      p1\n      p2\n      p3\n    \n    \n      grade\n      \n      \n      \n    \n  \n  \n    \n      A\n      70\n      25\n      30\n    \n    \n      B\n      0\n      0\n      10\n    \n  \n\n\n\n\n- aggfunc를 mean으로 구하기(default가 mean임)\n\ndata.pivot_table(index='grade', columns='prod_cd', values='pch_amt', aggfunc=np.mean)\n\n\n\n\n\n  \n    \n      prod_cd\n      p1\n      p2\n      p3\n    \n    \n      grade\n      \n      \n      \n    \n  \n  \n    \n      A\n      35.0\n      12.5\n      15.0\n    \n    \n      B\n      0.0\n      0.0\n      10.0\n    \n  \n\n\n\n\n\n\n\n\n\n\n\nCode\ndf = pd.DataFrame({\n    '지역': ['서울', '서울', '서울', '경기', '경기', '부산', '서울', '서울', '부산', '경기', '경기', '경기'],\n    '요일': ['월요일', '화요일', '수요일', '월요일', '화요일', '월요일', '목요일', '금요일', '화요일', '수요일', '목요일', '금요일'],\n    '강수량': [100, 80, 1000, 200, 200, 100, 50, 100, 200, 100, 50, 100],\n    '강수확률': [80, 70, 90, 10, 20, 30, 50, 90, 20, 80, 50, 10]})\n\ndf\n\n\n\n\n\n\n  \n    \n      \n      지역\n      요일\n      강수량\n      강수확률\n    \n  \n  \n    \n      0\n      서울\n      월요일\n      100\n      80\n    \n    \n      1\n      서울\n      화요일\n      80\n      70\n    \n    \n      2\n      서울\n      수요일\n      1000\n      90\n    \n    \n      3\n      경기\n      월요일\n      200\n      10\n    \n    \n      4\n      경기\n      화요일\n      200\n      20\n    \n    \n      5\n      부산\n      월요일\n      100\n      30\n    \n    \n      6\n      서울\n      목요일\n      50\n      50\n    \n    \n      7\n      서울\n      금요일\n      100\n      90\n    \n    \n      8\n      부산\n      화요일\n      200\n      20\n    \n    \n      9\n      경기\n      수요일\n      100\n      80\n    \n    \n      10\n      경기\n      목요일\n      50\n      50\n    \n    \n      11\n      경기\n      금요일\n      100\n      10\n    \n  \n\n\n\n\n\n\nstack : 컬럼 레벨에서 인덱스 레벨로 dataframe 변경 (gather?)\n즉, 데이터를 row 레벨로 쌓아올리는 개념으로 이해하면 쉬움\nunstack : 인덱스 레벨에서 컬럼 레벨로 dataframe 변경 (spread?)\nstack의 반대 operation\n둘은 역의 관계에 있음\n\n\n- ‘지역’, ‘요일’ 두개로 인덱스를 설정하고 별도의 DataFrame으로 설정 하기\n\nnew_df = df.set_index(['지역', '요일'])\nnew_df\n\n\n\n\n\n  \n    \n      \n      \n      강수량\n      강수확률\n    \n    \n      지역\n      요일\n      \n      \n    \n  \n  \n    \n      서울\n      월요일\n      100\n      80\n    \n    \n      화요일\n      80\n      70\n    \n    \n      수요일\n      1000\n      90\n    \n    \n      경기\n      월요일\n      200\n      10\n    \n    \n      화요일\n      200\n      20\n    \n    \n      부산\n      월요일\n      100\n      30\n    \n    \n      서울\n      목요일\n      50\n      50\n    \n    \n      금요일\n      100\n      90\n    \n    \n      부산\n      화요일\n      200\n      20\n    \n    \n      경기\n      수요일\n      100\n      80\n    \n    \n      목요일\n      50\n      50\n    \n    \n      금요일\n      100\n      10\n    \n  \n\n\n\n\n- 첫번째 레벨의 인덱스(지역)를 컬럼으로 이동 / 인덱스도 레벨이 있음\n\nnew_df.unstack(0)\n\n\n\n\n\n  \n    \n      \n      강수량\n      강수확률\n    \n    \n      지역\n      경기\n      부산\n      서울\n      경기\n      부산\n      서울\n    \n    \n      요일\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      금요일\n      100.0\n      NaN\n      100.0\n      10.0\n      NaN\n      90.0\n    \n    \n      목요일\n      50.0\n      NaN\n      50.0\n      50.0\n      NaN\n      50.0\n    \n    \n      수요일\n      100.0\n      NaN\n      1000.0\n      80.0\n      NaN\n      90.0\n    \n    \n      월요일\n      200.0\n      100.0\n      100.0\n      10.0\n      30.0\n      80.0\n    \n    \n      화요일\n      200.0\n      200.0\n      80.0\n      20.0\n      20.0\n      70.0\n    \n  \n\n\n\n\n- 두번째 레벨의 인덱스를 컬럼으로 이동\n\nnew_df.unstack(1)\n\n\n\n\n\n  \n    \n      \n      강수량\n      강수확률\n    \n    \n      요일\n      금요일\n      목요일\n      수요일\n      월요일\n      화요일\n      금요일\n      목요일\n      수요일\n      월요일\n      화요일\n    \n    \n      지역\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      경기\n      100.0\n      50.0\n      100.0\n      200.0\n      200.0\n      10.0\n      50.0\n      80.0\n      10.0\n      20.0\n    \n    \n      부산\n      NaN\n      NaN\n      NaN\n      100.0\n      200.0\n      NaN\n      NaN\n      NaN\n      30.0\n      20.0\n    \n    \n      서울\n      100.0\n      50.0\n      1000.0\n      100.0\n      80.0\n      90.0\n      50.0\n      90.0\n      80.0\n      70.0\n    \n  \n\n\n\n\n\nnew_df\n\n\n\n\n\n  \n    \n      \n      \n      강수량\n      강수확률\n    \n    \n      지역\n      요일\n      \n      \n    \n  \n  \n    \n      서울\n      월요일\n      100\n      80\n    \n    \n      화요일\n      80\n      70\n    \n    \n      수요일\n      1000\n      90\n    \n    \n      경기\n      월요일\n      200\n      10\n    \n    \n      화요일\n      200\n      20\n    \n    \n      부산\n      월요일\n      100\n      30\n    \n    \n      서울\n      목요일\n      50\n      50\n    \n    \n      금요일\n      100\n      90\n    \n    \n      부산\n      화요일\n      200\n      20\n    \n    \n      경기\n      수요일\n      100\n      80\n    \n    \n      목요일\n      50\n      50\n    \n    \n      금요일\n      100\n      10\n    \n  \n\n\n\n\n- new_df.unstack(0)상태에서 첫번째 레벨의 컬럼(강수량과 강수확률)을 인덱스로 이동(stack(0))\n\nnew_df.unstack(0)\n\n\n\n\n\n  \n    \n      \n      강수량\n      강수확률\n    \n    \n      지역\n      경기\n      부산\n      서울\n      경기\n      부산\n      서울\n    \n    \n      요일\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      금요일\n      100.0\n      NaN\n      100.0\n      10.0\n      NaN\n      90.0\n    \n    \n      목요일\n      50.0\n      NaN\n      50.0\n      50.0\n      NaN\n      50.0\n    \n    \n      수요일\n      100.0\n      NaN\n      1000.0\n      80.0\n      NaN\n      90.0\n    \n    \n      월요일\n      200.0\n      100.0\n      100.0\n      10.0\n      30.0\n      80.0\n    \n    \n      화요일\n      200.0\n      200.0\n      80.0\n      20.0\n      20.0\n      70.0\n    \n  \n\n\n\n\n\nnew_df.unstack(0).stack(0)\n\n\n\n\n\n  \n    \n      \n      지역\n      경기\n      부산\n      서울\n    \n    \n      요일\n      \n      \n      \n      \n    \n  \n  \n    \n      금요일\n      강수량\n      100.0\n      NaN\n      100.0\n    \n    \n      강수확률\n      10.0\n      NaN\n      90.0\n    \n    \n      목요일\n      강수량\n      50.0\n      NaN\n      50.0\n    \n    \n      강수확률\n      50.0\n      NaN\n      50.0\n    \n    \n      수요일\n      강수량\n      100.0\n      NaN\n      1000.0\n    \n    \n      강수확률\n      80.0\n      NaN\n      90.0\n    \n    \n      월요일\n      강수량\n      200.0\n      100.0\n      100.0\n    \n    \n      강수확률\n      10.0\n      30.0\n      80.0\n    \n    \n      화요일\n      강수량\n      200.0\n      200.0\n      80.0\n    \n    \n      강수확률\n      20.0\n      20.0\n      70.0\n    \n  \n\n\n\n\n\nnew_df.unstack(0).stack(1)\n\n\n\n\n\n  \n    \n      \n      \n      강수량\n      강수확률\n    \n    \n      요일\n      지역\n      \n      \n    \n  \n  \n    \n      금요일\n      경기\n      100.0\n      10.0\n    \n    \n      서울\n      100.0\n      90.0\n    \n    \n      목요일\n      경기\n      50.0\n      50.0\n    \n    \n      서울\n      50.0\n      50.0\n    \n    \n      수요일\n      경기\n      100.0\n      80.0\n    \n    \n      서울\n      1000.0\n      90.0\n    \n    \n      월요일\n      경기\n      200.0\n      10.0\n    \n    \n      부산\n      100.0\n      30.0\n    \n    \n      서울\n      100.0\n      80.0\n    \n    \n      화요일\n      경기\n      200.0\n      20.0\n    \n    \n      부산\n      200.0\n      20.0\n    \n    \n      서울\n      80.0\n      70.0\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n# pandas import\nimport pandas as pd\nimport numpy as np\n\n\n\npandas.concat 함수 (배열결합 : concatenate)\n데이터의 속성 형태가 동일한 데이터 셋 끼리 합칠때 사용 (DataFrame을 물리적으로 붙여주는 함수)\n열 or 행 레벨로 병합하는 것\n\n\n\n\n\nignore_index, axis 활용\n\n\ndf1 = pd.DataFrame({'key1' : [0,1,2,3,4], 'value1' : ['a', 'b', 'c','d','e']}, index=[0,1,2,3,4])\ndf2 = pd.DataFrame({'key1' : [3,4,5,6,7], 'value1' : ['c','d','e','f','g']}, index=[3,4,5,6,7])\n\n\ndf1,df2\n\n(   key1 value1\n 0     0      a\n 1     1      b\n 2     2      c\n 3     3      d\n 4     4      e,\n    key1 value1\n 3     3      c\n 4     4      d\n 5     5      e\n 6     6      f\n 7     7      g)\n\n\n- concat함수 옵션\n\nignore_index : 기존 index를 무시하고자 하는 경우\n\nFalse : 기존 index유지(default) / True : 기존 index무시(index재배열)\n\naxis\n\n0 : 위+아래로 합치기(row레벨) / 1 : 왼쪽+오른쪽으로 합치기(col레벨)\n\n\n\n# ignore_index에 대한이해  (직접 타이핑 해보세요)\npd.concat([df1, df2], ignore_index=False)\n\n\n\n\n\n  \n    \n      \n      key1\n      value1\n    \n  \n  \n    \n      0\n      0\n      a\n    \n    \n      1\n      1\n      b\n    \n    \n      2\n      2\n      c\n    \n    \n      3\n      3\n      d\n    \n    \n      4\n      4\n      e\n    \n    \n      3\n      3\n      c\n    \n    \n      4\n      4\n      d\n    \n    \n      5\n      5\n      e\n    \n    \n      6\n      6\n      f\n    \n    \n      7\n      7\n      g\n    \n  \n\n\n\n\n\n# ignore_index에 대한이해  (직접 타이핑 해보세요)\npd.concat([df1, df2], ignore_index=True)\n\n\n\n\n\n  \n    \n      \n      key1\n      value1\n    \n  \n  \n    \n      0\n      0\n      a\n    \n    \n      1\n      1\n      b\n    \n    \n      2\n      2\n      c\n    \n    \n      3\n      3\n      d\n    \n    \n      4\n      4\n      e\n    \n    \n      5\n      3\n      c\n    \n    \n      6\n      4\n      d\n    \n    \n      7\n      5\n      e\n    \n    \n      8\n      6\n      f\n    \n    \n      9\n      7\n      g\n    \n  \n\n\n\n\n\n# axis=0,1 비교해 보기  (직접 타이핑 해보세요)\npd.concat([df1, df2], axis =1)\n\n\n\n\n\n  \n    \n      \n      key1\n      value1\n      key1\n      value1\n    \n  \n  \n    \n      0\n      0.0\n      a\n      NaN\n      NaN\n    \n    \n      1\n      1.0\n      b\n      NaN\n      NaN\n    \n    \n      2\n      2.0\n      c\n      NaN\n      NaN\n    \n    \n      3\n      3.0\n      d\n      3.0\n      c\n    \n    \n      4\n      4.0\n      e\n      4.0\n      d\n    \n    \n      5\n      NaN\n      NaN\n      5.0\n      e\n    \n    \n      6\n      NaN\n      NaN\n      6.0\n      f\n    \n    \n      7\n      NaN\n      NaN\n      7.0\n      g\n    \n  \n\n\n\n\n\n# axis=0,1 비교해 보기  (직접 타이핑 해보세요)\npd.concat([df1, df2], axis =0)\n\n\n\n\n\n  \n    \n      \n      key1\n      value1\n    \n  \n  \n    \n      0\n      0\n      a\n    \n    \n      1\n      1\n      b\n    \n    \n      2\n      2\n      c\n    \n    \n      3\n      3\n      d\n    \n    \n      4\n      4\n      e\n    \n    \n      3\n      3\n      c\n    \n    \n      4\n      4\n      d\n    \n    \n      5\n      5\n      e\n    \n    \n      6\n      6\n      f\n    \n    \n      7\n      7\n      g\n    \n  \n\n\n\n\n\n\n\n\n\nconcat함수중에 join에 대한이해\njoin 방식은 outer의 경우 합집합, inner의 경우 교집합을 의미\n\n\n\ndf3 = pd.DataFrame({'a':['a0','a1','a2', 'a3'], 'b':['b0','b1','b2','b3'], 'c':['c0','c1','c2','c3']}, index = [0,1,2,3])\ndf4 = pd.DataFrame({'a':['a2','a3','a4', 'a5'], 'b':['b2','b3','b4','b5'], 'c':['c2','c3','c4','c5'], 'd':['d1','d2','d3','d4']}, index = [2,3,4,5])\n\n\ndf3\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      a0\n      b0\n      c0\n    \n    \n      1\n      a1\n      b1\n      c1\n    \n    \n      2\n      a2\n      b2\n      c2\n    \n    \n      3\n      a3\n      b3\n      c3\n    \n  \n\n\n\n\n\ndf4\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n      d\n    \n  \n  \n    \n      2\n      a2\n      b2\n      c2\n      d1\n    \n    \n      3\n      a3\n      b3\n      c3\n      d2\n    \n    \n      4\n      a4\n      b4\n      c4\n      d3\n    \n    \n      5\n      a5\n      b5\n      c5\n      d4\n    \n  \n\n\n\n\n\npd.concat([df3, df4], join='outer')\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n      d\n    \n  \n  \n    \n      0\n      a0\n      b0\n      c0\n      NaN\n    \n    \n      1\n      a1\n      b1\n      c1\n      NaN\n    \n    \n      2\n      a2\n      b2\n      c2\n      NaN\n    \n    \n      3\n      a3\n      b3\n      c3\n      NaN\n    \n    \n      2\n      a2\n      b2\n      c2\n      d1\n    \n    \n      3\n      a3\n      b3\n      c3\n      d2\n    \n    \n      4\n      a4\n      b4\n      c4\n      d3\n    \n    \n      5\n      a5\n      b5\n      c5\n      d4\n    \n  \n\n\n\n\n\npd.concat([df3, df4], join='inner')\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      a0\n      b0\n      c0\n    \n    \n      1\n      a1\n      b1\n      c1\n    \n    \n      2\n      a2\n      b2\n      c2\n    \n    \n      3\n      a3\n      b3\n      c3\n    \n    \n      2\n      a2\n      b2\n      c2\n    \n    \n      3\n      a3\n      b3\n      c3\n    \n    \n      4\n      a4\n      b4\n      c4\n    \n    \n      5\n      a5\n      b5\n      c5\n    \n  \n\n\n\n\n\n\n\n\n\nconcat함수중에 verify_integrity에 대한 이해\nverify_integrity=False가 default임으로 error발생을 하지 않음\nverify_integrity=True인 경우 error 발생\n\n\n\ndf5 = pd.DataFrame({'A':['A0','A1','A2'], 'B':['B0','B1','B2'], 'C':['C0','C1','C2'], 'D':['D0','D1','D2']}, index=['I0','I1','I2'])\ndf6 = pd.DataFrame({'A':['AA2','A3','A4'], 'B':['BB2','B3','B4'], 'C':['CC2','C3','C4'], 'D':['DD2','D3','D4']}, index=['I2','I3','I4'])\n\n\ndf5\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      I0\n      A0\n      B0\n      C0\n      D0\n    \n    \n      I1\n      A1\n      B1\n      C1\n      D1\n    \n    \n      I2\n      A2\n      B2\n      C2\n      D2\n    \n  \n\n\n\n\n\ndf6\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      I2\n      AA2\n      BB2\n      CC2\n      DD2\n    \n    \n      I3\n      A3\n      B3\n      C3\n      D3\n    \n    \n      I4\n      A4\n      B4\n      C4\n      D4\n    \n  \n\n\n\n\n\npd.concat([df5, df6], verify_integrity=False)\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      I0\n      A0\n      B0\n      C0\n      D0\n    \n    \n      I1\n      A1\n      B1\n      C1\n      D1\n    \n    \n      I2\n      A2\n      B2\n      C2\n      D2\n    \n    \n      I2\n      AA2\n      BB2\n      CC2\n      DD2\n    \n    \n      I3\n      A3\n      B3\n      C3\n      D3\n    \n    \n      I4\n      A4\n      B4\n      C4\n      D4\n    \n  \n\n\n\n\n\n# index중복이있는 경우 error가 남\npd.concat([df5, df6], verify_integrity=True)\n\nValueError: Indexes have overlapping values: Index(['I2'], dtype='object')"
  },
  {
    "objectID": "post/etc/ai study/2023-07-06-04.데이터 전처리.html",
    "href": "post/etc/ai study/2023-07-06-04.데이터 전처리.html",
    "title": "04. data processing",
    "section": "",
    "text": "데이터 분석/AI 모델링을 위하여 데이터를 정제하는 전처리 방법을 실습해 봅시다.\n우선 확보한 데이터를 전처리를 위해 불러옵니다.\nAI/DU 포탈의 데이터관리탭에 데이터를 Upload 했다면 아래 코드를 실행하여 JupyterLab 환경으로 불러 올 수 있습니다\n\n# #AIDU환경 사용자\nimport pandas as pd\n\n# from aicentro.session import Session\n# from aicentro.framework.keras import Keras as AiduFrm\n\n# aidu_session = Session(verify=False)\n# aidu_framework = AiduFrm(session=aidu_session) \n\n# #sc_cust_info_txn.csv대시 AI/DU 포탈내 데이터 관리에 있는 불러오기를 원하는 파일명을 입력해 주면 됩니다. \n# df = pd.read_csv(aidu_framework.config.data_dir + '/sc_cust_info_txn_v1.5.csv')\n\n개별 환경 사용자라면 ipynb 파일과 같은 Workspace에 sc_cust_info_txn_v1.5.csv파일을 넣어서 아래 코드를 실행하여 데이터를 불러 올 수 있습니다.\n\n#개별 환경 사용자\ndf=pd.read_csv(\"sc_cust_info_txn_v1.5.csv\")\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      0\n      202006\n      20200630\n      10001\n      C\n      F\n      28\n      0\n      N\n      N\n      2640.0000\n      792.000000\n      1584.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      1\n      202006\n      20200630\n      10001\n      _\n      _\n      _\n      1\n      N\n      N\n      300.0000\n      90.000000\n      180.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      2\n      202006\n      20200630\n      10001\n      E\n      F\n      24\n      1\n      N\n      N\n      16840.0000\n      2526.000000\n      6983.0000\n      0.0\n      6981.0000\n      N\n    \n    \n      3\n      202006\n      20200630\n      10001\n      F\n      F\n      32\n      1\n      N\n      N\n      15544.7334\n      2331.710010\n      6750.4666\n      0.0\n      6508.8000\n      N\n    \n    \n      4\n      202006\n      20200630\n      10001\n      D\n      M\n      18\n      1\n      N\n      N\n      4700.0000\n      0.000000\n      4502.0000\n      0.0\n      4507.7000\n      N\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9925\n      202006\n      20200630\n      10001\n      C\n      F\n      15\n      1\n      N\n      Y\n      1296.0999\n      194.414985\n      643.1001\n      0.0\n      852.5499\n      N\n    \n    \n      9926\n      202006\n      20200630\n      10001\n      G\n      M\n      12\n      1\n      N\n      N\n      13799.6666\n      2069.949990\n      10605.9266\n      0.0\n      10603.9266\n      N\n    \n    \n      9927\n      202006\n      20200630\n      10005\n      C\n      _\n      _\n      1\n      N\n      N\n      1396.2000\n      1206.000000\n      0.0000\n      1212.0\n      0.0000\n      N\n    \n    \n      9928\n      202006\n      20200630\n      10001\n      C\n      F\n      40\n      0\n      N\n      N\n      3140.0000\n      942.000000\n      1884.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      9929\n      202006\n      20200630\n      10001\n      C\n      F\n      59\n      1\n      N\n      N\n      2436.9000\n      365.535000\n      1839.9000\n      0.0\n      1919.7999\n      N\n    \n  \n\n9930 rows × 15 columns\n\n\n\n\n\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 9930 entries, 0 to 9929\nData columns (total 15 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   base_ym             9930 non-null   int64  \n 1   dpro_tgt_perd_val   9930 non-null   int64  \n 2   cust_ctg_type       9930 non-null   int64  \n 3   cust_class          9930 non-null   object \n 4   sex_type            9930 non-null   object \n 5   age                 9930 non-null   object \n 6   efct_svc_count      9930 non-null   int64  \n 7   dt_stop_yn          9930 non-null   object \n 8   npay_yn             9930 non-null   object \n 9   r3m_avg_bill_amt    9930 non-null   float64\n 10  r3m_A_avg_arpu_amt  9930 non-null   float64\n 11  r3m_B_avg_arpu_amt  9930 non-null   float64\n 12  r6m_A_avg_arpu_amt  9930 non-null   float64\n 13  r6m_B_avg_arpu_amt  9930 non-null   float64\n 14  termination_yn      9930 non-null   object \ndtypes: float64(5), int64(4), object(6)\nmemory usage: 1.1+ MB\n\n\nhead와 tail은 sample 데이터 확인이 가능합니다.\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      0\n      202006\n      20200630\n      10001\n      C\n      F\n      28\n      0\n      N\n      N\n      2640.0000\n      792.00000\n      1584.0000\n      0.0\n      0.0\n      Y\n    \n    \n      1\n      202006\n      20200630\n      10001\n      _\n      _\n      _\n      1\n      N\n      N\n      300.0000\n      90.00000\n      180.0000\n      0.0\n      0.0\n      Y\n    \n    \n      2\n      202006\n      20200630\n      10001\n      E\n      F\n      24\n      1\n      N\n      N\n      16840.0000\n      2526.00000\n      6983.0000\n      0.0\n      6981.0\n      N\n    \n    \n      3\n      202006\n      20200630\n      10001\n      F\n      F\n      32\n      1\n      N\n      N\n      15544.7334\n      2331.71001\n      6750.4666\n      0.0\n      6508.8\n      N\n    \n    \n      4\n      202006\n      20200630\n      10001\n      D\n      M\n      18\n      1\n      N\n      N\n      4700.0000\n      0.00000\n      4502.0000\n      0.0\n      4507.7\n      N\n    \n  \n\n\n\n\n\n df.tail()\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      9925\n      202006\n      20200630\n      10001\n      C\n      F\n      15\n      1\n      N\n      Y\n      1296.0999\n      194.414985\n      643.1001\n      0.0\n      852.5499\n      N\n    \n    \n      9926\n      202006\n      20200630\n      10001\n      G\n      M\n      12\n      1\n      N\n      N\n      13799.6666\n      2069.949990\n      10605.9266\n      0.0\n      10603.9266\n      N\n    \n    \n      9927\n      202006\n      20200630\n      10005\n      C\n      _\n      _\n      1\n      N\n      N\n      1396.2000\n      1206.000000\n      0.0000\n      1212.0\n      0.0000\n      N\n    \n    \n      9928\n      202006\n      20200630\n      10001\n      C\n      F\n      40\n      0\n      N\n      N\n      3140.0000\n      942.000000\n      1884.0000\n      0.0\n      0.0000\n      Y\n    \n    \n      9929\n      202006\n      20200630\n      10001\n      C\n      F\n      59\n      1\n      N\n      N\n      2436.9000\n      365.535000\n      1839.9000\n      0.0\n      1919.7999\n      N\n    \n  \n\n\n\n\ndescribe를 통해 수학적 통계를 확인해 보는 작업도 해봅시다.\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      base_ym\n      dpro_tgt_perd_val\n      cust_ctg_type\n      efct_svc_count\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      r6m_A_avg_arpu_amt\n      r6m_B_avg_arpu_amt\n    \n  \n  \n    \n      count\n      9930.0\n      9930.0\n      9930.000000\n      9930.000000\n      9.930000e+03\n      9.930000e+03\n      9.930000e+03\n      9.930000e+03\n      9930.000000\n    \n    \n      mean\n      202006.0\n      20200630.0\n      10001.372810\n      1.520040\n      1.181774e+04\n      1.897536e+03\n      6.395259e+03\n      8.496206e+02\n      4624.897630\n    \n    \n      std\n      0.0\n      0.0\n      1.605016\n      15.404037\n      1.397822e+05\n      1.235342e+04\n      8.346138e+04\n      1.235124e+04\n      4561.049131\n    \n    \n      min\n      202006.0\n      20200630.0\n      10001.000000\n      0.000000\n      0.000000e+00\n      0.000000e+00\n      0.000000e+00\n      0.000000e+00\n      0.000000\n    \n    \n      25%\n      202006.0\n      20200630.0\n      10001.000000\n      1.000000\n      3.624503e+03\n      3.240000e+02\n      1.260000e+03\n      0.000000e+00\n      0.000000\n    \n    \n      50%\n      202006.0\n      20200630.0\n      10001.000000\n      1.000000\n      8.284467e+03\n      1.593307e+03\n      4.768617e+03\n      0.000000e+00\n      3959.316700\n    \n    \n      75%\n      202006.0\n      20200630.0\n      10001.000000\n      1.000000\n      1.372000e+04\n      2.308360e+03\n      7.982000e+03\n      1.006125e+03\n      7741.006900\n    \n    \n      max\n      202006.0\n      20200630.0\n      10010.000000\n      905.000000\n      1.281568e+07\n      1.188998e+06\n      7.689409e+06\n      1.208498e+06\n      64947.092000\n    \n  \n\n\n\n\n\n\n\n- 데이터 처리에 필요한 10개 항목만 추출\n고객등급(cust_class), 성별(sex_type), 나이(age), \n\n사용서비스수(efct_svc_count), 서비스중지여부 (dt_stop_yn), 미납여부(npay_yn),\n\n3개월 평균 요금(r3m_avg_bill_amt), A서비스 3개월 평균요금(r3m_A_avg_arpu_amt), \n\nB서비스 3개월 평균요금(r3m_B_avg_arpu_amt), 해지여부(termination_yn)\n\ncust=df[[\"cust_class\",\"sex_type\",\"age\",\"efct_svc_count\",\"dt_stop_yn\",\"npay_yn\",\"r3m_avg_bill_amt\",\"r3m_A_avg_arpu_amt\",\"r3m_B_avg_arpu_amt\", \"termination_yn\"]]\ncust.head()\n\n\n\n\n\n  \n    \n      \n      cust_class\n      sex_type\n      age\n      efct_svc_count\n      dt_stop_yn\n      npay_yn\n      r3m_avg_bill_amt\n      r3m_A_avg_arpu_amt\n      r3m_B_avg_arpu_amt\n      termination_yn\n    \n  \n  \n    \n      0\n      C\n      F\n      28\n      0\n      N\n      N\n      2640.0000\n      792.00000\n      1584.0000\n      Y\n    \n    \n      1\n      _\n      _\n      _\n      1\n      N\n      N\n      300.0000\n      90.00000\n      180.0000\n      Y\n    \n    \n      2\n      E\n      F\n      24\n      1\n      N\n      N\n      16840.0000\n      2526.00000\n      6983.0000\n      N\n    \n    \n      3\n      F\n      F\n      32\n      1\n      N\n      N\n      15544.7334\n      2331.71001\n      6750.4666\n      N\n    \n    \n      4\n      D\n      M\n      18\n      1\n      N\n      N\n      4700.0000\n      0.00000\n      4502.0000\n      N\n    \n  \n\n\n\n\n- 컬럼명을 간결하고 직관적으로 변경.\n\ncust=cust.rename(columns = {\"cust_class\" : 'class',\"sex_type\":'sex', \"efct_svc_count\":'service', \"dt_stop_yn\":'stop',\"npay_yn\":'npay', \"r3m_avg_bill_amt\":'avg_bill', \"r3m_A_avg_arpu_amt\":\"A_bill\", \"r3m_B_avg_arpu_amt\":'B_bill', \"termination_yn\":'termination'})\ncust.head()\n\n\n\n\n\n  \n    \n      \n      class\n      sex\n      age\n      service\n      stop\n      npay\n      avg_bill\n      A_bill\n      B_bill\n      termination\n    \n  \n  \n    \n      0\n      C\n      F\n      28\n      0\n      N\n      N\n      2640.0000\n      792.00000\n      1584.0000\n      Y\n    \n    \n      1\n      _\n      _\n      _\n      1\n      N\n      N\n      300.0000\n      90.00000\n      180.0000\n      Y\n    \n    \n      2\n      E\n      F\n      24\n      1\n      N\n      N\n      16840.0000\n      2526.00000\n      6983.0000\n      N\n    \n    \n      3\n      F\n      F\n      32\n      1\n      N\n      N\n      15544.7334\n      2331.71001\n      6750.4666\n      N\n    \n    \n      4\n      D\n      M\n      18\n      1\n      N\n      N\n      4700.0000\n      0.00000\n      4502.0000\n      N\n    \n  \n\n\n\n\n\ncust.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 9930 entries, 0 to 9929\nData columns (total 10 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   class        9930 non-null   object \n 1   sex          9930 non-null   object \n 2   age          9930 non-null   object \n 3   service      9930 non-null   int64  \n 4   stop         9930 non-null   object \n 5   npay         9930 non-null   object \n 6   avg_bill     9930 non-null   float64\n 7   A_bill       9930 non-null   float64\n 8   B_bill       9930 non-null   float64\n 9   termination  9930 non-null   object \ndtypes: float64(3), int64(1), object(6)\nmemory usage: 775.9+ KB\n\n\n\n\n\n- 데이터를 불러온 후에는 반드시 데이터 타입을 확인하자.\n- 숫자형 데이터가 문자형으로 지정되어 있거나 혹은 그반의 경우 원하는 데이터 처리 결과가 도출 되지 않을 수 있음.\n- 숫자형 데이터가 문자형으로 저장된 경우의 사칙연산\n\ncust['age'][3]+cust['age'][4]\n\n'3218'\n\n\n- age 항목을 Intger type으로 변경\n\n결측치가 문자 ’_’로 되어있어 에러발생\n\n\ncust=cust.astype({'age': int})\n\nValueError: invalid literal for int() with base 10: '_'\n\n\n- “_” 항목을 NaN 값으로 치환\n\nimport numpy as np\ncust = cust.replace(\"_\", np.NaN)\n\n\ncust.info()\n\n\n#age 항목의 type 변경\n#NaN의 경우 int type을 지원하지 않아 float type으로 변경\ncust=cust.astype({'age': float })\n\n\ncust.info()\n\n\ncust.head()\n\n\n\n\n\n- 파이썬에서 Copy 메소드를 사용하지 않으면 주소값을 복사해서 사용기 때문에 원본 값을 변경 시킴\n- 따라서 원본 항목을 보전하면서 데이터를 보정하려면 copy 메소드를 사용하자.\n\n# Cust Data 복사\ncust_fix=cust.copy()\ncust_fix.head()\n\n\ntest=cust\ntest.head()\n\n\ntest['age']=0\ntest.head()\n\n\n# test 데이터 변경 후 Cust 데이터 확인\ncust.head()\n\n\n#Copy 메소드로 변경한 Cust_fix 데이터 확인\ncust_fix.head()\n\n- “test” 데이터프레임을 생성 후 값 변경 결과 원본데이터에 반영됨.\n- 그러나 copy method를 사용한 데이터프레임의 경우 원본 데이터의 변경이 일어나지 않음.\n\ncust=cust_fix.copy()\n\n\n\n- “fillna” 함수를 사용해서 특정 숫자나 문자로 결측치를 처리하는 방법\n\ncust_fix.info()\n\n\ncust=cust.fillna(15)\ncust.head()\n\n\ncust.info()\n\n- fillna의 Method 파라미터를 사용하면, 사용자가 지정하는 일괄적인 값이 아닌 주변값을 활용하여 결측치를 채울수 있다.\n- 뒤에 있는 data를 사용해서 결측치를 처리하는 방법\n\ncust=cust_fix.copy()\ncust=cust.fillna(method='backfill')\ncust.head()\n\n- 앞에 있는 data를 사용해서 결측치를 처리하는 방법\n\ncust=cust_fix.copy()\ncust=cust.fillna(method='ffill')\ncust.head()\n\n\\(\\divideontimes\\) Method 파리미터사용시 첫 Record 또는 마지막 Record가 결측치 인지 확인해야 합니다.\n- Replace 함수를 사용해도 결측치 채우기가 가능 (중앙값 이용)\n\ncust=cust_fix.copy()\ncust['age']=cust['age'].replace(np.nan, cust['age'].median())\ncust.head()\n\n- interpolate() 함수로 결측치 채우기 \\(\\to\\) 선형방식은 값들을 같은 간격으로 처리\n\ncust=cust_fix.copy()\ncust=cust.interpolate()\ncust.head()\n\n\n\n\n- 결측치를 채우기로 처리하다보면 경우에 따라 값의 왜곡이 크게 발생하는 발생 하는 경우가 있습니다.\n- 이때는 제거하기 기법을 사용하여 결측치를 처리하면 데이터의 정합성을 보존 할 수 있습니다.\n\nlistwise 방식 : record의 항목 중 1개의 값이라도 NA이면 해당 데이터 행 전체 제거\npairwise 방식 : 모든 항목이 NA인 데이터 행만 제거\n\n\n#listwise 방식으로 제거 하기\ncust=cust_fix.copy()\ncust=cust.dropna()\ncust.info()\n\n\n#pairwise 방식으로 제거하기\ncust=cust_fix.copy()\ncust=cust.dropna(how='all')\ncust.info()\n\n- pairwise 방식 확인을 위한 데이터 임의 변경\n\ncust=cust_fix.copy()\ncust[['npay','stop','termination']]=cust[['npay','stop','termination']].replace('N', np.nan)\n\ncust[['service','avg_bill','A_bill','B_bill']]=cust[['service','avg_bill','A_bill','B_bill']].replace(0, np.nan)\n\ncust=cust.dropna(how='all')\n\ncust[['npay','stop','termination']]=cust[['npay','stop','termination']].replace(np.nan,'N')\n\ncust[['service','avg_bill','A_bill','B_bill']]=cust[['service','avg_bill','A_bill','B_bill']].replace(np.nan, 0)\n\n\ncust.info()\n\n\ncust=cust_fix.copy()\ncust[(cust['avg_bill']==0)]\n\n- NA의 갯수에 따라 결측데이터를 판단하고자 한다면, thresh 파라미터를 사용\n- NA가 아닌값이 n개 이상인(한 개체에서) 경우만 남겨라는 뜻으로 해석하면 됩니다.\n- 임계치를 설정해서 제거하기\n\ncust=cust_fix.copy()\ncust=cust.dropna(thresh=10)\ncust.info()\n\n특정 열에 있는 NA만 참고하여 결측치를 제거하려면 Subset 파라미터를 사용\n특정열 안에서만 삭제하기\n\ncust=cust_fix.copy()\ncust=cust.dropna(subset=['class'])\ncust.info()\n\n\n\n\n\n- age를 제외한 모든 연속형 변수의 결측치를 0으로 바꾼 후, 결측치가 1개라도 있는 데이터는 제거\n\ncust=cust_fix.copy()\n\n\ncust.info()\n\n\ncust.service.unique()\n\n\ncust[['service','avg_bill','A_bill','B_bill']]=cust[['service','avg_bill','A_bill','B_bill']].fillna(0)\ncust.head()\n\n\ncust=cust.dropna()\n\n\n\n\n\n\n- 범주형 데이터의 경우 value_counts 메소드를 사용하면, 값의 분포와 함께 Trash 값이 있는지도 확인 가능합니다.\n- 범주형 데이터의 값 분포 확인\n\nprint(cust['sex'].value_counts())\n\n\nprint(cust['class'].value_counts())\n\n\nprint(cust['npay'].value_counts())\n\n- Class의 값에는 ’C, D, E, F, G, H’가 들어있는것을 확인\n\n실제로 그렇지는 않지만 ’H’가 실제로는 존재하지 않는 값이라고 가정 \\(\\to\\) 이상치\n\n- 이상치 제거하기\n\ncust_data=cust[(cust['class']!='H')]\n\nprint(cust_data['class'].value_counts())\n\n- 이상치 변경하기\n\ncust_data=cust.copy()\n\ncust_data['class']=cust_data['class'].replace('H','F')\n\nprint(cust_data['class'].value_counts())\n\n\n\n\n\ncust.describe()\n\n- Q1와 Q3가 거리의 1.5배가 넘어가는 값을 Outlier 라고 판단\n- 이 값들은 이상치로써 일반적으로 제거 또는 변경하여 데이터를 분석,학습\n(단, 이상치 분석시에는 제거하지 않습니다.)\n\\[\\text {IQR} = 1.5 \\times (\\text {Q}_3-\\text {Q}_1)\\]\n\\[ \\text {Outlier} < \\text{Q}_1 - \\text{IQR} \\quad \\&  \\quad \\text {Outlier} > \\text{Q}_3 + \\text{IQR}\\]\n\n#이상치를 제거하는 함수 만들기\ndef removeOutliers(x, column):\n    # Q1, Q3구하기\n    q1 = x[column].quantile(0.25)\n    q3 = x[column].quantile(0.75)\n    \n    # 1.5 * IQR(Q3 - Q1)\n    iqr = 1.5 * (q3 - q1)\n    \n    # 이상치를 제거\n    y=x[(x[column] < (q3 + iqr)) & (x[column] > (q1 - iqr))]\n    \n    return(y)\n\n\n#연속형 데이터의 이상치 제거하기\ncust_data=removeOutliers(cust, 'avg_bill')\ncust_data.describe()\n\n\ncust_data.info()\n\n- 동일한 함수로 나머지 연속형 변수에 대해서도 이상치를 처리\n\ncust_data=removeOutliers(cust_data, 'A_bill')\ncust_data=removeOutliers(cust_data, 'B_bill')\ncust_data.describe()\n\n\ncust_data.info()\n\n- 이상치를 변경하는 함수 만들기\n\nq1-IQR 값보다 작은값들을 q1-iqr 값으로 대체\nq3+IQR 값보다 큰값들을 q1+iqr 값으로 대체\n\n\ndef changeOutliers(data, column):\n    x=data.copy()\n    # Q1, Q3구하기\n    q1 = x[column].quantile(0.25)\n    q3 = x[column].quantile(0.75)\n    \n    # 1.5 * IQR(Q3 - Q1)\n    iqr = 1.5 * (q3 - q1)\n    \n    #이상치 대체값 설정하기\n    Min = 0\n    if (q1 - iqr) > 0 : Min=(q1 - iqr)\n        \n    Max = q3 + iqr\n    \n    # 이상치를 변경\n    # X의 값을 직졉 변경해도 되지만 Pyhon Warning이 나오기 떄문에 인덱스를 이용\n    x.loc[(x[column] > Max), column]= Max\n    x.loc[(x[column] < Min), column]= Min\n    \n    return(x)\n\n\n#연속형 데이터의 이상치 변경하기\ncust_data=changeOutliers(cust, 'avg_bill')\ncust_data.describe()\n\n\ncust.describe()\n\n\n# 동일한 함수로 나머지 연속형 변수에 대해서도 이상치를 처리\ncust_data=changeOutliers(cust_data, 'A_bill')\ncust_data=changeOutliers(cust_data, 'B_bill')\ncust_data.describe()\n\n\ncust_data.info()\n\n\n\n\n\n- 초기 데이터로부터 특징을 가공하고 생산하여 입력 데이터를 생성하는 과정\n- Feature Engineering을 통해 AI가 학습하기 좋은 형태로 데이터를 만들거나, 새로운 입력 데이터를 생성 할 수 있음.\n(이때 데이터를 수정하지 않도록 주의 합니다.)\n\ncust_data.head()\n\n\ncust_data['by_age']=cust_data['age']//10*10\n\n\n\n- 단어 뜻 그대로 자료를 일정한 규격의 통에 넣는것 입니다.-\n- 만약 관측치가 연속형이면서 범위가 너무 다양할 경우, 적절히 그룹을 지어주면 데이터를 이해하기가 더 쉬워질수 있기에 사용\n- 즉, 연속형 변수를 범주형 변수로 만드는 방법이라고 보시면 됩니다.\n- age를 활용하여 나이대(“by_age”) Feature 만들기\n\ncust_data.head()\n\n\ncust_data.info()\n\n\n#NaN값이 더이상 존재하지 않기 때문에 age와 by_age는 int로 형변환\ncust_data=cust_data.astype({'age': int, 'by_age':int})\n\n\n\n- Cut 함수를 사용하면 구간을 지정해서 쉽게 범주화 할 수 있다.\n- Bins 구간 사이의 값을 범주화 하여 Label에 지정된 카테고리명을 사용한다.\n\nq1=cust_data['avg_bill'].quantile(0.25)\nq3=cust_data['avg_bill'].quantile(0.75)\nprint(q1,q3)\n\n- cut 메소드를 활용하여 요금을 3개 구간으로 나누기\n\n?pd.cut # default : right = True\n\n\ncust_data['bill_rating'] = pd.cut(cust_data[\"avg_bill\"], \n                                  bins=[0,q1,q3,cust_data[\"avg_bill\"].max()] , \n                                  labels=['low', 'mid','high'])\ncust_data.head()\n\n\nprint(cust_data['bill_rating'].value_counts())\n\n\n\n\n- cut과 비슷하지만 같은 크기로 구간을 나누어 범주형 변수로 만든다\n- qcut 메소드를 활용하여 요금을 동일 비율로 3개 구간으로 나누기\n\ncust_data['bill_rating'] = pd.qcut(cust_data[\"avg_bill\"], \n                                       3 , \n                                       labels=['low', 'mid','high'])\n\ncust_data.head()\n\n\nprint(cust_data['bill_rating'].value_counts())\n\n\n\n\n\n- 각 컬럼에 들어있는 데이터의 상대적 크기에 따라 분석 결과나 모델링 결과가 달라질수 있다.\n- 0 ~ 1000까지의 값을 가지는 변수 A와 0 ~ 10까지의 값을 가지는 변수 B를 가지고 분석을 수행\n- 이 경우, 상대적으로 큰 숫자를 가지는 A변수의 영향이 더 크게 반영된다.\n- 따라서, 숫자데이터는 상대적 크기 차이를 제거할 필요가 있고, 이를 scaling이라고 합니다.\n\n\n- 정규 분포를 평균이 0 이고 분산이 1 인 표준 정규 분포로 변환합니다.\n\ncust_data_num = cust_data[['avg_bill', 'A_bill', 'B_bill']]\n#표준화\nStandardization_df = (cust_data_num - cust_data_num.mean())/cust_data_num.std()\nStandardization_df.head()\n\n\nStandardization_df.describe()\n\n\n\n\n값들을 모두 0과 1사이의 값으로 변환\n\\[\\text {normalization} = \\frac {X-min_{x}}{max_{x}-min_{x}}\\]\n- 사이킷런 패키지의 MinMaxScaler를 이용하여 Scaling\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\n\nnormalization_df=cust_data_num.copy()\n\nnormalization_df[:]=scaler.fit_transform(normalization_df[:])\n\nnormalization_df.head()\n\n\nnormalization_df.describe()\n\n- MinMaxScaler 함수 구현하기\n\nnormalization_df = (cust_data_num - cust_data_num.min())/(cust_data_num.max()-cust_data_num.min())\nnormalization_df.head()\n\n\n\n\n\n- 원-핫 인코딩은 간단히 말해 한 개의 요소는 True, 나머지 요소는 False로 만들어 주는 기법\n- 기계 학습의 주요 문제 중 하나는 많은 알고리즘이 범주형 데이터를 입력값으로 수용하지 않는다는 점이다.\n- 이를 One-Hot Encording을 통해 해결 할 수 있다.\n- 정수 치환으로 해결 할 수도 있지만 이럴 경우 성능이 저하되거나 예상치 못한 결과가 발생할 수 있다.\n- pandas에서는 get_dummies함수를 사용하면 쉽게 One-Hot Encording이 가능\n\npd.get_dummies(cust_data['class'])\n\n- columns를 사용해서 기존 테이블에 One-Hot Encording으로 치환된 변수 생성하기\n\ncust_data_end=pd.get_dummies(cust_data, columns=['class'])\ncust_data_end\n\n\n\n\n\n\ncust 데이터의 이상치는 제거하고, 나이는 5단위로 범주화, 평균 요금은 5개 구간으로 나누어 새로운 변수를 만들어 데이터를 저장\n\n이상치 : InterQuartile Range의 1.5배가 넘는 수 (처리 대상 Feature : avg_bill, A_bill, B_bill)\nby_age : 나이는 0 ~ 4세는 0/ 5 ~ 9세는 5 / 10 ~ 14세는 10으로 5단위 범주화\nbill_rating : 전체 평균 요금을 균등비율로 low / lowmid / mid / midhigh / high 5단계로 구분\n\n\n#이상치를 제거하는 함수 만들기\ndef removeOutliers(x, column):\n    # Q1, Q3구하기\n    q1 = x[column].quantile(0.25)\n    q3 = x[column].quantile(0.75)\n    \n    # 1.5 * IQR(Q3 - Q1)\n    iqr = 1.5 * (q3 - q1)\n    \n    # 이상치를 제거\n    y=x[(x[column] < (q3 + iqr)) & (x[column] > (q1 - iqr))]\n    \n    return(y)\n\n\ncust_data=removeOutliers(cust, 'avg_bill')\ncust_data=removeOutliers(cust_data, 'A_bill')\ncust_data=removeOutliers(cust_data, 'B_bill')\n\n\ncust_data['by_age']=cust_data['age']//5*5\ncust_data=cust_data.astype({'age': int, 'by_age':int})\n\n\ncust_data['bill_rating'] = pd.qcut(cust_data[\"avg_bill\"], 5 , labels=['low','lowmid', 'mid','midhigh','high'])\ncust_data.head()\n\n\ncust_data.to_csv('cust_data.csv', index=False)"
  },
  {
    "objectID": "post/etc/ai study/4. 데이터 시각화.html",
    "href": "post/etc/ai study/4. 데이터 시각화.html",
    "title": "05. data visualization",
    "section": "",
    "text": "Matplotlib는 파이썬에서 데이터를 차트나 플롯(Plot)으로 그려주는 가장 많이 사용되는 데이터 시각화(Data Visualization) 패키지로 라인 plot, 바 차트, 파이차트, 히스토그램, Box Plot, Scatter Plot 등을 비롯하여 다양한 차트와 plot 스타일을 지원합니다.\n웹페이지(https://matplotlib.org/index.html) 에서 다양한 샘플 차트를 볼 수 있습니다.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n\n\n\nMatplotlib를 사용하여 시각화를 하기 위해서는 아래 3가지 코드를 기억해야 합니다.\n\nplt.figure() - 시각화 그림을 표시할 영역 지정\nplt.plot() - 시각화 차트 및 값 지정\nplt.show() - 시각화 명령어\n\n\n#Matplotlib를 사용하여 간단한 차트를 그리기\nplt.figure()\nplt.plot([1,2,3], [100,120,110])\nplt.show()\n\n\n\n\n차트 크기 키우기\n차트의 크기는 그림을 표시할 영역의 크기를 키움으로 해결 할 수 있습니다.\nfigsize 옵션을 사용해서 조정 합니다. ( 기본 값은 (6,4) )\n\nplt.figure(figsize=(16,5))\nplt.plot([1,2,3], [100,120,110])\nplt.show()\n\n\n\n\n ## 2. Matplotlib 목적별 차트 그리기 ### 2-1.  선그래프(line plot) 그리기 선 그래프는 수치의 변화를 선으로 이어서 그려 줍니다.\n이 때문에 보통 선그래프는 시간에 따른 데이터의 변화 추세를 볼 때 사용합니다.\n\n#주간 일별 최고기온 리포트를 그리기\nplt.figure()\nplt.plot([\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"], [28,30,29,31,32,31,31] )\nplt.show()\n\n\n\n\nPandas에서 선그래프 그리기\n\n#cust_data.csv 파일을 DataFrame으로 불러오기\nimport pandas as pd\ndf=pd.read_csv(\"cust_data.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      class\n      sex\n      age\n      service\n      stop\n      npay\n      avg_bill\n      A_bill\n      B_bill\n      termination\n      by_age\n      bill_rating\n    \n  \n  \n    \n      0\n      C\n      F\n      28\n      0\n      N\n      N\n      2640.0000\n      792.00000\n      1584.0000\n      Y\n      25\n      low\n    \n    \n      1\n      E\n      F\n      24\n      1\n      N\n      N\n      16840.0000\n      2526.00000\n      6983.0000\n      N\n      20\n      high\n    \n    \n      2\n      F\n      F\n      32\n      1\n      N\n      N\n      15544.7334\n      2331.71001\n      6750.4666\n      N\n      30\n      high\n    \n    \n      3\n      D\n      M\n      18\n      1\n      N\n      N\n      4700.0000\n      0.00000\n      4502.0000\n      N\n      15\n      lowmid\n    \n    \n      4\n      C\n      F\n      78\n      1\n      N\n      N\n      1361.7999\n      1173.99990\n      0.0000\n      N\n      75\n      low\n    \n  \n\n\n\n\n\n#불러온 데이터를 사용하여 선그래프를 그리기\ndf.plot()\nplt.show()\n\n\n\n\n하나의 열에 대해서만 시각화 하기\n\n#avg_bill만으로 그래프 그리기\ndf['avg_bill'].plot(figsize=(12,8))\nplt.show()\n\n\n\n\n\n\n\n산점도는 키와 몸무게 같은 두 값 간의 관계를 표현 합니다.\n두 값이 양의 상관관계인지 음의 상관관계인지를 파악할 수 있습니다.\n산점도 위에 회귀선을 하나 그리면 하나의 값에 따라 다른 값이 어떻게 변하는지를 예측 할 수도 있습니다.\nplt.scatter()를 사용하고 X와 Y값을 지정해야 합니다.\n\n#avg_bill, age간의 관꼐를 알아보기 위해 산점도 그리기\nplt.figure(figsize=(12,6))\nplt.scatter(y=df[\"avg_bill\"], x=df[\"age\"])\nplt.show()\n\n\n\n\n\n\n\n히스토그램은 수치형 데이터의 분포를 나타 냅니다.\n흔히 빈도, 빈도밀도, 확율 등의 분포를 그릴때 사용합니다.\nplt.hist()를 사용하며, 구간값(bins)을 정해주지 않으면 10개 구간으로 나누어 그래프를 그려 줍니다.\n\n#A_bill에 대한 빈도를 10개 구간으로 그리기\nplt.figure()\nplt.hist(df[\"A_bill\"])\nplt.show()\n\n\n\n\n\n#age에 대한 빈도를 20개 구간으로 그리기\nplt.figure()\nplt.hist(df[\"age\"],bins=20)\nplt.show()\n\n\n\n\n\n\n\n수치적 자료를 표현하는 그래프입니다.\n수치적 자료로부터 얻어낸 통계량인 5가지 요약 수치(최소값, 제 1사분위값, 제 2사분위값, 제 3사분위값, 최대값)를 가지고 그리프를 만듭니다.\nplt.boxplot() 명령어를 사용합니다\n\n#임의의 값 리스트를 생성하고, 생성한 리스트를 사용하여 박스 그래프 그리기\nx=[5, 3, 7, 10, 9, 5, 3.5, 8]\n\nplt.boxplot(x=x)\nplt.show()\n\n\n\n\nPandas에서 가로 box plot 그리기\nby는 Group화 할 값(컬럼), column은 박스 그래프로 나타낼 값(컬럼)을 넣어주면 됩니다.\n\n#나이대별 총이용금액 분포를 박스 그래프로 그리기\ndf.boxplot(by=\"by_age\", column=\"avg_bill\", figsize=(16,8))\nplt.show()\n\n\n\n\n\n\n\n막대 그래프는 범주형 데이터의 수치를 요약해서 보여주니다.\n일반적으로 가로,세로,누적,그룹화 된 막대 그래프 등을 사용하며, plt.plot()대신 plt.bar() 명령어를 사용 할수있습니다.\n\n#임의의 값 리스트를 생성하고, 생성한 리스트를 사용하여 막대 그래프 그리기\ny=[5, 3, 7, 10, 9, 5, 3.5, 8]\nx=list(range(len(y)))\n\nplt.figure()\nplt.bar(x, y)\nplt.show()\n\n\n\n\npandas내 pivot_table 함수를 사용하면 쉽게 수치들에 대한 mean/sum/max등을 구할 수 있습니다.\n\n#service수 별 평균 요금과 나이에 대한 테이블을 만들기\ndf2=pd.pivot_table(df, index = ['service'])\nprint(df2)\n\n              A_bill        B_bill        age      avg_bill     by_age\nservice                                                               \n0        1878.392843   3860.988695  46.699844   6982.694778  44.618974\n1        1373.270092   5240.153191  47.019993   8885.312921  45.025216\n2        1619.646033   6445.699414  52.051696  11396.838680  50.020194\n3        1883.827355   7438.448824  53.407942  13573.878409  51.552347\n4        2137.489050   8525.301310  51.851351  15817.244532  49.864865\n5        2483.690873   9259.547658  52.947368  16468.355943  50.789474\n6        2759.059416   7154.133900  54.000000  15867.400000  51.666667\n7        4507.066600   9010.133400  47.000000  15548.166700  45.000000\n8        1699.500000   8058.963100  40.000000  11544.646115  40.000000\n9        1878.583350  12348.789850  55.500000  20850.559980  55.000000\n\n\nPandas에서 막대그래프 그리기\n\n#service수 별 A,B 서비스 요금에 대한 막대 그래프 그리기\ndf2[['A_bill', 'B_bill']].plot(kind='bar')\n\n<AxesSubplot:xlabel='service'>\n\n\n\n\n\n누적 막대그래프 만들기\n누적 막대 그래프는 막대 그래프 함수를 사용하고 stacked 옵션을 True로 변경하면 됩니\n\n#service수 별 A,B 서비스 요금에 대한 요금 누적 그래프 그리기\ndf2[['A_bill', 'B_bill']].plot(kind='bar', stacked=True)\n\n<AxesSubplot:xlabel='service'>\n\n\n\n\n\n ## 3. 차트 꾸미기 ### 3-1. 제목과 축 레이블 추가하기 Plot에 X,Y 축 레이블이나 제목을 붙이기 위해서는 plt.xlabel(축이름), plt.ylabel(축이름), plt.title(제목) 등의 함수를 사용합니다.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n\n#주간 최고기온 그래프 그리기\nplt.figure()\nplt.plot([\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"], [28,30,29,31,32,31,31] )\nplt.show()\n\n\n\n\n\n#주간 최고기온 그래프에 표제목과 X축, Y축 이름 넣기\nplt.figure()\nplt.plot([\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"], [28,30,29,31,32,31,31] )\nplt.xlabel('Day')\nplt.ylabel('Temp')\nplt.title('High Temperature')\nplt.show()\n\n\n\n\n\n\n\n차트에서 한글을 사용하기 위해서는 아래와 같이 한글을 지원하는 폰트를 설정해 주어야 합니다.\n\nimport matplotlib.font_manager as fm\nfm.findSystemFonts(fontpaths=None, fontext='ttf')\n\n['C:\\\\windows\\\\Fonts\\\\BELL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\SCDream3.otf',\n 'C:\\\\windows\\\\Fonts\\\\FRADMIT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\SAPGUI-icons.ttf',\n 'C:\\\\windows\\\\Fonts\\\\LG_Smart_UI-SemiBold.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\COOPBL.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NotoSansKR-Medium.otf',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquareB.otf',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquare_0.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\SCHLBKBI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\corbelb.ttf',\n 'C:\\\\windows\\\\Fonts\\\\Pretendard-SemiBold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\ebrimabd.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\DIGICOTTF-Bold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\Pretendard-Black.ttf',\n 'C:\\\\windows\\\\Fonts\\\\MaruBuri-Regular.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NanumBarunpenB.otf',\n 'C:\\\\windows\\\\Fonts\\\\georgiaz.ttf',\n 'C:\\\\windows\\\\Fonts\\\\msyhl.ttc',\n 'C:\\\\windows\\\\Fonts\\\\trebucbi.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\Cafe24Shiningstar.ttf',\n 'C:\\\\windows\\\\Fonts\\\\georgia.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\Gong Gothic Medium.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\msyh.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\NotoSansCJKkr-Bold.otf',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquareOTF.otf',\n 'C:\\\\Windows\\\\Fonts\\\\corbell.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\GOUDOSB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\cambriab.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\comicz.ttf',\n 'C:\\\\windows\\\\Fonts\\\\H2PORM.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\CALISTI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LCALLIG.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\JUICE___.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BOOKOSI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\DIGICO-Bold.otf',\n 'C:\\\\windows\\\\Fonts\\\\LEELAWAD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BOD_BI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LeelawUI.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NotoSansKR-Medium.otf',\n 'C:\\\\Windows\\\\Fonts\\\\H2PORM.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Candarali.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\ONYX.TTF',\n 'C:\\\\windows\\\\Fonts\\\\CASTELAR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\Gong Gothic Medium.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\TCCM____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\seguihis.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\georgiaz.ttf',\n 'C:\\\\windows\\\\Fonts\\\\msjhbd.ttc',\n 'C:\\\\windows\\\\Fonts\\\\mmrtextb.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\ARIALUNI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\impact.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\trebucbd.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\trebucit.ttf',\n 'C:\\\\windows\\\\Fonts\\\\H2SA1M.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Sitka.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\calibriz.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\georgiab.ttf',\n 'C:\\\\windows\\\\Fonts\\\\H2GTRM.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\mvboli.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\BRADHITC.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\SCHLBKB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BOD_PSTC.TTF',\n 'C:\\\\windows\\\\Fonts\\\\PER_____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMBARUNGOTHICULTRALIGHT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BOD_BLAR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BRLNSDB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Pretendard-ExtraBold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\msyi.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\Cafe24Ohsquareair.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NANUMBARUNGOTHIC.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ARIALN.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMBARUNGOTHICLIGHT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Candaral.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NotoSansCJKkr-DemiLight.otf',\n 'C:\\\\windows\\\\Fonts\\\\verdanai.ttf',\n 'C:\\\\windows\\\\Fonts\\\\GmarketSansBold.otf',\n 'C:\\\\windows\\\\Fonts\\\\JOKERMAN.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumBarunpenR.otf',\n 'C:\\\\windows\\\\Fonts\\\\NANUMGOTHIC.TTF',\n 'C:\\\\windows\\\\Fonts\\\\seguiemj.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\VLADIMIR.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Candara.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\Cafe24Dongdong.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\COPRGTB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\LBRITED.TTF',\n 'C:\\\\windows\\\\Fonts\\\\malgun.ttf',\n 'C:\\\\windows\\\\Fonts\\\\seguisbi.ttf',\n 'C:\\\\windows\\\\Fonts\\\\VINERITC.TTF',\n 'C:\\\\windows\\\\Fonts\\\\DIGICOTTF-Bold.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\Cafe24Dangdanghae.ttf',\n 'C:\\\\windows\\\\Fonts\\\\MATURASC.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\FRABK.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquare.ttf',\n 'C:\\\\windows\\\\Fonts\\\\PERI____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\GILLUBCD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\SB 어그로 L.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\GOUDYSTO.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\corbelz.ttf',\n 'C:\\\\windows\\\\Fonts\\\\TCB_____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\msgothic.ttc',\n 'C:\\\\windows\\\\Fonts\\\\georgiab.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\calibril.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\ebrimabd.ttf',\n 'C:\\\\windows\\\\Fonts\\\\LFAXD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquareEB.ttf',\n 'C:\\\\windows\\\\Fonts\\\\holomdl2.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\TCCB____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\arial.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\SCDream4.otf',\n 'C:\\\\Windows\\\\Fonts\\\\Pretendard-Thin.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\MTEXTRA.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquareOTF_0.otf',\n 'C:\\\\Windows\\\\Fonts\\\\verdana.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\LeelaUIb.ttf',\n 'C:\\\\windows\\\\Fonts\\\\MaruBuri-Bold.otf',\n 'C:\\\\windows\\\\Fonts\\\\NotoSansMonoCJKkr-Bold.otf',\n 'C:\\\\Windows\\\\Fonts\\\\MaruBuri-Bold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\arial.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\FREESCPT.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BOD_CBI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\KTfontLight.ttf',\n 'C:\\\\windows\\\\Fonts\\\\GOUDOSB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LTYPEB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\phagspab.ttf',\n 'C:\\\\windows\\\\Fonts\\\\MOD20.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquareEB.ttf',\n 'C:\\\\windows\\\\Fonts\\\\ariblk.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\OUTLOOK.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquareOTF_0.otf',\n 'C:\\\\Windows\\\\Fonts\\\\MaruBuri-Regular.otf',\n 'C:\\\\Windows\\\\Fonts\\\\himalaya.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\holomdl2.ttf',\n 'C:\\\\windows\\\\Fonts\\\\CALISTBI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ERASLGHT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\THEFACESHOP INKLIPQUID.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\mmrtext.ttf',\n 'C:\\\\windows\\\\Fonts\\\\courbi.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NirmalaS.ttf',\n 'C:\\\\windows\\\\Fonts\\\\seguisb.ttf',\n 'C:\\\\windows\\\\Fonts\\\\LSANSDI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\YuGothR.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\HMFMOLD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NotoSansCJKkr-Regular.otf',\n 'C:\\\\windows\\\\Fonts\\\\MaruBuri-Light.otf',\n 'C:\\\\windows\\\\Fonts\\\\NotoSansCJKkr-Black.otf',\n 'C:\\\\windows\\\\Fonts\\\\NANUMSQUAREROUNDB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ITCKRIST.TTF',\n 'C:\\\\windows\\\\Fonts\\\\palabi.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NANUMBARUNGOTHICBOLD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\consola.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\gulim.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMMYEONGJO-YETHANGUL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BOD_I.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMGOTHICBOLD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ROCKB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\cambriai.ttf',\n 'C:\\\\windows\\\\Fonts\\\\FRADMCN.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\OLDENGL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NotoSansCJKkr-Light.otf',\n 'C:\\\\Windows\\\\Fonts\\\\PARCHM.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NotoSansCJKkr-Regular.otf',\n 'C:\\\\Windows\\\\Fonts\\\\H2MJSM.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\verdanaz.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\comic.ttf',\n 'C:\\\\windows\\\\Fonts\\\\GOUDYSTO.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\HARNGTON.TTF',\n 'C:\\\\windows\\\\Fonts\\\\Cafe24Ssukssuk.ttf',\n 'C:\\\\windows\\\\Fonts\\\\sapdn.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\DIGICOTTF-Medium_0.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NotoSansCJKkr-Light.otf',\n 'C:\\\\windows\\\\Fonts\\\\SCDream9.otf',\n 'C:\\\\Windows\\\\Fonts\\\\cambriaz.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\GmarketSansTTFLight.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\HANBatang.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\H2MKPB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ELEPHNTI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\MISTRAL.TTF',\n 'C:\\\\windows\\\\Fonts\\\\msgothic.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\segoeuiz.ttf',\n 'C:\\\\windows\\\\Fonts\\\\SitkaZ.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\HANDotum.ttf',\n 'C:\\\\windows\\\\Fonts\\\\consolaz.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\gadugib.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NotoSansKR-Light.otf',\n 'C:\\\\windows\\\\Fonts\\\\trebuc.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\Cafe24Simplehae.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\seguibl.ttf',\n 'C:\\\\windows\\\\Fonts\\\\ALGER.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\TEMPSITC.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NANUMMYEONGJOECOR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\SCDream7.otf',\n 'C:\\\\Windows\\\\Fonts\\\\calibrii.ttf',\n 'C:\\\\windows\\\\Fonts\\\\BOOKOSI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\FRAHV.TTF',\n 'C:\\\\windows\\\\Fonts\\\\Cafe24SsurroundAir.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\BSSYM7.TTF',\n 'C:\\\\windows\\\\Fonts\\\\msjh.ttc',\n 'C:\\\\windows\\\\Fonts\\\\BOD_PSTC.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMMYEONGJOECOR.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\tahomabd.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\segoepr.ttf',\n 'C:\\\\windows\\\\Fonts\\\\AGENCYR.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\HMKMAMI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\MaruBuri-Light.otf',\n 'C:\\\\windows\\\\Fonts\\\\DIGICOTTF-Light.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\Pretendard-Light.ttf',\n 'C:\\\\windows\\\\Fonts\\\\RAGE.TTF',\n 'C:\\\\windows\\\\Fonts\\\\HARLOWSI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Pretendard-SemiBold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\javatext.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NanumPen.otf',\n 'C:\\\\windows\\\\Fonts\\\\COLONNA.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\GmarketSansTTFMedium.ttf',\n 'C:\\\\windows\\\\Fonts\\\\mmrtext.ttf',\n 'C:\\\\windows\\\\Fonts\\\\LATINWD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\GILBI___.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\simsunb.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\corbelli.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\GOUDOSI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\WINGDNG3.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NIAGENG.TTF',\n 'C:\\\\windows\\\\Fonts\\\\MTEXTRA.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\H2GTRM.TTF',\n 'C:\\\\windows\\\\Fonts\\\\segoeuii.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NANUMGOTHICCODING.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\arimonbd.ttf',\n 'C:\\\\windows\\\\Fonts\\\\TCM_____.TTF',\n 'C:\\\\windows\\\\Fonts\\\\TEMPSITC.TTF',\n 'C:\\\\windows\\\\Fonts\\\\verdanab.ttf',\n 'C:\\\\windows\\\\Fonts\\\\TCBI____.TTF',\n 'C:\\\\windows\\\\Fonts\\\\GOTHICI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\GILI____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMMYEONGJO.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ROCKEB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\segoeuisl.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\H2GTRE.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\GOTHIC.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\HMKMMAG.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\YuGothB.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\ARIALNBI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\corbelb.ttf',\n 'C:\\\\windows\\\\Fonts\\\\malgunsl.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\CENSCBK.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMBARUNGOTHICBOLD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\msjh.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\JUMJA.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ERASMD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMGOTHICCODINGBOLD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\pala.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\FRADMCN.TTF',\n 'C:\\\\windows\\\\Fonts\\\\Gong Gothic Light.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquareOTF_acL.otf',\n 'C:\\\\Windows\\\\Fonts\\\\Candarai.ttf',\n 'C:\\\\windows\\\\Fonts\\\\H2HDRM.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Cafe24Oneprettynight.ttf',\n 'C:\\\\windows\\\\Fonts\\\\CALIFI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\SCHLBKB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\GILI____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\palabi.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\javatext.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\BELL.TTF',\n 'C:\\\\windows\\\\Fonts\\\\CURLZ___.TTF',\n 'C:\\\\windows\\\\Fonts\\\\HMKMAMI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\seguihis.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquareR.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\FORTE.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquare_acL.ttf',\n 'C:\\\\windows\\\\Fonts\\\\BRLNSR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\HMFMPYUN.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\MAIAN.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\TCBI____.TTF',\n 'C:\\\\windows\\\\Fonts\\\\taileb.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\malgunsl.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\Gabriola.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\SCRIPTBL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LSANSD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NotoSansCJKkr-Medium.otf',\n 'C:\\\\Windows\\\\Fonts\\\\constanz.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\CHILLER.TTF',\n 'C:\\\\windows\\\\Fonts\\\\GLECB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquareEB.otf',\n 'C:\\\\Windows\\\\Fonts\\\\NEXON LV2 GOTHIC MEDIUM.TTF',\n 'C:\\\\windows\\\\Fonts\\\\comic.ttf',\n 'C:\\\\windows\\\\Fonts\\\\GIL_____.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BOOKOSBI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMBARUNGOTHIC.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NirmalaS.ttf',\n 'C:\\\\windows\\\\Fonts\\\\LG_Smart_UI-Bold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NotoSansKR-Bold.otf',\n 'C:\\\\windows\\\\Fonts\\\\timesbd.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\consola.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\SCHLBKI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NANUMSQUAREROUNDEB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\PERB____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\CALISTBI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMMYEONGJOBOLD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\SitkaZ.ttc',\n 'C:\\\\windows\\\\Fonts\\\\lucon.ttf',\n 'C:\\\\windows\\\\Fonts\\\\arimon__.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NanumBrush.otf',\n 'C:\\\\Windows\\\\Fonts\\\\LSANSDI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\batang.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\ROCK.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquareOTF.otf',\n 'C:\\\\windows\\\\Fonts\\\\PALSCRI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\corbelli.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\JOKERMAN.TTF',\n 'C:\\\\windows\\\\Fonts\\\\bahnschrift.ttf',\n 'C:\\\\windows\\\\Fonts\\\\H2GPRM.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquareOTF_acL.otf',\n 'C:\\\\Windows\\\\Fonts\\\\DIGICO-Medium.otf',\n 'C:\\\\windows\\\\Fonts\\\\SAPGUI-icons.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\SHOWG.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\SCDream6.otf',\n 'C:\\\\Windows\\\\Fonts\\\\H2SA1M.TTF',\n 'C:\\\\windows\\\\Fonts\\\\times.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\ELEPHNTI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BKANT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NotoSansKR-Black.otf',\n 'C:\\\\Windows\\\\Fonts\\\\FTLTLT.TTF',\n 'C:\\\\windows\\\\Fonts\\\\sylfaen.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\LG_Smart_UI-SemiBold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\FRABKIT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BELLB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\FELIXTI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\FRAHVIT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BOD_BLAI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\TCMI____.TTF',\n 'C:\\\\windows\\\\Fonts\\\\sapin.ttf',\n 'C:\\\\windows\\\\Fonts\\\\KTfontLight.ttf',\n 'C:\\\\windows\\\\Fonts\\\\msjhl.ttc',\n 'C:\\\\windows\\\\Fonts\\\\Inkfree.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\ROCCB___.TTF',\n 'C:\\\\windows\\\\Fonts\\\\webdings.ttf',\n 'C:\\\\windows\\\\Fonts\\\\KUNSTLER.TTF',\n 'C:\\\\windows\\\\Fonts\\\\SCHLBKBI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\calibrib.ttf',\n 'C:\\\\windows\\\\Fonts\\\\MaruBuri-Regular.otf',\n 'C:\\\\windows\\\\Fonts\\\\DIGICOTTF-Medium_0.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\MOD20.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ariali.ttf',\n 'C:\\\\windows\\\\Fonts\\\\segmdl2.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\KTfontMedium.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\FRSCRIPT.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NEXON LV2 GOTHIC MEDIUM.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquareB.otf',\n 'C:\\\\windows\\\\Fonts\\\\ARIALUNI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NotoSansCJKkr-Bold.otf',\n 'C:\\\\windows\\\\Fonts\\\\BRADHITC.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BOD_CR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\GmarketSansTTFBold.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\ELEPHNT.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BOD_I.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ARLRDBD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LEELAWDB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\couri.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\H2HDRM.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BOOKOS.TTF',\n 'C:\\\\windows\\\\Fonts\\\\gadugib.ttf',\n 'C:\\\\windows\\\\Fonts\\\\HYHWPEQ.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\GmarketSansTTFBold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\tahoma.ttf',\n 'C:\\\\windows\\\\Fonts\\\\TCMI____.TTF',\n 'C:\\\\windows\\\\Fonts\\\\JUMJA.TTF',\n 'C:\\\\windows\\\\Fonts\\\\MISTRAL.TTF',\n 'C:\\\\windows\\\\Fonts\\\\COPRGTL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ARIALN.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquare.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\RAVIE.TTF',\n 'C:\\\\windows\\\\Fonts\\\\AGENCYB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\FRAHV.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NANUMGOTHICEXTRABOLD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ALGER.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BELLI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\calibrili.ttf',\n 'C:\\\\windows\\\\Fonts\\\\HATTEN.TTF',\n 'C:\\\\windows\\\\Fonts\\\\segoeprb.ttf',\n 'C:\\\\windows\\\\Fonts\\\\MaruBuri-Light.ttf',\n 'C:\\\\windows\\\\Fonts\\\\LEELAWDB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\SB 어그로 M.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\consolaz.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\calibrili.ttf',\n 'C:\\\\windows\\\\Fonts\\\\corbell.ttf',\n 'C:\\\\windows\\\\Fonts\\\\SHOWG.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LBRITE.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\GOTHICBI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMSQUAREROUNDR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\constanb.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\MaruBuri-Light.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\esamanru Bold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\GmarketSansMedium.otf',\n 'C:\\\\windows\\\\Fonts\\\\symbol.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NANUMSQUAREROUNDL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquareB.ttf',\n 'C:\\\\windows\\\\Fonts\\\\comicbd.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\BRITANIC.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\FRADMIT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\phagspa.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NotoSansKR-Bold.otf',\n 'C:\\\\Windows\\\\Fonts\\\\comici.ttf',\n 'C:\\\\windows\\\\Fonts\\\\BOD_R.TTF',\n 'C:\\\\windows\\\\Fonts\\\\PLAYBILL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\palai.ttf',\n 'C:\\\\windows\\\\Fonts\\\\Cafe24Ohsquare.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NANUMSQUAREROUNDR.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NotoSansMonoCJKkr-Regular.otf',\n 'C:\\\\windows\\\\Fonts\\\\THEFACESHOP INKLIPQUID.ttf',\n 'C:\\\\windows\\\\Fonts\\\\GOUDOS.TTF',\n 'C:\\\\windows\\\\Fonts\\\\WINGDNG2.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NotoSansCJKkr-Black.otf',\n 'C:\\\\windows\\\\Fonts\\\\Cafe24Ssurround.ttf',\n 'C:\\\\windows\\\\Fonts\\\\cambriai.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\POORICH.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\DIGICOTTF-Bold_0.ttf',\n 'C:\\\\windows\\\\Fonts\\\\ANTQUAI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BELLB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\FRAHVIT.TTF',\n 'C:\\\\windows\\\\Fonts\\\\OCRB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\Candarali.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\seguibli.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\BMJUA_ttf.ttf',\n 'C:\\\\windows\\\\Fonts\\\\LBRITEDI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BASKVILL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumBarunpenR.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\cour.ttf',\n 'C:\\\\windows\\\\Fonts\\\\OUTLOOK.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NanumBarunpenR.otf',\n 'C:\\\\Windows\\\\Fonts\\\\ntailub.ttf',\n 'C:\\\\windows\\\\Fonts\\\\Cafe24Oneprettynight.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\courbd.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\BRLNSR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\LSANS.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\segoeuii.ttf',\n 'C:\\\\windows\\\\Fonts\\\\ERASBD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NotoSansKR-Black.otf',\n 'C:\\\\windows\\\\Fonts\\\\GmarketSansTTFLight.ttf',\n 'C:\\\\windows\\\\Fonts\\\\constani.ttf',\n 'C:\\\\windows\\\\Fonts\\\\GOUDOSI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NANUMMYEONGJO-YETHANGUL.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ANTQUAB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\impact.ttf',\n 'C:\\\\windows\\\\Fonts\\\\calibrii.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\ERASLGHT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BAUHS93.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\REFSAN.TTF',\n 'C:\\\\windows\\\\Fonts\\\\Cafe24Simplehae.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquareR.ttf',\n 'C:\\\\windows\\\\Fonts\\\\trebucit.ttf',\n 'C:\\\\windows\\\\Fonts\\\\BOD_B.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\SB 어그로 L.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\malgun.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\corbeli.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\ntailu.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\consolai.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\segoeprb.ttf',\n 'C:\\\\windows\\\\Fonts\\\\corbel.ttf',\n 'C:\\\\windows\\\\Fonts\\\\H2PORL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\CALISTB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\l_10646.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\LSANSI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Candaraz.ttf',\n 'C:\\\\windows\\\\Fonts\\\\H2GTRE.TTF',\n 'C:\\\\windows\\\\Fonts\\\\PERBI___.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\taile.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\arialbd.ttf',\n 'C:\\\\windows\\\\Fonts\\\\VIVALDII.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BOD_BLAI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\Gabriola.ttf',\n 'C:\\\\windows\\\\Fonts\\\\esamanru Bold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\Sitka.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\FRADM.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LEELAWAD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\FREESCPT.TTF',\n 'C:\\\\windows\\\\Fonts\\\\verdana.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NotoSansMonoCJKkr-Regular.otf',\n 'C:\\\\windows\\\\Fonts\\\\MAGNETOB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ariali.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\PRISTINA.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BOOKOS.TTF',\n 'C:\\\\windows\\\\Fonts\\\\wingding.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\wingding.ttf',\n 'C:\\\\windows\\\\Fonts\\\\Cafe24Ohsquareair.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\timesi.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\LBRITEI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\CENTAUR.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NotoSansCJKkr-DemiLight.otf',\n 'C:\\\\Windows\\\\Fonts\\\\DIGICOTTF-Medium.ttf',\n 'C:\\\\windows\\\\Fonts\\\\FRADM.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\GLSNECB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\trebuc.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMSQUAREROUNDEB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\COOPBL.TTF',\n 'C:\\\\windows\\\\Fonts\\\\monbaiti.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\GARABD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BKANT.TTF',\n 'C:\\\\windows\\\\Fonts\\\\Nirmala.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NANUMBARUNGOTHICULTRALIGHT.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ARIALNI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\constanb.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\ITCBLKAD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquare_0.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMBARUNGOTHIC-YETHANGUL.TTF',\n 'C:\\\\windows\\\\Fonts\\\\batang.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\SNAP____.TTF',\n 'C:\\\\windows\\\\Fonts\\\\himalaya.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\GARAIT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\msjhl.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\BOD_BI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BRITANIC.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LFAX.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\DIGICO-Bold.otf',\n 'C:\\\\windows\\\\Fonts\\\\ARLRDBD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LG_Smart_UI-Regular.ttf',\n 'C:\\\\windows\\\\Fonts\\\\LeelaUIb.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\BRUSHSCI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\SitkaI.ttc',\n 'C:\\\\windows\\\\Fonts\\\\FELIXTI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\SCDream3.otf',\n 'C:\\\\windows\\\\Fonts\\\\seguili.ttf',\n 'C:\\\\windows\\\\Fonts\\\\msyh.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\sapdn.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\H2GPRM.TTF',\n 'C:\\\\windows\\\\Fonts\\\\constan.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\GIL_____.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ENGR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\PARCHM.TTF',\n 'C:\\\\windows\\\\Fonts\\\\Cafe24Dongdong.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\TCCEB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\PER_____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NotoSansKR-Light.otf',\n 'C:\\\\Windows\\\\Fonts\\\\BMYEONSUNG_ttf.ttf',\n 'C:\\\\windows\\\\Fonts\\\\H2GSRB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\palai.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\HMFMMUEX.TTC',\n 'C:\\\\windows\\\\Fonts\\\\LFAXDI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\PERTIBD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\segoeui.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\STENCIL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\AGENCYB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\DIGICO-Light.otf',\n 'C:\\\\Windows\\\\Fonts\\\\ERASDEMI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\FRABK.TTF',\n 'C:\\\\windows\\\\Fonts\\\\WINGDNG3.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\arialbi.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\SCDream8.otf',\n 'C:\\\\windows\\\\Fonts\\\\BROADW.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NANUMGOTHICECOR.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BOD_CB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NANUMBARUNGOTHIC-YETHANGUL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\MSUIGHUR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\LTYPEO.TTF',\n 'C:\\\\windows\\\\Fonts\\\\LTYPE.TTF',\n 'C:\\\\windows\\\\Fonts\\\\KTfontThin.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NanumBarunpenB.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NANUMMYEONGJO.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMGOTHICLIGHT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BOD_CBI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BRLNSB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\CALIFR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquare_acR.ttf',\n 'C:\\\\windows\\\\Fonts\\\\arimonbd.ttf',\n 'C:\\\\windows\\\\Fonts\\\\LHANDW.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\RAGE.TTF',\n 'C:\\\\windows\\\\Fonts\\\\msyhbd.ttc',\n 'C:\\\\windows\\\\Fonts\\\\Pretendard-Light.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquare_acL.ttf',\n 'C:\\\\windows\\\\Fonts\\\\ROCKBI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\SCDream5.otf',\n 'C:\\\\windows\\\\Fonts\\\\segoeuisl.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\gadugi.ttf',\n 'C:\\\\windows\\\\Fonts\\\\VLADIMIR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NotoSansKR-Regular.otf',\n 'C:\\\\Windows\\\\Fonts\\\\simsun.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\OCRAEXT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\PERTIBD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ARIALNBI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\TCB_____.TTF',\n 'C:\\\\windows\\\\Fonts\\\\LTYPEB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\HTOWERTI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\segoescb.ttf',\n 'C:\\\\windows\\\\Fonts\\\\timesbi.ttf',\n 'C:\\\\windows\\\\Fonts\\\\verdanaz.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\TCM_____.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NANUMBARUNGOTHICLIGHT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquareL.otf',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquareL.ttf',\n 'C:\\\\windows\\\\Fonts\\\\LTYPEBO.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ROCKBI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\Candarai.ttf',\n 'C:\\\\windows\\\\Fonts\\\\simsunb.ttf',\n 'C:\\\\windows\\\\Fonts\\\\GIGI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Cafe24Syongsyong.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\comicbd.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMGOTHICCODING.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Pretendard-ExtraLight.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\SitkaB.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\timesbi.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\mmrtextb.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\Cafe24Danjunghae.ttf',\n 'C:\\\\windows\\\\Fonts\\\\COPRGTB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NANUMGOTHICBOLD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ENGR.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LHANDW.TTF',\n 'C:\\\\windows\\\\Fonts\\\\TCCB____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\verdanai.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\couri.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\times.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\GOTHICB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\CASTELAR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\MSUIGHUR.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ROCC____.TTF',\n 'C:\\\\windows\\\\Fonts\\\\DIGICOTTF-Bold_0.ttf',\n 'C:\\\\windows\\\\Fonts\\\\CALIST.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BERNHC.TTF',\n 'C:\\\\Users\\\\10094526\\\\AppData\\\\Local\\\\Microsoft\\\\Windows\\\\Fonts\\\\GmarketSansBold.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\seguili.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\SB 어그로 B.ttf',\n 'C:\\\\windows\\\\Fonts\\\\GARAIT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\MSUIGHUB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BERNHC.TTF',\n 'C:\\\\windows\\\\Fonts\\\\Cafe24Shiningstar.ttf',\n 'C:\\\\windows\\\\Fonts\\\\CENSCBK.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NANUMMYEONGJOBOLD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LeelUIsl.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\BOOKOSBI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\Pretendard-Thin.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NIAGENG.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\corbel.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\GmarketSansLight.otf',\n 'C:\\\\Windows\\\\Fonts\\\\YuGothM.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\consolab.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\WINGDNG2.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\calibri.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NanumBrush.otf',\n 'C:\\\\Windows\\\\Fonts\\\\REFSPCL.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquareOTF_acR.otf',\n 'C:\\\\Windows\\\\Fonts\\\\CENTURY.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\MaruBuri-ExtraLight.ttf',\n 'C:\\\\windows\\\\Fonts\\\\Cafe24Danjunghae.ttf',\n 'C:\\\\windows\\\\Fonts\\\\MaruBuri-ExtraLight.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\GILC____.TTF',\n 'C:\\\\windows\\\\Fonts\\\\arialbd.ttf',\n 'C:\\\\windows\\\\Fonts\\\\REFSAN.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\segoeuib.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\ROCKEB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\SitkaI.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\seguisbi.ttf',\n 'C:\\\\windows\\\\Fonts\\\\Pretendard-Medium.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\HMKMRHD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\LeelawUI.ttf',\n 'C:\\\\windows\\\\Fonts\\\\mvboli.ttf',\n 'C:\\\\windows\\\\Fonts\\\\seguisli.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquareL.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMMYEONGJOEXTRABOLD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\CALIFR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NanumBrush.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\Pretendard-Medium.ttf',\n 'C:\\\\windows\\\\Fonts\\\\corbeli.ttf',\n 'C:\\\\windows\\\\Fonts\\\\ntailu.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NanumPen.otf',\n 'C:\\\\Windows\\\\Fonts\\\\framdit.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\HARLOWSI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\HMFMMUEX.TTC',\n 'C:\\\\Windows\\\\Fonts\\\\Gong Gothic Bold.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\SCDream2.otf',\n 'C:\\\\windows\\\\Fonts\\\\tahomabd.ttf',\n 'C:\\\\windows\\\\Fonts\\\\Gong Gothic Bold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquareR.otf',\n 'C:\\\\Windows\\\\Fonts\\\\MaruBuri-Bold.otf',\n 'C:\\\\Windows\\\\Fonts\\\\timesbd.ttf',\n 'C:\\\\windows\\\\Fonts\\\\FTLTLT.TTF',\n 'C:\\\\windows\\\\Fonts\\\\MaruBuri-SemiBold.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\BOD_R.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ROCKB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\msyhl.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMSQUAREROUNDB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\SCDream8.otf',\n 'C:\\\\windows\\\\Fonts\\\\constanz.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NotoSansKR-Thin.otf',\n 'C:\\\\windows\\\\Fonts\\\\phagspab.ttf',\n 'C:\\\\windows\\\\Fonts\\\\RAVIE.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\constan.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\LG_Smart_UI-Bold.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\HATTEN.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\GOUDOS.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\segoeuil.ttf',\n 'C:\\\\windows\\\\Fonts\\\\palab.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\arimon__.ttf',\n 'C:\\\\windows\\\\Fonts\\\\seguibl.ttf',\n 'C:\\\\windows\\\\Fonts\\\\LFAXI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\OCRAEXT.TTF',\n 'C:\\\\windows\\\\Fonts\\\\HANDotum.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NirmalaB.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\KUNSTLER.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NotoSansMonoCJKkr-Bold.otf',\n 'C:\\\\windows\\\\Fonts\\\\KTfontMedium.ttf',\n 'C:\\\\windows\\\\Fonts\\\\SitkaB.ttc',\n 'C:\\\\windows\\\\Fonts\\\\GILLUBCD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\LSANSD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\segoeuib.ttf',\n 'C:\\\\windows\\\\Fonts\\\\STENCIL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\PERI____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\AGENCYR.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquareOTF_acR.otf',\n 'C:\\\\Windows\\\\Fonts\\\\ERASBD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LFAXD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NotoSansKR-Regular.otf',\n 'C:\\\\Windows\\\\Fonts\\\\sylfaen.ttf',\n 'C:\\\\windows\\\\Fonts\\\\MaruBuri-SemiBold.otf',\n 'C:\\\\windows\\\\Fonts\\\\Pretendard-Bold.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\Pretendard-Regular.ttf',\n 'C:\\\\windows\\\\Fonts\\\\TCCEB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\KTfontBold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\ROCK.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BRLNSB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\marlett.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\GILSANUB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NANUMMYEONGJOEXTRABOLD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquareR.otf',\n 'C:\\\\Windows\\\\Fonts\\\\NanumBarunpenB.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\MAGNETOB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\DIGICOTTF-Light_0.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\FRAMDCN.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\SCDream9.otf',\n 'C:\\\\Windows\\\\Fonts\\\\IMPRISHA.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LFAXDI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\palab.ttf',\n 'C:\\\\windows\\\\Fonts\\\\LBRITE.TTF',\n 'C:\\\\windows\\\\Fonts\\\\gulim.ttc',\n 'C:\\\\windows\\\\Fonts\\\\ROCKI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\malgunbd.ttf',\n 'C:\\\\windows\\\\Fonts\\\\Candaral.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMGOTHICEXTRABOLD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\CURLZ___.TTF',\n 'C:\\\\windows\\\\Fonts\\\\HMKMMAG.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BRLNSDB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\MaruBuri-SemiBold.otf',\n 'C:\\\\Windows\\\\Fonts\\\\mingliub.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\VINERITC.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ariblk.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\pala.ttf',\n 'C:\\\\windows\\\\Fonts\\\\DIGICOTTF-Medium.ttf',\n 'C:\\\\windows\\\\Fonts\\\\BOOKOSB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BOD_BLAR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\YuGothL.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\LATINWD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\PALSCRI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquareB.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\MaruBuri-SemiBold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\BOD_CR.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\seguiemj.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\sapin.ttf',\n 'C:\\\\windows\\\\Fonts\\\\FORTE.TTF',\n 'C:\\\\windows\\\\Fonts\\\\GILB____.TTF',\n 'C:\\\\windows\\\\Fonts\\\\Cafe24Syongsyong.ttf',\n 'C:\\\\windows\\\\Fonts\\\\LCALLIG.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BASKVILL.TTF',\n 'C:\\\\windows\\\\Fonts\\\\CENTURY.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\FRABKIT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\GmarketSansMedium.otf',\n 'C:\\\\windows\\\\Fonts\\\\calibrib.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NGULIM.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\PAPYRUS.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\CALIFB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\CALIFB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\DIGICOTTF-Light.ttf',\n 'C:\\\\windows\\\\Fonts\\\\GmarketSansTTFMedium.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NANUMGOTHICLIGHT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LBRITED.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquareL.otf',\n 'C:\\\\Windows\\\\Fonts\\\\cambriab.ttf',\n 'C:\\\\windows\\\\Fonts\\\\3of9.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\msyhbd.ttc',\n 'C:\\\\windows\\\\Fonts\\\\LBRITEI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\IMPRISHA.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NIAGSOL.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ONYX.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LTYPEO.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\SCDream5.otf',\n 'C:\\\\windows\\\\Fonts\\\\KTfontBold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\MaruBuri-Bold.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\CALIFI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\trebucbd.ttf',\n 'C:\\\\windows\\\\Fonts\\\\comici.ttf',\n 'C:\\\\windows\\\\Fonts\\\\corbelz.ttf',\n 'C:\\\\windows\\\\Fonts\\\\PERTILI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ANTQUAI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\GOTHICB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ERASDEMI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\SCDream4.otf',\n 'C:\\\\windows\\\\Fonts\\\\NanumSquareEB.otf',\n 'C:\\\\Windows\\\\Fonts\\\\NGULIM.TTF',\n 'C:\\\\windows\\\\Fonts\\\\HMFMOLD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\simsun.ttc',\n 'C:\\\\windows\\\\Fonts\\\\seguisym.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\ANTQUAB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\PLAYBILL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\bahnschrift.ttf',\n 'C:\\\\windows\\\\Fonts\\\\BSSYM7.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ELEPHNT.TTF',\n 'C:\\\\windows\\\\Fonts\\\\framd.ttf',\n 'C:\\\\windows\\\\Fonts\\\\REFSPCL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\BOD_B.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BMYEONSUNG_ttf.ttf',\n 'C:\\\\windows\\\\Fonts\\\\CHILLER.TTF',\n 'C:\\\\windows\\\\Fonts\\\\CALISTB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\SCRIPTBL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Cafe24SsurroundAir.ttf',\n 'C:\\\\windows\\\\Fonts\\\\GLSNECB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ROCKI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\CENTAUR.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumBrush.ttf',\n 'C:\\\\windows\\\\Fonts\\\\HARNGTON.TTF',\n 'C:\\\\windows\\\\Fonts\\\\GILSANUB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\LFAX.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\trebucbi.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\lucon.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\georgiai.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\Pretendard-Black.ttf',\n 'C:\\\\windows\\\\Fonts\\\\Pretendard-Regular.ttf',\n 'C:\\\\windows\\\\Fonts\\\\segoepr.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\Cafe24Ssurround.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\webdings.ttf',\n 'C:\\\\windows\\\\Fonts\\\\micross.ttf',\n 'C:\\\\windows\\\\Fonts\\\\ANTQUABI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMSQUAREROUNDL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\OPTITimes-Roman.otf',\n 'C:\\\\Windows\\\\Fonts\\\\GILB____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\monbaiti.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\msjhbd.ttc',\n 'C:\\\\windows\\\\Fonts\\\\BMJUA_ttf.ttf',\n 'C:\\\\windows\\\\Fonts\\\\Cafe24Dangdanghae.ttf',\n 'C:\\\\windows\\\\Fonts\\\\calibri.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NanumPen.ttf',\n 'C:\\\\windows\\\\Fonts\\\\POORICH.TTF',\n 'C:\\\\windows\\\\Fonts\\\\SCDream2.otf',\n 'C:\\\\windows\\\\Fonts\\\\YuGothR.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\GARA.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\segoeui.ttf',\n 'C:\\\\windows\\\\Fonts\\\\Candara.ttf',\n 'C:\\\\windows\\\\Fonts\\\\consolab.ttf',\n 'C:\\\\windows\\\\Fonts\\\\ntailub.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\COLONNA.TTF',\n 'C:\\\\windows\\\\Fonts\\\\INFROMAN.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ARIALNB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\PAPYRUS.TTF',\n 'C:\\\\windows\\\\Fonts\\\\segoesc.ttf',\n 'C:\\\\windows\\\\Fonts\\\\GARABD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\PERBI___.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ERASMD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\timesi.ttf',\n 'C:\\\\windows\\\\Fonts\\\\SCHLBKI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\YuGothB.ttc',\n 'C:\\\\windows\\\\Fonts\\\\LG_Smart_UI-Regular.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\H2GSRB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ITCEDSCR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ITCBLKAD.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ARIALNI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\seguisli.ttf',\n 'C:\\\\windows\\\\Fonts\\\\BAUHS93.TTF',\n 'C:\\\\windows\\\\Fonts\\\\MTCORSVA.TTF',\n 'C:\\\\windows\\\\Fonts\\\\cambriaz.ttf',\n 'C:\\\\windows\\\\Fonts\\\\ARIALNB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\calibriz.ttf',\n 'C:\\\\windows\\\\Fonts\\\\HTOWERTI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\MAIAN.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\YuGothL.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\micross.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\MTCORSVA.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NanumBarunpenB.otf',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMGOTHICECOR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\BRUSHSCI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\H2MJSM.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\SCDream1.otf',\n 'C:\\\\Windows\\\\Fonts\\\\HYHWPEQ.TTF',\n 'C:\\\\windows\\\\Fonts\\\\cambria.ttc',\n 'C:\\\\windows\\\\Fonts\\\\LSANSI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NanumBarunpenR.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NANUMGOTHICCODINGBOLD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\FRSCRIPT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\cambria.ttc',\n 'C:\\\\windows\\\\Fonts\\\\GILBI___.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\PERB____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\COPRGTL.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ebrima.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NanumPen.ttf',\n 'C:\\\\windows\\\\Fonts\\\\segoeuil.ttf',\n 'C:\\\\windows\\\\Fonts\\\\calibril.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\ITCEDSCR.TTF',\n 'C:\\\\windows\\\\Fonts\\\\SNAP____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\ebrima.ttf',\n 'C:\\\\windows\\\\Fonts\\\\GOTHICBI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Candarab.ttf',\n 'C:\\\\windows\\\\Fonts\\\\georgiai.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\symbol.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NirmalaB.ttf',\n 'C:\\\\windows\\\\Fonts\\\\NotoSansKR-Thin.otf',\n 'C:\\\\windows\\\\Fonts\\\\arialbi.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\BOOKOSB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\GmarketSansBold.otf',\n 'C:\\\\Windows\\\\Fonts\\\\H2PORL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Cafe24Ohsquare.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\GIGI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\l_10646.ttf',\n 'C:\\\\windows\\\\Fonts\\\\phagspa.ttf',\n 'C:\\\\windows\\\\Fonts\\\\SCDream1.otf',\n 'C:\\\\windows\\\\Fonts\\\\cour.ttf',\n 'C:\\\\windows\\\\Fonts\\\\MaruBuri-ExtraLight.otf',\n 'C:\\\\Windows\\\\Fonts\\\\ANTQUABI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LG_Smart_UI-Light.ttf',\n 'C:\\\\windows\\\\Fonts\\\\consolai.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\courbi.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\LTYPEBO.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Cafe24Ssukssuk.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\BROADW.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\georgia.ttf',\n 'C:\\\\windows\\\\Fonts\\\\TCCM____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\KTfontThin.ttf',\n 'C:\\\\windows\\\\Fonts\\\\SB 어그로 M.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NotoSansCJKkr-Thin.otf',\n 'C:\\\\Windows\\\\Fonts\\\\msyi.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\BELLI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Nirmala.ttf',\n 'C:\\\\windows\\\\Fonts\\\\BOD_CI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LBRITEDI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ROCC____.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NANUMGOTHIC.TTF',\n 'C:\\\\windows\\\\Fonts\\\\ITCKRIST.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\MaruBuri-Regular.ttf',\n 'C:\\\\windows\\\\Fonts\\\\HTOWERT.TTF',\n 'C:\\\\windows\\\\Fonts\\\\H2MJRE.TTF',\n 'C:\\\\windows\\\\Fonts\\\\DIGICOTTF-Light_0.ttf',\n 'C:\\\\windows\\\\Fonts\\\\DIGICO-Medium.otf',\n 'C:\\\\windows\\\\Fonts\\\\MSUIGHUB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\courbd.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\Inkfree.ttf',\n 'C:\\\\windows\\\\Fonts\\\\YuGothM.ttc',\n 'C:\\\\windows\\\\Fonts\\\\HMKMRHD.TTF',\n 'C:\\\\windows\\\\Fonts\\\\seguibli.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\SCDream7.otf',\n 'C:\\\\Windows\\\\Fonts\\\\PERTILI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\LeelUIsl.ttf',\n 'C:\\\\windows\\\\Fonts\\\\taile.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\INFROMAN.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\seguisb.ttf',\n 'C:\\\\windows\\\\Fonts\\\\FRAMDCN.TTF',\n 'C:\\\\windows\\\\Fonts\\\\Pretendard-ExtraLight.ttf',\n 'C:\\\\windows\\\\Fonts\\\\Candaraz.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\MaruBuri-ExtraLight.otf',\n 'C:\\\\Windows\\\\Fonts\\\\taileb.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\GOTHICI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LTYPE.TTF',\n 'C:\\\\windows\\\\Fonts\\\\GILC____.TTF',\n 'C:\\\\windows\\\\Fonts\\\\NotoSansCJKkr-Thin.otf',\n 'C:\\\\windows\\\\Fonts\\\\ROCCB___.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\constani.ttf',\n 'C:\\\\windows\\\\Fonts\\\\H2MKPB.TTF',\n 'C:\\\\windows\\\\Fonts\\\\OLDENGL.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\NIAGSOL.TTF',\n 'C:\\\\windows\\\\Fonts\\\\GmarketSansLight.otf',\n 'C:\\\\windows\\\\Fonts\\\\DIGICO-Light.otf',\n 'C:\\\\windows\\\\Fonts\\\\framdit.ttf',\n 'C:\\\\windows\\\\Fonts\\\\gadugi.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NanumSquare_acR.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\MATURASC.TTF',\n 'C:\\\\windows\\\\Fonts\\\\Pretendard-ExtraBold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\comicz.ttf',\n 'C:\\\\windows\\\\Fonts\\\\BOD_CB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\LFAXI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Gong Gothic Light.ttf',\n 'C:\\\\windows\\\\Fonts\\\\CALISTI.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\CALIST.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\H2MJRE.TTF',\n 'C:\\\\windows\\\\Fonts\\\\HANBatang.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\BOD_CI.TTF',\n 'C:\\\\windows\\\\Fonts\\\\segoescb.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\GLECB.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\Pretendard-Bold.ttf',\n 'C:\\\\windows\\\\Fonts\\\\GOTHIC.TTF',\n 'C:\\\\windows\\\\Fonts\\\\OPTITimes-Roman.otf',\n 'C:\\\\Windows\\\\Fonts\\\\segoesc.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\HTOWERT.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\segmdl2.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\VIVALDII.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\framd.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\malgunbd.ttf',\n 'C:\\\\windows\\\\Fonts\\\\LG_Smart_UI-Light.ttf',\n 'C:\\\\windows\\\\Fonts\\\\GARA.TTF',\n 'C:\\\\windows\\\\Fonts\\\\JUICE___.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\verdanab.ttf',\n 'C:\\\\windows\\\\Fonts\\\\Candarab.ttf',\n 'C:\\\\windows\\\\Fonts\\\\SCDream6.otf',\n 'C:\\\\windows\\\\Fonts\\\\SB 어그로 B.ttf',\n 'C:\\\\windows\\\\Fonts\\\\mingliub.ttc',\n 'C:\\\\Windows\\\\Fonts\\\\LSANS.TTF',\n 'C:\\\\Windows\\\\Fonts\\\\seguisym.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\NotoSansCJKkr-Medium.otf',\n 'C:\\\\windows\\\\Fonts\\\\segoeuiz.ttf',\n 'C:\\\\Windows\\\\Fonts\\\\tahoma.ttf',\n ...]\n\n\n\n#찾은 폰트를 기본 폰트로 설정하기\nplt.rc('font', family='NanumGothicCoding')\nplt.rc('axes', unicode_minus=False)\n\n\n#주간 최고기온 그래프에 표제목과 X축, Y축 이름을 한글로 넣기\nplt.plot([\"월\",\"화\",\"수\",\"목\",\"금\",\"토\",\"일\"], [28,30,29,31,32,31,31] )\nplt.xlabel('일')\nplt.ylabel('온도')\nplt.title('일별 최고 기온')\nplt.show()\n\n\n\n\n\n\n\n플롯에 여러 개의 라인들을 추가하기 위해서는 plt.plot()을 plt.show() 이전에 여러 번 사용하면 됩니다.\n또한, 각 라인에 대한 범례를 추가하기 위해서는 plt.legend() 함수를 사용하여 각 라인에 대한 범례를 순서대로 지정합니다.\n\n#두개선을 가지는 임의의 선그래프를 그리고 범례 추가하기\nplt.plot([1,2,3], [1,4,9])\nplt.plot([2,3,4],[5,6,7])\nplt.xlabel('Quarter')\nplt.ylabel('Score')\nplt.title('Game Result')\nplt.legend(['A team', 'B team'])\nplt.show()\n\n\n\n\n한줄의 plt.plot이라도 여러개의 X,Y 데이터를 가질수 있으며, plot 여러개를 사용한 것과 같이 다수개의 그래프가 만들어 집니다.\n\n#한개의 Plot 함수로 2개의 선을 가지는 그래프 그리기\nplt.plot([1,2,3],[1,4,9],[2,3,4],[5,6,7])\nplt.xlabel('Quarter')\nplt.ylabel('Score')\nplt.title('Game Result')\nplt.legend(['A team', 'B team'])\nplt.show()\n\n\n\n\n\n\n\n\nx=list(range(0,10))\ny1=list(range(0,10))\ny2=list(range(0,20,2))\ny3=list(range(0,40,4))\n\n\n#(x,y1)(x,y2)(x,y3)의 선그래프를 그리기\nplt.plot(x,y1,x,y2, x, y3)\nplt.show()\n\n\n\n\n마커를 변경 하려면, 각 x,y값 뒤에 원하는 스타일 문자열을 넣어줍니다.\n스타일 문자열은 색깔(color), 마커(marker), 선 종류(line style)의 순서로 지정하고,\n만약 이 중 일부가 생략되면 기본 값이 적용된다.\n색, 마커, 선 종류에 대한 설명은 plot의 도움말을 참고하도록 합니다.\n\n#3개의 선을 각기 다른 색과 마커, 선을 사용해 표현하기\nplt.plot(x,y1,'r--', x,y2, 'bs' ,x, y3, 'g^:')\nplt.show()\n\n\n\n\n\n\n\n지금까지는 하나의 Plot에 여러개의 라인을 추가 하였으나, 한번에 여러개의 plot을 그리는것도 가능합니다.\nplt.show를 사용하여 그래프를 출력하기 전에 여러개의 figure를 열면 여러개의 그래프를 한번에 만들 수 있습니다.\n\nx=list(range(0,10))\ny1=[0,1,2,3,4,5,6,7,8,9]\ny2=[0,1,4,9,16,25,36,49,64,81]\n\n\n#(x,y), (x,y2)를 두개의 그래프로 동시에 그리기\nplt.figure()\nplt.plot(x,y1)\nplt.figure()\nplt.plot(x,y2)\n\nplt.show()\n\n\n\n\n\n\n\nsubplot을 사용하여 원하는 배치를 해서 한번에 출력하는것도 가능 합니다.\n이때 subplot에 넘겨주는 값은 3개의 숫자로 앞에서부터 차례대로 행/열/순서 배치순서가 됩니다.\n\n#subplot을 사용하여 3개의 그래프 영역을 동시에 그리기\nplt.figure(figsize=(20,12))\n\nplt.subplot(221)\nplt.subplot(222)\nplt.subplot(212)\n\nplt.show()\n\n\n\n\nplt.subplot(221)는 2행 2열 배치에 첫번째 영역이 됩니다.\nplt.subplot(222)는 2행 2열 배치에 두번째 영역이 됩니다.\nplt.subplot(212)는 2행 1열 배치의 두번째 영역 즉, 하단 긴 영역이 됩니다.\n\n#subplot을 사용하여 임의의 그래프 3개를 동시에 그리기\nplt.figure(figsize=(20,12))\n\nplt.subplot(221)\nplt.plot([1,2,3], [110,130,120])\nplt.grid()\n\nplt.subplot(222)\nplt.plot([\"월\",\"화\",\"수\",\"목\",\"금\",\"토\",\"일\"], [28,30,29,31,32,31,31] )\nplt.xlabel('요일')\nplt.ylabel('기온')\nplt.title('최고기온')\nplt.grid()\n\nplt.subplot(212)\ny = [5, 3, 7, 10, 9, 5, 3.5, 8]\nx = range(len(y))\nplt.barh(x, y, height=0.7, color=\"red\")\nplt.grid()\n\nplt.show()\n\n\n\n\n\n\n\n\ncust_data.csv 데이터를 불러와서 3개의 그래프를 그려봅시다.\n\n서비스 수에 따라 요금 평균이 어떻게 변화 되는지 확인해 봅시다.\n\n빨간 라인으로, 네모 마커를 사용하여 표기 합니다.\n\n고객의 나이 분포를 확인해 봅시다.\n\n20개 구간으로 나누고, X축에는 ‘나이’, Y축에는 ’고객수’로 축이름을 설정합니다.\n\n나이와 전체 요금간의 관계를 확인하는 그래프를 그려봅시다.\n\n그래프의 크기는 (20,10)으로 설정하고, 색은 녹색으로 합니다\n\n\n\nplt.figure(figsize=(20,5))\n\nplt.subplot(121)\nplt.plot(df2['avg_bill'],'rs-')\n\n\nplt.subplot(122)\nplt.hist(df['age'], bins=20)\nplt.xlabel('나이')\nplt.ylabel('고객수')\n\nplt.figure(figsize=(20,10))\nplt.scatter(x=df['age'], y=df['avg_bill'], c='green')\n\nplt.show()\n\n\n\n\n\n\n\n  \n\n\n\n\n\nSeaborn은 Matplotlib을 기반으로 다양한 색상 테마와 통계용 차트 등의 기능을 추가한 시각화 패키지입니다.\n기본적인 시각화 기능은 Matplotlib 패키지에 의존하게 됩니다. Seaborn에 대한 자세한 내용은 웹사이트(http://seaborn.pydata.org/ )를 참조하세요.\n\n\n\n\n%pip install seaborn\n\nRequirement already satisfied: seaborn in c:\\programdata\\anaconda3\\lib\\site-packages (0.11.2)\nRequirement already satisfied: matplotlib>=2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (3.5.2)\nRequirement already satisfied: numpy>=1.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (1.19.5)\nRequirement already satisfied: pandas>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (1.4.4)\nRequirement already satisfied: scipy>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (1.9.1)\nRequirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (9.2.0)\nRequirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (4.25.0)\nRequirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\nRequirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.4.2)\nRequirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (3.0.9)\nRequirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (21.3)\nRequirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.23->seaborn) (2022.1)\nRequirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.15.0)\nCould not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)'))) - skipping\nNote: you may need to restart the kernel to use updated packages.\n\n\nWARNING: There was an error checking the latest version of pip.\n\n\n\n#seaborn 패키지 불러오기\n%matplotlib inline\nimport seaborn as sns\n\n\n#matplotlib 패키지 불러오기\n#seaborn을 사용할떄는 반드시 Matplotlib 패키지도 함께 불러와야 함\nimport matplotlib.pyplot as plt\n\n\n\n\n\n#사용할 데이터 불러오기\nimport pandas as pd\ndf=pd.read_csv(\"cust_data.csv\")\ndf.head()\n\n\n\n\n\n  \n    \n      \n      class\n      sex\n      age\n      service\n      stop\n      npay\n      avg_bill\n      A_bill\n      B_bill\n      termination\n      by_age\n      bill_rating\n    \n  \n  \n    \n      0\n      C\n      F\n      28\n      0\n      N\n      N\n      2640.0000\n      792.00000\n      1584.0000\n      Y\n      25\n      low\n    \n    \n      1\n      E\n      F\n      24\n      1\n      N\n      N\n      16840.0000\n      2526.00000\n      6983.0000\n      N\n      20\n      high\n    \n    \n      2\n      F\n      F\n      32\n      1\n      N\n      N\n      15544.7334\n      2331.71001\n      6750.4666\n      N\n      30\n      high\n    \n    \n      3\n      D\n      M\n      18\n      1\n      N\n      N\n      4700.0000\n      0.00000\n      4502.0000\n      N\n      15\n      lowmid\n    \n    \n      4\n      C\n      F\n      78\n      1\n      N\n      N\n      1361.7999\n      1173.99990\n      0.0000\n      N\n      75\n      low\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n앞서 배웠듯 산점도는 두 값간의 상관관계를 확인해 볼 수 있습니다.\n\n#avg_bill과 age간의 상관관계를 확인해 봅시다.\nsns.scatterplot(x='age', y='avg_bill', data=df)\nplt.show()\n\n\n\n\n\n\n\ncatplot은 색상(hue)과 열(col) 또는 행(row) 등을 동시에 사용하여 3 개 이상의 카테고리 값에 의한 분포 변화를 보여줍니다.\n\n#age와 avg_bill간의 관계를 class별로 확인하기\nsns.catplot(x='age', y='avg_bill',data=df ,col=\"class\", col_wrap=2)\nplt.show()\n\n\n\n\n\n\n\nlmplot은 산점도에 회귀선을 그려주어 하나의 값에 따라 다른 값이 어떻게 변하는지를 예측 할수 있습니다.\n\n#lmplot을 사용하여 avg_bill과 B_bill의 상관관계를 확인하기\nplt.figure(figsize=(10,5))\nsns.lmplot(x='avg_bill', y='B_bill', data=df,line_kws={'color': 'red'})\nplt.show()\n\n<Figure size 1000x500 with 0 Axes>\n\n\n\n\n\n\n#hue값으로 성별에 따라 어떻게 달라지는지 구분하기 \nsns.lmplot(x='avg_bill', y='B_bill', data=df, hue='sex')\nplt.show()\n\n\n\n\n\n\n\n항목 별 갯수를 확인 할 때 사용합니다.\n\n#나이대별 bill_rating분포를 확인하기\nplt.figure(figsize=(10,5))\nsns.countplot(x=\"by_age\", hue=\"bill_rating\", data=df)\nplt.show()\n\n\n\n\n히스토그램과 countplot은 두 그래프 모두 항목의 갯수분포를 확인하는 비슷한 그래프로 보이지만\n히스토그램은 연속형 변수만 가능하고, coutplot은 범주형 변수에도 적용이 가능하다는 차이가 있습니다.\n\n#연속형 변수인 age로 count plot과 비슷한 그래프 그리기 \nplt.figure(figsize=(10,5))\nsns.histplot(x=\"age\",bins=20, hue=\"bill_rating\",data=df, multiple='dodge', shrink=0.8)\nplt.show()\n\n\n\n\nY축 값을 지정하면 가로 막대 그래프로 표현됩니다.\n\n#성별 고객등급 분포를 가로 막대 그래프로 확인하기\nsns.countplot(y=\"class\", hue=\"sex\", data=df)\nplt.show()\n\n\n\n\n\n\npalette값을 설정하면 원하는 톤의 그래프를 그릴 수 있습니다.\n더 많은 palette색상은 담은을 참고하세요. https://seaborn.pydata.org/tutorial/color_palettes.html\n\n#palette값을 spring로 설정하여 색을 변경하기\nsns.countplot(y=\"class\", hue=\"sex\", data=df, palette='spring')\nplt.show()\n\n\n\n\n\n\n\n\n산점도와 countplot을 한번에 보여주어 데이터의 분포와 상관관계를 한번에 볼 수 있습니다.\n\n#jointplot을 사용하여 avg_bill과 age간의 관계 확인하기\nsns.jointplot(x=\"avg_bill\", y=\"age\", data=df)\nplt.show()\n\n\n\n\nkind 값을 사용하면 산점도 영역의 그래프를 다른 그래프로 변경 할 수 있습니다\n\n#kind 값을 변경하여 다양한 그래프를 그리기\nsns.jointplot(x=\"avg_bill\", y=\"age\", data=df, kind='hex')\nplt.show()\n\n\n\n\n\n\n\n상관관계를 확인하기 위해서 주의해야 할 점은 모든 데이터가 ’continous’해야 한다는 점입니다.\ncorr()은 상관관계를 측정 할 수 있는 함수입니다.\n\ndf.corr()\n\n\n\n\n\n  \n    \n      \n      age\n      service\n      avg_bill\n      A_bill\n      B_bill\n      by_age\n    \n  \n  \n    \n      age\n      1.000000\n      0.092643\n      -0.327182\n      0.029134\n      -0.359188\n      0.997385\n    \n    \n      service\n      0.092643\n      1.000000\n      0.271239\n      0.068715\n      0.203498\n      0.093625\n    \n    \n      avg_bill\n      -0.327182\n      0.271239\n      1.000000\n      0.340294\n      0.806813\n      -0.325668\n    \n    \n      A_bill\n      0.029134\n      0.068715\n      0.340294\n      1.000000\n      -0.006415\n      0.030240\n    \n    \n      B_bill\n      -0.359188\n      0.203498\n      0.806813\n      -0.006415\n      1.000000\n      -0.357902\n    \n    \n      by_age\n      0.997385\n      0.093625\n      -0.325668\n      0.030240\n      -0.357902\n      1.000000\n    \n  \n\n\n\n\nheatmap은 열을 의미하는 heat와 지도를 뜻하는 map을 합친 단어입니다.\n데이터의 배열을 생삭으로 표현해 주는 그래프입니다.\nheatmap을 사용하면 두개의 카테고리 값에 대한 값변화를 한 눈에 알기 쉽기 때문 두 값의 상관관계를 나타낼때 주로 사용 합니다.\n\n# '-' 부호 출력 경고를 제거 하기 위한 코드\nplt.rc('axes', unicode_minus=False)\n\n\n#컬럼별 상과관계를 heatmap 그래프로 그리기\nsns.heatmap(df.corr())\nplt.show()\n\nNameError: name 'sns' is not defined\n\n\n\n\n\n수치적 자료를 표현하는 그래프입니다.\n수치적 자료로부터 얻어낸 통계량인 5가지 요약 수치(최소값, 제 1사분위값, 제 2사분위값, 제 3사분위값, 최대값)를 가지고 그리며,\n전처리에서 배웠던 describe()함수와 함께 데이터 분포를 확인하는데도 유용하게 사용할수있습니다.\n\n# 나이대별 avg_bill에 대한 boxplot 그리기 \nplt.figure(figsize=(16,8))\nsns.boxplot(y=df[\"avg_bill\"], x=df[\"by_age\"],width=0.9)\nplt.show()\n\n\n\n\n\n\n\nviolinplot은 Boxplot과 모양은 비슷하지만 밀집도를 함께 볼 수 있어 데이터 탐색에 유용하게 사용할 수 있습니다.\n\n# 나이대별 A상품 요금에 대한 violinplot을 그리기\nplt.figure(figsize=(16,8))\nsns.violinplot(y=df[\"A_bill\"], x=df[\"class\"],width=1)\nplt.show()"
  },
  {
    "objectID": "post/etc/toeic speaking/part1.html",
    "href": "post/etc/toeic speaking/part1.html",
    "title": "part 1",
    "section": "",
    "text": "- 주어진 지문을 따라 읽기\n- 답변 준비시간 : 각 45초, 답변 시간 : 45초\n- 참고영상\n\n\n\n\n준비시간 : 큰소리로 지문 전체를 한 번 읽고 남는 시간에는 어려웠던 발음을 연습\n답변시간 : 지문을 자신감 있게 읽고 남는 시간 묵음, 읽다가 틀리면 그 부분만 다시 읽기"
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html",
    "href": "post/etc/toeic speaking/part2.html",
    "title": "part 2",
    "section": "",
    "text": "- 사진묘사\n- Q3-4, 답변 준비 시간 각 45초, 답변 시간 각 30초\n- 참고영상"
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#실점-원인-6가지",
    "href": "post/etc/toeic speaking/part2.html#실점-원인-6가지",
    "title": "part 2",
    "section": "(1) 실점 원인 6가지",
    "text": "(1) 실점 원인 6가지\n\n준비시간 활용을 잘 못하는 경우\n아주 기본적인 단어를 모르는 경우\n외운 템플릿에서는 빠른속도, 내가 영작해야 하는 부분에서는 느려짐\n세부 사항에 집착을 하다가 사진의 전반적인 것들을 설명 못함\n기본적인 문법실수가 너무 심하게 반복 (ex, 한 문장에 동사가 2개, equipment 같은 불가산 명사에 (a)를 붙임)\n사진에 대한 묘사 보다 내 의견 위주로 말하는 경우"
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#만접-비법-3가지",
    "href": "post/etc/toeic speaking/part2.html#만접-비법-3가지",
    "title": "part 2",
    "section": "(2) 만접 비법 3가지",
    "text": "(2) 만접 비법 3가지\n\n템플릿, 빈출 표현 입을 움직여서 암기\n지엽적, 한가지 대상만 오래 설명하지말고, “공통문장”을 써서 전반적인 묘사를 하자.\n모르는 단어는 더큰 범위의 단어로 표현\n\n\nex1) a hammer \\(\\to\\) a tool\nex2) a pitchfork(쇠스랑) \\(\\to\\) a garden tool\nex3) a yellow cardigan \\(\\to\\) yellow clothes\nex4) a beaker \\(\\to\\) equipment"
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#summary",
    "href": "post/etc/toeic speaking/part2.html#summary",
    "title": "part 2",
    "section": "summary",
    "text": "summary\n\n준비시간: 필기비추, 최대 중요 포인트 몇가지 잡고 순서만 정하자.\n답변시간 : 지엽적, 한가지 대상만 오래설명하지말고 공통문장을 사용해서 “핵심적, 전반적”인 묘사를 사용하자."
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#서론",
    "href": "post/etc/toeic speaking/part2.html#서론",
    "title": "part 2",
    "section": "(1) 서론",
    "text": "(1) 서론\n\nThis is a picture taken at/in 장소\nThis is a picture taken indoors/outdoors"
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#전체요약",
    "href": "post/etc/toeic speaking/part2.html#전체요약",
    "title": "part 2",
    "section": "(2) 전체요약",
    "text": "(2) 전체요약\n\n때에 따라 생략가능\n\nThe first thing I can see from this picture is 대표적인 사람/사물"
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#세부사항",
    "href": "post/etc/toeic speaking/part2.html#세부사항",
    "title": "part 2",
    "section": "(3) 세부사항",
    "text": "(3) 세부사항\n\n(3)-1. 정면\n\n\n\n\n\n(3)-2. 중심\n\n\n\n\n\n(3)-3. 왼쪽/오른쪽\n\n\nOn the right/left side of the picture, there is/are 사람/사물 I can see 단/복수명사\n\n\nOn the left side of the picture, there is a man with a beard.(턱수염이 있는 남자)\nOn the right side of the picture, there is a man sitting on a chair\n\n\n\n(3)-4. 배경\n\n\nIn the background of the picture, I can see three people."
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#마무리",
    "href": "post/etc/toeic speaking/part2.html#마무리",
    "title": "part 2",
    "section": "(4) 마무리",
    "text": "(4) 마무리\n\n1인 유형을 제외하고는 생략하는 것을 추천\n\n\n\nOverall, It seems/looks like 주어 + 동사 \\(\\to\\) Overall, It seems like the woman is busy."
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#주의해야할-문법",
    "href": "post/etc/toeic speaking/part2.html#주의해야할-문법",
    "title": "part 2",
    "section": "(5) 주의해야할 문법",
    "text": "(5) 주의해야할 문법"
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#분사-vs-현재진행형",
    "href": "post/etc/toeic speaking/part2.html#분사-vs-현재진행형",
    "title": "part 2",
    "section": "(5)-1. 분사 vs 현재진행형",
    "text": "(5)-1. 분사 vs 현재진행형\n\n분사 : N + ing\n\n\nI can see a man working hard (열심히 일하고 있는 남자를 볼 수 있다.)\n\n\n현재진행형 : be + ing\n\n\nHe is working hard. (그는 열심히 일하고 있는 중이다.)\n\n\na lot of는 셀 수 있는/없는 명사 모두 수식 가능"
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#q1",
    "href": "post/etc/toeic speaking/part2.html#q1",
    "title": "part 2",
    "section": "Q1",
    "text": "Q1\n\nThis is a picture taken indoors. (서론)\nThe first thing I can see from this picture is a woman, (전체요약)\nShe is working and using a drill. Also, she is holding a piece of wood and looking at it.(인물묘사)\nIn front of her, there is a table. On the right side of the picture, I can see shelves. (기타사물배경)\nOverall, it seems like the woman is busy."
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#q2",
    "href": "post/etc/toeic speaking/part2.html#q2",
    "title": "part 2",
    "section": "Q2",
    "text": "Q2\n\nThis is a picture taken at cafe.\nThe first thing I can see from this picture is a woman,\nshe is sitting at a table. she is wearning a blue shirt(셜트), and she has long hair.\nshe is holing a smartphone and looking at the screen.\nIn front of her, there is a table. On the table, I can see a pick cup.\nOverall, it seems like the woman is busy."
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#q1-1",
    "href": "post/etc/toeic speaking/part2.html#q1-1",
    "title": "part 2",
    "section": "Q1",
    "text": "Q1\n\nThis is a picture taken outdoors.\nThe first thing I can see from this picture is two people.\nThe man on the left is smiling and is wearing a black T-shirt.\nThey are wearing helmets, sunglasses, and sportswear. Also, they are riding bicycles.\nIn the background of the picture, I can see green grass."
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#q2-1",
    "href": "post/etc/toeic speaking/part2.html#q2-1",
    "title": "part 2",
    "section": "Q2",
    "text": "Q2\n\nThis is a picture taken outdoors.\nThe first thing I can see from this picture is two people.\nThey are taking a picture. Also, they are wearing sunglasses and smiling.\nThe woman in the picture is wearing blue cloths. she is holding a smartphone and taking a picture.\nThe man is holing a camera and waving his hand."
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#q1-2",
    "href": "post/etc/toeic speaking/part2.html#q1-2",
    "title": "part 2",
    "section": "Q1",
    "text": "Q1\n\nThis is a picture taken outdoors.\nThe first thing I can see from this picture is some people.\nOn the rigth side of the picture, I can see two people. They are taking a walk.\nOne of them is a woman. She is walking a dog.\nThe other is a a girl. She has a ponytail and she is wearing a blue shirt.\nIn the background of the picture, I can see a house (and on old lady (pushing a stroller)유모차를 밀다.) \\(\\to\\) 생략가능"
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#q2-2",
    "href": "post/etc/toeic speaking/part2.html#q2-2",
    "title": "part 2",
    "section": "Q2",
    "text": "Q2\n\nThis is a picture taken outdoors.\nThe first thing I can see from this picture is some people\nOn the left side of the picture, I can see a man sitting on the ground and taking a picture.\nOn the right side of the picture, I can see amen walking on the street.\nIn the back ground of the picture, I can see buildings, parasols, and more people walking around(서성거리다.)."
  },
  {
    "objectID": "post/etc/toeic speaking/part2.html#set2-q1",
    "href": "post/etc/toeic speaking/part2.html#set2-q1",
    "title": "part 2",
    "section": "set2-Q1",
    "text": "set2-Q1\n\nThis is a picture taken outdoors.\n(전체요약 생략)\nIn the middle of the picture, I can see a lot of people crossing the street.\nSome of them are carrying bags.(Most of them are wearing casual clothes.)\nOn the right side of the picture, there is a car. Next to it, there is a sidewalk.\nIn the backgroud of the picture, I can see two big buildings,(street lights, signboards(가로등), and traffic lights(산호등))"
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html",
    "href": "post/etc/toeic speaking/part3.html",
    "title": "part 3",
    "section": "",
    "text": "- 문제 번호 : Q5-7 (3문제)\n- 답변 준비 시간 : 각 3초\n- 답변시간\n* 5, 6번 : 15초\n* 7번 : 30초\n- 참고영상"
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#문제는-내-친구",
    "href": "post/etc/toeic speaking/part3.html#문제는-내-친구",
    "title": "part 3",
    "section": "(1) 문제는 내 친구",
    "text": "(1) 문제는 내 친구\n\n문제를 참조해서 답변으로 바꾸어 읽기\n의문사 만능공식을 활용해 답변형식으로 변형하자."
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#어렵게-말하기-x",
    "href": "post/etc/toeic speaking/part3.html#어렵게-말하기-x",
    "title": "part 3",
    "section": "(2) 어렵게 말하기 X",
    "text": "(2) 어렵게 말하기 X"
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#진짜-내-얘기-x",
    "href": "post/etc/toeic speaking/part3.html#진짜-내-얘기-x",
    "title": "part 3",
    "section": "(3) 진짜 ‘내 얘기’ x",
    "text": "(3) 진짜 ‘내 얘기’ x\n\n만능 답변 등을 이용한 쉬운 답변 이용"
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#답변의-70-미리준비",
    "href": "post/etc/toeic speaking/part3.html#답변의-70-미리준비",
    "title": "part 3",
    "section": "(4) 답변의 70% 미리준비",
    "text": "(4) 답변의 70% 미리준비\n\n의문사 만능동식 6개\n부연설명 쉽게 만드는 팁 2개\n만능 문장 50개"
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#가급적-yes로-대답",
    "href": "post/etc/toeic speaking/part3.html#가급적-yes로-대답",
    "title": "part 3",
    "section": "(5) 가급적 Yes로 대답",
    "text": "(5) 가급적 Yes로 대답\n\n5번 문항에서 부정적으로 답변하면 뒤에 6, 7번에서 동일 주제에 대한 문제가 연속으로 나오기 때문에 뒤에서 답변하는데 약간의 차질이 생길 수 있음"
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#who",
    "href": "post/etc/toeic speaking/part3.html#who",
    "title": "part 3",
    "section": "(1) Who",
    "text": "(1) Who\nQ. Who do you usually go with when you go to a cafe?\nA. I usually go with my friend when I go to a cafe"
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#where",
    "href": "post/etc/toeic speaking/part3.html#where",
    "title": "part 3",
    "section": "(2) Where",
    "text": "(2) Where\n\n(2)-1. 어디서 사니?\n(where do you buy ~?)\n\nOn the Internet\nAt a shopping mall\nAt a department store 백화점\n\nQ. where do you buy (purchase) clothes?\nA. I usually buy cloths on the Internet.\n\n\n\n(2)-2. 어디서 하니\n(Where do you usually play ~?)\n\nat home/school\non a subway\n\nQ. where do you usually play mobile game?\nA. I usually play mobile game at home/school/on a subway.\n\n\n\n(2)-3. 어디서 정보를 얻니?\n(Where do you get information ~ ?)\n\non the internet\n\nQ. Where do you get information about where to buy home cleaning products?\nA. I usually get information about where to buy home cleaning products on the Internet."
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#when",
    "href": "post/etc/toeic speaking/part3.html#when",
    "title": "part 3",
    "section": "(3) When",
    "text": "(3) When\n\n(3)-1. 보통 언제 하니\n(When do you usually~?)\n\nWhen I have spare time(빈출)\non weekends / in 계절\n\nQ. When do you usually visit museums?\nA. I usually visit museums when I have spare time.\n\n\n\n(3)-2. 언제 했니?, 언제 마지막으로 했니?\n(When did you) ~ ? \\(\\to\\) two weeks ago / yesterday / last year * When was the last time you 과거동사? \\(\\to\\) The last time I 과거동사 was two weeks ago.\nQ. When was the last time you went to a park?\nA. The last time I went to a park was two weeks ago\ncf. When did you last go to a park?\nA. I last went to a park two weeks ago\n\n\n\n(3)-3. 하루 중 언제 하니?\n(What time of the day do you usually~?)\n\nin the moring/evening\n답변 만능 문장\n\nIt’s part of my routine. 내 일과 중 일부이다.\nIt’s less crowded in the morning. 아침에 덜 붐빈다.\n\n\nQ. What time of the day do you usually go to the movies?\nA. I usually go to the movies in the morning. because it’s less crowded in the morning.\n\n\n\n(3)-4. 일 년 중에 언제 하니?\n(what time of the year~?)\n\nin 계절\n\n\nwhat time of the year do you read books most?\n\nA. I read books in winter most."
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#how",
    "href": "post/etc/toeic speaking/part3.html#how",
    "title": "part 3",
    "section": "(4) How",
    "text": "(4) How\n\n(4)-1. 얼마나 자주\n(How often, How frequently, How many times~?) \\(\\to\\) 빈출\n\ntwice a week 일주일에 두 번\nalmost every day 거의 매일\nonce in a while 가끔씩\n\nQ. How often do you shop for cloths?\nA. I shop for cloths twice a week / almost every day / once in a while.\n\n\n(4)-2. 얼마나 오래\n(How long have you p.p~?) \\(\\to\\) 빈출\n\nI have p.p + for 기간(5 minutes / 2 hours / a year)\n\nQ. How long have you lived in your current neighborhood?\nA. I have lived in my current neighborhood for 3 years.\n\n\n(4)-3. V 하는데에 얼마가 걸리니?\n(How long does it take 사람 to V?)\n\nIt takes 기간 (for 목적어) to V\n\nQ. How long does it take you to finish a book?\nA. It takes 3 hours for me to finish a book.\n\n\n(4)-4. 얼마나 오래 시간을 보내니?\n(How much time do you spend~?) \\(\\to\\) 빈출\n\nI spend minutes / hours / days\n\nQ. How much time do you spend surfing the Internet every day?\nA. I spend 3 hours surfing the Internet every day.\n\n\n\n(4)-5. 얼마나 멀리 있니?\n(How far)\n\nabout 10 minutes away\n\nQ. How far is the closest bank from where you live?\nA. The closet bank from where I live is about 10 minutes away.\n\n\n(4)-6. 가격이 얼마니?\n(How much~?)\n\nabout 20 dollars\n\nQ. How much do you usually spend when you go to a restaurant with your friends?\nA. I usually spend about 20 dollars when I go to a restaurant with my friends.\n\n\n\n(4)-7. 몇 개니?\n(How many~?)\nQ. How many bookstores are there in your town?\nA. There are two bookstores in my town.\n\n\n\n(4)-8. 이동 수단\n(How do you get to ~?)\nQ. How do you usually get to school?\nA. I usually get to school by bus."
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#what",
    "href": "post/etc/toeic speaking/part3.html#what",
    "title": "part 3",
    "section": "(5) What",
    "text": "(5) What\n(What kind / type / sort / genre of N) \\(\\to\\) 빈출\n\nK-pop / Fiction / action movies\n\nQ. What kind of social media do people in your company normally use?\nA. People in my country normally use Instagram."
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#해본적-있니",
    "href": "post/etc/toeic speaking/part3.html#해본적-있니",
    "title": "part 3",
    "section": "~해본적 있니?",
    "text": "~해본적 있니?\n(Have you ever p.p~?)\n\nYes, I have p.p / No, I haven’t p.p\n\nQ. Have you ever used a photo-editing software to imporve your picture images?\nA. Yes, I have used a photo-editing software to improve my picture images."
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#매우-좋아한다.",
    "href": "post/etc/toeic speaking/part3.html#매우-좋아한다.",
    "title": "part 3",
    "section": "(1). 매우 좋아한다.",
    "text": "(1). 매우 좋아한다.\n\nI like/love N very much.\\(\\to\\) 빈출\n\nQ. who do you usually go with when you shop for clothes?\nA. I usually go with my friends when I shop for clothes / because we like shopping very much."
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#형용사로-표현",
    "href": "post/etc/toeic speaking/part3.html#형용사로-표현",
    "title": "part 3",
    "section": "(2). 형용사로 표현",
    "text": "(2). 형용사로 표현\n\n쉽고 간결하며 더 좋은 점수를 받을 수 있음\n\n\n\n(2)-1. It’s very + 형용사 (빈출)\n\n\n\n(2)-2. 비교급\n\n\n\n(2)-3. 유용하고 간편하게 쓸 수 있는 형용사\n\n\n\n\n(2)-4. 비교급 주의\n\n\n\n(2)-5. 빈출 예시\n\n\n\n(2)-6. 문장 연결어\n\n- 파트3 만능문장50"
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#set1.-해외여행",
    "href": "post/etc/toeic speaking/part3.html#set1.-해외여행",
    "title": "part 3",
    "section": "set1. 해외여행",
    "text": "set1. 해외여행\n\nQ5. when was the last time you traveled overseas and what is your favorite places?\nanswer\nThe last time I traveld overseas was last year, and my favorite places is canda.\n\nQ6. What kind of activities do you enjoy when traveling overseas?\nanswer\nI enjoy shopping when traveling overseas because it’s favorite thing to do and it makes me happy.\n\nQ7. Which of the following activity do you think is the most important when you travel overseas?\n\nLearning local languages, - Visiting historical spot, - Meeting local people\n\nA1. 기본형\nI think meeting local people is the most important when I travel overseas.\nFirst, I can meet foregin people and make new friends.\nAlso, I can get a lot of useful information from these new friends(local people, them).\nTherefore, I think this way.\n\nA2. 과거형\nI think meeting local people is the most important when I travel overseas.\nbecause, I can meet foregin people and make new friends.\nLast year, I traveled overseas and I met local people. I really liked it. It was awesome!\nbecause, I was able to meet foreign people and make new friends"
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#set2.-기부",
    "href": "post/etc/toeic speaking/part3.html#set2.-기부",
    "title": "part 3",
    "section": "set2. 기부",
    "text": "set2. 기부"
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#set3.-거주지",
    "href": "post/etc/toeic speaking/part3.html#set3.-거주지",
    "title": "part 3",
    "section": "set3. 거주지",
    "text": "set3. 거주지"
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#set4.-놀이공원",
    "href": "post/etc/toeic speaking/part3.html#set4.-놀이공원",
    "title": "part 3",
    "section": "set4. 놀이공원",
    "text": "set4. 놀이공원"
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#set5.-악기수업",
    "href": "post/etc/toeic speaking/part3.html#set5.-악기수업",
    "title": "part 3",
    "section": "set5. 악기수업",
    "text": "set5. 악기수업"
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#set11.-운동기구",
    "href": "post/etc/toeic speaking/part3.html#set11.-운동기구",
    "title": "part 3",
    "section": "set11. 운동기구",
    "text": "set11. 운동기구"
  },
  {
    "objectID": "post/etc/toeic speaking/part3.html#set14.-호텔",
    "href": "post/etc/toeic speaking/part3.html#set14.-호텔",
    "title": "part 3",
    "section": "set14. 호텔",
    "text": "set14. 호텔"
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html",
    "href": "post/etc/toeic speaking/part4.html",
    "title": "part 4",
    "section": "",
    "text": "표 읽는시간 45초, 답변 준비시간 문항당 3초\n10 번을 제외하고 전부 1회 청취가능\n답변시간 : 8, 9번 \\(\\to\\) 15초, 10번 \\(\\to\\) 30초"
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#표-읽는-시간",
    "href": "post/etc/toeic speaking/part4.html#표-읽는-시간",
    "title": "part 4",
    "section": "(1) 표 읽는 시간",
    "text": "(1) 표 읽는 시간\n\n전반적인 내용\n중요한 부분(반복적 키워드)\n특이사항\n유형별 중요한 포인트"
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#고정된-템플릿",
    "href": "post/etc/toeic speaking/part4.html#고정된-템플릿",
    "title": "part 4",
    "section": "(2) 고정된 템플릿",
    "text": "(2) 고정된 템플릿\n\n동일한 표현을 썼더라도 문제에 대한 정확한 답변을 했을때 고득점을 받음!`"
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#듣기-잘-듣는-법",
    "href": "post/etc/toeic speaking/part4.html#듣기-잘-듣는-법",
    "title": "part 4",
    "section": "(3) 듣기 잘 듣는 법",
    "text": "(3) 듣기 잘 듣는 법\n- 파트 4는 유사한 패턴이 반족 출제 되므로 문제를 예측해서 키워드를 캐치하자!\n- 듣기 문제 예측방법\n\n표가 힌트다 \\(\\to\\) 표가 계속 화면에 떠있음\n의문사를 잘듣자 \\(\\to\\) 8번 문제의 경우 시간, 날짜, 장소, 금액 등 고정적으로 묻는 문제들이 출제됨\n키워드 듣기 \\(\\to\\) 9, 10번의 경우, 표에 나와있는 키워드를 언급을 한다면 그 부분이 정답이 되는 경우가 많음"
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#유형별-정리",
    "href": "post/etc/toeic speaking/part4.html#유형별-정리",
    "title": "part 4",
    "section": "(4) 유형별 정리",
    "text": "(4) 유형별 정리\n\n일정표\n개인 일정표\n이력서\n면접 일정표\n수업 시간표"
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#날짜-숫자-시간",
    "href": "post/etc/toeic speaking/part4.html#날짜-숫자-시간",
    "title": "part 4",
    "section": "(1) 날짜, 숫자, 시간",
    "text": "(1) 날짜, 숫자, 시간\n\nA. 서수 읽기\n- 날짜는 ’서수’로 읽는다. ‘숫자 + th’\n- 나머지는 교재 보고 읽기 연습\n\n\nB. 전치사 사용\n\n1. 시간\n- on + 날짜/요일\n- At + 시각\n- in + 월/계절\n- From A to B\n\n\n2. 장소\n- At : 특정 장소\n- in : 방안(실내장소), 도시, 나라\n- On : 층, 거리명, 교통편\n\n\n3. 기타\n\nby/with + 사람\n\nby : 그 사람에 의해서 진행되거나 이끌어 지는 경우\nwith : 그 사람이 진행 하거나 진행하지는 않지만, 그 사람과 함께 하는 경우\n\non/about + 주제 : 둘의 딱히 구별은 없음\nfrom + 소속\nthe, as + (a) 직책\n\nas는 이력서 유형에서 경력을 말할 때!"
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#일정표-템플릿",
    "href": "post/etc/toeic speaking/part4.html#일정표-템플릿",
    "title": "part 4",
    "section": "일정표 템플릿",
    "text": "일정표 템플릿\n\nQuestion 8\n- It will be held + 시간/장소 = it wiil take place + 시간/장소\n- It will start at 시간/장소 = It will begin at 시간/장소\n- It will finish at 시각\n\n\nQuestion 9\n- No, I’m afraid that you have the wrong information, Actually, + 맞는정보\n\n시간이 부족하다면 “No, Actually, 맞는정보~~” 로 가자.\n\n\n\nQuestion 10\n- There are ___ session. First, Next, Finally, there is 일정 on 주제 by 사람\n\n\nset 1\n- 교재 126\n\n- Q8. when and where is it scheduled to take place?\n- Answer\nIt will be held on Monday, November 2nd, at Washington Convention Center\n- Q8-1. On what date does the conference take place and what time does it start?\n- Answer\nIt will be held on Monday, November 2nd and it will start at 10 a.m\n- Q8-2\nHow much is the registration fee per person if I”m going with five other people?\n- Answer\nIt’s 15 dollars per person because you can get a group discount\n\n- Q9. I’m going with some other people. so will be a group of six people. Can we get a group discount? and, if so, how much is the discounted registration fee per person?\n- Answer\nYes, you can get a group discount, and it’s 15 dollars per person.\n- Q9-1. I heard that Murphy davis will give a presentation in the aftenoon is that right?\n- answer\nNo. I’m afraid that you have the wrong information. Actually Murphy Davis will give a presentation at 11 am.\n- Q9-2. I was told that there will be a seminar by jane Morris at 1 p.m. Can you confirm that?\n- answer\nNo, actually, there was supposed to be a seminar by Jane Morris, but it has been cancled.\n- Q9-3. I’m afraid I’ll have to leave after 4 p.m. Will I miss anything?\n- answer\nYes, from 4 to 5 p.m, there is a lecture on Customer Service Skills for Small Business Owners by Susan Foster\n\n- Q10. I’m looking forward to sessions that deal with customer service. Could you give me all the details of the sessions related to customer service?\n- answer\nSure. First, at 2 p.m. there is a workshop on how to provide great customer service by Ashley Parker. Next, at 4 p.m, there is a lecture on customer service skills for small business owners by Susan Poster\n- Q10-1. I want to know about the details of the sessions that deal with customer service. Can you tell me about them?\n- answer\nSure. First, at 2 p.m. there is a workshop on how to provide great customer service by Ashley Parker. Next, at 4 p.m, there is a lecture on customer service skills for small business owners by Susan Poster\n\n\nset 3\n- 교재 128\n\n- Q8. When is the first day of the orientation, and what is the first thing on the schedule?\n- answer\nSaturday, August 15th, is the first day of the orientaton. and the first thing on the schedule is an opening address at 10 a.m\n- Q9. I heard that there will be a company tour in the morning. Am I right?\n- answer\nNo. I’m afraid that you have the wrong information. actually, the company tour will be held at 1 p.m\n- Q10, Someone told me that I would find the lectures particularly helpful. Can you give me all the details of the lectures offered at the orientation?\n- answer\nSure, There are two lectures on Sunday. First, at 9 am. there is a lecture on how to succeed in a workplace by jennifer owen. Then, at noon, there is a lecture on Teamwork is Everything by Cindy Peterson"
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#수업시간표",
    "href": "post/etc/toeic speaking/part4.html#수업시간표",
    "title": "part 4",
    "section": "수업시간표",
    "text": "수업시간표\n\n템플릿1. 금액, 등록\n- You have to pay 금액 for 강좌\n- it’s 숫지 dollars.\n-You should register(=sign up) by 등록 마감일\n- If you V, it’s ~dollars\n- It’s free\n- It’s for beginners/intermediate students/advanced students.\n\n\n템플릿 2. 수업 소개\n- There is a 과목명 class/course\n- 강사 will teach 과목명 class/coures\n- There is a class on 주제\n- 강사 will teach a class on 주제\n\n\nset 10\n- 교재 135\n\n- Q8. How much is the fee per course, and when is the deadline for registration?\n- answer\nIt’s 50 dollars per course, you have to register by December 2nd.\n-Q9. I heared that Elliot Ross will be teaching History of video Artwork. Can you confirm that?\n- answer\nNo. I’m afraid that you have wrong information. actually, Andrew Morgan will teach history of video Artworks.\n\n- Q10. I’m busy with my work these days. so I only have time on mondays and saturdays. Can you tell me all the details of the classes scheduled on mondays and saturdays?\n- answer\nSure. First, on mondays, there is a class on the importance of artworks appreciation(어프리쉬에이션) by Shellakeen from 1 to 2 pm. Next, on Saturdays. there is an oil painting for beginners by kate carter from 4 to 5 pm.\n\n\nSet 4\n- 가끔 출제되는 유형\n\n- Q8. What time does the first movie start, and what is the title of the movie?\n- answer\nIt will be held at 9 am. and the title of the movie is Seed of Evil\n- Q9. I’m a big fan of Sean Adams. and I want to watch his movie. A man in Love. I heard that the fee is 10 dollars per ticket. Is that right?\n- answer\nNo, actually, it’s a free. So you don’t have to pay for the ticket.\n- Q10. Could you tell me about all the screenings being held in the New york Theater?\n- answer\nThere are two films. First, at 11 am, there is Gloomy Fate. it’s 7 dollars per ticket. Next, at 3 p.m, there is Heaven. It’s 6 dollars per ticket."
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#개인일정표",
    "href": "post/etc/toeic speaking/part4.html#개인일정표",
    "title": "part 4",
    "section": "개인일정표",
    "text": "개인일정표\n\n템플릿 1. ~ 할것이다\n- You will depart from 출발지\n- You will arrive in 도착지\n- You will take 교통편\n- You will stay at 숙소\n- You will have lunch/ dinner/ a meeting.\n- You will give a sppech/ a lecture/ a presentation\n\n\n템플릿 2. 취소/ 연기 일정 변경\n-There was supposed to (비 서포스투) be 일정, but it has been canceled.\n- There was supposeed to be 일정, but it has been postponed.\n- There was supposed to be 일정 but it has been rescheduled to 바뀐날짜\n\n시간이 부족할 땐 It has been canceled/postponed/recheduled to 바뀐날짜만 쓸 수 있다.\n\n\n\nset 5\n\n- Q8. What time will the team meeting take place. and what we be discussing?\n- answer\nit will be held at 1 pm. and the topic will be reviewing safety issues\n-Q9. I’m planning to have a lunch appointment with my friend at noon. Would that be posssible?\n- answer\nNo, Actually it’s not possible, You have a lunch meeting with your clients, John Brown\n- Q9-1. As far as I know. I’m supposd to attend a staff meeting in the morning. Can you confirm that?\n- answer\nNo, actually, There is a staff meeting at 2 pm\n\n- Q10.I want to know about the things I have to do before noon. Can you tell me all the items scheduled before noon?\n- answer\nYes, First, at 10 am, there is a meeting with samuel Greene, sales director, next, at 11 am, there is a meeting with Shawna choi, regional(리지너) director.\n\n\nset 8\n\n- Q8. Which hotel wiil I be staying at, and what time do I arrive in Chicago on Wednesday, June 20th?\n- answer\nYou wiil stay at Glen Hotel, and You will arrive in Chicago at 10:30 am. on wednesday.\n- Q9. When I arrive in Chicao, do **I have to take a taxi to get to the hotel?\n- answer\nNo. You don’t have to, Actually, a rental car will be reserved at the airport on June 20th.\n\n- Q10. Can You tell me all the details of my schedule on Friday, June 22nd?\n- answer\nSure. First, you will depart from Chicago at 6 pm on Friday. You will take Los Angeles airlines. The flight number is 105,\nNext, You will arrive in San Antonio at 9:30 pm on Friday."
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#이력서",
    "href": "post/etc/toeic speaking/part4.html#이력서",
    "title": "part 4",
    "section": "이력서",
    "text": "이력서\n\n템플릿 1. 학력\n- He got a bachelor’s(배칠러스) degree/master’s degree in 전공 from 학교 in 년도\n\n\n템플릿 2. 경력\n- From 시작일 to 종료일, He worked at 회사명 as 직책\n- From 시작일 up to now, He has worked at 회사명 as 직책\n\n\n템플릿 3. 기타 능력\n- I think he is qualified because ~\n- he is fluent in 언어명\n- He is certified(설디파이드) in 자격\n- he has a certificate(설티피킷).\n- he has experience in 분야\n\n\nset 9\n\n- Q8. What school did he get his master’s degree from, and what was his major?\n- answer\nhe got a master’s degree in computer enginering from the university of southern california in 2016.\n- Q9. I’d like to hire an applicant who understands software enginerring management. Do you think he is a qualified applicant?\n- answer\nYes. I think he is qualified because he is certified in software engineering management.\n\n- Q9-1. We are planning to work with Chinese companies next year and I want to hire an employee who can help us with that project. Is he qualified?\n- answer\nYes, I think he is qualified because he is fluent in Chinese.\n- Q10, we want to hire a highly experienced employee. What details can you tell me about his work experience?\n- answer\nYes, First, from 2017 to 2018, he woaked at Jefferson Software as an intern, Next, from 2018 until now, he has worked at STA Eletronics as Chief Engineer."
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#면접일정표",
    "href": "post/etc/toeic speaking/part4.html#면접일정표",
    "title": "part 4",
    "section": "면접일정표",
    "text": "면접일정표\n\n템플릿 1. 면접일정\n- You will interview 사람.\n- there is an interview with 사람 from 회사명 (Who is applying for 직책).\n- he has 5 years of experience.\n- he is applying for editor position.\n\n\nset 7\n\n- Q8. What date will the job interview be held, and where are they being held?\n- answer\nIt will be held on Monday, january 10th, and they will be held in metting room A\n- Q8-1. What time does the first inteview start and who am I interviewing?\n- answer\nThe first interview will start at 9 am. and you will interview Michael West from Smart Design.\n- Q9. I’m afraid that I have an urgent video conference with my client at 10 am. Would it be a problem?\n- answer\nFortuately(폴츄널리), there was supposed to be an interview with Aiden Thomas. but it has ben canceled. (So, it wouldn’t be a problem at all.)\n\n- Q9-1. I know that I’m supposed to have an interview with aiden thomas from creative minds. Am I right?\n- answer\nNo, actually, there was supposed to be an interview with Aiden Thomas. but it has ben canceled.\n- Q10. Can you give me all the details of applicants who are currently working at Smart Design?\n- answer\nThere are two applicants(어플리컨츠). First, at 9 am. there is an interview with Michael West. he is applying for web designer position. next, at 2 pm, there is an interview with Heater Stevenson. he is applying for Graphic Designer position.\n- Q10-1. Can you tell me about the details of the applicants who are applying for a web designer posution?\n- answer\nThere are two applicants. First, at 9 am. there is an interview with Michael West from Smart Design. Next, at 1 pm. there is an interview with Dylan Paley Desi from gn Master."
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#복기",
    "href": "post/etc/toeic speaking/part4.html#복기",
    "title": "part 4",
    "section": "복기",
    "text": "복기\n\nset3, set4 처럼 예상치 못한 질문\nset5 \\(\\to\\) 사람이름, the + 직책\nset7 \\(\\to\\) 주제 명사 복수인지, 단수인지 확인(다시 풀자..)"
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html",
    "href": "post/etc/toeic speaking/part5.html",
    "title": "part 5",
    "section": "",
    "text": "- : 준비시간 45초, 답변시간 60초\n- part5 이론강의영상\n\n\n\n\n\nkeypoint : “문제에 대한 적절합 답변을 하는 것!”\n15초 이내로 문제 해석을 끝내는 것이 가장 좋음\n\n\n\n\n\n“파트 5 만능 문장, 파트3 만능 문장50가지” 를 이용하여 2문장 이상을 준비시간에 생각해내자\n파트 5 만능문장\n\n\n\n\n\n5글자 이상 되는 단어는 앞의 4-5글자만 쓰고, 동사 위주로만 필기하자.\n\n\n\n\n\n- 전반적인 순서 : 서론(결론) \\(\\to\\) 이유1 \\(\\to\\) 예시1\\(\\to\\) 이유2 \\(\\to\\) 예시2 \\(\\to\\) Therefore, I agree with the statement~\n- 할 수 있다면 만능문장과, 모범답안을 계속 모방해서 연습하자(고득점 전략!)\n\\(\\divideontimes\\) 동문서답금지 \\(\\to\\) 외운 답변을 문제에 적절하지 않게그대로 말하면 큰 실점이 된다.\n\n\n\n- 130 ~ 150 (IH) : 문제에 적합한 답변을 채점자가 알아 듣게만 하자 \\(\\to\\) 모범 답안의 70 ~ 80%의 퍼포먼스\n- 160이상 (AL 이상) : 파트5 만능 문장을 2문장 이상 다양하게 이용해서 말한다.\n- 파트 3의 만능 문장 50의 쉬운 문장들도 함께 이용한다.\n\n- 만능 문장을 적용해서 답변하는 연습 (30 set구비)\n\n130 + vs 160 : 구체성의 차이!\n\nA. IH 답변 : From my experience, I studied overseas\nB. AL 답변 : From my experience, I studied in Japan as an exchange student last year.\n\n\n\n\n\n\n\n\nThey can learn new things.\nThey can meet new people and expand their network.\nThey can have a lot of (new) experience and broaden their perspective.\nThey can’t make good decisions because they are not mature enough.\n\n\n\n\n\nThey will be distracted / if a teacher is not strict : strict(엄격한)\nThey can’t focus on their studies/work.\nThey can’t get good grades at school.\nThey will fall behind in class.\nThey can’t work efficiently.\n\n\n\n\n\nThey can save money.\nThe cost of living is too high.\nThey can’t make a living.\nI can get a high(er) salary.\nThe cost of N is too expensive.\nIt’s waste of money.\nThat’s good investment because it makes lives better.\n\n\n\n\n\nThey can focus better.\nThey will not be distracted by others.\nThey can set their own schedule.\nThey can have more freedom.\nThey feel more comfortable.\nIt’s fun and entertaining.\nThey can get information and share it with other people.\nIt feels more like a family.\n\n\n\n\n\nThey can get a lot of (useful information/latest information) on the Internet.\n\n기기나 인터넷 상에서 하는 것 앞에는 전치사 ‘on’ 을 쓴다.\n\nThey can communicate with their friends anytime anywhere on their smartphones.\nIt’s faster and convenient.\nThere is a lot of inaccurate information on the internet so it’s not reliable.\nit is very distracting for S so S can’t focus on their studies/work.\nI can get responses right away\nI can understand the feeling of the speaker more accurately.\n\n\n\n\n\nThey can make a friendly (work) atmosphere.\nThey can communicate with others better.\nThey can be good team players and make good relationships with others.\nThey can have a good reputation.\nThey can be very influential.\nThey can motivate others.\nEverything is always changing and there is a lot of competition.\nThey face a lot of challenges and difficulties.\nHe is able to handle a variety of situation due to his N.\nThey have a lot of experience/knowledge.\n\n\n\n\n\nEmployees can work more efficiently and productively.\nEmployees can be more satisfied with their jobs.\nIt can make a better work environment.\nThey might appear less professional.\nCustomers will feel satisfied and remain loyal.\nIt will attract more customers.\nThe business will be more successful.\nPeople frequently use N so it will be very effective.\n\n\n\n\n\nIt relieves their stress and they can relax.\nIt is good for their (Physical / mental) health.\nIt is not good for their health.\n\n\n\n\n\nIt is good for environment.\nPollution is a serious issue these days.\nIt can make a cleaner environment.\nWe will be able to protect the environment."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#서론",
    "href": "post/etc/toeic speaking/part5.html#서론",
    "title": "part 5",
    "section": "(1) 서론",
    "text": "(1) 서론\n- 서론 문장은 문제에 맞게 결론부터 말한다.\n- 유형별 빈출 서론문장\n1. 동의/비동의 : agree/disagree with the statement.\n\n2. 장/단점 : There are some advantages/disadvantages of N.\n\n3. 옵션 선택 : I think N is the most important.\n\n4. Do you think S + V : I think that S + V / I don't think S + V\n\n5. 선호도 질문 : I prefer A to B"
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#연결문장",
    "href": "post/etc/toeic speaking/part5.html#연결문장",
    "title": "part 5",
    "section": "(2) 연결문장",
    "text": "(2) 연결문장\n- Let me explain why I think this way."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#이유-1",
    "href": "post/etc/toeic speaking/part5.html#이유-1",
    "title": "part 5",
    "section": "(3) 이유 1",
    "text": "(3) 이유 1\n- 첫 번째 이유는 만능 문장을 이용해서 말하되, 문장에 맞게 적절하게 변형해서 이용한다.\n\nexample : Most of all, 첫번째 이유 S + V \\(\\to\\) 무엇보다도, 첫번째 이유"
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#예시-1",
    "href": "post/etc/toeic speaking/part5.html#예시-1",
    "title": "part 5",
    "section": "(4) 예시 1",
    "text": "(4) 예시 1\n- 첫 번째 예시에서는 경험 이야기를 하자.\nFrom my experienece, S + V\n- 경험 세부사항 말하는 법\n1) For me, it was very helpful because S + V\n\n2) It was great!, because \n\nex) For me, it was not helpful at all : 나에게 전혀 도움이 되지 않았다."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#이유-2",
    "href": "post/etc/toeic speaking/part5.html#이유-2",
    "title": "part 5",
    "section": "(5) 이유 2",
    "text": "(5) 이유 2\nOn top of that, 두 번째 이유 (if / When S + V) \\(\\to\\) 게다가 ~ 한다면, ~ 할 때 두 번째 이유다."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#예시-2",
    "href": "post/etc/toeic speaking/part5.html#예시-2",
    "title": "part 5",
    "section": "(6) 예시 2",
    "text": "(6) 예시 2\n- 뉴스기사와 전문가들의 견해를 연결하여 말하자.\nAccording to a recent news report,. the majority of 사람들 in korea said that S + V (if / when S + V)\n- 위 S + V 에는 주장을 쓰자 (문제와 내 노트 테이킹 한거를 다시 말하기!)"
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#결론",
    "href": "post/etc/toeic speaking/part5.html#결론",
    "title": "part 5",
    "section": "(7) 결론",
    "text": "(7) 결론\n- Therefore, 서론"
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#경험-교육",
    "href": "post/etc/toeic speaking/part5.html#경험-교육",
    "title": "part 5",
    "section": "(1) 경험 & 교육",
    "text": "(1) 경험 & 교육\n\nset2\n- 교재 154\nDo you agree or disagree with the statement :\nTeenagers Should get free admission to art galleries\n\nanswer\n- keyword: learn, expand, experience, broaden\n- 서론 : I agree with the statement\n- 연결문장 : Let me explain why I think this way.\n- 이유1\nMost of all, Teenagers can learn new things about the arts (만능문장1번)\n- 예시 1\nFrom my experienece, When I was a high school student, I got free admission to art gallery.(문제 질문을 그대로 인용)\nFor me, It was very helpful because I learned new things about the arts and art history (만능문장 1번 변형)\n- 이유 2\nOn top of that, they can have a lot of new experience and broaden their perspective.(만능문장 3번) when they visit art galleries.\n- 예시 2\nAccording to a recent news report, the majority of education experts in korea said that visiting art galleries(for free) is very beneficial for teenagers because they can experience many things and broaden their persfectives. (만능문장 3번 변형)\n- 결론\nTherefore, I think this way.\n\n\n\nset1\n- 교재 153 page\nWhat are advantages of learning how to play musical instruments for teenagers?\nUse specific reasons and examples to suppport your opinions.\n\nanswer\n- keyword: learn, expand, experience, broaden\n- 서론\nThere are some advangtages of learning how to play musical instruments for teenagers.(서론 만능 템플릿!)\n- 연결문장\nLet me explain why I think this way.\n- 이유 1\nMost of all, teenagers can learn new things such as music skills (만능문장 1번 변형)\n- 예시 1\nFrom my experience, When I was a highschool student, I took a piano lesson.\nFor me, It was very helpful because I learned new things such as piano skills and music skiils (이유 1에서 이용한 만능문장 그대로 사용)\n- 이유 2\nOn top of that, they can have a lot of new experience and broaden their perspective.(만능문장 3번)\n- 예시 2\nAccording to a recent news report, the majority of education experts in korea said that learnging how to play musical instruments is very beneficial for teenagers because they can have a lot of new experience and broaden their perspective (만능문장 3번)\n- 결론\ntherefore, I think this way\n\n\n\nset 24\n- 교재 187\nDo you agree or disagree with the following statement : Participating in an international internship is the best way to imporve one’s work performance\n\nanswer\n- keyword : hav/exper, broad/per, meet, expand/net, learn\n- 서론\nI agree with the statement\n- 연결문장\nLet me explain why I think this way.\n- 이유 1\nMost of all, if employees participate in an international internship, they can have a lot of experience and broaden their perspective (만능문장 3번 변형)\nAlso, they can meet new people and expand their business network overseas. (만능문장 2번 변형)\n- 이유 2\nOn top of that, employees can learn new things such as language skiis and culture.(만능문장 1번 변형)\n- 예시\nAccording to a recent news report, the majority of succesful CEOS in korea said that Participating in an international internship is very beneficial for employees because they can improve theirs work performance.\nTherefore, I think this way."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#집중",
    "href": "post/etc/toeic speaking/part5.html#집중",
    "title": "part 5",
    "section": "(2) 집중",
    "text": "(2) 집중\n\nset 3\n- 교재 156\nDo you agree or disagree with the statments :\nElementary school studentes should not be allowed to have their own smartphones at school.\nuse specific reasons and examples to support tour opinions.\n\nanswer\n- keyword : distracted, focus/study, work/effi, can’t get grades\n- 서론 및 연결문장\nI agree with the statement.\nLet me explain why I think this way.\n-이유 1\nMost of all, If elementary school students are allowed to have their own smartphones at school (문제는 우리의 참고서!), they wiil be distracted and they can’t focus on their studies (만능문장 5번 6번)\n- 예시 1\nFrom my exeperience, When I was an elementary school student, I had my own smartphones.(문제를 꼼꼼히 보자)\nFor me , it was not helpful at all becasue I was distracted by my smartphones. So, I couldn’t focus on my studies. (주장을 한번 더 부각하고, 만능문장 6번 변형)\n- 이유 2\nOn top of that, according to a recent news report, the majority of education experts in korea said that elementary school students should not be allowed to have their smartphones at school because they can be distracted and can’t learn efficiently.(만능문장 5번 9번)\nTherefore, I agree with the statement.\n\\(\\divideontimes\\) 위 문제 같은 경우는 서론-이유1-예시1-예시2-결론 형태이다. 익숙해지면 나에게 편한 방식으로 뱉어보자"
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#돈",
    "href": "post/etc/toeic speaking/part5.html#돈",
    "title": "part 5",
    "section": "(3) 돈",
    "text": "(3) 돈\n\nset 4\n- 교재 158\nwhen you choose a place to work, which of the following is the most important?\n- a high salary\n- a flexible working schedule\n- more opportunities to work abroad\n\nanswer\n- keyword : save/money, cost/high, make/living, satisfied, work/effi\n- 서론\nI think a high salary is the most important.\nLet me explain I why think this way.\n- 이유 1\nMost of all, I can save money. (만능문장 10번)\nAs you know, the cost of living is too high these days. So, I can’t make a living without a high salary (만능문장 11, 12)\n- 이유 2\nOn top of that, I can be more satisfied with my job if I earn(get) a high salary.(만능문장 45)\n- 예시 2\nAccroding to a recent news report, the majority of offices workers in korea said that getting a high salary is very important (서론이용)\nbecause they can be more satisfied with their jobs and they can work more efficiently and productively when they get a high salary. (만능문장 45, 44)\ntherefore, I think a high salary is the most impoartant.\n\n\n\nset 5\n- 교재 159\nDo you agree with the following statement :\nLocal goverments should provide more financial support for making more green spaces such as parks.\nuse specific reasons and examples to support your opinons.\n\nanswer\n- keyword : investment, make/ppl’s lives, waste, good 4 env, protect\n- 서론\nI agree with the statement.\nLet me explan why I think this way.\n- 이유 1\nMost of all, making more green spaces such as parks is a good investment because it makes people’s lives better (만능문장 16) because It’s good for people’s health. so it’s not a waste of money. (만능문장 15, 53)\n- 이유 2\nOn top of that, making green spaces is good for the environment. (만능문장 55)\nPolloution is a serious issues these days,(만능문장 56). if the local goverments provide more financia(빠이낸셜) support for making green speaces, we will be able to protect the environment.(만능문장 58)\nTherefore, I agree with the statement."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#혼자-vs-같이",
    "href": "post/etc/toeic speaking/part5.html#혼자-vs-같이",
    "title": "part 5",
    "section": "(4) 혼자 vs 같이",
    "text": "(4) 혼자 vs 같이\n\nset 6\n-교재 161\nWhen you do projects, do you prefer to work with the people in your own department everytime or collaborate with employees of other departments? why?\nuse specific reasons and examples to supoort your opinions.\n\nanswer\n- keywords : get/inf, share/ppl, meet/ppl, expand/net\n- 서론\nWhen I do projects, I prefer to collaborate with employees of other departments.\nLet me explain why I think this way.\n- 이유 1\nMost of all, I can get information from employees from other departments and share it with other people. (만능문장 23)\n- 예시 1\nFrom my experience, I used to work at a company. At that times, I collaborated with employess from other departments. For me, It was very helpful because I was able to get new information. (만능문장 25)\n- 예시 2\nOn top of that, I was able to meet new people from other departments and expand my network. (만능문장 2) (시간이 부족하면 생략가능)\nTherefore, When I do projects, I prefer to collaborate with employees of other departments.\n\n\n\nset 7\n- 교재 162\nwhat are some advantages of taking a class that provides one-on-one teaching rather than taking a big class for college student?\nuse specific reasons and examples to support your opinions.\n\nanswer\n- keywords : focus, not distract, set/schedule, feel/freedom\n- 서론\nthere are some advantages of taking a class that providing one-on-one teaching.\nLet me explain why I think this way.\n- 이유 1\nMost of all, Students can focus better and they will not be distracted by other students if a class provide one-on-one teaching. (만능문장 17, 20)\n- 예시 1\nFrom my experience, when I was college a student, I took a class that provided one-on-one teaching.\nFor me. it was very helpful because I was able to focus better and not be distracted by other students.(I feel more comfotable \\(\\to\\) 만능문장 21)\n- 이유 2\nOn top of that, students can set their own schedule(만능문장 19) if they take a class that provides one-on-one teaching. Then, They can have more freedom.\nTherefore, I think this way."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#기술",
    "href": "post/etc/toeic speaking/part5.html#기술",
    "title": "part 5",
    "section": "(5) 기술",
    "text": "(5) 기술\n\nset 8\n- 교재 165\nWhich of the following inventions do you think improved the quality of our lives more?\n\nsocial media, TV\n\n\nanswer\n- keywords : comm/any/ppl, get/inf\n- 서론\nI think social media improved the quality of our lives more.\nLet me explain why I think this way.\n- 이유 1, 예시 1\nMost of all, people can communicate with other people anytime anywhere on social media. (만능문장 26)\nFrom my experience, I use social media when I communicate with my friends.\nFor me, It is very helpful because i can communicate with my friends anytime anywhere, and It’s faster and more convenient.\n- 예시 2\nAccording to a recent news report, the majority of people in korea said that social media improved the qulity of our lives because they can get the latest information on social medial.\ntherefore, I think this way.\n\n\n\n\nset 10\n- 교재 167\nWhen having a meeting with clients at work, which one do you think is a more efficient way?\n\nhaving a video conference, - talking face-to-face\n\n\nanswer\n- keywords : under/feeling, builds, satisfied\n- 서론\nI think talking face-to-face is more efficient way.\nLet me explain why I think this way.\n- 이유 1\nMost of all, employees can understand the feeling of the clients more accurately. so it causes less misunderstanding. (만능문장 33)\n- 예시 1\nFrom my experience, I used to work at a company, and I had video conference with my clients.\nFor me, It was not helpful at all because we couldn’t talk face-to-face.\n- 이유 2\nOn top of that, talking face-to-face is more personal and builds a closer relationship with clients.\n- 예시 2\nAccording to a recent news report, the majoriy of successful CEOs in korea said that talking face-to-face with clients is very important because it builds a closer relationship with clients and clients will be more satisfied.\nTherefore, I think this way."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#특징",
    "href": "post/etc/toeic speaking/part5.html#특징",
    "title": "part 5",
    "section": "(6) 특징",
    "text": "(6) 특징\n\nset 11\n- 교재 170\nWhich of the following is the most important quality of a good business leader?\n\nKnowing how to deal with conflicts\nknowing how to listen to others\norganizational skills\n\n\nanswer\nI think knowing how to listen to others is the most important.\nlet me explain why I think this way.\nMost of all, leaders can communicate with other employees better when they know how to listen to others.\nFrom my experience, I used to work at a company, and my manager knew how to listen to others. It was great because he was able to commnuicate with other employees very well.\nHe was able to be a good team player and make good relationships wtih other employees.\nOn top of that, accroding to a recent news report. the majority of successful CEOs in korea said that knowing how to listen to others is very imortant for leaders because those leaders can make a friendly work atmosphere. (만능문장 34)\nTherefore, I think knowing how to listen to others is the most important.\n\n\n\nset 12\n- 교재 171\nDo you agree or disagree with the following statement : New employees should be creative in order to succeed in their careers.\n\nanswer\n- keywords : handle, every/chaining/competition, face/challenges/diff, deal with\nI agree with the statement.\nLet me explain why i think this way.\nMost of all, creative employees are able to handle a variety of situation due to their creativity. (만능문장42)\nEverything is always changing, and there is a lot of competition, and they face a lot of challenges and difficulties.(만능문장 40, 41)\nIf new employees are creative, they can handle a variety of situation (due to creativity. 생략가능)\nOn top of that, according to a recent news report, the majority of successful CEOs in korea said that creativity is very important for new employeees because they can deal with problems more effectively if they are creative.\nTherefore, I agree with the statement.\n\n\n\nset 13\n- 교재 172\nDo you think pop stars can be good role models for other people?\n\nanswer\n- keywords : hav/reputation, influe, motivate, bts\nI agree with the statement.\nLet me explain why I think this way.\nMost of all, pop stars can have a good reputation, and they can be very influential. So, they can motivate other people.(만능문장 37, 38, 39).\nFor example, the korean pop group, BTS has a good reputation and is very influential. BTS motivates a lot of teenagers.\nOn top of that, according to a recent news report, the majoriy of teenagers in korea said that pop stars are their good role models because pop stars motivate teenagers.\nTherfore, I agree with the statement."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#업무환경기업성공",
    "href": "post/etc/toeic speaking/part5.html#업무환경기업성공",
    "title": "part 5",
    "section": "(7) 업무환경/기업성공",
    "text": "(7) 업무환경/기업성공\n\nset 14\n- 교재 174\nDo you think telecommuting is better than working at an office?\n\nanswer\n- keywords : work/eff/prod, feel/comfor, make g env, satisfied\nI think telecommuting is better than working at an office.\nLet me explain why I think this way.\nMost of all. employees can work more efficiently and productively.(만능문장 44)\nFrom my experience. I used to work from home. For me, it was very helpful becasue I felt very comfortable at home. It made a better environment, so I worked more efficiently and productively (만능문장 44, 46 혼합).\nAlso, I was more satisfied with my job when I worked from home (만능문장 45).\nOn top of that, employees can be more satisfied with their jobs if they work from home.\nAccording to a rencent news report, the majority of office worker in korea said that they prefer to work from home.\ntherefore, I think telecommuing is better than working at an office.\n\n\n\nset 15\n- 교재 175\nWhat is the most important quality of start-up company should have in order to succeed in a market?\n\ngood location\nvariety of products\ngreat customer service\n\n\nanswer\n- keywords : satisfied, ramain loyal, attract\ni think a great customer service is the most important.\nLet me explain why I think this way.\nMost of all, customers will feel satisfied and remain loyal if a start-up company offers great customer service. (만능문장 48)\nFrom my experience, I visited a new restaurant in my town, and they offerd great customer service. I was very satisfied and became a regular customer.\nOn top of that, according to a recent news report, the majority of successful CEOs in korea said that a great customer service is very important in order to succeed in a market because it can attract a lot of customers.\ntherfore, I think a great customer service is the most important."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#스트레스건강",
    "href": "post/etc/toeic speaking/part5.html#스트레스건강",
    "title": "part 5",
    "section": "(8) 스트레스/건강",
    "text": "(8) 스트레스/건강\n\nset 16\n- 교재 177\nWhich one do you think is the most important subject for high school students to learn?\n\nphysical education\nmath and science\narts and crafts\n\n\nanswer\n- keywords : reli, relax/str, be good 4 health\nI think Physical education is the most important.\nLet me explain why I think this way.\nMost of all, physical education relives their stress and they can relax. (만능문장 52)\nFrom my experience, When I was a high school student, I had a lot of stress.\nhowever, I took a physical education classes, and they relived my stresss, so I was able to relax. It was good for my mental health.\nOn top of that, it is good for their physical health. (만능문장 53)\nAccording to a recent news report, the majoriy of doctors in korea said that physical education is very beneficial for high school students because it is good for their health. (만능문장 54)\ntherefore, I think this way.\n\n\n\nset 17\n- 교재 178\nDo you agree or disagree with the following statement :\nVending machines should not be allowed in cafeterias(캐퍼티어리어스) of schools or workplaces.\n\nanswer\n- keywords : not good 4 health, develop unhealthy\nI agree with the statement.\nLet me explain why I think this way.\nMost of all, It is not good for people’s health.\nFrom my experience, when I was a high school student, There was a vending machine in my school cafeteria, and I used to buy sodas every day.\nFor me, it was not helpful at all. It developed unhealthy habits.\nOn top of that, accroding to a recent news report, the majority of doctors in korea said that vending machines should not be allowed in cafeterias because people can develop unhealthy habits.\ntherefore. I agree with the statement.\n\n\n\nset 18\nWhich of the following pastime activities would be the best way to relieve people’s stress\n\nplaying mobile game\nknitting\nwalking a dog\n\n\nanswer\n- keywords : fun/entertan, don’t get bored, relive/relax str\nI think playing mobile game is the best way to relieve people’s stress\nLet me explain why I think this way.\nMost of all. It’s more fun and entertaining more knitting and walking a dog\nFrom my experience, I like to play mobile game in my free time. it’s great because it’s fun and entertaining, and I don’t get bored.\nOn top of that, mobile games are very popular, so people love them.\nAccording to a recent news report, the majority of people in korea said that playing mobile games is the best way to relieve their stress. because it makes them happy and they can forget about their worries\ntherefore, I think this way"
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#환경",
    "href": "post/etc/toeic speaking/part5.html#환경",
    "title": "part 5",
    "section": "(9) 환경",
    "text": "(9) 환경\n\nset 19\n- 교재 181\nDo you think the goverment should make laws to have more people use public transportation?\n\nanswer\n- keywords : be good 4 env, pollu is serious, make cleaner env, protect\nI agree with the statement.\nLet me explain why I think this way.\nMost of all, it is good for the environment.(만능문장 55)\nPollution is a serious issue these days. (만능문장 56)\nif the goverment makes laws to have more people use public transportation, it can make a cleaner environment. (만능문장 57)\nThen, we will able to protect environment.(만능문장 58)\nOn top of that, According to a recent news report, the majority of environmental experts in korea said that public transfortation is very important because it is good for the environment. (만능문장 55)\ntherfore, I think the goverment should make laws to have more people use public trasportation."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#과거-현재-비교",
    "href": "post/etc/toeic speaking/part5.html#과거-현재-비교",
    "title": "part 5",
    "section": "(10) 과거 현재 비교",
    "text": "(10) 과거 현재 비교\n\nset 20\n- 교재 182 ~ 183\nDo you agree or disagree with the following statement :\nthese days, people spend more time using their cell phones than in the past.\n\nanswer\n- keywords : anytime~, spend\nI agree with the statement.\nLet me explain why I think this way.\nMost of all, thanks to technology, people can do everything anytime anywhere on their smartphones.\nSo, people spend more time using their smartphones than in the past.\nHowever, in the past, they didn’t have smartphones, so they were not able to do many things on their smartphones.\nSo, people spent less time using their cell phones than now.\ntherefore, I agree with the statement.\n\n\n\nset 21\n- 교재 184\nWhen you work at a company as a new employee, which one do you think is the best option.\n\na bonus\na long vacation\nflexible working hours\n\n\nanswer\n- keywords : relive, relax, work/effi/prod, satisfied\nWhen I work at a company as a new employee, I think a long vacation is the best option for me.\nLet me explain why I think this way.\nMost of all, it relieves my stress, and I can relax if I have a long vacation.\nFrom my experience, I used to work at a company, and the company offered a long vacation for employees.\nFor me, it was very helpful because I was able to relax and work more efficiently and productively after the vacation. I was able to be more satisfied with my job.\nOn top of that, employeess can learn new things during the vacation.\naccording to a recent news report, the majority of successful CEOs in korea said that a long vacation is very beneficial for employees because they can learn new thing and broaden their pespectives. (during the vacation)\ntherefore. I think this way.\n\n\n\nset 22\n- 교재 185\nDo you think high school students should have organizational skills to do well at school? why?\n\nanswer\n- keywords : save/time, get/grades, not distract, focus\nI think high school students should have organizational skiils to do well at school.\nlet me explain why I think this way.\nMost of all, they can save time if they have organizational skills.\nAs you know, there is a lot of competition at school.\nHowever, if they have organizational skiils and can save time, they can get good grades at school and will not fall behind in class.\nOn top of that, students can set their own schedule better if they have organizational skills.\nThen, they will not be distracted by other things and they can focus better on their studies.\ntherfore, I think this way."
  },
  {
    "objectID": "post/etc/toeic speaking/summary.html",
    "href": "post/etc/toeic speaking/summary.html",
    "title": "summary",
    "section": "",
    "text": "Affordable (어포러블)\nAppliances (어플라이언시스)\nProperty (프로펄리)\nExecutive (이그제키디브??)\ncompetitive (컴페더디브)\nAlternate (얼터너트)\nPurchase (펄쳐스)\nRecord (V 리코드, N 레코드)\nInterior\nApproximately\nRepersentaitve (레프레젠테티브)\nHesitate\nApplause (어플로우즈)\nAttorney (어 thㅗㄹ 니)\nreward\nModerate. (머더리트)\nPedstrians (퍼데스트리언스)\nquarter/quarterly\nHumid / Humidity\nGroceries (그로우서리스)\nTremendous (트레멘데스)\nNutritious (뉴트리셔스)\nExterior\nCuisine (쿠이진)\nMaintenance (메인티넌스)\nAwareness (어웰너스)\nExothic (이그조릭)\nResume (레저미)\nTemporaray / Temporarily (템포러ㄹ리, 템포렐러리)\nLodging (로징)\nUnforgettable (언폴게러블)\nRenowned (리노운ㅌ)\nAppropriate (어프러푸레이트)\nStrategies (스트레드지스)\nProcedure\nOrganize / Organization (올게나이즈, 올가니제이션)\ninformative (인폴메티브)\nAssociate\nSculpture (스컬ㅍ쳐)\nImmediate / Immediately (이미디어트, 이미디엍리)\nRegion (리즌)\nVehicle (비이클)\nCreative. (크리에이리브)\nLatest (레이리스트)\ncashier (캐시얼)\nModern (모어런)\nContemporary\nCurrently(커ㄹㅌ리)\nCooperation\nEnvironmental\nReception (리셒션)\nInterested / Interesting"
  },
  {
    "objectID": "post/etc/toeic speaking/summary.html#경험-교육",
    "href": "post/etc/toeic speaking/summary.html#경험-교육",
    "title": "summary",
    "section": "1. 경험 & 교육",
    "text": "1. 경험 & 교육\n\nThey can learn new things (such as ~, about ~)\nThey can meet new people and expand their network \\(\\to\\) (회사 인턴십, 워크숍, 다른부서하고 일할 때)\nThey can have a lot a experience and broaden their perspectives. \\(\\to\\) (박물관, 해외 여행, 회사 인턴십, 워크숍, 다른부서하고 일할 때)\nThey can’t make good decisions because they are not mature enough.\nIt makes me happy and I can have a great experience.\nIt makes me happy and I can forget about my worries.\nThey provide a happy environment and a pleasant experience.\nIt relives my stress. I’m stressed out these days. So, I need this. (발음 듣기)\nIt relives their stress and they can relax.\nIt’s good for their physical/mental health.(템플릿 발음)\nIt’s my favorite thing to do."
  },
  {
    "objectID": "post/etc/toeic speaking/summary.html#집중",
    "href": "post/etc/toeic speaking/summary.html#집중",
    "title": "summary",
    "section": "2. 집중",
    "text": "2. 집중\n\nThey will be distracted. (by others) \\(\\to\\) (스마트폰, 그룹스터디 등등)\nThey can’t focus on their studies/works \\(\\to\\) (스마트폰, 그룹스터디 등등)\nThey can’t get good grades at school. \\(\\to\\) (스마트폰, 그룹스터디 등등)\nThey fall behind in class. \\(\\to\\) (스마트폰, 그룹스터디 등등)\nThey can’t work efficiently \\(\\to\\) (스마트폰, 그룹스터디 등등)\nI feel more comfortable and I can focus better.\nI don’t have to waste time waiting for other people.(others) \\(\\to\\) 혼자 쇼핑"
  },
  {
    "objectID": "post/etc/toeic speaking/summary.html#돈-시간",
    "href": "post/etc/toeic speaking/summary.html#돈-시간",
    "title": "summary",
    "section": "3. 돈, 시간",
    "text": "3. 돈, 시간\n\nIt’s cheaper, so I can save money. \\(\\to\\) (연봉, 보너스)\nThe price is reasonable. \\(\\to\\) (연봉, 보너스, 템플릿발음)\nThe cost of living is too high, so employees can’t make a living (without bonus, a high salary) \\(\\to\\) (연봉, 보너스)\nI can get a high salary.\nIt’s waste of money/time.\nThat’s a good investment because It makes lives better.\nI’m a student so my budget is tight.\nI can’t afford to buy expensive things.\nI don’t want to waste (too much) money(time) on that.\nI’m a student so I’m busy with my school work.\nI don’t have much time.\nIt’s faster, so I can save time."
  },
  {
    "objectID": "post/etc/toeic speaking/summary.html#혼자-같이-기술",
    "href": "post/etc/toeic speaking/summary.html#혼자-같이-기술",
    "title": "summary",
    "section": "4. 혼자 & 같이 & 기술",
    "text": "4. 혼자 & 같이 & 기술\n\nThey can focus better. (if, when ~)\nThey can set their own schedule.\nThey can have/feel more freedom/comfortable. (at home) \\(\\to\\) (혼자 일할 때)\nIt’s fun and entertaining. so I don’t get bored.\nThey can get a lot of useful information (from my friends, books, on the Internet) and share it with others.\nI think it’s more fun to do things in a group.\nI can meet new people and make friends (expand their network.)\nI can save time because I don’t have to waste time going out.\nThey can do everything anytime anywhere on their smartphones.\nThey can communicate with their friends anytime anywhere on social media.\nIt’ faster and convinient.\nThere is a lot of inaccurate information on the internet so it’s not reliable.\nIt’s reliable, so the information is more trustworthy.\nIt is very distracting for student because they can’t focus on their studies/work.\ni can get responses right away.\nI can understand the feeling of speaker more accurately. \\(\\to\\) (대면 수업, 일대일 강의)\nIt is more personal, and bulids a closer relationship.\nIt causes less misunderstanding.\nThey can make a friendly atmosphere.\nThey can communicate with others better.\nThey can be good team players and make good relationships with others."
  },
  {
    "objectID": "post/etc/toeic speaking/summary.html#좋은-성품-성격",
    "href": "post/etc/toeic speaking/summary.html#좋은-성품-성격",
    "title": "summary",
    "section": "5. 좋은 성품, 성격",
    "text": "5. 좋은 성품, 성격\n\nThey can have a good reputaion.\nThey can be very influential. (템플릿 발음)\nThey can motivate others."
  },
  {
    "objectID": "post/etc/toeic speaking/summary.html#지식-능력-재능",
    "href": "post/etc/toeic speaking/summary.html#지식-능력-재능",
    "title": "summary",
    "section": "6. 지식, 능력, 재능",
    "text": "6. 지식, 능력, 재능\n\nEverything is always chainging and There is a lot of competition.\nThey face a lot of challenges and difficulties. (템플릿 발음)\nHe is able to handle a varity of situation due to his creativity. (템플릿 발음)\nThey have a lot of experience / knowledge.\n\n\\(\\divideontimes\\) They can give me some good advice."
  },
  {
    "objectID": "post/etc/toeic speaking/summary.html#업무-환경-기업-성공",
    "href": "post/etc/toeic speaking/summary.html#업무-환경-기업-성공",
    "title": "summary",
    "section": "7. 업무 환경 & 기업 성공",
    "text": "7. 업무 환경 & 기업 성공\n\nEmployees can work more efficiently and productively (if company offer a high salary)\nEmployees can be more satisfied with their jobs.\nIt can make a better work environment.\nCustomers will feel satisfied and remain loyal.(They will become regular customer.)\nIt will attract more customers.\nThe business will be more successful.\npeople frequently use it so it will be very effective.\nIt has great facilities.\nIt’s a well-liked place so people love it.\n\n\\(\\divideontimes\\) It can develop healthy/unhealthy habits. (건강에 안좋은 음식 문제?)"
  },
  {
    "objectID": "post/etc/toeic speaking/summary.html#환경",
    "href": "post/etc/toeic speaking/summary.html#환경",
    "title": "summary",
    "section": "8. 환경",
    "text": "8. 환경\n\nIt’s good for the enviorment.\nPollution is a serious issue these days.(if ~, it can makes a cleaner environment)\nWe will able to protect the environment."
  },
  {
    "objectID": "post/etc/toeic speaking/summary.html#기타",
    "href": "post/etc/toeic speaking/summary.html#기타",
    "title": "summary",
    "section": "9. 기타",
    "text": "9. 기타\n\nThey are too old/outdated so I think it’s good to have new ones.\nIf we have more stores here, it would be more convinient.\nIt’s very necessary for me. (템플릿 발음)\nIt’s popular item so people will love it.\nI like to try new things.\nIt’s a good gift (템플릿 발음).\nit has sentimental value.\nIt’s part of my routine.\nIt’s my habit.\nI really liked it.\nIt was great/awesome."
  },
  {
    "objectID": "post/Lecture/IP/2023-05-16-01wk.html",
    "href": "post/Lecture/IP/2023-05-16-01wk.html",
    "title": "01. 자료형",
    "section": "",
    "text": "- 0차원 자료형 : int, float, bool\n- 1차원 자료형 : str, list, tuple, dict, set\n\n\n\na = 100\ntype(a)\n\nint\n\n\n\n\n\n\na = 1.2*3\ntype(a)\n\nfloat\n\n\n\na?\n\n\n\n\n\na = True\nb = False\n\n\ntype(a),type(b)\n\n(bool, bool)\n\n\n\na + b\n\n1\n\n\n\n\n\n\na = 1 + 2j\n\nb = 2-2j\n\n\nc = a + b\n\n\nc\n\n(3+0j)\n\n\n\ntype(a), type(b), type(c)\n\n(complex, complex, complex)\n\n\n\n\n\n\n\n\na = 3.4\n\n_a = int(a)\n\n\ntype(a), type(_a)\n\n(float, int)\n\n\n\na, _a\n\n(3.4, 3)\n\n\n- 정보의 손실이 발생한 것임\n\n\n\n\na = 3\n_a = float(a)\n\n\ntype(a), type(_a)\n\n(int, float)\n\n\n\na, _a\n\n(3, 3.0)\n\n\n\n\n\n- 예시 1\n\na = True\n_a1 = float(a)\n_a2 = int(a)\n\n\ntype(a), type(_a1), type(_a2)\n\n(bool, float, int)\n\n\n- 예시 2\n\na1 = 1\na2 = 1.0\n\n_a1 = bool(a1)\n_a2 = bool(a2)\n\n\ntype(a1), type(a2), type(_a1), type(_a2)\n\n(int, float, bool, bool)\n\n\n\n_a1,_a2\n\n(True, True)\n\n\n\n\n\n\nbool(-3.14)\n\nTrue\n\n\n\nint(3.14)\n\n3\n\n\n- 형태변환이 항상 가능한 것도 아님\n\nfloat(3+0j)\n\nTypeError: can't convert complex to float\n\n\n\n\n\n\nTrue + 1 \n\n2\n\n\n\n1*1.0\n\n1.0\n\n\n\nTrue + True\n\n2\n\n\n\n\n\n\n\na = \"gangcheol\"\n\n\na\n\n'gangcheol'\n\n\n\n\n\na = \"X\"\n\nb= \"2\"\n\na+b\n\n'X2'\n\n\n\n\n\n\na = \"X\"\n\nb= \"2\"\n\na-b\n\nTypeError: unsupported operand type(s) for -: 'str' and 'str'\n\n\n\n\n\n\na*3\n\n'XXX'\n\n\n\nb=2\na*b\n\n'XX'\n\n\n- 이런건 수행되지 않음.\n\na = \"X\"\nb = \"Y\"\na*b\n\nTypeError: can't multiply sequence by non-int of type 'str'\n\n\n\n\n\n\na/b\n\nTypeError: unsupported operand type(s) for /: 'str' and 'str'\n\n\n\n\n\n\na = \"gangcheol\"\n\n\na[0]\n\n'g'\n\n\n\na[-1]\n\n'l'\n\n\n\na[1:-1]\n\n'angcheo'\n\n\n\n\n\n\na = \"ABCD\"\nb = \"efgh\"\n\n\na,b\n\n('ABCD', 'efgh')\n\n\n\na.lower(),b.upper()\n\n('abcd', 'EFGH')"
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/Supervised Learning.html",
    "href": "post/Lecture/RFD/데이터마이닝/Supervised Learning.html",
    "title": "01. Supervised Learning",
    "section": "",
    "text": "절차 : 목적 설정 -> 데이터 준비 -> 가공 -> 기법적용 -> 검증\n세부적인 절차는 아래와 같다.\n\n\n데이터를 로드 한 후 데이터의 형(type), 기초통계량 확인\n훈련용 데이터와 테스트 데이터를 분류\n모형 적합 후 모형확인 -> 유의하지 않은 변수가 많을 경우 step 함수를 이용하여 적절히 변수선택법을 사용\n모형 적합 후 검증 : ROC curve, AUC, 정확도, 특이도, 민감도 등을 확인"
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/Supervised Learning.html#r을-이용한-이항-로지스틱",
    "href": "post/Lecture/RFD/데이터마이닝/Supervised Learning.html#r을-이용한-이항-로지스틱",
    "title": "01. Supervised Learning",
    "section": "1-1. R을 이용한 이항 로지스틱",
    "text": "1-1. R을 이용한 이항 로지스틱\n\nsetwd(\"C:\\\\Users\\\\lee\\\\Desktop\\\\PART 05 실습용 데이터\")\ncredit_data <- read_csv(\"credit_final.csv\")\n#glimpse(credit_data) ## credit.rating의 경우 factor 변수이므로 변환\ncredit_data$credit.rating <- as_factor(credit_data$credit.rating)\n\nRows: 1000 Columns: 21\n\n-- Column specification ------------------------------------------------------------------------------------------------\nDelimiter: \",\"\ndbl (21): credit.rating, account.balance, credit.duration.months, previous.c...\n\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n# summary(credit_data)\n## 데이터 분할 각각의 데이터를 7:3으로 나눔\ntrain <- credit_data %>% sample_frac(0.7)\ntest <- credit_data %>% setdiff(train)\n\n\n## 로지스틱 회귀모형 적합\nlogit_fit <-glm(credit.rating~., data =train, \n                family = \"binomial\") ## 이진분류이므로 \"binomial\"이라고 기입\n\n# summary(logit_fit)\n\nstep.logit <- step(glm(credit.rating~., data =train, \n    family = \"binomial\"),direction = 'both')\n\n# summary(step.logit) ## 총 12개의 변수가 채택됨\n# length(step.logit$coefficients)\n\n\npred <- predict(step.logit, test %>% select(-1),type=\"response\") %>% as_tibble()\npred <- pred %>% mutate(hat_y = as.factor(ifelse(value>0.5,1,0)))\n\nStart:  AIC=695.13\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + guarantor + residence.duration + \n    current.assets + age + other.credits + apartment.type + bank.credits + \n    occupation + dependents + telephone + foreign.worker\n\n                                 Df Deviance    AIC\n- residence.duration              1   653.14 693.14\n- dependents                      1   653.18 693.18\n- occupation                      1   653.19 693.19\n- age                             1   653.21 693.21\n- guarantor                       1   653.34 693.34\n- bank.credits                    1   653.76 693.76\n- foreign.worker                  1   654.15 694.15\n<none>                                653.13 695.13\n- employment.duration             1   655.13 695.13\n- other.credits                   1   655.68 695.68\n- apartment.type                  1   656.90 696.90\n- credit.amount                   1   656.97 696.97\n- telephone                       1   658.59 698.59\n- credit.duration.months          1   658.67 698.67\n- marital.status                  1   659.91 699.91\n- previous.credit.payment.status  1   661.76 701.76\n- installment.rate                1   662.17 702.17\n- savings                         1   663.97 703.97\n- current.assets                  1   665.97 705.97\n- credit.purpose                  1   666.41 706.41\n- account.balance                 1   710.79 750.79\n\nStep:  AIC=693.14\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + guarantor + current.assets + \n    age + other.credits + apartment.type + bank.credits + occupation + \n    dependents + telephone + foreign.worker\n\n                                 Df Deviance    AIC\n- dependents                      1   653.20 691.20\n- occupation                      1   653.20 691.20\n- age                             1   653.25 691.25\n- guarantor                       1   653.35 691.35\n- bank.credits                    1   653.77 691.77\n- foreign.worker                  1   654.18 692.18\n<none>                                653.14 693.14\n- employment.duration             1   655.28 693.28\n- other.credits                   1   655.70 693.70\n- apartment.type                  1   656.90 694.90\n- credit.amount                   1   656.99 694.99\n+ residence.duration              1   653.13 695.13\n- telephone                       1   658.64 696.64\n- credit.duration.months          1   658.71 696.71\n- marital.status                  1   659.93 697.93\n- previous.credit.payment.status  1   661.81 699.81\n- installment.rate                1   662.18 700.18\n- savings                         1   664.13 702.13\n- current.assets                  1   666.17 704.17\n- credit.purpose                  1   666.51 704.51\n- account.balance                 1   711.31 749.31\n\nStep:  AIC=691.2\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + guarantor + current.assets + \n    age + other.credits + apartment.type + bank.credits + occupation + \n    telephone + foreign.worker\n\n                                 Df Deviance    AIC\n- occupation                      1   653.27 689.27\n- age                             1   653.29 689.29\n- guarantor                       1   653.41 689.41\n- bank.credits                    1   653.84 689.84\n- foreign.worker                  1   654.22 690.22\n<none>                                653.20 691.20\n- employment.duration             1   655.31 691.31\n- other.credits                   1   655.78 691.78\n- apartment.type                  1   656.91 692.91\n- credit.amount                   1   657.03 693.03\n+ dependents                      1   653.14 693.14\n+ residence.duration              1   653.18 693.18\n- telephone                       1   658.71 694.71\n- credit.duration.months          1   658.75 694.75\n- marital.status                  1   659.95 695.95\n- previous.credit.payment.status  1   661.92 697.92\n- installment.rate                1   662.21 698.21\n- savings                         1   664.16 700.16\n- current.assets                  1   666.24 702.24\n- credit.purpose                  1   666.62 702.62\n- account.balance                 1   711.60 747.60\n\nStep:  AIC=689.27\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + guarantor + current.assets + \n    age + other.credits + apartment.type + bank.credits + telephone + \n    foreign.worker\n\n                                 Df Deviance    AIC\n- age                             1   653.35 687.35\n- guarantor                       1   653.48 687.48\n- bank.credits                    1   653.93 687.93\n- foreign.worker                  1   654.28 688.28\n<none>                                653.27 689.27\n- employment.duration             1   655.52 689.52\n- other.credits                   1   655.88 689.88\n- apartment.type                  1   656.97 690.97\n- credit.amount                   1   657.05 691.05\n+ occupation                      1   653.20 691.20\n+ dependents                      1   653.20 691.20\n+ residence.duration              1   653.25 691.25\n- credit.duration.months          1   658.90 692.90\n- telephone                       1   659.76 693.76\n- marital.status                  1   659.97 693.97\n- previous.credit.payment.status  1   661.95 695.95\n- installment.rate                1   662.22 696.22\n- savings                         1   664.17 698.17\n- current.assets                  1   666.30 700.30\n- credit.purpose                  1   666.99 700.99\n- account.balance                 1   711.99 745.99\n\nStep:  AIC=687.35\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + guarantor + current.assets + \n    other.credits + apartment.type + bank.credits + telephone + \n    foreign.worker\n\n                                 Df Deviance    AIC\n- guarantor                       1   653.57 685.57\n- bank.credits                    1   653.97 685.97\n- foreign.worker                  1   654.39 686.39\n<none>                                653.35 687.35\n- other.credits                   1   655.96 687.96\n- employment.duration             1   656.02 688.02\n- credit.amount                   1   657.10 689.10\n+ age                             1   653.27 689.27\n+ occupation                      1   653.29 689.29\n+ dependents                      1   653.30 689.30\n+ residence.duration              1   653.32 689.32\n- apartment.type                  1   657.85 689.85\n- credit.duration.months          1   659.25 691.25\n- marital.status                  1   659.98 691.98\n- telephone                       1   660.07 692.07\n- previous.credit.payment.status  1   662.05 694.05\n- installment.rate                1   662.25 694.25\n- savings                         1   664.35 696.35\n- current.assets                  1   666.35 698.35\n- credit.purpose                  1   666.99 698.99\n- account.balance                 1   712.02 744.02\n\nStep:  AIC=685.57\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + current.assets + other.credits + \n    apartment.type + bank.credits + telephone + foreign.worker\n\n                                 Df Deviance    AIC\n- bank.credits                    1   654.17 684.17\n- foreign.worker                  1   654.67 684.67\n<none>                                653.57 685.57\n- other.credits                   1   656.21 686.21\n- employment.duration             1   656.25 686.25\n- credit.amount                   1   657.25 687.25\n+ guarantor                       1   653.35 687.35\n+ age                             1   653.48 687.48\n+ occupation                      1   653.52 687.52\n+ dependents                      1   653.52 687.52\n+ residence.duration              1   653.55 687.55\n- apartment.type                  1   658.04 688.04\n- credit.duration.months          1   659.38 689.38\n- telephone                       1   660.23 690.23\n- marital.status                  1   660.32 690.32\n- previous.credit.payment.status  1   662.15 692.15\n- installment.rate                1   662.49 692.49\n- savings                         1   664.37 694.37\n- credit.purpose                  1   667.27 697.27\n- current.assets                  1   667.27 697.27\n- account.balance                 1   712.07 742.07\n\nStep:  AIC=684.17\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + current.assets + other.credits + \n    apartment.type + telephone + foreign.worker\n\n                                 Df Deviance    AIC\n- foreign.worker                  1   655.39 683.39\n<none>                                654.17 684.17\n- employment.duration             1   656.69 684.69\n- other.credits                   1   657.30 685.30\n+ bank.credits                    1   653.57 685.57\n+ guarantor                       1   653.97 685.97\n- credit.amount                   1   658.07 686.07\n+ occupation                      1   654.09 686.09\n+ dependents                      1   654.10 686.10\n+ age                             1   654.12 686.12\n+ residence.duration              1   654.16 686.16\n- apartment.type                  1   658.67 686.67\n- credit.duration.months          1   659.92 687.92\n- telephone                       1   660.69 688.69\n- marital.status                  1   660.79 688.79\n- previous.credit.payment.status  1   662.55 690.55\n- installment.rate                1   662.92 690.92\n- savings                         1   665.50 693.50\n- current.assets                  1   667.58 695.58\n- credit.purpose                  1   668.17 696.17\n- account.balance                 1   712.69 740.69\n\nStep:  AIC=683.39\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + current.assets + other.credits + \n    apartment.type + telephone\n\n                                 Df Deviance    AIC\n<none>                                655.39 683.39\n- employment.duration             1   657.93 683.93\n+ foreign.worker                  1   654.17 684.17\n- other.credits                   1   658.53 684.53\n+ bank.credits                    1   654.67 684.67\n- credit.amount                   1   658.86 684.86\n+ guarantor                       1   655.13 685.13\n+ occupation                      1   655.31 685.31\n+ age                             1   655.32 685.32\n+ dependents                      1   655.33 685.33\n+ residence.duration              1   655.36 685.36\n- apartment.type                  1   659.85 685.85\n- telephone                       1   661.73 687.73\n- credit.duration.months          1   661.88 687.88\n- marital.status                  1   662.40 688.40\n- previous.credit.payment.status  1   664.08 690.08\n- installment.rate                1   664.36 690.36\n- savings                         1   666.61 692.61\n- credit.purpose                  1   668.81 694.81\n- current.assets                  1   669.65 695.65\n- account.balance                 1   713.23 739.23\n\n\n\nconfusionMatrix(data=pred$hat_y,reference = test$credit.rating,positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  40  23\n         1  56 181\n                                         \n               Accuracy : 0.7367         \n                 95% CI : (0.683, 0.7856)\n    No Information Rate : 0.68           \n    P-Value [Acc > NIR] : 0.0192801      \n                                         \n                  Kappa : 0.3343         \n                                         \n Mcnemar's Test P-Value : 0.0003179      \n                                         \n            Sensitivity : 0.8873         \n            Specificity : 0.4167         \n         Pos Pred Value : 0.7637         \n         Neg Pred Value : 0.6349         \n             Prevalence : 0.6800         \n         Detection Rate : 0.6033         \n   Detection Prevalence : 0.7900         \n      Balanced Accuracy : 0.6520         \n                                         \n       'Positive' Class : 1              \n                                         \n\n\n\noptions(repr.plot.res=150,repr.plot.width=8,repr.plot.height=5)\npred.roc <- prediction(as.numeric(pred$hat_y),as.numeric(test$credit.rating))\nplot(performance(pred.roc,\"tpr\",\"fpr\"),lwd=2)\nabline(a=0,b=1,lty=\"dashed\",col=\"red\",lwd=2)\nperformance(pred.roc,\"auc\")@y.values\n\n\n    0.651960784313725\n\n\n\n\n\n\n\n위를 살펴보면 기존의 로지스틱회귀 모형을 적합했을 때 유의미하지 않은 변수가 다수 보여 단계적 변수 선택법을 적용하여 모델을 적합시켰다.\n정확도는 약 73%, 민감도와 특이도는 각각 85%, 44%로 측정되었으며, AUC값은 0.6484로 산출되었다. 그닥 훌륭한 모델이라고 보기 어렵다."
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/Supervised Learning.html#다항로지스틱",
    "href": "post/Lecture/RFD/데이터마이닝/Supervised Learning.html#다항로지스틱",
    "title": "01. Supervised Learning",
    "section": "1-2. 다항로지스틱",
    "text": "1-2. 다항로지스틱\n\n##iris 데이터를 이용한 다항 로지스틱 회귀분석\ntrain.iris <- iris %>% sample_frac(0.7)\ntest.iris <- iris %>% setdiff(train.iris)\n\nmul.iris <- multinom(Species~.,train.iris)\nsummary(mul.iris)\n\n# weights:  18 (10 variable)\ninitial  value 115.354290 \niter  10 value 11.452176\niter  20 value 3.153238\niter  30 value 2.649388\niter  40 value 2.341641\niter  50 value 2.256572\niter  60 value 1.995702\niter  70 value 1.245143\niter  80 value 0.904025\niter  90 value 0.621338\niter 100 value 0.603201\nfinal  value 0.603201 \nstopped after 100 iterations\n\n\nCall:\nmultinom(formula = Species ~ ., data = train.iris)\n\nCoefficients:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    51.48231    -30.82089   -21.44772     85.88038    24.74862\nvirginica    -96.62251    -68.87111   -61.97239    164.13765    94.06323\n\nStd. Errors:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    121.5106     132.8265    97.66829     74.74537    36.83325\nvirginica     118.3141     144.8161   106.79819     64.21134    33.88914\n\nResidual Deviance: 1.206402 \nAIC: 21.2064 \n\n\n\npred.mul <- predict(mul.iris,test.iris %>% select(-5)) \nconfusionMatrix(pred.mul,test.iris$Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         13          0         0\n  versicolor      0         16         0\n  virginica       0          1        15\n\nOverall Statistics\n                                          \n               Accuracy : 0.9778          \n                 95% CI : (0.8823, 0.9994)\n    No Information Rate : 0.3778          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.9665          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.9412           1.0000\nSpecificity                 1.0000            1.0000           0.9667\nPos Pred Value              1.0000            1.0000           0.9375\nNeg Pred Value              1.0000            0.9655           1.0000\nPrevalence                  0.2889            0.3778           0.3333\nDetection Rate              0.2889            0.3556           0.3333\nDetection Prevalence        0.2889            0.3556           0.3556\nBalanced Accuracy           1.0000            0.9706           0.9833"
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/Supervised Learning.html#binary-classification",
    "href": "post/Lecture/RFD/데이터마이닝/Supervised Learning.html#binary-classification",
    "title": "01. Supervised Learning",
    "section": "2-1. binary classification",
    "text": "2-1. binary classification\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\n\ndt.model <- rpart(credit.rating~.,method=\"class\", ## 회귀가 아닌 분류이므로 \"class\" 기입\n                                   data = train, \n                                   control = rpart.control(maxdepth=5,\n                                                           minsplit=15)) ## 의사결정나무의 최대 깊이는 5, 노드에서의 최소 관측치는 15개이 상\n\n\noptions(repr.plot.res=200)\nrpart.plot(dt.model,extra=2) ## extra 인자를 이용해 각각 범주의 분류된 개수를 파악, extra=3 은 오분류된 개수를 보여주\n\n\n\n\n\n총 700개의 관측치 중 496개의 관측치를 1로 분류하였으며, account.balance <3 인 377개의 노드 중 210개가 1로 분류되었음을 의미한다.\ncptable을 확인하여 xerror가 가장 낮은 split 개수를 택한 후 가지치기를 적용하자.\nxerror는해당 CP에서 교차검증오류를 나타내며 CP는 복잡성을 나타낸다.\n\n\ndt.model$cptable\n\n\n\nA matrix: 4 × 5 of type dbl\n\n    CPnsplitrel errorxerrorxstd\n\n\n    10.0526960801.00000001.00000000.05893547\n    20.0245098040.78921570.92647060.05757874\n    30.0147058850.76470590.88725490.05678633\n    40.0100000070.73529410.88725490.05678633\n\n\n\n\n\n확인 결과 xerror가 가장 낮은 분할 횟수는 7이며, 앞선 모형의 그래프를 봤을 때 모델이 분할을 7번까지 한다고 할 수 있다.\n해당 위치에서의 CP 값을 전달받자\n\n\nopt <- which.min(dt.model$cptable[,\"xerror\"])\ncp  <- dt.model$cptable[opt,\"CP\"]\n\n\nplotcp를 그려본결과 8번째 분할에서 xerror가 가장 낮지만 과적합을 막기위해 cptable에서 산출한 cp값을 이용하여 가지치기를 하자.\n\n\nplotcp(dt.model)\n\n\n\n\n\n가지치기 수행\n\n\noptions(repr.plot.res=200)\nrpart.plot(dt.model,extra=2) ## extra 인자를 이용해 각각 범주의 분류된 개수를 파악, extra=3 은 오분류된 개수를 보여준다\n\n\n\n\n\nprune.c <- prune(dt.model,cp=cp)\nrpart.plot(prune.c,extra=2)\n\n\n\n\n\n기존의 모델보다 복잡성이 줄어든 것으로 볼 수 있다.\n예측을 통하여 우리가 생성한 모델의 타당성을 평가하자.\n\n\npred.dt <- predict(dt.model, test %>% select(-1),type=\"class\")\n\n\nconfusionMatrix(data=pred.dt,reference= test$credit.rating)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  37  19\n         1  59 185\n                                          \n               Accuracy : 0.74            \n                 95% CI : (0.6865, 0.7887)\n    No Information Rate : 0.68            \n    P-Value [Acc > NIR] : 0.014           \n                                          \n                  Kappa : 0.3285          \n                                          \n Mcnemar's Test P-Value : 1.006e-05       \n                                          \n            Sensitivity : 0.3854          \n            Specificity : 0.9069          \n         Pos Pred Value : 0.6607          \n         Neg Pred Value : 0.7582          \n             Prevalence : 0.3200          \n         Detection Rate : 0.1233          \n   Detection Prevalence : 0.1867          \n      Balanced Accuracy : 0.6461          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n정분류율은 0.73이며 특이도, 민감도는 각각 0.8578, 0.4270으로 산출되었다.\n\n\npred.dt.roc <- prediction(as.numeric(pred.dt),as.numeric(test$credit.rating))\n\n\nplot(performance(pred.dt.roc,\"tpr\",\"fpr\"),lwd=2)\nabline(a=0,b=1,lty=2,col=\"red\",lwd=2)\n\n\n\n\n\nperformance(pred.dt.roc,\"auc\")@y.values\n\n\n    0.646139705882353\n\n\n\n\n산출된 AUC값은 0.642로 좋은 모델은 아니다."
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/Supervised Learning.html#multiple-classification",
    "href": "post/Lecture/RFD/데이터마이닝/Supervised Learning.html#multiple-classification",
    "title": "01. Supervised Learning",
    "section": "2-2. multiple classification",
    "text": "2-2. multiple classification\n\ndt.model2 <- rpart(Species ~. ,data=train.iris)\n\n\nrpart.plot(dt.model2,extra=2)\n\n\n\n\n\n총 105개의 데이터에서 39개의 데이터가 virginica로 분류되었고, Petal.Length < 2.5인 34개의 데이터는 모두 setosa로 분류되었다.\n\n\npred.dt2 <- predict(dt.model2, test.iris %>% select(-5),type=\"class\")\n\n\nconfusionMatrix(pred.dt2,test.iris$Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         13          0         0\n  versicolor      0         16         3\n  virginica       0          1        12\n\nOverall Statistics\n                                          \n               Accuracy : 0.9111          \n                 95% CI : (0.7878, 0.9752)\n    No Information Rate : 0.3778          \n    P-Value [Acc > NIR] : 1.099e-13       \n                                          \n                  Kappa : 0.8655          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.9412           0.8000\nSpecificity                 1.0000            0.8929           0.9667\nPos Pred Value              1.0000            0.8421           0.9231\nNeg Pred Value              1.0000            0.9615           0.9062\nPrevalence                  0.2889            0.3778           0.3333\nDetection Rate              0.2889            0.3556           0.2667\nDetection Prevalence        0.2889            0.4222           0.2889\nBalanced Accuracy           1.0000            0.9170           0.8833\n\n\n\n정확도의 0.9556, 각각의 class에서 특이도와 민감도를 확인해보니 적절한 분류가 이루어졌음을 알 수 있다."
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html",
    "href": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html",
    "title": "02. unsupervised learning",
    "section": "",
    "text": "개체간 유사성에 따라 집단을 분류하고, 군집 내 유사성과 군집 간 상이성을 규명하는 다변량 분석 기법이다.\n군집분석에서는 관측 데이터 간 유사성이나 근접성을 측정해 어느 군집으로 묶을 수 있는지 판단해야 한다.\n\n\n\n\n\n\n유클리디안거리 (euclidean) : 가장 널리 사용되나 통계적 개념이 내포되어 있지 않아 변수들의 산포 정도가 전혀 감안되어 있지 않음.\n표준화 거리(Standardized) : 해당변수의 표준편차로 척도 변환 후 유클리드안 거리를 계산하는 방법. 분산 차이로 인한 왜곡을 피할 수 있음.\n마할라노비스 거리(Mahalanobis) : 통계적 개념이 포함된 거리로 두 벡터 사이의 거리를 표본공분산으로 나눠주어야 한다. 그룹에 대한 사전 지식 없이는 표본공분산 S를 계산할 수 없으므로 사용하기 곤란하다.\n체비셰프 거리 (Chebyshev) : \\(d(x,y) = max_{i} |x_i-y_i|\\)\n멘하탄 거리 (Manhattan) : 유클리디안과 비슷 -> \\(d(x,y) = \\sum |x_i-y_i|\\)\n캔버라 거리(Canberra), 민코우스키(Minkowski) 거리 등이 있음\n\n\n\n\n\n자카드 거리\n\n\\[ 1-  J(A,B) = \\frac {|A\\cup B|- |A\\cap B|}{|A\\cup B|}\\]\n\n코사인 유사도 : 두 개체의 백터 내적의 코사인 값을 이용하여 측정된 벡터간의 유사한 정도\n\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html#종류",
    "href": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html#종류",
    "title": "02. unsupervised learning",
    "section": "1-1. 종류",
    "text": "1-1. 종류\n\n최단연결법 (single linkage, nearest neighbor)\n\nnxn 거리 행렬에서 가장 가까운 데이터를 묶어서 군집을 형성한다.\n\n최장연결법 (complete linkage) : 데이터 간 거리를 계산할 때 최장거리를 거리로 계산하여 거리행렬을 수정하는 방법\n평균연결법 (average linkage)\n와드연결법(Ward linkage) : 군집 내 편차들의 제곱합을 고려한 방법, 군집간 정보의 손실을 최소화 하기 위해 군집화를 진행 \\(\\to sse\\)를 고려한다고 생각"
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html#절차",
    "href": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html#절차",
    "title": "02. unsupervised learning",
    "section": "1-2. 절차",
    "text": "1-2. 절차\nstep 1. 거리행렬을 통해 가장 가까운 거리의 객체들 간의 관계를 규명하고 덴드로그램을 그린다.\nstep 2. 덴드로그램을 보고 군집의 개수를 변화해 가면서 적절한 군집 수를 선정한다.\nstep 3. 군집의 수는 분석 목적에 따라 선정할수 있지만 대부분 5개 이상의 군집은 잘 활용하지 않는다."
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html#r-실습",
    "href": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html#r-실습",
    "title": "02. unsupervised learning",
    "section": "1-3. R 실습",
    "text": "1-3. R 실습\n\ndist(data, method)\n\nmethod : 거리측정방법, euclidean, maximum, manhattan, canberra, binary, minkowski가 있음\n\nhclust (data, method)\n\ndata : dist 함수로 거리가 측정된 데이터\nmethod : 거리측정 밥법, single, complete, average, median, ward.D 가 있음\n\n\n\nUS <- USArrests\nhead(US)\n\n\n\nA data.frame: 6 × 4\n\n    MurderAssaultUrbanPopRape\n    <dbl><int><int><dbl>\n\n\n    Alabama13.22365821.2\n    Alaska10.02634844.5\n    Arizona 8.12948031.0\n    Arkansas 8.81905019.5\n    California 9.02769140.6\n    Colorado 7.92047838.7\n\n\n\n\n\nUS.dist <- dist(US, \"euclidean\")\n\n\ndist^2을 한 이유는 거리의 차이를 많이 두어 군집이 나뉘는 것을 쉽게 확인하기 위해서임.\n\n\n# hclust 함수를 이용하여 계층적 군집분석\noptions(repr.plot.res=200,repr.plot.hight=5,repr.plot.width=7)\nUS.single <- hclust(US.dist^2,method=\"single\")\nplot(US.single)\n\n\n\n\n\n# cutree함수를 이용하여 계층적 군집결과를 그릅 나누기\ngroup <- cutree(US.single,k=5) ## k는 군집의 수, h는 높이\ngroup\n\nAlabama1Alaska2Arizona1Arkansas3California1Colorado3Connecticut3Delaware1Florida4Georgia3Hawaii3Idaho3Illinois1Indiana3Iowa3Kansas3Kentucky3Louisiana1Maine3Maryland1Massachusetts3Michigan1Minnesota3Mississippi1Missouri3Montana3Nebraska3Nevada1New Hampshire3New Jersey3New Mexico1New York1North Carolina5North Dakota3Ohio3Oklahoma3Oregon3Pennsylvania3Rhode Island3South Carolina1South Dakota3Tennessee3Texas3Utah3Vermont3Virginia3Washington3West Virginia3Wisconsin3Wyoming3\n\n\n\n## 각각의 그룹을 사각형으로 그룹지어 나타나기\nplot(US.single)\nrect.hclust(US.single,k=5,border=\"red\")"
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html#측도",
    "href": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html#측도",
    "title": "02. unsupervised learning",
    "section": "4-1. 측도",
    "text": "4-1. 측도\n\\(\\divideontimes\\) tip:측도의 단어 뜻을 직관적으로 받아들여 해석하자.\n\n지지도 (Support) : 전체 거래 중 항목 A와 항목 B를 동시에 포함하는 거래의 비율로 정의한다.\n\n\\(\\quad P (A \\cup B)\\)\n\n신뢰도 (Confidence) : 항목 A를 포함한 거래중 항목 B도 같이 포함될 확률\n\n\\(\\begin {eqnarray*}\\quad P(B|A) = \\frac {P(A\\cap B)}{P(A)} \\end{eqnarray*}\\)\n\n향상도 (Lift) : A가 구매되지 않았을 때 품목 B의 구매확률에 비해 A가 구매됐을 때 품목 B의 구매확률의 증가비\n\n\\(\\quad \\begin {eqnarray*} \\frac {P(B|A)}{P(B)}\\end{eqnarray*}\\)"
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html#apriori-알고리즘",
    "href": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html#apriori-알고리즘",
    "title": "02. unsupervised learning",
    "section": "4-2. Apriori 알고리즘",
    "text": "4-2. Apriori 알고리즘\n\n빈발항목집합(frequent item set) : 최소 지지도 보다 큰 지지도 값을 갖는 품목의 집합\nApriori 알고리즘 : 모든 품목집합에 대한 지지도를 전부 계산하는 것이 아니라, 최소 지지도 이상의 빈발항목집합을 찾은 후 그것들에 대해서만 연관규칙을 계산하는 것.\n지지도가 낮은 후보 집합 생성 시 아이템의 개수가 많아지면 계산 복잡도가 증가한다는 문제점을 가지고 있다."
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html#r실습",
    "href": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html#r실습",
    "title": "02. unsupervised learning",
    "section": "4-3. R실습",
    "text": "4-3. R실습\n\nas함수를 이용하여 데이터프레임을 transactions 함수로 변형 후에 진행하여야 하나 예제 데이터는 이미 변형이 된 데이터이다.\n\n\nlibrary(arules)\n\n\ndata(Groceries)\ninspect(Groceries[1:3]) ##연관규칙 확인\n\n    items                \n[1] {citrus fruit,       \n     semi-finished bread,\n     margarine,          \n     ready soups}        \n[2] {tropical fruit,     \n     yogurt,             \n     coffee}             \n[3] {whole milk}         \n\n\n\nrules <- apriori(Groceries,\n                 parameter = list(support=0.01,\n                                  confidence = 0.3))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n        0.3    0.1    1 none FALSE            TRUE       5    0.01      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 98 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].\nsorting and recoding items ... [88 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 done [0.00s].\nwriting ... [125 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\n\n\n총 125개의 연관규칙이 생성되었다.\n\n\ninspect(sort(rules,by=c(\"confidence\"),decreasing = T)[1:5])\n\n    lhs                                 rhs                support   \n[1] {citrus fruit,root vegetables}   => {other vegetables} 0.01037112\n[2] {tropical fruit,root vegetables} => {other vegetables} 0.01230300\n[3] {curd,yogurt}                    => {whole milk}       0.01006609\n[4] {other vegetables,butter}        => {whole milk}       0.01148958\n[5] {tropical fruit,root vegetables} => {whole milk}       0.01199797\n    confidence coverage   lift     count\n[1] 0.5862069  0.01769192 3.029608 102  \n[2] 0.5845411  0.02104728 3.020999 121  \n[3] 0.5823529  0.01728521 2.279125  99  \n[4] 0.5736041  0.02003050 2.244885 113  \n[5] 0.5700483  0.02104728 2.230969 118  \n\n\n\n좌항에서 우항, 우항에서 좌항의 규칙이 겹치는 경우가 있으므로 중복규칙 제거함수 구현\n\n\nprune.dup.rules <- function(rules) {\n                    rule.subset.matrix <- is.subset(rules,rules,sparse=F)\n                    rule.subset.matrix[lower.tri(rule.subset.matrix, diag =T)] <- NA\n                    dup.rules <- colSums(rule.subset.matrix,na.rm =T) >=1\n                    pruned.rules <- rules[!dup.rules]\n                    return (pruned.rules)\n                    }\n\n\n만약 우변의 아이템 구매를 이끌 아이템을 찾고 싶다면?\n\n\nmetric.params <- list(supp=0.001,conf=0.5,minlen=2) ## minlen은 좌항과 우항을 합친 최소 물품수\nrules <- apriori(Groceries,\n                 parameter = metric.params,\n                 appearance = list(default = \"lhs\",rhs=\"soda\"),\n                 control = list(verbose=F))\n\n\napperance : 우측의 soda를 사기위해 좌항의 아이템을 찾는 것으로 설정\nverbose : apriori 함수 실행 결과를 나타낼지의 여부를 묻는 인자\n\n\nrules <- prune.dup.rules(rules)\n\n\nrules <- sort(rules,decreasing=T,by=\"confidence\")\n\n\ninspect(rules[1:5])\n\n    lhs                                     rhs    support     confidence\n[1] {coffee,misc. beverages}             => {soda} 0.001016777 0.7692308 \n[2] {sausage,bottled water,bottled beer} => {soda} 0.001118454 0.7333333 \n[3] {sausage,white bread,shopping bags}  => {soda} 0.001016777 0.6666667 \n[4] {rolls/buns,bottled water,chocolate} => {soda} 0.001321810 0.6500000 \n[5] {pastry,misc. beverages}             => {soda} 0.001220132 0.6315789 \n    coverage    lift     count\n[1] 0.001321810 4.411303 10   \n[2] 0.001525165 4.205442 11   \n[3] 0.001525165 3.823129 10   \n[4] 0.002033554 3.727551 13   \n[5] 0.001931876 3.621912 12   \n\n\n\n만약 좌변의 아이템 세트를 가지고 있을 때 물품을 찾고 싶다면?\n\n\nmetric.params <- list(supp=0.001,conf=0.3,minlen=2) ## minlen은 좌항과 우항을 합친 최소 물품수\nrules <- apriori(Groceries,\n                 parameter = metric.params,\n                 appearance = list(default = \"rhs\",lhs=c(\"yogurt\",\"sugar\")),\n                 control = list(verbose=F))\n\n\nrules <- prune.dup.rules(rules)\n\n\nrules <- sort(rules,decreasing=T, by=\"confidence\")\n\n\ninspect(rules,decreasing=T,by=\"confidence\")\n\n    lhs               rhs                  support     confidence coverage   \n[1] {sugar}        => {whole milk}         0.015048297 0.4444444  0.033858668\n[2] {yogurt}       => {whole milk}         0.056024403 0.4016035  0.139501779\n[3] {sugar}        => {other vegetables}   0.010777834 0.3183183  0.033858668\n[4] {yogurt}       => {other vegetables}   0.043416370 0.3112245  0.139501779\n[5] {yogurt,sugar} => {whipped/sour cream} 0.002135231 0.3088235  0.006914082\n[6] {yogurt,sugar} => {root vegetables}    0.002135231 0.3088235  0.006914082\n    lift     count\n[1] 1.739400 148  \n[2] 1.571735 551  \n[3] 1.645119 106  \n[4] 1.608457 427  \n[5] 4.308198  21  \n[6] 2.833283  21"
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html#extra.-som-자기조직화지도",
    "href": "post/Lecture/RFD/데이터마이닝/Unsupervised Learning(1204).html#extra.-som-자기조직화지도",
    "title": "02. unsupervised learning",
    "section": "Extra. SOM (자기조직화지도)",
    "text": "Extra. SOM (자기조직화지도)\n\nlibrary(kohonen) ## somgmgid를 위한함수\n\n\n\ntrain <- sample(1:150, 100) #무작위로 100개 추출 (학습데이터)\ntrain_Set <-  list( x = as.matrix(iris[train,-5]), Species = as.factor(iris[train,5])) #학습데이터 list형\ntest_Set <- list(x = as.matrix(iris[-train,-5]), Species = as.factor(iris[-train,5])) #테스트 데이터 list형\ngr <- somgrid(xdim = 3, ydim = 5, topo = \"hexagonal\") #grid 갯수 및 모양 설정\nss <- supersom(train_Set, gr, rlen = 200, alpha = c(0.05, 0.01)) #som 학습하기\n\n\ngrid : 경쟁층의 가충치 테이블을 5 x 4로 설정 topology 위상방법은 rectangular, hexagonal 두가지 중 하나를 설정한다.\nrlen : 전체 데이트 세트가 네트워크에 표시되는 횟수 (= 학습 횟수)\nalpha : 학습율, 변화의 양을 나타내는 두 개의 숫자의 벡터. 기본값은 rlen 업데이트에 대해 0.05에서 0.01로 선형적으로 감소하는 것이다.\n\n\nsummary(ss)\n\nSOM of size 3x5 with a hexagonal topology and a bubble neighbourhood function.\nThe number of data layers is 2.\nDistance measure(s) used: sumofsquares, tanimoto.\nTraining data included: 100 objects.\nMean distance to the closest unit in the map: 0.008.\n\n\n\n요약결과 가장 가까운 노드까지의 평균 거리가 0.008 이다.\n학습을 거듭하면서 뉴런과 학습 데이터의 거리가 짧아짐을 아래와 같이 확인할 수 있었다.\n\n\nplot(ss,type=\"change\")\n\n\n\n\n\n또한 학습된 som모델의 각 뉴런이 몇 개의 학습 데이터와 매핑이 되는 지를 확인 할 수 있었다.\n결과적으로 보았을 때 매핑이 더ㅣ지않은 뉴런이 존재하는 것으로 보아 생성한 som 모델의 신경망 크기가 크다는 것을 의미한다.\n\n\nplot(ss, type=\"count\")\n\n\n\n\n\n아래와 같이 통합거리매트릭스를 그려 각 뉴런의 이웃간 거리를 표현할 수 있었다. 값이 높을 수록 해당 뉴런은 이웃뉴련과 비유사하다고 할 수 있다.\n\n\nplot(ss, type=\"dist.neighbours\")\n\n\n\n\n\n또한 아래그림처럼 각 뉴런에서 Species의 속성들 가중치 기여율을 알 수 있다.\n\n\nplot(ss, type=\"codes\")\n\n\n\n\n\n\n\n\n학습된 모델을 통해 학습에 사용된 데이터 분류와 모델분류를 확인하기 위해 혼동행렬을 그려보면 아래와 같다.\n\n\nsom.prediction <- predict(ss, newdata = test_Set[1],type=\"class\")\n\n\nsom.prediction$predictions$Species\n\n\nsetosasetosasetosasetosasetosasetosasetosasetosasetosasetosasetosasetosa<NA>versicolorversicolorversicolorversicolorversicolorversicolorversicolor<NA>versicolorversicolorversicolorversicolorversicolorversicolorversicolorvirginicavirginicavirginicavirginicavirginicavirginicavirginicavirginicaversicolorvirginicavirginicavirginicavirginicavirginicavirginicavirginicavirginicavirginicavirginicavirginicavirginicavirginica\n\n\n    \n        Levels:\n    \n    \n    'setosa''versicolor''virginica'\n\n\n\n\nlibrary(caret)\n\n\nconfusionMatrix(som.prediction$predictions$Species,test_Set$Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         12          0         0\n  versicolor      0         14         1\n  virginica       0          0        21\n\nOverall Statistics\n                                          \n               Accuracy : 0.9792          \n                 95% CI : (0.8893, 0.9995)\n    No Information Rate : 0.4583          \n    P-Value [Acc > NIR] : 3.148e-15       \n                                          \n                  Kappa : 0.9677          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                   1.00            1.0000           0.9545\nSpecificity                   1.00            0.9706           1.0000\nPos Pred Value                1.00            0.9333           1.0000\nNeg Pred Value                1.00            1.0000           0.9630\nPrevalence                    0.25            0.2917           0.4583\nDetection Rate                0.25            0.2917           0.4375\nDetection Prevalence          0.25            0.3125           0.4375\nBalanced Accuracy             1.00            0.9853           0.9773"
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/텍스트마이닝.html",
    "href": "post/Lecture/RFD/데이터마이닝/텍스트마이닝.html",
    "title": "03. Text Mining",
    "section": "",
    "text": "텍스트 마이닝을 수행하기 전에 tm 패키지를 활용해 Corpus를 만들고 생성된 Corpus를 전처리 하고 분석에 활용하여야 한다.\nVectorSource 함수를 사용하고 문서로 완성된 데이터를 VCorpus 함수를 이용하여 Corpus로 만든다.\n\n\nlibrary(tm)\n\n\ncrude 데이터는 tm 패키지 내에 내장된 Corpus데이터 이다.\ninspect 하수를 이용해 첫 번째 문서의 정보를 확인한다.\n\n\ninspect(crude[1])\n\n<<VCorpus>>\nMetadata:  corpus specific: 0, document level (indexed): 0\nContent:  documents: 1\n\n$`reut-00001.xml`\n<<PlainTextDocument>>\nMetadata:  15\nContent:  chars: 527\n\n\n\n\n\n\ntm_map(x, tolower) : 소문자로 만들기\ntm_map(x, stemDocument) : 어근만 남기기\ntm_map(x, stripWhitespace) : 공백 제거\ntm_map(x, removePunctuation) : 문장부호 제거\ntm_map(x, removeNumbers) : 숫자 제거\ntm_map(x, removeWords, “words”) : 특정 단어 제거\ntm_map(x, removeWords,stopwords (“english)) : 불용어 제거\ntm_map(x, PlainTextDocument) : TextDocument로 변환\n\n\nsetwd(\"C:\\\\Users\\\\rkdcj\\\\OneDrive\\\\바탕 화면\\\\PART 06 실습용 데이터\")\n\n\nlibrary(tm)\n\n\n데이터 로드\n\n\nnews <- readLines(\"키워드_뉴스.txt\")\n\n\nnews.corpus <- VCorpus(VectorSource(news))\n\n\nnews.corpus\n\n<<VCorpus>>\nMetadata:  corpus specific: 0, document level (indexed): 0\nContent:  documents: 10\n\n\n\n전처리 함수 생성\n\n\nclean_txt <- function(txt) {\n        txt <- tm_map(txt, removeNumbers) ## 숫자제거\n        txt <- tm_map(txt, removePunctuation) ## 문장부호 제거\n        txt <- tm_map(txt, stripWhitespace)  ## 공백제거\n        \n        return(txt)\n}\n\n\nclean.news <- clean_txt(news.corpus)\n\n\nnews.corpus[[1]]$content\n\n'동아대학교(총장 한석정)가 \\'수요자 데이터기반 스마트헬스케어 서비스\\'분야 ‘4차 산업혁명 혁신선도대학으로 최종선정됐습니다. 동아대가 혁신선도대학으로 펼치게 될 ‘수요자 데이터기반 스마트헬스케어 서비스’ 산업은 리빙데이터(운동·영양·약물)와 메디컬데이터(생체계측·진료기록)를 종합 분석, 다양한 헬스케어 서비스를 제공하는 것입니다. 동아대는 건강과학대학과 의료원, 재활요양병원 등 경쟁력 있는 인프라를 바탕으로 신뢰도 높은 정밀 분석을 실시, ‘헬스케어 기획 전문가’와 ‘헬스케어 데이터분석 전문가’ 등 수요자 맞춤형 헬스케어 서비스 분야를 선도하는 전문 인재를 키워나갈 계획입니다. ‘스마트헬스케어 융합전공’을 신설, 경영정보학과를 중심으로 한 빅데이터 분석, 식품영양학과·의약생명공학과·건강관리학과 중심의 헬스케어 등 학문 간 경계는 교육혁신도 이뤄나갈 방침입니다. '\n\n\n\nclean.news[[1]]$content\n\n'동아대학교총장 한석정가 수요자 데이터기반 스마트헬스케어 서비스분야 ‘차 산업혁명 혁신선도대학으로 최종선정됐습니다 동아대가 혁신선도대학으로 펼치게 될 ‘수요자 데이터기반 스마트헬스케어 서비스’ 산업은 리빙데이터운동·영양·약물와 메디컬데이터생체계측·진료기록를 종합 분석 다양한 헬스케어 서비스를 제공하는 것입니다 동아대는 건강과학대학과 의료원 재활요양병원 등 경쟁력 있는 인프라를 바탕으로 신뢰도 높은 정밀 분석을 실시 ‘헬스케어 기획 전문가’와 ‘헬스케어 데이터분석 전문가’ 등 수요자 맞춤형 헬스케어 서비스 분야를 선도하는 전문 인재를 키워나갈 계획입니다 ‘스마트헬스케어 융합전공’을 신설 경영정보학과를 중심으로 한 빅데이터 분석 식품영양학과·의약생명공학과·건강관리학과 중심의 헬스케어 등 학문 간 경계는 교육혁신도 이뤄나갈 방침입니다 '\n\n\n\n확인 결과 ” ’, \\(\\cdot\\) “와 같은 부호는 제거 되지않음\n아래와 같은 함수를 사용하여 제거해준다.\n\n\ntxt<- gsub(\"[[:punct:]]\",\"\",clean.news)"
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/텍스트마이닝.html#news-data-not-use-corpus",
    "href": "post/Lecture/RFD/데이터마이닝/텍스트마이닝.html#news-data-not-use-corpus",
    "title": "03. Text Mining",
    "section": "news data not use Corpus",
    "text": "news data not use Corpus\n\nclean_txt2 <- function(txt) {\n        txt <- removeNumbers(txt) ## 숫자제거\n        txt <- removePunctuation(txt) ## 문장부호 제거\n        txt <- stripWhitespace(txt) ## 공백제거\n        txt <- gsub(\"[^[:alnum:]]\",\" \",txt) ## 영어와 숫자를 제거\n        return(txt)\n}\n\n\nclean.news <- clean_txt2(news)\n\n\n전처리 결과 푸드테크, 스타트업 등과 같은 복합명사가 분리되어 출력되는 것을 확인할 수 있었다.\n따라서 복합명사를 명사로 인식할 수 있도록 사전에 등록하고 다시 분석 결과를 확인하면 복합명사도 하나의 명사로 추가된 것으로 확인할 수 있다.\n\n\nhead(extractNoun(clean.news[5]))\n\n\n'푸드''테크''스타트''업''빅데이터''기반'\n\n\n\nbuildDictionary(ext_dic= \"sejong\",\n                user_dic=data.frame(c(read.table(\"food.txt\"))))\n\n370965 words dictionary was built.\n\n\n\nread.table(\"food.txt\")\n\n\n\nA data.frame: 4 × 2\n\n    V1V2\n    <chr><chr>\n\n\n    푸드테크    ncn\n    스타트업    ncn\n    빅데이터    ncn\n    우아한형제들ncn\n\n\n\n\n\nhead(extractNoun(clean.news[5]))\n\n\n'푸드테크''스타트업''들이''빅데이터''기반''시스템'\n\n\n\n위와 같이 복합명사도 하나의 명사로 추출된 것을 확인할 수 있다."
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/텍스트마이닝.html#simplepos22를-활용해-형용사-추출하기",
    "href": "post/Lecture/RFD/데이터마이닝/텍스트마이닝.html#simplepos22를-활용해-형용사-추출하기",
    "title": "03. Text Mining",
    "section": "SimplePos22를 활용해 형용사 추출하기",
    "text": "SimplePos22를 활용해 형용사 추출하기\n\nlibrary(stringr) ## R에서 문자열을 처리할 수 있는 패키지\n\n\ndoc1 <- paste(SimplePos22(clean.news[[2]]))\n\n\nhead(doc1)\n\n\n'첨단/NC''정보통신기술/NC+에/JC''AI/F''등/NB+이/JC''더하/PA+어/EC+지/PX+면서/EC''무기체계/NC+가/JC'\n\n\n\n형용사의 품사는 PA 이므로 아래와 같은 함수를 이용하여 추출\n\n\ndoc2 <- str_match(doc1,\"([가-힣]+)/PA\") \n\n\nhead(doc2)\n\n\n\nA matrix: 6 × 2 of type chr\n\n    NA     NA  \n    NA     NA  \n    NA     NA  \n    NA     NA  \n    더하/PA더하\n    NA     NA  \n\n\n\n\n\ndoc3 <- doc2 %>% na.omit()\ndoc3[,2]\n\n\n'더하''빠르''새롭''빠르''빠르''이러하''걸맞'"
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/텍스트마이닝.html#stemming",
    "href": "post/Lecture/RFD/데이터마이닝/텍스트마이닝.html#stemming",
    "title": "03. Text Mining",
    "section": "Stemming",
    "text": "Stemming\n\n어간추출은 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 할 수 있다.\n\n즉, 공통 어간을 가지는 단어를 묶는 작업을 Stemming이라고 한다.\n\nR에서는 tm패키지에서 stemDocument 함수를 통해 공통으로 들어가지 않은 부분을 제외하고 stemCompletion 함수를 통해 stemming된 단어와 완성을 위한 dictionary를 함께 넣으면 기본적인 어휘로 완성해주는 역할을 한다.\n\n\ntest <- stemDocument(c(\"analyze\",\"analyzed\",\"analyzing\"))\ntest\n\n\n'analyz''analyz''analyz'\n\n\n\n아래에서 중요한것은 stemCompletion 함수를 사용할 때에는 단어의 완성을 위해 반드시 dictionary가 필요하다.\n\n\nstemCompletion(test,dictionary =c(\"analyze\",\"analyzed\",\"analyzing\") )\n\nanalyz'analyze'analyz'analyze'analyz'analyze'"
  },
  {
    "objectID": "post/Lecture/RFD/데이터마이닝/텍스트마이닝.html#tdm",
    "href": "post/Lecture/RFD/데이터마이닝/텍스트마이닝.html#tdm",
    "title": "03. Text Mining",
    "section": "TDM",
    "text": "TDM\n\nR에서 tm패키지의 TermDocumentMatric을 사용하여 문서-단어 빈도행렬을 만들 수 있다. 단 객체는 Corpus 객체를 이용하여야함.\n\n\nVC.news <- VCorpus(VectorSource(clean.news))\n\n\ninspect(TermDocumentMatrix(VC.news))\n\n<<TermDocumentMatrix (terms: 1011, documents: 10)>>\nNon-/sparse entries: 1144/8966\nSparsity           : 89%\nMaximal term length: 14\nWeighting          : term frequency (tf)\nSample             :\n            Docs\nTerms        1 10 2 3 4 5 6 7 8 9\n  관계자는   0  1 0 1 0 3 0 1 0 0\n  데이터     0  4 3 6 8 3 7 1 4 5\n  밝혔다     0  1 0 1 2 0 1 0 2 0\n  분석을     1  0 0 1 2 0 2 0 0 0\n  브라이틱스 0  0 0 0 9 0 0 0 0 0\n  빅데이터   1  1 3 1 2 5 0 2 1 4\n  빅데이터를 0  2 0 0 0 5 0 0 0 1\n  서비스를   1  2 1 0 0 2 2 0 0 0\n  전문가     2  0 1 0 1 0 1 0 2 0\n  활용한     0  0 2 0 1 0 2 0 2 1\n\n\n\n총 10개의 기사에서 11의 단어가 추출된 것을 확인하였다.\n\n\n명사만 추출하여 TDM을 만들기\n\n사용자 정의 함수 생성\n\n\nwords <- function(doc) {\n    doc <- as.character(doc)\n    extractNoun(doc)\n}\n\n\n명사만 추출한 TDM 생성\n\n\nTDM.news2 <- TermDocumentMatrix(VC.news, control = list(tokenize = words))\n\n\ninspect(TDM.news2)\n\n<<TermDocumentMatrix (terms: 289, documents: 10)>>\nNon-/sparse entries: 360/2530\nSparsity           : 88%\nMaximal term length: 14\nWeighting          : term frequency (tf)\nSample             :\n            Docs\nTerms        1 10 2 3 4  5  6 7 8 9\n  경쟁력     1  0 0 2 0  1  1 0 1 1\n  경진대회   0  0 0 0 1  0  0 2 0 5\n  관계자     0  1 0 1 0  3  0 1 0 0\n  광양제철소 0  0 0 0 0  0  0 0 0 6\n  데이터     3  6 3 7 8  7 10 3 5 5\n  브라이틱스 0  0 0 0 9  0  0 0 0 0\n  빅데이터   1  4 4 1 3 16  0 2 1 6\n  서비스     3  3 3 0 0  7  3 1 0 0\n  시스템     0  2 0 2 1  3  0 0 0 0\n  전문가     2  1 1 4 1  1  1 1 2 1\n\n\n\n총 289개의 단어가 추출되었다.\nTDM으로 나타난 단어들의 빈도 체크\n\n\ntdm2 <- as.matrix(TDM.news2)\ntdm3 <- rowSums(tdm2)\ntdm4 <- tdm3[order(tdm3,decreasing = T)]\n\n\ntdm4[1:10]\n\n데이터57빅데이터38서비스20전문가15브라이틱스9경진대회8시스템8경쟁력7관계자6광양제철소6\n\n\n\n단어 사전을 정의하여 특정 단어들에 대해서만 분석 결과를 확인해보자.\n\n\nmydict <- names(tdm4)[1:10]\n\n\nmy.news <- TermDocumentMatrix(VC.news, control = list(tokenize=words,dictionary=mydict))\n\n\nmy.news\n\n<<TermDocumentMatrix (terms: 10, documents: 10)>>\nNon-/sparse entries: 54/46\nSparsity           : 46%\nMaximal term length: 5\nWeighting          : term frequency (tf)\n\n\n\n상위 10개 단어에 대해서만 추출하엿다.\n\n\ninspect(my.news)\n\n<<TermDocumentMatrix (terms: 10, documents: 10)>>\nNon-/sparse entries: 54/46\nSparsity           : 46%\nMaximal term length: 5\nWeighting          : term frequency (tf)\nSample             :\n            Docs\nTerms        1 10 2 3 4  5  6 7 8 9\n  경쟁력     1  0 0 2 0  1  1 0 1 1\n  경진대회   0  0 0 0 1  0  0 2 0 5\n  관계자     0  1 0 1 0  3  0 1 0 0\n  광양제철소 0  0 0 0 0  0  0 0 0 6\n  데이터     3  6 3 7 8  7 10 3 5 5\n  브라이틱스 0  0 0 0 9  0  0 0 0 0\n  빅데이터   1  4 4 1 3 16  0 2 1 6\n  서비스     3  3 3 0 0  7  3 1 0 0\n  시스템     0  2 0 2 1  3  0 0 0 0\n  전문가     2  1 1 4 1  1  1 1 2 1"
  },
  {
    "objectID": "post/Lecture/RFD/데이터전처리/결측치(1205).html",
    "href": "post/Lecture/RFD/데이터전처리/결측치(1205).html",
    "title": "01. 결측치 처리",
    "section": "",
    "text": "is.na(x) : 벡터에서 결측가 있는 경우 True를 반환\ncomplete.cases(x) : 해당 데이터프레임에서 어떤 객체가 가지는 변수 중에 한 개라도 NA가 있을 경우 False 를 뱉음"
  },
  {
    "objectID": "post/Lecture/RFD/데이터전처리/결측치(1205).html#r실습",
    "href": "post/Lecture/RFD/데이터전처리/결측치(1205).html#r실습",
    "title": "01. 결측치 처리",
    "section": "R실습",
    "text": "R실습\n\nOzone 변수에 존재하는 na의 개수 산출\n\n\nsum(is.na(airquality$Ozone))\n\n37\n\n\n\ntable(is.na(airquality$Ozone))\n\n\nFALSE  TRUE \n  116    37 \n\n\n\napply함수를 이용하여 각 변수의 na값이 몇 개가 있는지 확인\n\n\n#summary(airquality)\napply(airquality,2,function(x) sum(is.na(x)))\n\nOzone37Solar.R7Wind0Temp0Month0Day0\n\n\n\ncomplete.case 함수를 이용하여 airquality 데이터에서 na값이 하나라도 존재하는 행들을 air_na 변수에 저장하고, na값을 하나도 가지지 않는 행들을 air_com 변수에 저장하기.\n\n\nair_na <- airquality[!complete.cases(airquality),]\n\n\nhead(air_na)\n\n\n\nA data.frame: 6 × 6\n\n    OzoneSolar.RWindTempMonthDay\n    <int><int><dbl><int><int><int>\n\n\n    5NA NA14.3565 5\n    628 NA14.9665 6\n    10NA194 8.669510\n    11 7 NA 6.974511\n    25NA 6616.657525\n    26NA26614.958526\n\n\n\n\n\nair_com <- airquality[complete.cases(airquality),]\n\n\nhead(air_com)\n\n\n\nA data.frame: 6 × 6\n\n    OzoneSolar.RWindTempMonthDay\n    <int><int><dbl><int><int><int>\n\n\n    141190 7.46751\n    236118 8.07252\n    31214912.67453\n    41831311.56254\n    723299 8.66557\n    819 9913.85958"
  },
  {
    "objectID": "post/Lecture/RFD/데이터전처리/결측치(1205).html#r-실습",
    "href": "post/Lecture/RFD/데이터전처리/결측치(1205).html#r-실습",
    "title": "01. 결측치 처리",
    "section": "R 실습",
    "text": "R 실습\n\nairquality의 Ozone 변수 값이 존재하지 않는 경우, 해당 변수를 평균값으로 대체하자.\n\n\nairquality$Ozone[is.na(airquality$Ozone)] <- mean(airquality$Ozone, na.rm=T)\n\n\nsum(is.na(airquality$Ozone))\n\n0\n\n\n\nDMwR 패키지\n\nlibrary(DMwR2)\n\n\nair_before <- airquality\n\n\nair_after <- centralImputation(airquality) ## 결측치를 중앙값으로 대치\n\n\nna_indx <- which(!complete.cases(airquality)) ## Na인덱스를 추출\n\n\nhead(air_before[na_indx,])\n\n\n\nA data.frame: 6 × 6\n\n    OzoneSolar.RWindTempMonthDay\n    <dbl><int><dbl><int><int><int>\n\n\n    542.12931NA14.3565 5\n    628.00000NA14.9665 6\n    11 7.00000NA 6.974511\n    2742.12931NA 8.057527\n    9678.00000NA 6.9868 4\n    9735.00000NA 7.4858 5\n\n\n\n\n\nhead(air_after)\n\n\n\nA data.frame: 6 × 6\n\n    OzoneSolar.RWindTempMonthDay\n    <dbl><dbl><dbl><int><int><int>\n\n\n    141.00000190 7.46751\n    236.00000118 8.07252\n    312.0000014912.67453\n    418.0000031311.56254\n    542.1293120514.35655\n    628.0000020514.96656\n\n\n\n\n\n아래의 값을 살펴본 결과 결측치가 중앙값으로 잘 대치 되었음을 확인하였다.\n\n\nmedian(airquality$Solar.R,na.rm=T)\n\n205\n\n\n\nk최근접 이웃 알고리즘을 이용하여 na값을 대치해보기\n\n\napply(knnImputation(air_before,k=3),2,function(x) sum(is.na(x)))\n\nOzone0Solar.R0Wind0Temp0Month0Day0"
  },
  {
    "objectID": "post/Lecture/RFD/데이터전처리/날짜 데이터 전처리.html",
    "href": "post/Lecture/RFD/데이터전처리/날짜 데이터 전처리.html",
    "title": "03. 날짜 데이터",
    "section": "",
    "text": "today <- Sys.Date()\ntoday\n\n2021-12-05\n\n\n\nclass(today)\n\n'Date'\n\n\n\ntime <- Sys.time()\ntime\n\n[1] \"2021-12-05 14:46:38 KST\"\n\n\n\nclass(time)\n\n\n'POSIXct''POSIXt'\n\n\n\n각각의 형식을 보면 생성된 객체들의 특성을 파악할 수 있다."
  },
  {
    "objectID": "post/Lecture/RFD/데이터전처리/날짜 데이터 전처리.html#현재-날자로부터-100일-후의-날짜-구하기",
    "href": "post/Lecture/RFD/데이터전처리/날짜 데이터 전처리.html#현재-날자로부터-100일-후의-날짜-구하기",
    "title": "03. 날짜 데이터",
    "section": "현재 날자로부터 100일 후의 날짜 구하기",
    "text": "현재 날자로부터 100일 후의 날짜 구하기\n\nSys.Date() + 100\n\n2022-03-15"
  },
  {
    "objectID": "post/Lecture/RFD/데이터전처리/날짜 데이터 전처리.html#로-표현된-데이터로부터-365일-후-의-날짜-구하기",
    "href": "post/Lecture/RFD/데이터전처리/날짜 데이터 전처리.html#로-표현된-데이터로부터-365일-후-의-날짜-구하기",
    "title": "03. 날짜 데이터",
    "section": "“2020-01-01”로 표현된 데이터로부터 365일 후 의 날짜 구하기",
    "text": "“2020-01-01”로 표현된 데이터로부터 365일 후 의 날짜 구하기\n\nas.Date(\"2020-01-01\",format=\"%Y-%m-%d\") + 365\n\n2020-12-31"
  },
  {
    "objectID": "post/Lecture/RFD/데이터전처리/날짜 데이터 전처리.html#과-2025-01-01-사이의-일-수-구하기",
    "href": "post/Lecture/RFD/데이터전처리/날짜 데이터 전처리.html#과-2025-01-01-사이의-일-수-구하기",
    "title": "03. 날짜 데이터",
    "section": "“1990-01-01”과 “2025-01-01” 사이의 일 수 구하기",
    "text": "“1990-01-01”과 “2025-01-01” 사이의 일 수 구하기\n\nas.Date(\"2025-01-01\")-as.Date(\"1990-01-01\")\n\nTime difference of 12784 days"
  },
  {
    "objectID": "post/Lecture/RFD/데이터전처리/날짜 데이터 전처리.html#구한-일수만-알고싶다면",
    "href": "post/Lecture/RFD/데이터전처리/날짜 데이터 전처리.html#구한-일수만-알고싶다면",
    "title": "03. 날짜 데이터",
    "section": "구한 일수만 알고싶다면?",
    "text": "구한 일수만 알고싶다면?\n\nas.numeric(as.Date(\"2025-01-01\")-as.Date(\"1990-01-01\"))\n\n12784\n\n\n\ndifftime(\"2025-01-01\",\"1990-01-01\")\n\nTime difference of 12784 days"
  },
  {
    "objectID": "post/Lecture/RFD/데이터전처리/날짜 데이터 전처리.html#시간-차이를-구할-경우",
    "href": "post/Lecture/RFD/데이터전처리/날짜 데이터 전처리.html#시간-차이를-구할-경우",
    "title": "03. 날짜 데이터",
    "section": "시간 차이를 구할 경우?",
    "text": "시간 차이를 구할 경우?\n\nas.difftime(\"09:40:00\")-as.difftime(\"18:30:00\")\n\nTime difference of -8.833333 hours"
  },
  {
    "objectID": "post/Lecture/RFD/데이터전처리/표준화와 정규화(1205).html",
    "href": "post/Lecture/RFD/데이터전처리/표준화와 정규화(1205).html",
    "title": "02. 표준화와 정규화",
    "section": "",
    "text": "각 개체들이 평균을 기준으로 얼마나 떨어져 있는지를 나타내는 값으로 변환하는 과정\nZ-Score 표준화는 각 요소의 값에서 평균을 뺀 후 표준편차로 나누어 수행\n변환 후 데이터의 평균은 0, 표준편차는 1의 값을 갖게 된다."
  },
  {
    "objectID": "post/Lecture/RFD/데이터전처리/표준화와 정규화(1205).html#r실습",
    "href": "post/Lecture/RFD/데이터전처리/표준화와 정규화(1205).html#r실습",
    "title": "02. 표준화와 정규화",
    "section": "R실습",
    "text": "R실습\n\nscale\n\ncenter : TRUE이면 데이터에서 해당벡터의 평균을 뺌\nscale :\n\ncenter = T, scale = T 이면 데이터를 해당 벡터의 표준편차로 나눔\ncenter = F, scale = T 이면 데이터를 해당 벡터의 제곱평균제곱근으로 나눔\nscale = F 이면 데이터를 어떤 값으로도 나누지 않음\n\n\nmtcars 데이터의 mpg, hp 변수로만 이루어진 데이터프레임을 생성하고 각 변수를 표준화한 새로운 변수를 추가해보자.\n\n\nlibrary(tidyverse)\n\n\ntest <- mtcars %>%  select(mpg,hp)\n\n\ntest %>% transmute_at(vars(mpg,hp),scale) %>% head()\n##scale(test$mpg, center=T,scale=T) 안되면 걍이런식으로 노가다 붙이자\n\n\n\nA data.frame: 6 × 2\n\n    mpghp\n    <dbl[,1]><dbl[,1]>\n\n\n    Mazda RX4 0.1508848-0.5350928\n    Mazda RX4 Wag 0.1508848-0.5350928\n    Datsun 710 0.4495434-0.7830405\n    Hornet 4 Drive 0.2172534-0.5350928\n    Hornet Sportabout-0.2307345 0.4129422\n    Valiant-0.3302874-0.6080186"
  },
  {
    "objectID": "post/Lecture/RFD/데이터전처리/표준화와 정규화(1205).html#r실습-1",
    "href": "post/Lecture/RFD/데이터전처리/표준화와 정규화(1205).html#r실습-1",
    "title": "02. 표준화와 정규화",
    "section": "R실습",
    "text": "R실습\n\n사용자 정의함수 생성\n\n\nnormal <- function(x)\n    {\n    return ((x- min(x)) / (max(x)-min(x)))\n}\n\n\niris2 <- iris %>% transmute_at(vars(-Species),normal)\n\n\nhead(iris); head(iris2)\n\n\n\nA data.frame: 6 × 5\n\n    Sepal.LengthSepal.WidthPetal.LengthPetal.WidthSpecies\n    <dbl><dbl><dbl><dbl><fct>\n\n\n    15.13.51.40.2setosa\n    24.93.01.40.2setosa\n    34.73.21.30.2setosa\n    44.63.11.50.2setosa\n    55.03.61.40.2setosa\n    65.43.91.70.4setosa\n\n\n\n\n\n\nA data.frame: 6 × 4\n\n    Sepal.LengthSepal.WidthPetal.LengthPetal.Width\n    <dbl><dbl><dbl><dbl>\n\n\n    10.222222220.62500000.067796610.04166667\n    20.166666670.41666670.067796610.04166667\n    30.111111110.50000000.050847460.04166667\n    40.083333330.45833330.084745760.04166667\n    50.194444440.66666670.067796610.04166667\n    60.305555560.79166670.118644070.12500000"
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/T-검정.html",
    "href": "post/Lecture/RFD/통계분석/T-검정.html",
    "title": "02. T-test",
    "section": "",
    "text": "개념 : 단일모집단에서 관심이 있는 연속형 벼수의 평균값을 특정기준값과 비교하고자 할 때 사용\n\n\\[ H_0 : \\mu_0 = \\mu_1 \\quad H_1 : not\\,\\, H_0\\]\n\n모집단이 정규성을 따른다고 가정하고 표본의 크기가 30보다 클 경우, 중심극한정리에 따라 정규분포를 따른다고 가정함.\n정규성을 만족할 경우 t-test, 그렇지 않은 경우 wilcox.test 함수를 이용하여 T-검정을 수행\n\n\n\n\\[H_0 : \\mu = 2.6 \\quad H_1 :  not \\, H_0\\]\n\nlibrary(MASS)\n\n\n str(cats) ## \n\n'data.frame':   144 obs. of  3 variables:\n $ Sex: Factor w/ 2 levels \"F\",\"M\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Bwt: num  2 2 2 2.1 2.1 2.1 2.1 2.1 2.1 2.1 ...\n $ Hwt: num  7 7.4 9.5 7.2 7.3 7.6 8.1 8.2 8.3 8.5 ...\n\n\n\n표본의 크기가 30보다 크므로 정규성을 만족한다고 가정\n\n\nt.test(cats$Bwt,mu=2.6,alternative= \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  cats$Bwt\nt = 3.0565, df = 143, p-value = 0.002673\nalternative hypothesis: true mean is not equal to 2.6\n95 percent confidence interval:\n 2.643669 2.803553\nsample estimates:\nmean of x \n 2.723611 \n\n\n\n검정통계량의 근거한 p-value 값을 보았을 때 귀무가설을 기각한다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/T-검정.html#r-실습-1",
    "href": "post/Lecture/RFD/통계분석/T-검정.html#r-실습-1",
    "title": "02. T-test",
    "section": "2-1. R 실습",
    "text": "2-1. R 실습\n\ndata <- data.frame(before = c(7,3,4,5,2,1,6,6,5,4),\n                  after = c(8,4,5,6,2,3,6,8,6,5))\n\n\nshapiro.test(c(data$before,data$after)) ## 정규성을 만족\n\n\n    Shapiro-Wilk normality test\n\ndata:  c(data$before, data$after)\nW = 0.95961, p-value = 0.5362\n\n\n\nt.test(data$before,data$after, paired=T,alternative=\"less\")\n\n\n    Paired t-test\n\ndata:  data$before and data$after\nt = -4.7434, df = 9, p-value = 0.0005269\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n       -Inf -0.6135459\nsample estimates:\nmean of the differences \n                     -1 \n\n\n\n결과적으로 영양제 복용 후 수면시간이 더 길었다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/T-검정.html#r-실습-2",
    "href": "post/Lecture/RFD/통계분석/T-검정.html#r-실습-2",
    "title": "02. T-test",
    "section": "3-1. R 실습",
    "text": "3-1. R 실습\n\nlibrary(MASS)\n\n\nstr(cats)\n\n'data.frame':   144 obs. of  3 variables:\n $ Sex: Factor w/ 2 levels \"F\",\"M\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Bwt: num  2 2 2 2.1 2.1 2.1 2.1 2.1 2.1 2.1 ...\n $ Hwt: num  7 7.4 9.5 7.2 7.3 7.6 8.1 8.2 8.3 8.5 ...\n\n\n\nvar.test(Bwt~Sex,data=cats)\n\n\n    F test to compare two variances\n\ndata:  Bwt by Sex\nF = 0.3435, num df = 46, denom df = 96, p-value = 0.0001157\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.2126277 0.5803475\nsample estimates:\nratio of variances \n         0.3435015 \n\n\n\n등분산 검정결과 성별에 따른 두집단은 등분산성을 만족하지 않음\n\n\nt.test(Bwt~Sex, data=cats, alternative=\"two.sided\",var.equal = F)\n\n\n    Welch Two Sample t-test\n\ndata:  Bwt by Sex\nt = -8.7095, df = 136.84, p-value = 8.831e-15\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n -0.6631268 -0.4177242\nsample estimates:\nmean in group F mean in group M \n       2.359574        2.900000 \n\n\n\n“고양이들의 성별에 따른 평균 몸무게에는 통계적으로 유의한 차이가 존재한다.” 고 결론을 내릴 수 있다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/고급회귀분석.html",
    "href": "post/Lecture/RFD/통계분석/고급회귀분석.html",
    "title": "07. regression analysis-2",
    "section": "",
    "text": "정규화 선형회귀는 선형회귀 계수에 대한 제약 조건을 추가하여 모델이 과도하게 최적화되는 현상을 막는 방법이다.\n즉, 불편성은 포기하되 모형의 해석력을 단순화하자는 것이 초점이다.\n\n\n\n\n교차타당법에 의한 선택이 일반적임\n또는 임의의 \\(\\lambda\\) grid를 선택하고 가능한 모든 값 중 가장 우수한 \\(\\lambda\\)를 선택"
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/고급회귀분석.html#r-실습",
    "href": "post/Lecture/RFD/통계분석/고급회귀분석.html#r-실습",
    "title": "07. regression analysis-2",
    "section": "R 실습",
    "text": "R 실습\n\nlibrary(tidyverse)\nlibrary(mosaicData)\nlibrary(glmnet) ## 리지 및 라쏘 회귀를 적합하기 위한 패키지\n\n\nglimpse(RailTrail)\n\nRows: 90\nColumns: 11\n$ hightemp   <int> 83, 73, 74, 95, 44, 69, 66, 66, 80, 79, 78, 65, 41, 59, 50,~\n$ lowtemp    <int> 50, 49, 52, 61, 52, 54, 39, 38, 55, 45, 55, 48, 49, 35, 35,~\n$ avgtemp    <dbl> 66.5, 61.0, 63.0, 78.0, 48.0, 61.5, 52.5, 52.0, 67.5, 62.0,~\n$ spring     <int> 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,~\n$ summer     <int> 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,~\n$ fall       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,~\n$ cloudcover <dbl> 7.6, 6.3, 7.5, 2.6, 10.0, 6.6, 2.4, 0.0, 3.8, 4.1, 8.5, 7.2~\n$ precip     <dbl> 0.00, 0.29, 0.32, 0.00, 0.14, 0.02, 0.00, 0.00, 0.00, 0.00,~\n$ volume     <int> 501, 419, 397, 385, 200, 375, 417, 629, 533, 547, 432, 418,~\n$ weekday    <lgl> TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, TR~\n$ dayType    <chr> \"weekday\", \"weekday\", \"weekday\", \"weekend\", \"weekday\", \"wee~\n\n\n\nmodel.matrix는 기본적으로 절편을 포함한 모형을 산정한다. 절편을 제외시킨 모형을 고려해보자\n\n\nx <- model.matrix(volume~.-1 ,RailTrail) \n\n\ny <- RailTrail$volume\n\n\n모형적합\n\n\nridge.fit <- cv.glmnet(x,y,alpha=0) ## 모형적합 alpha=0 은 릿지를 말함\n\n\n교차타당에러를 최소로하는 람다값을 구한결과 11.757로 산출되었다.\n\n\nbestlam <- ridge.fit$lambda.min\nlog(bestlam)\n\n2.65058474242334\n\n\n\ncoef(ridge.fit, s = \"lambda.min\")\n\n12 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)     97.879570\nhightemp         3.923302\nlowtemp         -1.148241\navgtemp          1.971339\nspring          10.236204\nsummer           3.316273\nfall           -27.057463\ncloudcover      -8.082976\nprecip         -90.969378\nweekdayFALSE    13.410258\nweekdayTRUE    -13.487794\ndayTypeweekend  13.616159\n\n\n\noptions(repr.plot.res=200,repr.plot.height=4,repr.plot.width=10)\n\n\nbestlam에서 MSE가 최소가 되는 지점을 확인\n\n\nplot(ridge.fit)\nabline(v=log(bestlam),lty=\"dashed\",lwd=2,col=\"blue\")\n\n\n\n\n\n산출된 best.lam값으로 모형을 다시 적합 후 MSE를 구해보자.\n\n\ntrain  <- sample(1:nrow(x),nrow(x)*0.7)\ntest  <- -train\ny.test <- y[test]\ny.train <- y[train]\n\n\nridge.fit <- glmnet(x[train,],y.train,alpha=0,lambda=bestlam,family=\"gaussian\")\n\n\nridge.pred <- predict(ridge.fit,s=bestlam,newx=x[test,]) ## type=\"coefficients\"로 하면 예측된 베타계수를 보여줌\n\n\nridge.coef <- predict(ridge.fit,s=bestlam,newx=x[test,],type=\"coefficients\") ## type=\"coefficients\"로 하면 예측된 베타계수를 보여줌\n\n\nridge.coef\n\n12 x 1 sparse Matrix of class \"dgCMatrix\"\n                        s1\n(Intercept)     64.9007304\nhightemp         3.9758591\nlowtemp         -0.7261573\navgtemp          2.0855905\nspring          20.3027908\nsummer         -13.1841751\nfall           -18.2167429\ncloudcover      -6.5169836\nprecip         -59.6275608\nweekdayFALSE    12.1230181\nweekdayTRUE    -12.6441270\ndayTypeweekend  12.3701010\n\n\n\nmean((ridge.pred - y.test)^2)\n\n11360.201935448\n\n\n\nmse가 만 단위가 넘게 나왔는데. 지나치게 모형을 단순화 하였다는 생각이 든다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/교차분석.html",
    "href": "post/Lecture/RFD/통계분석/교차분석.html",
    "title": "04. chi-square test",
    "section": "",
    "text": "개념 : 범주형 자료(명목/서열)인 두 변수 간의 관계를 알아보기 위해 실시하는 분석 기법\n적합성, 독립성, 동질성 검정에 사용되며, 카이제곱($ ^2$) 검정 통계량을 이용한다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/교차분석.html#r-실습",
    "href": "post/Lecture/RFD/통계분석/교차분석.html#r-실습",
    "title": "04. chi-square test",
    "section": "R 실습",
    "text": "R 실습\n$H_0 : $ 전체 응답자 중 왼손잡이의 비율이 20%, 오른손잡이의 비율이 80% 이다.\n\\(H_1 : not\\,\\,H_0\\)\n\nlibrary(MASS)\n\n\nstr(survey)\n\n'data.frame':   237 obs. of  12 variables:\n $ Sex   : Factor w/ 2 levels \"Female\",\"Male\": 1 2 2 2 2 1 2 1 2 2 ...\n $ Wr.Hnd: num  18.5 19.5 18 18.8 20 18 17.7 17 20 18.5 ...\n $ NW.Hnd: num  18 20.5 13.3 18.9 20 17.7 17.7 17.3 19.5 18.5 ...\n $ W.Hnd : Factor w/ 2 levels \"Left\",\"Right\": 2 1 2 2 2 2 2 2 2 2 ...\n $ Fold  : Factor w/ 3 levels \"L on R\",\"Neither\",..: 3 3 1 3 2 1 1 3 3 3 ...\n $ Pulse : int  92 104 87 NA 35 64 83 74 72 90 ...\n $ Clap  : Factor w/ 3 levels \"Left\",\"Neither\",..: 1 1 2 2 3 3 3 3 3 3 ...\n $ Exer  : Factor w/ 3 levels \"Freq\",\"None\",..: 3 2 2 2 3 3 1 1 3 3 ...\n $ Smoke : Factor w/ 4 levels \"Heavy\",\"Never\",..: 2 4 3 2 2 2 2 2 2 2 ...\n $ Height: num  173 178 NA 160 165 ...\n $ M.I   : Factor w/ 2 levels \"Imperial\",\"Metric\": 2 1 NA 2 2 1 1 2 2 2 ...\n $ Age   : num  18.2 17.6 16.9 20.3 23.7 ...\n\n\n\ndata <- table(survey$W.Hnd)\n\n\nchisq.test(data, p = c(0.2,0.8))\n\n\n    Chi-squared test for given probabilities\n\ndata:  data\nX-squared = 22.581, df = 1, p-value = 2.015e-06\n\n\n\n유의확률이 0.05보다 작으므로 “전체 응답자 중 왼손잡이의 20%, 오른손잡이의 80%이다.” 라는 귀무가설을 기각한다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/교차분석.html#r-실습-1",
    "href": "post/Lecture/RFD/통계분석/교차분석.html#r-실습-1",
    "title": "04. chi-square test",
    "section": "R 실습",
    "text": "R 실습\n$H_0 : $ 주로 사용하는 손(W.Hnd)와 운동 빈도(Exer)는 독립이다.\n\\(H_1: not \\,\\, H_0\\)\n\ndata <- table(survey$W.Hnd,survey$Exer)\nchisq.test(data)\n\nWarning message in chisq.test(data):\n\"Chi-squared approximation may be incorrect\"\n\n\n\n    Pearson's Chi-squared test\n\ndata:  data\nX-squared = 1.2065, df = 2, p-value = 0.547\n\n\n\n검정결과 주로 사용하는 손과 운동 빈도는 서로 독립이라고 말할 수 있다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/다차원척도법(1204).html",
    "href": "post/Lecture/RFD/통계분석/다차원척도법(1204).html",
    "title": "10. Multidimensional scaling",
    "section": "",
    "text": "1. 객체간 근접성(Proximity)을 시각화하는 통계기법.\n2. 군집분석과 같이 개체들을 대상으로 변수들을 측정한 후 개체들 사이의 유사성/비유사성을 측정하여 2차원 또는 3차원 공간상에 점으로 표현"
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/다차원척도법(1204).html#가.-계량적-mdsmetric-mds",
    "href": "post/Lecture/RFD/통계분석/다차원척도법(1204).html#가.-계량적-mdsmetric-mds",
    "title": "10. Multidimensional scaling",
    "section": "가. 계량적 MDS(Metric MDS)",
    "text": "가. 계량적 MDS(Metric MDS)\n\n데이터가 구간 또는 비율척도인 경우 활용\nN개의 케이스에 대해서 p개의 특성변수가 있는 경우, 각 개체들간의 유클리드 거리행렬을 계산하고 개체들간의 비유사성 \\(S\\)를 공간상에 표현한다.\n\n\nR 실습\n\nlibrary(MASS)\n\n\nMASS 패키지 안에 내장된 자료를 이용\n\n\nloc <- cmdscale(eurodist,k=2) ## 거리행렬을 다시 x,y좌표로 바꾸는 작업?\n\n\nx <- loc[,1]\ny <- -loc[,2]  ## y축은 북쪽 도시를 상단에 표시하기 위해 부호를 바꾼다.\n\n\noptions(repr.plot.res=200,repr.plot.height=5,repr.plot.width=10)\n\n\nplot(x,y,type=\"n\",asp=1,main=\"metric MDS\" )\ntext(x,y,rownames(loc),cex=0.7)\nabline(v=0,h=0,lty=2,lwd=0.5)\n\n\n\n\n\n즉 유사한 도시들은 가까이 밀집해있고 그렇지 않은 도시들은 멀리 떨어져있음을 확인할 수 있다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/다차원척도법(1204).html#나.-비계량적-mds-nonmetric-mds",
    "href": "post/Lecture/RFD/통계분석/다차원척도법(1204).html#나.-비계량적-mds-nonmetric-mds",
    "title": "10. Multidimensional scaling",
    "section": "나. 비계량적 MDS (Nonmetric MDS)",
    "text": "나. 비계량적 MDS (Nonmetric MDS)\n\n데이터가 순서척도 인 경우 활용한다.\n개체들간의 거리가 순서로 주어진 경우에는 순서척도를 거리의 속성과 같도록 변환\n그 후 거리를 생성하여 적용한다.\nR 에서는 isoMDS 함수를 이용한다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/다차원척도법(1204).html#r-실습-1",
    "href": "post/Lecture/RFD/통계분석/다차원척도법(1204).html#r-실습-1",
    "title": "10. Multidimensional scaling",
    "section": "R 실습",
    "text": "R 실습\n\n기존의 데이터프레임을 행렬구조로 변환해줌\n\n\nswiss.x <- as.matrix(swiss[,-1])\n\n\nswiss.dist <- dist(swiss.x)\n\n\nswiss.mds <- isoMDS(swiss.dist)\n\ninitial  value 2.979731 \niter   5 value 2.431486\niter  10 value 2.343353\nfinal  value 2.338839 \nconverged\n\n\n\nplot(swiss.mds$points,type=\"n\")\ntext(swiss.mds$points,labels=as.character(1:nrow(swiss.x)))"
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/데이터 샘플링.html",
    "href": "post/Lecture/RFD/통계분석/데이터 샘플링.html",
    "title": "01. sampling",
    "section": "",
    "text": "1. 단순 임의 추출\n\nn <- nrow(iris)\n\n\nidx <- sample(1:n,n*0.7,replace=F)\n\n\ntrain <- iris[idx,]\ntest <- iris[-idx]\n\n\n\n2. 층화 임의 추출\n\nmethod  1. srswor : 비복원 단순 임의 추출  2. srswr : 복원 단순 임의 추출  3. possion : 포아송 추출  4. systematic : 계통 추출 \n\n\nlibrary(sampling)\n\n\nsamples <- strata(iris,c(\"Species\"),size=c(20,15,15),method=\"srswor\")\n\n\niris_sample <- getdata(iris,samples)\n\n\n\n3. 계통 추출\n\nformula : ~ 우축에 나열한 이름에 따라 데이터가 그룹으로 묶임\nfrac = 0.1 : 추출할 샘플 비율 기본값은 10%\nreplace : 복원 추출 여부\ndata = parent.frame() : 추출할 데이터 프레임\nsystematic = F : 계통 추출(Systematic Sampling)을 사용할지 여부\n\n\nlibrary(doBy)\n\n\nhead(sampleBy(~ Species,frac = 0.3,data = iris, systematic =T))\n\n\n\nA data.frame: 6 × 5\n\n    Sepal.LengthSepal.WidthPetal.LengthPetal.WidthSpecies\n    <dbl><dbl><dbl><dbl><fct>\n\n\n    setosa.15.13.51.40.2setosa\n    setosa.44.63.11.50.2setosa\n    setosa.74.63.41.40.3setosa\n    setosa.115.43.71.50.2setosa\n    setosa.144.33.01.10.1setosa\n    setosa.175.43.91.30.4setosa"
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/분산분석(ANOVA).html",
    "href": "post/Lecture/RFD/통계분석/분산분석(ANOVA).html",
    "title": "03. ANOVA",
    "section": "",
    "text": "T-검정과 달리 두 개 이상의 다수 집단 간 평균을 비교하는 통계분석 방법\n반응값에 대한 하나의 범주형 변수의 영향을 알아보기 위해 사용한다.\n가정 : 각 집단의 측정치는 서로 독립적이며, 정규분포를 따른다. 또한 각 집단 측정치의 분산은 같다.\n등분산 검정은 : 정규성을 만족할 경우 Bartlett.test, 그렇지 않은 경우Levene.test(lawstat 패키지)를 사용\n정규성 가정이 깨졌다면? \\(\\to\\) kruskal-Wallis Rank Sum Test\n정규성 가정이 깨지지 않았다면? ->F통계량에 근거한 p-value 값으로 가설의 유의성을 검증한다.\n사후 검정 : 귀무가설이 기각된 경우 어떠한 집단들에 대해서 평균의 차이가 존재하는지를 알아보기 위해 실시하는 분석\n\n\n\n\nby( iris$Sepal.Width,iris$Species,shapiro.test)\n\niris$Species: setosa\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.97172, p-value = 0.2715\n\n------------------------------------------------------------ \niris$Species: versicolor\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.97413, p-value = 0.338\n\n------------------------------------------------------------ \niris$Species: virginica\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.96739, p-value = 0.1809\n\n\n\n정규성 검정결과 세 가지 종 모두 정규성을 만족한다.\n\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\nbartlett.test(Sepal.Width~Species ,data= iris)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  Sepal.Width by Species\nBartlett's K-squared = 2.0911, df = 2, p-value = 0.3515\n\n\n\n검정결과 3가지 종의 꽃잎길이는 등분산성을 만족한다.\n정규성과 등분산성을 만족한다는 가정하에 일원배치 분산분석을 수행\n\n\nresult <- oneway.test(Sepal.Width~Species, data=iris,var.equal=T)\nresult\n\n\n    One-way analysis of means\n\ndata:  Sepal.Width and Species\nF = 49.16, num df = 2, denom df = 147, p-value < 2.2e-16\n\n\n\n검정결과 귀무가설을 기각 즉, 적어도 어느 하나의 종의 Speal.Width가 나머지 종들과 통계적으로 유의한 차이가 있다고 말할 수 있다.\n사후검정을 통해 확인\n\n\nlibrary(stats)\n\n\npairwise.t.test(iris$Sepal.Width,iris$Species,pool.sd=F,p.adj=\"bonf\") ## pool.sd=F는 합동공분산 사용여부 \n\n\n    Pairwise comparisons using t tests with non-pooled SD \n\ndata:  iris$Sepal.Width and iris$Species \n\n           setosa  versicolor\nversicolor 7.5e-15 -         \nvirginica  1.4e-08 0.0055    \n\nP value adjustment method: bonferroni \n\n\n\n$H_0 : $ 집단들 사이의 평균은 같다.\n$H_1 : $ 집단들 사이의 평균은 같지 않다.\n\\(p-value\\) 값을 살펴본 결과 모든 종들에 대해서 꽃받침 폭의 평균값은 통계적으로 유의한 차이가 있다는 것을 알 수 있다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/분산분석(ANOVA).html#r-실습-1",
    "href": "post/Lecture/RFD/통계분석/분산분석(ANOVA).html#r-실습-1",
    "title": "03. ANOVA",
    "section": "R 실습",
    "text": "R 실습\n\n실린더 개수와 변속기 종류에 따른 주행거리의 차이가 있는지 검정\n\n\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n\nmtcars$cyl <- as.factor(mtcars$cyl)\nmtcars$am <- as.factor(mtcars$am)\n\n\nlibrary(tidyverse)\n\n\n#mtcars %>% \n #       group_by(cyl,am) %>% \n  #          summarise(statistic = shapiro.test(mpg)$statistic,\n  #                    p.value = shapiro.test(mpg)$p.value) \n\n\n본래는 위와 같이 검정을 하여야 하나 샘플사이즈가 3보다 적은 그룹이 있어 계산 불가…\n\n\ncar_aov <- aov(mpg~cyl*am,mtcars)\n\n\nsummary(car_aov)\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncyl          2  824.8   412.4  44.852 3.73e-09 ***\nam           1   36.8    36.8   3.999   0.0561 .  \ncyl:am       2   25.4    12.7   1.383   0.2686    \nResiduals   26  239.1     9.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n검정결과 실린더개수(cyl) 에 따른 주행거리 차이만 통계적으로 유의하다는 결론을 내렸다.\n\n\noptions(repr.plot.res=200,repr.plot.height=5,repr.plot.width=10)\n\n\ninteraction.plot(mtcars$cyl,mtcars$am,mtcars$mpg,col=c(\"red\",\"blue\"),lwd=2)\n\n\n\n\n\n일반적으로 상호작용 그래프에서 두 선이 서로 교차하고 있을 시에는 \\(x\\) 축에 있는 독립변수와 그래프에서 시각화된 독립변수 간에는 상호작용이 존재한다고 해석할 수 있다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/상관분석.html",
    "href": "post/Lecture/RFD/통계분석/상관분석.html",
    "title": "05. cor-test",
    "section": "",
    "text": "$H_ 0 : $ 변수간에는 상관관계가 없다( 상관계수 \\(= 0\\))\n$H_1 : $ 변수간에는 상관관계가 있다.( 상관계수 \\(\\neq 0\\) )\n\n피어슨 상관계수\n\n두 연속형 자료가 모두 정규성을 따른다는 가정하에 선형적 상관관계를 측정\n\n스피어만 상관계수\n\n데이터가 정규성을 만족하지 않거나 순위 및 순서 형태로 주어지는 경우 사용\n피어슨 상관계수와 달리 비선형 관계의 연관성을 파악할 수 있다.\n비모수적 방법\n\n켄달의 순위상관계수\n\n\\(X_i\\)가 커짐에 따라 \\(Y_i\\)도 커질 경우 부합, 작아질 경우 비부합이라고 본다.\n전체 데이터에서 비부합쌍에 대한 부합쌍의 비율로 상관계수를 산출한다.\n순위상관계수가 -1 일 경우 비부합쌍의 비율이 100%, 0일 경우 두 변수 \\(X,Y\\)는 상관성이 없음을 의미한다.\n\n\n\\(\\divideontimes\\) 원래의 경우 독립이면 상관계수는 0이지만, 이것에 대한 역은 반드시 성립하지 않는다.\nhttps://techntalk.tistory.com/entry/%ED%86%B5%EA%B3%84%EC%A0%81%EC%9C%BC%EB%A1%9C-%EB%91%90-%EB%B3%80%EC%88%98%EC%9D%98-%EB%8F%85%EB%A6%BDindependence%EA%B3%BC-%EC%83%81%EA%B4%80%EA%B3%84%EC%88%98correlation%EC%99%80%EC%9D%98-%EA%B4%80%EA%B3%84\n\n\n\nlibrary(tidyverse)\n\n\ndata(\"airquality\")\n\n\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n\n\nair <- airquality %>% select(-c(Day,Month))\n\n\nair_cor <- cor(air,use=\"pairwise.complete.obs\",method=\"pearson\")\n\n\nuse\n\neverything : 결측값 존재 시 NA출력\nall.obs : 결측값 존재 시 오류 메시지 출력\ncomplete.obs : 변수별로 결측값을 제외하고 상관계수 계산\npairwise.complete.obs : 모든 변수 쌍에서 결측값이 없는 데이터들에 대해 상관계수 계산\n\n\n\nlibrary(corrplot)\n\n\ntestRes <- cor.mtest(air,method=\"pearson\")\n\n\noptions(repr.plot.res=200,repr.plot.height=5,repr.plot.width=10)\ncorrplot(air_cor,diag=F,type=\"upper\",p.mat=testRes$p,\n            method=\"circle\",number.cex=1.5,addCoef.col=\"black\")\n\n\n\n\n\n위 그래프를 해석하면 Wind와 Solar.R간에는 상관관계가 없다고 해석할 수 있다.\n\n\ncor.test(air$Wind,air$Solar.R) ##실제 검정 결과도 동일하다.\n\n\n    Pearson's product-moment correlation\n\ndata:  air$Wind and air$Solar.R\nt = -0.6826, df = 144, p-value = 0.496\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.2172359  0.1066406\nsample estimates:\n        cor \n-0.05679167"
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/시계열 분석.html",
    "href": "post/Lecture/RFD/통계분석/시계열 분석.html",
    "title": "11. Time-seris analysis",
    "section": "",
    "text": "정상 시계열과 비정상 시계열을 구분할 줄 알아야한다.\n정상성을 만족하지 못하는 시계열 자료는 다른 시기로 일반화 할 수 없기 때문에 정상 시계열로 전환 후에 모형을 적합해주어야 한다.\nARIMA 모형과 분해 시계열 분석을 할 수 있다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/시계열 분석.html#가.-분석-방법",
    "href": "post/Lecture/RFD/통계분석/시계열 분석.html#가.-분석-방법",
    "title": "11. Time-seris analysis",
    "section": "가. 분석 방법",
    "text": "가. 분석 방법\n\nA. 단기 예측 - 이동평균법\n\n과거로부터 현재까지의 시계열 자료를 대상으로 일정기간별 이동평균을 계산하고, 이들의 추세를 파악하여 다음 기간을 예측하는 방법\n즉 표본평균처럼 관측값 전부에 동일한 가중치를 주는 대신에 최근 \\(m\\) 개의 값들만을 이용하여 평균을 구하는 방법이다.\n예측보다는 원 시계열 자료에서 계절변동과 불규칙 변동을 제거하여 추세변동과 순환변동만 가진 시계열로 변환하는 방법으로도 사용됨\n\n이론적인 설명은 저건데 모형 적합시에는 \\(\\hat {Z}_t = \\hat {T}_t + \\hat {S}_t\\) 로 예측값을 추정함\n가법모형은 계절성분의 진폭이 시계열 수준에 상관없이 일정할 때 사용 \\(\\hat {Z}_t = \\hat {T}_t + \\hat {S}_t\\)\n승법모형은 계절성분의 진폭이 시계열 수준에 따라 달라질 때 사용 \\(\\hat {Z}_t = \\hat {T}_t \\times \\hat {S}_t\\)\n\n\\(n+1\\) 시점의 데이터를 \\(m\\)개의 데이터를 가지고 예측하는 경우는 다음과 같다.\n\n\\[F_{n+1} = \\frac {1}{m}(Z_n + Z_{n-1} + \\dots Z_{n-m+1}) =\\frac 1m \\sum_{t}^{n}Z_t, \\quad t=n-m+1\\]\n\nPractice with R\n\nlibrary(forecast) ## 예측값을 구하기 위한 패키지\nlibrary(TTR) ## 이동평균법을 이용하기 위한 패키지\nlibrary(lmtest) ## 더비-왓슨 검정 \nlibrary(tidyverse) ## 전처리 패키지\n\n\nsetwd(\"C:\\\\Users\\\\lee\\\\Desktop\\\\고급시계열 분석\\\\제5판_시계열분석_프로그램\\\\제5판 시계열분석 data\")\n\n\n데이터 호출 : 주기는 12로 월별 데이터임\n\n\nz <- scan(\"food.txt\")\nt <- 1:length(z)\nfood <- ts(z, start=c(1981,1), frequency=12) \n\n\n시도표 확인  1. 추세성분이 보인다.  2. 계절성분이 보인다.  3. 이분산성이 보인다. \n\n\noptions(repr.plot.res=200,repr.plot.height=6,repr.plot.width=10)\nplot.ts(food)\n\n\n\n\n\n이분산성을 제거하기 위해 로그변환을 시도\n\n\nlog_food <- log(food)\nplot.ts(log_food)\n\n\n\n\n\n\nstl\n\nstl function을 이용하여 모델 적합\n\n\nstl_fit1  <- stl(log_food,s.window=12) ## s.window=12 : 계절성분의 주기가 12이다.\n\n\nstl_fit1$time.series[1:10,]\n\n\n\nA matrix: 10 × 3 of type dbl\n\n    seasonaltrendremainder\n\n\n    -0.090355043.789108 0.09223150\n    -0.147931393.787232 0.04957932\n    -0.019896143.785355-0.03017285\n     0.038295803.783478-0.04901299\n     0.094538723.782254-0.04815173\n     0.043787293.781031-0.01593573\n     0.016322463.779807-0.01421511\n     0.054057673.779134-0.03770230\n     0.039065453.778461-0.04937347\n    -0.017797403.777788 0.02192421\n\n\n\n\n\n계절성분, 추세성분, 불규칙성분의 값들을 볼 수 있음.\n추세성분으로 추정한 경우 \\(\\hat {\\beta}_0 + \\hat{\\beta}_1z_t= 0.07 + 3.7z_t\\)로 앞서 구한 데이터와 동일(이건 수업시간에 한거에 나와있음)\n\n\nplot(stl_fit1)\n\n\n\n\n\n추정값을 구해보자\n\n\npred_stl <- stl_fit1$time.series[,1] + stl_fit1$time.series[,2] ## 추세성분 + 계절성분\n\nts.plot(log_food, pred_stl, col=1:2, lty=1:2, ylab=\"food\", xlab=\"time\",\n        main=\"원시계열과 분해법에 의한 추정값\",lwd=2)\nlegend(\"topleft\", lty=1:2, col=1:2, c(\"원시계열\", \"추정값\"))\n\n\n\n\n\nirregular <- log_food-pred_stl\nsum(irregular^2)/(length(log_food)-1) ## MSE 계산\n\n0.001245257904363\n\n\n\n\ndecompose\n\ndecompose function을 이용하여 모형 적합\n\n\ndec_fit  <- decompose(log_food)\n\n\ndec_fit$trend[1:10]\n\n\n<NA><NA><NA><NA><NA><NA>3.776909155269433.768171473872113.765649627472043.77219261288612\n\n\n\ndec_fit$seasonal[1:10]\n\n\n-0.0799020938844753-0.146043800620445-0.01274493183601310.04049677614460120.09068184389565770.03967866993284980.01048177141592280.0587619091303550.041341551797343-0.0179374241117825\n\n\n\ndec_fit$random[1:10] ## 불규칙성분\n\n\n<NA><NA><NA><NA><NA><NA>-0.00547660660422888-0.0314441938302674-0.03883854426093690.0276591313067893\n\n\n\nplot(dec_fit)\n\n\n\n\n\n추정치 구하기\n\n\npred_dec <-dec_fit$trend+dec_fit$seasonal\n\nts.plot(pred_stl, pred_dec, col=1:2, lty=1:2, ylab=\"food\", xlab=\"time\",\n        main=\"원시계열과 분해법에 의한 추정값\",lwd=2)\nlegend(\"topleft\", lty=1:2, col=1:2, c(\"원시계열\", \"추정값\"),lwd=2)\n\n\n\n\n\nstl vs decompose\n\n\nts.plot(pred_stl, pred_dec, col=1:2, lty=1:2, ylab=\"food\", xlab=\"time\",\n        main=\"stl vs. decompose\",lwd=2)\nlegend(\"topleft\", lty=1:2, col=1:2, c(\"stl\", \"decompose\"),lwd=2)\n\n\n\n\n\nstl과 달리 decompose는 승법모형을 지원해준다.\n\n\ndec_fit2 <- decompose(food, type = \"multiplicative\")\ndec_fit2$trend[1:10]\n\n\n<NA><NA><NA><NA><NA><NA>43.729166666666743.379166666666743.291666666666743.5875\n\n\n\ndec_fit2$seasonal[1:10]\n\n\n0.9219246356829520.8621213373421160.9844960475951441.038004198013531.091650489534581.037859336820561.008002905607491.059170985651181.042754035631620.980759172033513\n\n\n\ndec_fit2$random[1:10]\n\n\n<NA><NA><NA><NA><NA><NA>0.995936238493310.9685292269139350.9591835261281731.0269284401149\n\n\n\nplot(dec_fit2)\n\n\n\n\n\npred_dec2 <-dec_fit2$trend*dec_fit2$seasonal\n\nts.plot(food, pred_dec2, col=1:2, lty=1:2, ylab=\"food\", xlab=\"time\",\n        main=\"원시계열과 분해법에 의한 추정값\",lwd=2)\nlegend(\"topleft\", lty=1:2, col=1:2, c(\"원시계열\", \"추정값\"),lwd=2)\n\n\n\n\n\n가법모형 vs 승법모형\n\n\nts.plot(exp(pred_dec), pred_dec2, col=1:2, lty=1:2, ylab=\"food\", xlab=\"time\",\n        main=\"가법모형과 승법모형 추정값\")\nlegend(\"topleft\", lty=1:2, col=1:2, c(\"가법모형\", \"승법모형\"))\n\n\n\n\n\n승법모형과 가법모형 비교 결과 거의 동일한 모형임을 확인할 수 있다.\n\n\n\n\n\nB. 단기 예측 - 지수평활법\n\n이동평균법이 \\(m\\)개의 데이터의 평균을 이용하여(=동일한 가중치를 부여) \\(n+1\\) 시점의 예측값을 구했다면\n지수평활법은 최근 시점에 관측치에 더 많은 가중치를 부여하여 미래를 예측하는 방법이다.\n\n$F_{n+1} = Z_n + (1-) Z_{n-1} + (1-)^nZ_0 $\n\n\\(\\alpha=\\) 지수평활계수로 작을수록 과거시점에 가중치를 더 많이 주고, 클 수록 현시점에 가중치를 많이 준다.\n\n\\(\\alpha\\)는 예측오차를 비교하여 예측오차가 가장 작은값을 선택하는 것이 바람직함\n불규칙변동이 큰 시계열의 경우 작은 값을, 작을 경우 큰 값을 적용\n\n또한 이동평균법과 달리 모든 \\(m\\)개가 아닌 전체 데이터를 고려하여 예측값을 산출한다.\n추세성분과 계절성분이 없을 경우 \\(\\to\\) 단순지수평활법\n추세성분이 있고 계절성분이 없는 경우 \\(\\to\\) 이중지수평활법\n추세성분과 계절성이 모두 관측된 경우 \\(\\to\\) winters 계절지수 평활법\n\n\nPractice with R\n\n\n단순지수 평활법\n\nrain  <- scan(\"precip1.txt\",skip=1)\nrainseries <- ts(rain,start=c(1813))\nrainseries ## 1년 주기 데이터\n\nA Time Series:\n23.5626.0721.8631.2423.6523.8826.4122.6731.6923.8624.1132.4323.2622.572327.8825.3225.0827.7619.8224.7820.1224.3427.4219.4421.6327.4919.4331.1323.0925.8522.6522.7526.3617.729.8122.9319.2220.6335.3425.8918.6523.0622.2122.1818.7728.2132.2422.2727.5721.5916.9329.4831.626.2523.425.4221.3225.0233.8622.6718.8228.4426.1628.1734.0833.8230.2827.9227.1424.420.3526.6427.0119.2127.7423.8521.2328.1522.6119.827.9421.4723.5222.8617.6922.5423.2822.1720.8438.120.6522.9724.2623.0123.6726.7525.3624.7927.88\n\n\n\n시도료를 살펴본결과 추세와 계절성분이 관측되지 않아 단순지수평활법을 사용하겠음\n\n\nplot.ts(rainseries)\n\n\n\n\n\n[단순지수평활법] HoltWinters\n\n\nrainforecasts = HoltWinters(rainseries, beta=FALSE, gamma=FALSE) \nrainforecasts ## alpha=단순지수 평활법, beta = 추세성분 가중치 , gamma = 계절성분 가중치\n## alpha 값을 지정하지 않으면 최적의 alpha를 찾아줌\n\nHolt-Winters exponential smoothing without trend and without seasonal component.\n\nCall:\nHoltWinters(x = rainseries, beta = FALSE, gamma = FALSE)\n\nSmoothing parameters:\n alpha: 0.02412151\n beta : FALSE\n gamma: FALSE\n\nCoefficients:\n      [,1]\na 24.67819\n\n\n\n\\(\\alpha = 0.024\\dots\\) 로 산출\n\\(\\alpha\\)가 작다는 것은 과거값의 가중치를 더 많이 주는 것이므로 smoothing 효과가 더욱 커진다.\n\n\nhead(rainforecasts$fitted)\n\n\n\nA Time Series: 6 × 2\n\n    xhatlevel\n\n\n    181423.5600023.56000\n    181523.6205423.62054\n    181623.5780823.57808\n    181723.7629023.76290\n    181823.7601723.76017\n    181923.7630623.76306\n\n\n\n\n\nplot(rainforecasts) ## 빨간색은 smoothing 효과가 반영된 것이다.\n\n\n\n\n\n오차제곱합 계산\n\n\nrainforecasts$SSE\n\n1828.8548918638\n\n\n\n\\(\\alpha\\) 값을 변화해가며 비교\n\n\npar(mfrow=c(3,1))\nplot(HoltWinters(rainseries, alpha=0.3,\n                 beta=FALSE, gamma=FALSE), main=\"Alpha=0.3\")\n\nplot(HoltWinters(rainseries, alpha=0.7,\n                 beta=FALSE, gamma=FALSE), main=\"Alpha=0.7\")\n\nplot(HoltWinters(rainseries, alpha=1,\n                 beta=FALSE, gamma=FALSE), main=\"Alpha=1\") ## 1은 직전값만 보겠다는 뜻이다.\n\n\n\n\n\nalpha1=HoltWinters(rainseries, alpha=1, beta=FALSE, gamma=FALSE)\nalpha1$SSE\nalpha07=HoltWinters(rainseries, alpha=0.7, beta=FALSE, gamma=FALSE)\nalpha07$SSE\nalpha03=HoltWinters(rainseries, alpha=0.3, beta=FALSE, gamma=FALSE)\nalpha03$SSE\n\n3738.1784\n\n\n2849.29917342505\n\n\n2101.5603686547\n\n\n\n[단순지수평활] 초기값 변경 \\(\\to\\) 기존의 경우 관측값의 첫 번째 값을 사용\n\n\nrainforecasts35 = HoltWinters(rainseries, beta=FALSE, gamma=FALSE, l.start=35)\nrainforecasts35\n\nHolt-Winters exponential smoothing without trend and without seasonal component.\n\nCall:\nHoltWinters(x = rainseries, beta = FALSE, gamma = FALSE, l.start = 35)\n\nSmoothing parameters:\n alpha: 0.1955854\n beta : FALSE\n gamma: FALSE\n\nCoefficients:\n      [,1]\na 25.28555\n\n\n\n\\(\\alpha\\) 값이 앞에 초기값 설정 전 보다 작아짐\n스무딩 효과도 줄어들었다.\n\n\nrainforecasts35$SSE\n\n2209.42164806169\n\n\n\nplot(rainforecasts35)\n\n\n\n\n\nrainforecasts2 <- forecast(rainforecasts, h=5) ## h=예측할 시점, 즉 5개 시점을 예측해서 보겠다\nplot(rainforecasts2) ## 회색은 95%신뢰구간, 보라색은 80%신뢰구간\n\n\n\n\n\nZ <- scan(\"mindex.txt\")\n\n\nmindex  <- ts(Z,start=c(1986,1),frequency = 12) ## 1986년 1월부터 주기가 12인 데이터를 생성\n\n\nmindex\n\n\n\nA Time Series: 9 × 12\n\n    JanFebMarAprMayJunJulAugSepOctNovDec\n\n\n    1986 9.310.713.314.117.818.119.418.819.118.418.017.0\n    198719.520.119.415.715.616.114.916.014.618.318.223.0\n    198822.222.118.817.713.812.716.515.616.310.710.4 7.0\n    1989 4.7 4.5 4.0 6.0 6.2 5.7 4.4 4.2 5.0 5.8 6.4 4.9\n    1990 7.9 8.211.810.011.111.712.415.214.015.212.918.0\n    199114.412.7 8.311.511.911.610.3 8.511.612.314.511.1\n    199211.812.412.7 9.810.010.2 9.6 6.9 5.3 4.8 4.6 1.9\n    1993 3.8 4.7 7.7 7.0 7.2 7.8 8.611.410.711.811.316.0\n    199413.212.0 8.511.4                                \n\n\n\n\n\n시도표를 확인해본 결과 추세와 계절 성분이 없는 것을 확인하였다. 따라서 단순지수평활법을 적용\n\n\nplot.ts(mindex)\n\n\n\n\n\nmindexforecasts = HoltWinters(mindex, beta=FALSE, gamma=FALSE)\nmindexforecasts\n\nHolt-Winters exponential smoothing without trend and without seasonal component.\n\nCall:\nHoltWinters(x = mindex, beta = FALSE, gamma = FALSE)\n\nSmoothing parameters:\n alpha: 0.9036403\n beta : FALSE\n gamma: FALSE\n\nCoefficients:\n      [,1]\na 11.15433\n\n\n\npar(mfrow=c(3,1))\nplot(HoltWinters(mindex, alpha=0.3,\n                 beta=FALSE, gamma=FALSE), main=\"Alpha=0.3\")\n\nplot(HoltWinters(mindex, alpha=0.7,\n                 beta=FALSE, gamma=FALSE), main=\"Alpha=0.7\")\n\nplot(HoltWinters(mindex, alpha=1,\n                 beta=FALSE, gamma=FALSE), main=\"Alpha=1\")\n\n\n\n\n\nalpha1=HoltWinters(mindex, alpha=1, beta=FALSE, gamma=FALSE)\nalpha1$SSE\nalpha07=HoltWinters(mindex, alpha=0.7, beta=FALSE, gamma=FALSE)\nalpha07$SSE\nalpha03=HoltWinters(mindex, alpha=0.3, beta=FALSE, gamma=FALSE)\nalpha03$SSE\n\n443.23\n\n\n462.503411392955\n\n\n792.968599702168\n\n\n\n직접적으로 최적의 평활상수를 구해보자\n\n\nw <-c(seq(0.1,0.8,0.1), seq(0.81, 0.99, 0.01)) \nsse <- sapply(w, function(x) \n  return(sum(ses(mindex, alpha = x)$residuals^2)))\n\n\nw1 = w[-c(1:6)]  # xaxis from 0.7 to 1.0\nsse1 = sse[-c(1:6)]\nplot(w1,sse1, type=\"o\", xlab=\"weight\", ylab=\"sse\", pch=16,\n     main=\"1 시차 후 예측오차의 제곱합\")\n\n\n\n\n\nopt_w <- w[which.min(sse)]  # 최적 평활상수값\nopt_w\n\n0.9\n\n\n\nfit1 <- ses(mindex, alpha=w[which.min(sse)], h=6) ## ses 함수는 더 예측할 시점을 설정하여 모형을 적합할 수 있음\nfit1\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nMay 1994       11.14643 8.438328 13.85454 7.004743 15.28812\nJun 1994       11.14643 7.503050 14.78982 5.574359 16.71851\nJul 1994       11.14643 6.762982 15.52989 4.442522 17.85034\nAug 1994       11.14643 6.130952 16.16192 3.475916 18.81695\nSep 1994       11.14643 5.570103 16.72276 2.618171 19.67470\nOct 1994       11.14643 5.060723 17.23214 1.839142 20.45372\n\n\n\nplot(fit1, xlab=\"\", ylab=\"\", \n     main=paste0(\"중간재 출하지수와 단순지수평활값 alpha=\",opt_w), \n     lty=1,col=\"black\" )\nlines(fitted(fit1), col=\"red\", lty=2)\nlegend(\"topright\", legend=c(\"Mindex\", opt_w), \n       lty=1:2,col=c(\"black\",\"red\"))\n\n\n\n\n\n\n이중지수 평활법\n\n치맛단 길이 데이터를 사용\n\n\nskirts=scan(\"http://robjhyndman.com/tsdldata/roberts/skirts.dat\",\n            skip=5)\nskirtsseries <- ts(skirts,start=c(1866))\nskirtsseries # 주기가 1년인 데이터\n\nA Time Series:\n6086176256366576917287848168769499971027104710491018102110121018991962921871829822820802821819791746726661620588568542551541557556534528529523531\n\n\n\n시도표를 살펴본 결과 올라가다가 내려가는? 추세를 볼 수 있다.\n또한 추세가 일정한 것보다는 시간에 따라 추세가 변하는 것을 볼 수 있다\n따라서 일반적인 추세모형을 적합할 수 없으므로 이중지수평활법을 사용\n\n\nplot.ts(skirtsseries)\n\n\n\n\n[이중지수평활법] gamma =F\n\nskirtforecasts <- HoltWinters(skirtsseries, gamma=FALSE)\nskirtforecasts \n\nHolt-Winters exponential smoothing with trend and without seasonal component.\n\nCall:\nHoltWinters(x = skirtsseries, gamma = FALSE)\n\nSmoothing parameters:\n alpha: 0.8383481\n beta : 1\n gamma: FALSE\n\nCoefficients:\n        [,1]\na 529.308585\nb   5.690464\n\n\n\n\\(\\beta = 1, \\alpha = 0.83\\dots\\)로 최근값에 가중치가 더 많이 들어간 것으로 확인된다.\n\n\nplot(skirtforecasts)\n\n\n\n\n[이중지수평활법] 예측\n\nskirtsforecasts2=forecast(skirtforecasts, h=10)\nplot(skirtsforecasts2)\n\n\n\n\n\nz <- scan(\"stock.txt\")\nstock <- ts(z, start=c(1984,1), frequency=12)\nstock\n\n\n\nA Time Series: 8 × 12\n\n    JanFebMarAprMayJunJulAugSepOctNovDec\n\n\n    1984118.60129.84129.12133.23131.74133.53133.15135.08134.74132.21132.94138.34\n    1985139.98135.72134.30136.14133.17137.35135.77136.54136.34140.21145.26135.37\n    1986157.54168.83187.77206.02209.57239.30260.24268.28262.02243.64257.00237.30\n    1987289.09320.53365.85368.66383.97387.28454.00483.98481.52509.43477.53489.00\n    1988579.48644.36624.94643.46673.32710.98696.84693.39675.05713.39778.15884.34\n    1989884.23902.21965.89959.12936.94900.75886.35922.35951.58913.00898.67901.80\n    1990898.00867.18845.48785.50771.65766.70698.17637.42602.21682.79696.44712.46\n    1991647.53654.72670.55638.34630.19604.73646.25710.56684.34705.16668.07625.06\n\n\n\n\n\n레벨과 추세가 시간에 따라 변하는 것을 알 수 있다.\n\n\nplot.ts(stock, main='월별주가지수')\n\n\n\n\n\nfit4 = HoltWinters(stock, alpha=0.6, beta=0.6, gamma=FALSE) \nfit4\n\nHolt-Winters exponential smoothing with trend and without seasonal component.\n\nCall:\nHoltWinters(x = stock, alpha = 0.6, beta = 0.6, gamma = FALSE)\n\nSmoothing parameters:\n alpha: 0.6\n beta : 0.6\n gamma: FALSE\n\nCoefficients:\n       [,1]\na 650.78081\nb -26.14166\n\n\n\nplot(fit4)\n\n\n\n\n\n\\(\\alpha=1, \\beta = 0.109\\dots\\) 알파는 최근값, 베타는 과거값에 가중치를 더 많이 주었다.\n\n\nfit5 = HoltWinters(stock, gamma=FALSE) \nfit5\n\nHolt-Winters exponential smoothing with trend and without seasonal component.\n\nCall:\nHoltWinters(x = stock, gamma = FALSE)\n\nSmoothing parameters:\n alpha: 1\n beta : 0.1094451\n gamma: FALSE\n\nCoefficients:\n        [,1]\na 625.060000\nb  -7.097122\n\n\n\nfit5_2=forecast(fit5, h=10)\nplot(fit5_2)\n\n\n\n\n\n\n계절지수 평활법\n\nsouvenir=scan(\"http://robjhyndman.com/tsdldata/data/fancy.dat\")\nsouvenirtimeseries=ts(souvenir, frequency=12, start=c(1987,1))\n\nsouvenirtimeseries\n\n\n\nA Time Series: 7 × 12\n\n    JanFebMarAprMayJunJulAugSepOctNovDec\n\n\n    1987  1664.81  2397.53  2840.71  3547.29  3752.96  3714.74  4349.61  3566.34  5021.82  6423.48  7600.60 19756.21\n    1988  2499.81  5198.24  7225.14  4806.03  5900.88  4951.34  6179.12  4752.15  5496.43  5835.10 12600.08 28541.72\n    1989  4717.02  5702.63  9957.58  5304.78  6492.43  6630.80  7349.62  8176.62  8573.17  9690.50 15151.84 34061.01\n    1990  5921.10  5814.58 12421.25  6369.77  7609.12  7224.75  8121.22  7979.25  8093.06  8476.70 17914.66 30114.41\n    1991  4826.64  6470.23  9638.77  8821.17  8722.37 10209.48 11276.55 12552.22 11637.39 13606.89 21822.11 45060.69\n    1992  7615.03  9849.69 14558.40 11587.33  9332.56 13082.09 16732.78 19888.61 23933.38 25391.35 36024.80 80721.71\n    1993 10243.24 11266.88 21826.84 17357.33 15997.79 18601.53 26155.15 28586.52 30505.41 30821.33 46634.38104660.67\n\n\n\n\n\n시도표를 그려본 결과 추세성분과 계절성분이 관측되었고 이분산성도 관측되었음\n따라서 로그변환 후 계절지수평활법을 적용\n\n\nplot.ts(souvenirtimeseries) \n\n\n\n\n\nlogsouvenirtimeseries <- log(souvenirtimeseries)\nplot.ts(logsouvenirtimeseries)\n\n\n\n\n\nsouvenirforecasts <- HoltWinters(logsouvenirtimeseries)\nsouvenirforecasts\n\nHolt-Winters exponential smoothing with trend and additive seasonal component.\n\nCall:\nHoltWinters(x = logsouvenirtimeseries)\n\nSmoothing parameters:\n alpha: 0.413418\n beta : 0\n gamma: 0.9561275\n\nCoefficients:\n           [,1]\na   10.37661961\nb    0.02996319\ns1  -0.80952063\ns2  -0.60576477\ns3   0.01103238\ns4  -0.24160551\ns5  -0.35933517\ns6  -0.18076683\ns7   0.07788605\ns8   0.10147055\ns9   0.09649353\ns10  0.05197826\ns11  0.41793637\ns12  1.18088423\n\n\n\n\\(s_1,\\dots s_{12}\\)는 계절성분의 평활상수 이다.\n\n\nplot(souvenirforecasts)\n\n\n\n\n\nsouvenirforecasts2 <- forecast(souvenirforecasts, h=36)\nplot(souvenirforecasts2)\n\n\n\n\n\nz <- scan(\"koreapass.txt\")\npass <- ts(z, start=c(1981,1), frequency=12) \n\npass\n\n\n\nA Time Series: 9 × 12\n\n    JanFebMarAprMayJunJulAugSepOctNovDec\n\n\n    1981112696103070122800142496150064136128143033158223138626148761140539117693\n    1982133390117378136309149476157107147160153156164352139584165633147054129396\n    1983137638132128141178154545169127156056167749200228154371169199144594126590\n    1984132362131123146756160249175131169655178023193169171479186443172378150952\n    1985156240144008177073186693187296190206196132218560183651189003177325159672\n    1986163846158125193446193743205408210435219326249612205442234143216913185724\n    1987195346183669215524224547238201238304249401274307224528229684217576182898\n    1988199252193565230360242953274802271599293280328824236158305662312164267947\n    1989309654279943324963319020352602356546376214398079355931363955339887308085\n\n\n\n\n\nplot.ts(pass)\n\n\n\n\n\n가법모형 적용\n\n\nfit6 = HoltWinters(pass, seasonal=\"additive\") \nfit6\n\nHolt-Winters exponential smoothing with trend and additive seasonal component.\n\nCall:\nHoltWinters(x = pass, seasonal = \"additive\")\n\nSmoothing parameters:\n alpha: 0.4810767\n beta : 0.0383379\n gamma: 0.7345988\n\nCoefficients:\n          [,1]\na   347794.753\nb     3363.251\ns1  -12186.666\ns2  -33643.322\ns3    4855.643\ns4    5000.713\ns5   29085.909\ns6   22953.006\ns7   32200.195\ns8   49687.643\ns9  -11655.430\ns10  10218.813\ns11  -4226.391\ns12 -38683.394\n\n\n\n잔차 그림\n\n\nts.plot(resid(fit6), ylab=\"residual\", \n        main=\"가법모형의 예측오차\"); abline(h=0)\n\n\n\n\n\nfit6_2=forecast(fit6, h=12)\nplot(fit6_2)\n\n\n\n\n\n승법모형 적용\n\n\nfit7 =  HoltWinters(pass, seasonal=\"multiplicative\") \nfit7\n\nHolt-Winters exponential smoothing with trend and multiplicative seasonal component.\n\nCall:\nHoltWinters(x = pass, seasonal = \"multiplicative\")\n\nSmoothing parameters:\n alpha: 0.5623303\n beta : 0.03452066\n gamma: 0.3506508\n\nCoefficients:\n            [,1]\na   3.560417e+05\nb   3.517556e+03\ns1  9.199511e-01\ns2  8.489190e-01\ns3  9.796617e-01\ns4  1.016528e+00\ns5  1.095078e+00\ns6  1.060795e+00\ns7  1.089621e+00\ns8  1.178393e+00\ns9  9.706046e-01\ns10 1.065056e+00\ns11 9.983082e-01\ns12 8.552947e-01\n\n\n\nts.plot(resid(fit7), ylab=\"residual\", \n        main=\"승법모형의 예측오차\"); abline(h=0)\n\n\n\n\n\nfit6$SSE; fit7$SSE\n\n13764737658.0239\n\n\n12633778874.3409\n\n\n\nfit7_2=forecast(fit7, h=12)\nplot(fit7_2)"
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/시계열 분석.html#가.-자기회귀-모형-ar-모형-autoregressive-model",
    "href": "post/Lecture/RFD/통계분석/시계열 분석.html#가.-자기회귀-모형-ar-모형-autoregressive-model",
    "title": "11. Time-seris analysis",
    "section": "가. 자기회귀 모형 (AR 모형, Autoregressive Model)",
    "text": "가. 자기회귀 모형 (AR 모형, Autoregressive Model)\n\n\\(p\\) 시점 전의 자료가 현재 자료에 영향을 주는 모형\n\n\\[Z_t = \\phi_1 + Z_{t-1} + \\dots \\phi_pZ_{t-p} + \\alpha_t\\]\n\n\\(\\alpha_t \\sim WN(0,\\sigma^2)\\) 로 대표적인 정상시계열이다.\n자기상관함수(ACF)는 지수적으로 빠르게 감소, 부분자기상관함수(PACF)는 \\(p+1\\)시점에서 절단점을 가진다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/시계열 분석.html#나.-이동평균-모형-ma-모형-moving-average-model",
    "href": "post/Lecture/RFD/통계분석/시계열 분석.html#나.-이동평균-모형-ma-모형-moving-average-model",
    "title": "11. Time-seris analysis",
    "section": "나. 이동평균 모형 (MA 모형, Moving Average Model)",
    "text": "나. 이동평균 모형 (MA 모형, Moving Average Model)\n\\[ Z_t =\\alpha_t - \\theta_1\\alpha_{t-1}-\\theta_2\\alpha_{t-2}\\dots -\\theta_p \\alpha_{t-p}\\]\n\n유한개 개수의 백색잡음의 결합이므로 언제나 정상성을 만족한다.\n부분자기상관함수(PACF)는 지수적으로 빠르게 감소, 자기상관함수(ACF)는 \\(p+1\\)시점에서 절단점을 가진다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/시계열 분석.html#다.-자기회귀누적이동평균-모형-arima-pdq-모형-autoregressive-integrated-moving-average-model",
    "href": "post/Lecture/RFD/통계분석/시계열 분석.html#다.-자기회귀누적이동평균-모형-arima-pdq-모형-autoregressive-integrated-moving-average-model",
    "title": "11. Time-seris analysis",
    "section": "다. 자기회귀누적이동평균 모형 (ARIMA (p,d,q) 모형 , Autoregressive integrated moving average model)",
    "text": "다. 자기회귀누적이동평균 모형 (ARIMA (p,d,q) 모형 , Autoregressive integrated moving average model)\n\n\\(p \\to AR, \\, q\\to MA\\) 와 대응된다.\n\\(ARIMA\\) 모형은 비정상시계열 모형이다.\n\\(Z_t\\)가 비정상 시계열일 때 \\(d\\)번 차분한 시계열이 \\(ARMA(p,q)\\) 이면 \\(Z_t\\)는 \\(ARIMA(p,d,q)\\) 모형을 갖는다고 한다.\n\\(ARIMA(0,1,1)\\) 일 경우 1차분 후 \\(MA(1)\\)을 이용\n\\(ARIMA(1,1,0)\\) 일 경우 1차분 후 \\(AR(1)\\)을 이용"
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/시계열 분석.html#practice-with-r-2",
    "href": "post/Lecture/RFD/통계분석/시계열 분석.html#practice-with-r-2",
    "title": "11. Time-seris analysis",
    "section": "Practice with R",
    "text": "Practice with R\n\n데이터 로드 후 시도표\n딱히 이렇다할 추세나 계절 성분은 보이진 않는다.\n\n\nking <- scan(\"http://robjhyndman.com/tsdldata/misc/kings.dat\",skip=3)\nking.ts <- ts(king) ## 1년 주기로 데이터를 생성\nplot(king.ts)\n\n\n\n\n\nacf값과 pacf값을 확인\n\n\npar(mfrow=c(1,2))\nacf(king.ts); pacf(king.ts)\n\n\n\n\n\n또한 PACF 도표를 그려 확인한 결과 \\(lag\\, 2\\)에서 절단 점을 가지므로 \\(AR(1)\\) 모형을 1차적으로 고려해볼 수 있다.\n그러나 실제 ACF가 유의한지 아닌 지 검증을 수행하여 차분여부를 결정하자\n\n\nlibrary(fUnitRoots)\n\n\nadfTest(king.ts,lags=0, type=\"c\") ## type=\"c\" 는 적합할 모형에 상수항(절편)이 존재하는 경우\n                                  ## type= \"nc\"는 적합할 모형에 상수항이 0인 경우\n                                  ## type= \"ct\" 적합할 모형에 추세와 절편이 존재하는 경우\nadfTest(king.ts,lags=1, type=\"c\")\nadfTest(king.ts,lags=2, type=\"c\")\n\nWarning message in adfTest(king.ts, lags = 0, type = \"c\"):\n\"p-value smaller than printed p-value\"\n\n\n\nTitle:\n Augmented Dickey-Fuller Test\n\nTest Results:\n  PARAMETER:\n    Lag Order: 0\n  STATISTIC:\n    Dickey-Fuller: -4.0902\n  P VALUE:\n    0.01 \n\nDescription:\n Fri Nov 26 15:50:29 2021 by user: lee\n\n\n\nTitle:\n Augmented Dickey-Fuller Test\n\nTest Results:\n  PARAMETER:\n    Lag Order: 1\n  STATISTIC:\n    Dickey-Fuller: -3.0002\n  P VALUE:\n    0.04631 \n\nDescription:\n Fri Nov 26 15:50:29 2021 by user: lee\n\n\n\nTitle:\n Augmented Dickey-Fuller Test\n\nTest Results:\n  PARAMETER:\n    Lag Order: 2\n  STATISTIC:\n    Dickey-Fuller: -2.1483\n  P VALUE:\n    0.2665 \n\nDescription:\n Fri Nov 26 15:50:29 2021 by user: lee\n\n\n\n시차 0,1,2에서 검정 결과 차분을 1회할 경우 정상성을 만족할 것으로 보인다.\n검정결과만 보면 굳이 차분을 안해도 될 것 같긴하다.\n\n\npar(mfrow=c(1,2))\nking.ff1 <- diff(king.ts,differences=1)\nplot.ts(king.ts);plot.ts(king.ff1)\n\n\n\n\n\n시도표를 그려보았을 때는 기존보다 평균과 분산이 어느 정도 일정해짐을 볼 수 있었다.\n\n\npar(mfrow=c(1,2))\nacf(king.ff1); pacf(king.ff1)\n\n\n\n\n\nacf 그래프를 본 결과 lag 2부터 절단점을 가진다. \\(\\to MA(1)\\)\npacf 그래프를 살펴본 결과 lag 4부터 절단점을 가진다. \\(\\to AR(3)\\)\n모형의 해석력을 고려하여 \\(MA(1)\\)을 선택하는 것이 좋겠지만, auto.arima함수를 이용하여 적절한 모형을 선택\n\n\nlibrary(forecast)\n\n\nauto.arima(king) ##기존 데이터를 넣어 주어야함\n\nSeries: king \nARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.7218\ns.e.   0.1208\n\nsigma^2 estimated as 236.2:  log likelihood=-170.06\nAIC=344.13   AICc=344.44   BIC=347.56\n\n\n\n함수를 이용하여 산출된 모형은 \\(ARIMA(0,1,1)\\)이다.\n\n\\[ Z_t = \\varepsilon_t + 0.7218\\times \\varepsilon_{t-1}\\]\n\n따라서 \\(ARIMA(0,1,1)\\) 모형을 적합\n예측값을 살펴본 결과 43~52 번째 왕의 사망예측 나이는 67.75살로 추정된다.\n\n\nking.arima <- arima(king,order = c(0,1,1))\nking.forecasts <- forecast(king.arima)\nking.forecasts\nplot(king.forecasts)\n\n   Point Forecast    Lo 80    Hi 80    Lo 95     Hi 95\n43       67.75063 48.29647 87.20479 37.99806  97.50319\n44       67.75063 47.55748 87.94377 36.86788  98.63338\n45       67.75063 46.84460 88.65665 35.77762  99.72363\n46       67.75063 46.15524 89.34601 34.72333 100.77792\n47       67.75063 45.48722 90.01404 33.70168 101.79958\n48       67.75063 44.83866 90.66260 32.70979 102.79146\n49       67.75063 44.20796 91.29330 31.74523 103.75603\n50       67.75063 43.59372 91.90753 30.80583 104.69543\n51       67.75063 42.99472 92.50653 29.88974 105.61152\n52       67.75063 42.40988 93.09138 28.99529 106.50596"
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/요인분석(1205).html#참고",
    "href": "post/Lecture/RFD/통계분석/요인분석(1205).html#참고",
    "title": "09. Factor Analysis",
    "section": "참고",
    "text": "참고\n### 직각회전 : 회전된 인자들이 서로 상관되지 않도록 제약\n1. 베리맥스 (Variance is maximized) : 요인행렬을 변환할 때 행렬의 열을 기준으로 하여 더 큰 값은 크게, 작은 값은 더 작게 회전하는 길을 찾는다.\n2. 쿼티맥스 : 요인행렬의 행을 기준으로 분산을 극대화한다. 제 1요인만 과대 해석하고 기타 요인은 과소해석하는 문제가 있어, 단일요인 구조가 존재한다는 확신이 있을 때에나 한정적으로 사용한다.\n\n비직각회전(=사각회전) : 회전 결과에서 요인간의 상관이 0이 아님\n1. 회전된 요인간의 상관이 있다고 확신이 들 때 사용\n2.오블리민과 프로맥스가 있으나 R에서는 프로맥스를 제공\n\n\n요약\n\ndisplay_png(file = \"C:\\\\Users\\\\rkdcj\\\\OneDrive\\\\바탕 화면\\\\요인분석.png\")"
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/요인분석(1205).html#요인분석파트-주성분분석-함수써서-정리해야됨",
    "href": "post/Lecture/RFD/통계분석/요인분석(1205).html#요인분석파트-주성분분석-함수써서-정리해야됨",
    "title": "09. Factor Analysis",
    "section": "요인분석파트 주성분분석 함수써서 정리해야됨",
    "text": "요인분석파트 주성분분석 함수써서 정리해야됨"
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/주성분분석(1204).html",
    "href": "post/Lecture/RFD/통계분석/주성분분석(1204).html",
    "title": "08. PCA",
    "section": "",
    "text": "서로 상관성이 높은 변수들의 선형결합으로 만들어 기존의 상관성이 높은 변수들을 축소하는 기법이다.\n요인분석과 다르게 요인의 이름을 명명하지 않고 제1 주성분, 제2 주성분 등으로 표현된다.\n주성분분석의 결과에서 누적기여율 (cumulative proportion)이 85% 이상이면 주성분의 수로 결정할 수 있다.\n또한 scree plot을 그려 기울기가 급격히 줄어드는 전단계로 주성분의 수를 선택한다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/주성분분석(1204).html#princomp-vs-prcomp",
    "href": "post/Lecture/RFD/통계분석/주성분분석(1204).html#princomp-vs-prcomp",
    "title": "08. PCA",
    "section": "princomp vs prcomp",
    "text": "princomp vs prcomp\n\n둘의 차이는 princomp는 원데이터의 상관계수 또는 공분산행렬에 고유값 분해를\nprcomp는 원데이터에 특이값 분해를 적용하여 PCA를 수행한다는 것이다.\n\n\nlibrary(IRdisplay)\n\n\nsetwd(\"C:\\\\Users\\\\rkdcj\\\\OneDrive\\\\바탕 화면\")\n\n\ndisplay_png(file=\"pca.png\",width=800,height=700)\n\n\n\n\n\nsummary(princomp(USArrests,cor=T))\n\nImportance of components:\n                          Comp.1    Comp.2    Comp.3     Comp.4\nStandard deviation     1.5748783 0.9948694 0.5971291 0.41644938\nProportion of Variance 0.6200604 0.2474413 0.0891408 0.04335752\nCumulative Proportion  0.6200604 0.8675017 0.9566425 1.00000000\n\n\n\nsummary(prcomp(USArrests,scale=T))\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.5749 0.9949 0.59713 0.41645\nProportion of Variance 0.6201 0.2474 0.08914 0.04336\nCumulative Proportion  0.6201 0.8675 0.95664 1.00000"
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/주성분분석(1204).html#다시-돌아와서",
    "href": "post/Lecture/RFD/통계분석/주성분분석(1204).html#다시-돌아와서",
    "title": "08. PCA",
    "section": "다시 돌아와서",
    "text": "다시 돌아와서\n\nus.prin <- princomp(USArrests,cor=T)\nresult <- summary(us.prin)\nscreeplot(us.prin, npcs=4,type=\"lines\")\n\n\n\n\n\n누적기여율을 보았을 때 제 2주성분까지의 채택이 적절하다고 판단된다.\n또한 screeplot을 그려본결과 분산 감소 기울기가 급격히 줄어들기 전 시점인 2시점에서 제 2주성분을 채택하는 것이 적절하다고 판단된다.\n\n\nloadings(us.prin)\n\n\nLoadings:\n         Comp.1 Comp.2 Comp.3 Comp.4\nMurder    0.536  0.418  0.341  0.649\nAssault   0.583  0.188  0.268 -0.743\nUrbanPop  0.278 -0.873  0.378  0.134\nRape      0.543 -0.167 -0.818       \n\n               Comp.1 Comp.2 Comp.3 Comp.4\nSS loadings      1.00   1.00   1.00   1.00\nProportion Var   0.25   0.25   0.25   0.25\nCumulative Var   0.25   0.50   0.75   1.00\n\n\n\n\\(Comp1 = 0.536 \\times Muder + 0.583 \\times Assault + 0.278 \\times UrbanPop + 0.543 \\times Rape\\)\n제 1주성분은 범죄중에서도 muder, Assault와 관련있는 변수라고 볼 수 있다.\n아래는 각 주성분의 선형식을 통해 각 지역별로 얻은 결과이다.\n\n\nhead(us.prin$scores)\n\n\n\nA matrix: 6 × 4 of type dbl\n\n    Comp.1Comp.2Comp.3Comp.4\n\n\n    Alabama 0.9855659 1.1333924 0.44426879 0.156267145\n    Alaska 1.9501378 1.0732133-2.04000333-0.438583440\n    Arizona 1.7631635-0.7459568-0.05478082-0.834652924\n    Arkansas-0.1414203 1.1197968-0.11457369-0.182810896\n    California 2.5239801-1.5429340-0.59855680-0.341996478\n    Colorado 1.5145629-0.9875551-1.09500699 0.001464887\n\n\n\n\n\noptions(repr.plot.res=200, repr.plot.height=7,repr.plot.width=12)\n\n\nbiplot(us.prin,scale=0)\n\n\n\n\n\nbiplot은 원 변수와 주성분 간의 관계를 그래프로 표현한 것으로 그래프를 통해 각 주성분의 의미를 해석하고 각 개체들의 특성을 파악할 수 있다.\n화살표는 원 변수와 주성분의 상관계수를 의미하며, 주성분과 평행할수록 상관계수가 크므로 해당 주성분에 큰 영향을 끼친다.\n그리고 화살표가 같은 방향으로 인접해 있을수록 같은 주성분으로 생성될 수 도 있음을 알 수 있다.\n제 2주성분을 기준으로 Urbampop은 다른 변수들과 방향이 다르므로 상관관계가 낮다.\n제 1주성분을 기준으로 범죄와 관련된 3변수들은 같은 방향으로 인접해 있는 것을 확인할 수 있다.\n여기서 이상치인 도시는 Vermont, West Virginia 등은 변수 방향, 상관관계가 동떨어져 이상치로 판정될 수 있는 데이터이다.\n만약 이상치의 특성을 파악하라는 문제가 출제된다면, 위 결과에서 이상치라고 판단되는 값들 중 West Virginia는 범죄비율들과 도시인구비율이 적으므로 “범죄가 없는 시골” 이라고 해석하여 분석 결과를 본 미국 시민들이 그 도시로 몰릴 수도 있다고 판단할 수 있다."
  },
  {
    "objectID": "post/Lecture/RFD/통계분석/회귀분석.html",
    "href": "post/Lecture/RFD/통계분석/회귀분석.html",
    "title": "06. regression analysis",
    "section": "",
    "text": "변수들간의 인과관계를 밝히고 모형을 적합하여 관심 있는 변수를 예측하거나 추론하기 위해 사용하는 분석기법\n선형회귀분석의 가정\n\n오차의 등분산성\n오차의 독립성\n오차의 정규성 : Q-Q plot, Kolmogorov-Smirnov 검정, Shapiro-Wilk 검정을 확인하여 정규성을 확인한다.\n\n\n\n\n1. 모형 내의 개별 회귀계수에 대한 검정\n2. 모형에 설명력 \\(R^2\\)값을 통해 확인, 독립변수의 수가 많아지면 \\(adj-R^2\\) 값을 확인\n3. 회귀모형이 통계적으로 유의한가 확인\n4. 잔차 plot을 통해 모형의 진단\n\n\n\nCars93 데이터의 엔진크기(EngineSize)를 독립변수, 가격(Price)를 종속변수로 선정하여 단순 선형회귀분석을 실시한 후, 추정된 회귀모형에 대해 해석해보자.\n\nlibrary(MASS)\nlibrary(lmtest) ## 더비왓슨 테스트를 위함\nlibrary(tidyverse)\nselect <- dplyr::select\n\n\nfit1 <- lm(Price~EngineSize,data=Cars93)\nsummary(fit1)\n\n\nCall:\nlm(formula = Price ~ EngineSize, data = Cars93)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.684  -4.627  -1.795   2.592  39.429 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.6692     2.2390   2.085   0.0398 *  \nEngineSize    5.5629     0.7828   7.107 2.59e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.789 on 91 degrees of freedom\nMultiple R-squared:  0.3569,    Adjusted R-squared:  0.3499 \nF-statistic: 50.51 on 1 and 91 DF,  p-value: 2.588e-10\n\n\n\nfit1.1 <- lm(Price~ EngineSize +Horsepower +RPM + Width + Length + Weight,Cars93)\nsummary(fit1.1)\n\n\nCall:\nlm(formula = Price ~ EngineSize + Horsepower + RPM + Width + \n    Length + Weight, data = Cars93)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.659  -3.022  -0.144   2.376  27.955 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 77.117445  24.957836   3.090  0.00270 ** \nEngineSize   0.405792   1.699372   0.239  0.81184    \nHorsepower   0.141609   0.028745   4.926 4.02e-06 ***\nRPM         -0.001575   0.001933  -0.815  0.41751    \nWidth       -1.714985   0.386433  -4.438 2.68e-05 ***\nLength       0.152837   0.076182   2.006  0.04798 *  \nWeight       0.006586   0.002479   2.657  0.00939 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.45 on 86 degrees of freedom\nMultiple R-squared:  0.7024,    Adjusted R-squared:  0.6817 \nF-statistic: 33.83 on 6 and 86 DF,  p-value: < 2.2e-16\n\n\n\noptions(repr.plot.res=200,repr.plot.width=10,repr.plot.height=4)\n\n\nplot(Cars93$EngineSize,Cars93$Price,lwd=2)\nabline(a=coefficients(fit1)[2],b=coefficients(fit1)[1],col=\"red\",lwd=2)\n\n\n\n\n\npar(mfrow=c(1,2))\nplot(fit1,1); plot(fit1,2)\n\n\n\n\n\nshapiro.test(resid(fit1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(fit1)\nW = 0.85365, p-value = 3.886e-08\n\n\n\ndwtest(fit1,alternative=\"two.sided\")\n\n\n    Durbin-Watson test\n\ndata:  fit1\nDW = 1.1716, p-value = 2.236e-05\nalternative hypothesis: true autocorrelation is not 0\n\n\n1. 추정된 회귀계수는 모두 통계적으로 유의하다.\n2. 결정계수값과 수정된 결정계수 값이 각각 0.3569, 0.3499 로 산출되었다.\n3. F-통계량의 근거한 p-value값을 보아도 생성된 모델은 통계적으로 유의하다.\n4. 잔차 plot 을 그려본 결과 오차항의 정규성과 독립성 가정이 위배된 것 같다. * 실제로 test 결과 위배되었다는 결론이 통계적으로 유의미했다.\n5. 따라서 모형의 식별 단계로 돌아가 새로운 모형을 적합할 필요가 있어보인다.\n\ntest <- Cars93 %>% select(EngineSize)  %>% sample_n(5)\n\n\npredict(fit1,test,interval=\"none\") ##점추정\n\n117.4639509954058218.576539050536311.901010719755413.5698928024502513.0135987748851\n\n\n\npredict(fit1,test,interval=\"confidence\") # 회귀계수에 대한 신뢰구간을 고려한 구간\npredict(fit1,test,interval=\"prediction\") # 회귀계수에 대한 신뢰구간과 오차항을 고려한 구간\n\n\n\nA matrix: 5 × 3 of type dbl\n\n    fitlwrupr\n\n\n    117.4639515.76082419.16708\n    218.5765416.95120220.20188\n    311.90101 9.23710314.56492\n    413.5698911.26122015.87857\n    513.0136010.59070215.43650\n\n\n\n\n\n\nA matrix: 5 × 3 of type dbl\n\n    fitlwrupr\n\n\n    117.46395 1.89943633.02847\n    218.57654 3.02034434.13273\n    311.90101-3.79771227.59973\n    413.56989-2.07246929.21225\n    513.01360-2.64602928.67323\n\n\n\n\n\n\n\n\niris 데이터를 사용\nR에 lm함수는 범주형 변수를 자동으로 더미변수로 변환해줌\n\n\nfit2 <- lm(Petal.Length~.,data=iris)\nsummary(fit2) \n\n\nCall:\nlm(formula = Petal.Length ~ ., data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.78396 -0.15708  0.00193  0.14730  0.65418 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -1.11099    0.26987  -4.117 6.45e-05 ***\nSepal.Length       0.60801    0.05024  12.101  < 2e-16 ***\nSepal.Width       -0.18052    0.08036  -2.246   0.0262 *  \nPetal.Width        0.60222    0.12144   4.959 1.97e-06 ***\nSpeciesversicolor  1.46337    0.17345   8.437 3.14e-14 ***\nSpeciesvirginica   1.97422    0.24480   8.065 2.60e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2627 on 144 degrees of freedom\nMultiple R-squared:  0.9786,    Adjusted R-squared:  0.9778 \nF-statistic:  1317 on 5 and 144 DF,  p-value: < 2.2e-16\n\n\n\ndwtest(fit2,alternative=\"two.sided\")\n\n\n    Durbin-Watson test\n\ndata:  fit2\nDW = 1.6772, p-value = 0.03042\nalternative hypothesis: true autocorrelation is not 0\n\n\n\nshapiro.test(resid(fit2))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(fit2)\nW = 0.99389, p-value = 0.78\n\n\n\n\n\n\n\n\n1. 전진 선택법 (forward selection) : 절편만 있는 상수모형에서 시작하여 중요하다고 생각되는 설명변수부터 차례로 추가한다.\n2. 후진 제거법 (backward elimination) : 모든 독립변수를 포함한 모형에서 출발하여 종속변수에 가장 적은 영향을 주는 변수부터 하나씩 제거하면서 더 이상 제거할 변수가 없을 때의 모형을 선택한다.\n3. 단계적 방법 (stepwise method) : 전진선택법에 의해 변수를 추가하면서 새롭게 추가된 변수에 의해 기존 변수의 중요도가 약화되면 해당변수를 제거한다.\n\n\n\n\n모형의 복잡도에 따라 벌점을 주는 방식으로 \\(AIC, BIC\\) 값이 주로 사용된다.\n\n\n\n\n\nfit3 <- step(lm(Price~ EngineSize +Horsepower +RPM + Width + Length + Weight,Cars93),direction = \"both\")\nsummary(fit3)\n\nStart:  AIC=322.11\nPrice ~ EngineSize + Horsepower + RPM + Width + Length + Weight\n\n             Df Sum of Sq    RSS    AIC\n- EngineSize  1      1.69 2556.1 320.17\n- RPM         1     19.71 2574.1 320.82\n<none>                    2554.4 322.11\n- Length      1    119.55 2674.0 324.36\n- Weight      1    209.73 2764.2 327.45\n- Width       1    585.01 3139.4 339.29\n- Horsepower  1    720.84 3275.3 343.22\n\nStep:  AIC=320.17\nPrice ~ Horsepower + RPM + Width + Length + Weight\n\n             Df Sum of Sq    RSS    AIC\n- RPM         1     49.36 2605.5 319.95\n<none>                    2556.1 320.17\n+ EngineSize  1      1.69 2554.4 322.11\n- Length      1    140.92 2697.0 323.16\n- Weight      1    208.09 2764.2 325.45\n- Width       1    593.56 3149.7 337.59\n- Horsepower  1   1476.65 4032.8 360.57\n\nStep:  AIC=319.95\nPrice ~ Horsepower + Width + Length + Weight\n\n             Df Sum of Sq    RSS    AIC\n<none>                    2605.5 319.95\n+ RPM         1     49.36 2556.1 320.17\n+ EngineSize  1     31.34 2574.1 320.82\n- Length      1    132.02 2737.5 322.54\n- Weight      1    279.31 2884.8 327.42\n- Width       1    562.10 3167.6 336.12\n- Horsepower  1   1898.74 4504.2 368.86\n\n\n\nCall:\nlm(formula = Price ~ Horsepower + Width + Length + Weight, data = Cars93)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.956  -2.578  -0.182   2.114  28.448 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 53.005861  16.532269   3.206  0.00188 ** \nHorsepower   0.129653   0.016190   8.008 4.46e-12 ***\nWidth       -1.480623   0.339813  -4.357 3.56e-05 ***\nLength       0.152968   0.072440   2.112  0.03755 *  \nWeight       0.007339   0.002389   3.071  0.00283 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.441 on 88 degrees of freedom\nMultiple R-squared:  0.6965,    Adjusted R-squared:  0.6827 \nF-statistic: 50.48 on 4 and 88 DF,  p-value: < 2.2e-16\n\n\n\n\n\\[ C_p = \\frac{1}n (SSE+2p\\hat{\\sigma}^2)\\]\n\nCp 값이 p(변수의 개수)와 비슷한 경우 : bias가 작고 우수한 모델을 의미\nCp값이 p 보다 큰 경우 : bias가 크고 추가적인 변수가 필요한 모델을 의미\nCp값이 p 보다 작은 경우 : 분산의 증가폭보다 편향의 감소폭이 더 크며, 필요 없는 변수가 모델에 있다는 것을 의미\n일반적으로 Cp값이 작고, p+상수에 가까운 모형을 선택한다.\n\n\nlibrary(olsrr)\n\n\nols_mallows_cp(fit1, fit1.1)## fit1.1은 fullmodel이라고 생각\n\n96.8503933522773\n\n\n\nols_mallows_cp(fit3,fit1.1) ## fit1.1은 fullmodel이라고 생각\n\n4.71880013093038\n\n\n\nCp 통계량을 기준으로 보았을 때 \\(AIC\\)값과 단계적 선택법을 고려한 fit3가 fit1보다 적합한 모델이다."
  },
  {
    "objectID": "post/Lecture/STBD/2020-04-12-(6주차) 과제.html",
    "href": "post/Lecture/STBD/2020-04-12-(6주차) 과제.html",
    "title": "06. layer 과제",
    "section": "",
    "text": "- 케라스를 이용하여 아래를 만족하는 적절한 \\(\\beta_0\\)와 \\(\\beta_1\\)을 구하라. 적합결과를 시각화하라. (애니메이션 시각화 X)\npython\n\\[\\hat {y} \\approx 2.5+ 4e^{-x}  \\]"
  },
  {
    "objectID": "post/Lecture/STBD/2020-04-12-(6주차) 과제.html#solution",
    "href": "post/Lecture/STBD/2020-04-12-(6주차) 과제.html#solution",
    "title": "06. layer 과제",
    "section": "Solution",
    "text": "Solution\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nnp.random.seed(43052) \nN= 100\nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) +epsilon\n\n\nX= np.stack([np.ones(N),np.exp(-x)],axis=1)\ny= y.reshape(N,1)\n\n\nnet = tf.keras.Sequential()\n\n\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\nnet.compile(tf.keras.optimizers.SGD(0.1), loss='mse') \n\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7f77b5e4c250>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_8/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.4630797],\n        [3.996811 ]], dtype=float32)>]\n\n\n\nbeta_hat = net.weights\ny_hat = (X @ beta_hat).reshape(-1)\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt \n\n\nplt.plot(x,y,\".\")\nplt.plot(x,y_hat,\"--\")"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-08-(1주차).html#모형의-매트릭스화",
    "href": "post/Lecture/STBD/2022-03-08-(1주차).html#모형의-매트릭스화",
    "title": "01. 단순선형회귀",
    "section": "모형의 매트릭스화",
    "text": "모형의 매트릭스화\n\n모형을 행렬로 표현하면 변수가 여러개인 multiple linear regression 에서도 단순형태로 표현이 가능하다.\n\n우리의 모형은 아래와 같다.\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i=1,2,\\dots,10\\)\n풀어서 쓰면\n\\(\\begin{cases} y_1 = \\beta_0 +\\beta_1 x_1 + \\epsilon_1 \\\\ y_2 = \\beta_0 +\\beta_1 x_2 + \\epsilon_2 \\\\ \\dots \\\\ y_{10} = \\beta_0 +\\beta_1 x_{10} + \\epsilon_{10} \\end{cases}\\)\n아래와 같이 쓸 수 있다.\n$\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\dots \\\\\ny_{10}\n\\end{bmatrix}\\]\n=\n\\[\\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\dots & \\dots \\\\\n1 & x_{10}\n\\end{bmatrix}\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\]\n\n\\[\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\dots \\\\\n\\epsilon_{10}\n\\end{bmatrix}\\]\n$\n\n벡터와 매트릭스 형태로 정리하면\n\\({\\bf y} = {\\bf X} {\\boldsymbol \\beta} + \\boldsymbol{\\epsilon}\\)\n- 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다.\n\\(loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2\\)\n이것을 벡터표현으로 하면 아래와 같다.\n\\(loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\)\n풀어보면\n\\(loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\)\n- 미분하는 과정의 매트릭스화\nloss를 최소화하는 \\({\\boldsymbol \\beta}\\)를 구해야하므로 loss를 \\({\\boldsymbol \\beta}\\)로 미분한식을 0이라고 놓고 풀면 된다.\n\\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} loss = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\)\n$= 0 - {}^- {}^ + 2{}^ $\n따라서 \\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}loss=0\\)을 풀면 아래와 같다.\n$= ({}){-1}{}^ $\n- 공식도 매트릭스로 표현하면 : \\(\\left(\\boldsymbol{\\hat\\beta}= ({\\bf X}^\\top {\\bf X})^{-1}{\\bf X}^\\top {\\bf y}\\right) \\leftarrow\\) 외우자 이건..\n- 적용을 해보자.\n(X를 만드는 방법1)\n\nX=tf.transpose(tf.concat([[[1.0]*10],[x]],0)) # \nX \n\n<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[ 1. , 20.1],\n       [ 1. , 22.2],\n       [ 1. , 22.7],\n       [ 1. , 23.3],\n       [ 1. , 24.4],\n       [ 1. , 25.1],\n       [ 1. , 26.2],\n       [ 1. , 27.3],\n       [ 1. , 28.4],\n       [ 1. , 30.4]], dtype=float32)>\n\n\n(X를 만드는 방법2)\n\ntf.concat([[[1.0]*10],[x]],0)\n\n<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\narray([[ 1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ],\n       [20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]],\n      dtype=float32)>\n\n\n\ntf.concat([[[1.0]*10],[x]],0).T\n\nAttributeError: \n        'EagerTensor' object has no attribute 'T'.\n        If you are looking for numpy-related methods, please run the following:\n        from tensorflow.python.ops.numpy_ops import np_config\n        np_config.enable_numpy_behavior()\n\n\n\n위 처럼 하면 error가 남\nerror를 읽어보면 numpy 스타일로 구성하고 싶을 경우 np_config.enable_numpy_behavior() 을 이용하라는 문구가 나옴\n\n\nfrom tensorflow.python.ops.numpy_ops import np_config\nnp_config.enable_numpy_behavior()\n\n\nX = tf.concat([[[1.0]*10],[x]],0).T\n\n오 이제된다.\n\nX\n\n<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[ 1. , 20.1],\n       [ 1. , 22.2],\n       [ 1. , 22.7],\n       [ 1. , 23.3],\n       [ 1. , 24.4],\n       [ 1. , 25.1],\n       [ 1. , 26.2],\n       [ 1. , 27.3],\n       [ 1. , 28.4],\n       [ 1. , 30.4]], dtype=float32)>\n\n\n\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([9.945015 , 2.2156935], dtype=float32)>\n\n\n결과를 보면 $ (_0,_1) = (9.94…,2.21…)$ 로 산출되었다.\n- 잘 구해진다.\n- 그런데..\n\nbeta0_estimated,beta1_estimated\n\n(<tf.Tensor: shape=(), dtype=float32, numpy=9.94458>,\n <tf.Tensor: shape=(), dtype=float32, numpy=2.2157042>)\n\n\n값이 좀 다르다..?\n- 같은 값입니다! 신경쓰지 마세요! 텐서플로우가 좀 대충계산합니다.\n- 실제로 조금 더 정확히 계산하기 위해서는 tensorflow 안에 내장된 numpy 를 사용한다.\n\nimport tensorflow.experimental.numpy as tnp \n\n\nx=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) \ny=10.2 + 2.2*x + epsilon \n\n\nbeta1_estimated = sum((x-sum(x)/10)*(y-sum(y)/10)) / sum((x-sum(x)/10)**2)\nbeta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 \n\n\nbeta0_estimated, beta1_estimated\n\n(<tf.Tensor: shape=(), dtype=float64, numpy=9.944573294798559>,\n <tf.Tensor: shape=(), dtype=float64, numpy=2.2157046054834106>)\n\n\n\nX=tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\n<tf.Tensor: shape=(2,), dtype=float64, numpy=array([9.94457329, 2.21570461])>"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-08-(1주차).html#회귀계수-추정",
    "href": "post/Lecture/STBD/2022-03-08-(1주차).html#회귀계수-추정",
    "title": "01. 단순선형회귀",
    "section": "회귀계수 추정",
    "text": "회귀계수 추정\n\\[Loss = \\sum (y-\\beta_0-\\beta_1 x)\\]\n\n\\(\\beta_0\\) 추정\n\n\\[L=Loss = \\sum (y-\\beta_0-\\beta_1 x)\\]\n\\[\\frac {d L}{d \\beta_0} = -2\\sum(y-\\beta_0-\\beta_1x) = 0\\]\n\\[\\therefore\\,\\,\\hat \\beta_0 = \\bar y -\\beta_1 \\bar x\\]\n\n\\(\\beta_1\\) 추정\n\n\\[\\begin{align}\\frac {d L}{d \\beta_1} &= \\sum x \\left(y-\\beta_0-\\beta_1x\\right) \\\\ \\\\ \\nonumber\n                                        & = \\left (\\sum xy -\\beta_1 x^2\\right )- n\\bar x\\left (\\bar y -\\beta_1\\bar x \\right) \\\\ \\\\ \\nonumber\n                                         &= \\left (\\sum xy -\\bar x \\bar y\\right ) -\\beta_1 \\left (\\sum x^2 - (\\bar x)^2\\right )\\nonumber\\end{align}\\]\n\\[\\begin{align}\n\\therefore \\hat {\\beta_1} &=  \\frac{\\sum xy -\\bar x \\bar y}{\\sum x^2 - (\\bar x)^2}  \\nonumber \\\\ \\\\\n&= \\frac {S_{xy}}{S_{xx}}  \\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-08-(1주차).html#벡터-미분-매트릭스-미분",
    "href": "post/Lecture/STBD/2022-03-08-(1주차).html#벡터-미분-매트릭스-미분",
    "title": "01. 단순선형회귀",
    "section": "벡터 미분 / 매트릭스 미분",
    "text": "벡터 미분 / 매트릭스 미분\n\\[L=loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\]\n\n벡터 미분\n\\[\\begin{align}  x^{\\top}y &= \\begin{bmatrix}x_1\\dots x_n\\end{bmatrix}\\begin{bmatrix} y_1   \\\\ \\dots \\\\ y_n\\end{bmatrix}\\nonumber  \\\\  \\\\\n      &=  x_1y_1 + x_2y_2+\\dots x_ny_n\\nonumber  \\end{align}\\]\n위를 미분하면\n\\[\\begin{align} \\frac {d x^{\\top}y}{d x} &= \\, \\begin{bmatrix} \\frac {d}{d x_1}  \\\\\n                                                                                       \\dots \\\\ \\frac {d}{d x_n}\n                                                                                        \\end{bmatrix} ( x_1y_1 + x_2y_2+\\dots x_ny_n) \\nonumber \\\\ \\\\  \n                                                         &=  \\, \\begin{bmatrix} y_1   \\\\ \\dots \\\\ y_n\\end{bmatrix}=y                         \\nonumber  \\end{align}\\]\n\n\n벡터 미분의 다른풀이\n\n(1)\n\\[ \\frac {d x^{\\top}y}{d x} = \\left (\\frac {d x^{\\top}}{d x}\\right) y=y\\]\n\\[\\begin{align}\\left( \\frac {d x^{\\top}}{d x} \\right)  = \\begin{bmatrix} \\frac {d}{d x_1} \\\\  \\dots \\\\ \\frac {d}{d x_{n}} \\end {bmatrix} \\begin{bmatrix} x_1 \\dots x_n \\end{bmatrix} =\\begin{bmatrix}\\frac {d x_1}{d x_1} & \\dots & \\frac {d x_n}{d x_1} \\\\\n                                                                         \\dots & \\dots  &\\dots  \\\\\n                                                                        \\frac {d x_1}{d x_n} & \\dots & \\frac {d x_n}{d x_n}     \\end{bmatrix} = \\mathbf{I} \\nonumber \\end{align}\\]\n\\[\\therefore \\quad \\frac {d x^{\\top}y}{d x} = \\left (\\frac {d x^{\\top}}{d x}\\right) y = \\mathbf{I} y = y\\]\n\n\n(2)\n\\[ \\frac {d y^{\\top}x}{d x} = \\left (\\frac {d y^{\\top}x}{d x}\\right) =y \\]\n$ y^{} x $는 \\(1 \\times 1\\) 차원이므로 인간이면 이해할 수 있을 듯?\n\n\n(3)\n\\[\\frac {d}{d \\boldsymbol{\\beta}} \\left ( \\mathbf{y^{\\top}X\\boldsymbol \\beta}\\right ) = \\mathbf{X^{\\top}y}\\]\n\\(\\mathbf{y^{\\top}X\\boldsymbol \\beta}\\) 는 \\(1 \\times 1\\) 인 스칼라 이므로\n\\[\\mathbf{y^{\\top}X\\boldsymbol \\beta} = \\left (\\mathbf{y^{\\top}X\\boldsymbol \\beta}\\right)^{\\top} = \\boldsymbol \\beta^{\\top}\\mathbf{X^{\\top} y}\\]\n따라서\n\\[\\begin{align} \\frac {d}{d \\boldsymbol{\\beta}} \\left ( \\mathbf{y^{\\top}X\\boldsymbol \\beta}\\right ) &= \\frac {d}{d \\boldsymbol{\\beta}} \\left (\\boldsymbol \\beta^{\\top}\\mathbf{X^{\\top} y}\\right )\\nonumber \\\\ \\\\\n                                                  &= \\left ( \\frac{d}{d \\boldsymbol{\\beta}}\\boldsymbol{\\beta}^{\\top}\\right)\\mathbf{X^{\\top y}} \\nonumber\\\\ \\\\\n                                                    &= \\mathbf{I\\,X^{\\top}y} \\nonumber \\\\ \\\\ &= \\mathbf{X^{\\top}y}    \\nonumber      \\end{align}\\]\n\n\n(4)\n\\[ \\frac {d }{d \\mathbf {y}} (\\mathbf{y^{\\top}y}) = 2\\mathbf{y}\\]\n\\[ d\\, \\mathbf{y} = \\left [\\frac {d}{y_1}, \\,\\frac {d}{y_2}\\dots\\dots \\frac {d}{y_n}\\right ]\\]\n\\[\\mathbf{y^{\\top}y} =  \\sum{y_i}^2\\]\n\\[\\therefore  \\quad \\frac {d }{d \\mathbf {y}} (\\mathbf{y^{\\top}y}) = 2\\mathbf{y}\\]\n\n\n참고할 틀린풀이\n\n아래와 같은 풀이는 1번의 벡터 미분의 다른 풀이처럼 풀면 안된다.\n\n\\[ \\frac {d }{d \\mathbf {y}} (\\mathbf{y^{\\top}y}) = \\mathbf{y}\\]\n\\[ \\frac {d\\,\\mathbf{y^{\\top}y}}{d \\,\\mathbf{y}} = \\left (\\frac {d\\, \\mathbf{y^{\\top}y}}{d\\, \\mathbf y}\\right) \\mathbf y= \\mathbf{I} y \\neq y\\]\nbecause 스칼라 경우를 생각해보자\n(틀린풀이 )\\(\\quad \\frac {d }{d\\,y} y^2 = \\left (\\frac {d}{d\\,y } yy \\right) = y\\) ?\n(올바른 풀이) \\(\\quad \\frac {d }{d\\,y} y^2 = \\left (\\frac {d}{d\\,y } y_1 \\right)+ \\left (\\frac {d}{d\\,y } y_2 \\right) = 2y\\)\n스칼라를 예제로 들었는데 벡터에서 이런 느낌이라고 생각하자 이게 표준적으로 사용되는 설명은 아니지만 이해적? 으로는 간편한 듯\n다른풀이 (3)번의 경우도 원래는 안되는데 값이 스칼라 이므로 틀린풀이 처럼 안되는 경우이나 1 x 1 행렬이므로 가능한 것이다.\n다시 벡터로 돌아오면\n(올바른 풀이) \\(\\quad \\left ( \\frac {d}{d\\,\\mathbf{y}} (\\mathbf{y^{\\top}y})\\right ) = A + B\\)\n\\(A = f(\\mathbf y)\\) , \\(B = g(\\mathbf y)\\) 라고 생각하자\n$ A =( ) = = $\n\\(B\\) 의 경우도 위와 동일하므로\n\\[\\frac {d\\,(A+B)}{d\\,\\mathbf{y}} = \\{f(\\mathbf {y})\\}^{\\prime} +\\{g(\\mathbf {y})\\}^{\\prime} = 2\\mathbf{y}\\]\n\n\n(5)\n\\[\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta} = 2\\mathbf{X^{\\top}X}\\boldsymbol \\beta\\]\n4번의 원리를 이용하면 이지이지"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-08-(1주차).html#loss를-미분",
    "href": "post/Lecture/STBD/2022-03-08-(1주차).html#loss를-미분",
    "title": "01. 단순선형회귀",
    "section": "loss를 미분",
    "text": "loss를 미분\n\\[L=loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\]\n\\[L = {\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\]\n\\(L\\) 을 미분하면\n\\[\\begin{align} \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} L &= \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta} \\nonumber \\\\ \\\\\n            &=  0 - \\mathbf{X^{\\top}y} - \\mathbf{X^{\\top}y} + 2\\mathbf{X^{\\top}X\\boldsymbol \\beta}\\nonumber \\end{align}\\]\n따라서 아래와 같은식이 성립한다.\n\\[  \\mathbf{X^{\\top}y}= \\mathbf{X^{\\top} X} \\boldsymbol \\beta \\]\n\\[\\hat {\\boldsymbol \\beta} = \\mathbf{\\left (X^{\\top}X\\right)^{-1}Xy}  \\]"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-14-(2주차).html",
    "href": "post/Lecture/STBD/2022-03-14-(2주차).html",
    "title": "02. Tensorflow-1",
    "section": "",
    "text": "단순선형회귀의 경우 일반적인 베타계수의 추정치\n\n\\(\\hat {\\beta_0} = \\bar y - \\beta _1\\bar x, \\quad \\hat {\\beta_1} = \\frac {S_{xy}} {S_{xx}}\\)\n\n다중 회귀의 경우\n\n\\[L=loss =({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\]\n\\[ L= {\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\]\n\n위를 미분하면\n\n\\[\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} L = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}  \\]\n\\[\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} L=- \\mathbf{X^{\\top}y} - \\mathbf{X^{\\top}y} + 2\\mathbf{X^{\\top}X\\boldsymbol \\beta}\\]\n따라서\n\\[\\quad \\bf{X^{\\top}X}\\beta\n= \\bf{X^{\\top}Y}\\]\n\\[\\therefore \\quad \\hat {\\beta} = \\left(\\bf{X^{\\top}X}^{-1}\\right)XY\\]"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-14-(2주차).html#예비학습-중첩리스트",
    "href": "post/Lecture/STBD/2022-03-14-(2주차).html#예비학습-중첩리스트",
    "title": "02. Tensorflow-1",
    "section": "예비학습: 중첩리스트",
    "text": "예비학습: 중첩리스트\n- 리스트\n\nlst = list(range(6))\nlst\n\n[0, 1, 2, 3, 4, 5]\n\n\n\nlst[-1]\n\n5\n\n\n- 리스트 안에 리스트 생성\n\nlst =[[1,2,],[3,4]]\nlst\n\n[[1, 2], [3, 4]]\n\n\n\nprint(lst[1][0],lst[0][0])\n\n3 1\n\n\n- 위 같은 2차원의 리스트 구조를 행렬로 생각할 수 있다\n1 2 \\ 3 4\n또는\n1 \\ 2 \\ 3 \\ 4\n- (4,1) 행렬 느낌의 리스트\n\nlst = [[1],[2],[3],[4]]\nlst\n\n[[1], [2], [3], [4]]\n\n\n\nnp.array(lst)\n\narray([[1],\n       [2],\n       [3],\n       [4]])\n\n\n- (1,4) 행렬 느낌의 리스트\n\nlst = [1,2,3,4]\nlst\n\n[1, 2, 3, 4]\n\n\n\nnp.array(lst)\n\narray([1, 2, 3, 4])"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-14-(2주차).html#변수-선언",
    "href": "post/Lecture/STBD/2022-03-14-(2주차).html#변수-선언",
    "title": "02. Tensorflow-1",
    "section": "변수 선언",
    "text": "변수 선언\n\n스칼라\n\ntf.constant(3.14)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=3.14>\n\n\n\ntf.constant(3.14) + tf.constant(3.14)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=6.28>\n\n\n\n\n벡터\n\n_vector = tf.constant([1,2,3 ])\n_vector\n\n<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>\n\n\n\n_vector[0]\n\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n\n\n\n\n매트릭스 생성\n\n_matrix = tf.constant([[1,0],[0,1]])\n_matrix\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 0],\n       [0, 1]], dtype=int32)>\n\n\n\n\n텐서 == 3차원 이상의 배열\n\nnp.array([[[0,1],[1,2]],[[0,1],[1,2]]])\n\narray([[[0, 1],\n        [1, 2]],\n\n       [[0, 1],\n        [1, 2]]])\n\n\n\ntf.constant([[[0,1],[1,2]],[[0,1],[1,2]]])\n\n<tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\narray([[[0, 1],\n        [1, 2]],\n\n       [[0, 1],\n        [1, 2]]], dtype=int32)>\n\n\n\n\n타입\n\ntype(tf.constant([[[0,1],[1,2]],[[0,1],[1,2]]]))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n- 끝에 EagerTensor 가 나오는 것을 기억하자\n\n\n인덱싱\n\n_matrix  = tf.constant([[1,2],[3,4]])\n_matrix\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)>\n\n\n\n_matrix[0]\n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>\n\n\n\n_matrix[0,:]\n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>\n\n\n\n_matrix[0,0]\n\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n\n\n\n_matrix[0][0]\n\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n\n\n\n\ntf.constant는 불편하다.\n- 각 컬럼의 데이터 타입이 전부 동일하여야 한다.\n- 원소 수정이 불가능함.\n\n a= tf.constant([1,22,33])\n a\n\n<tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 1, 22, 33], dtype=int32)>\n\n\n\na[0] =11\n\nTypeError: ignored\n\n\n- 묵시적(간접적) 형변환이 불가능하다.\n\n1+3.14\n\n4.140000000000001\n\n\n\ntf.constant(1) + tf.constant(3.14)\n\nInvalidArgumentError: ignored\n\n\n- 같은 float 도 안되는 경우가 있음\n\ntf.constant(1.0, dtype= tf.float64) + tf.constant(3.14)\n\nInvalidArgumentError: ignored\n\n\n\n\ntf.constant \\(\\to\\) 넘파이\n\nnp.array(tf.constant(1))\n\narray(1, dtype=int32)\n\n\n\na = tf.constant(3.14)\ntype(a)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\na.numpy()\n\n3.14"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-14-(2주차).html#연산",
    "href": "post/Lecture/STBD/2022-03-14-(2주차).html#연산",
    "title": "02. Tensorflow-1",
    "section": "연산",
    "text": "연산\n\n더하기\n\na = tf.constant([1,2])\nb = tf.constant([3,4])\na+b\n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)>\n\n\n\ntf.add(a,b) ## 이건 예전버전 \n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)>\n\n\n\n\n곱하기\n- 결과가 조금 이상하다. 일반적인 행렬연사이 아니다\n\na = tf.constant([[1,2],[3,4]])\nb = tf.constant([[5,6],[7,8]])\na*b\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 5, 12],\n       [21, 32]], dtype=int32)>\n\n\n- but matrix의 곱은\n\na = tf.constant([[1,0],[0,1]])\nb = tf.constant([[5],[7]])\na@b\n\n<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\narray([[5],\n       [7]], dtype=int32)>\n\n\n\ntf.matmul(a,b) ## 위와 같은 표현\n\n\n\n역행렬\n\na = tf.constant([[1,0],[0,2]])\na\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 0],\n       [0, 2]], dtype=int32)>\n\n\n\ntf.linalg.inv(a)\n\nInvalidArgumentError: ignored\n\n\n\n위의 경우는 자료가 int 형이여서 안되는 거임\n\n\n?tf.constant\n\n\n아래오 같이 자료형을 선언해 주어야함\n\n\na = tf.constant([[1,0],[0,2]],dtype=float)\ntf.linalg.inv(a)\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1. , 0. ],\n       [0. , 0.5]], dtype=float32)>\n\n\n\n\ndeterminant\n\na = tf.constant([[1,2],[3,4]],dtype=float)\nprint(a)\ntf.linalg.det(a)\n\ntf.Tensor(\n[[1. 2.]\n [3. 4.]], shape=(2, 2), dtype=float32)\n\n\n<tf.Tensor: shape=(), dtype=float32, numpy=-2.0>\n\n\n\n\nTrace\n\ntf.linalg.trace(a)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=5.0>"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-14-(2주차).html#형태변환",
    "href": "post/Lecture/STBD/2022-03-14-(2주차).html#형태변환",
    "title": "02. Tensorflow-1",
    "section": "형태변환",
    "text": "형태변환\n- 1 x 4 행렬을 \\(\\to\\) 4 x 1\n\na = tf.constant([1,2,3,4])\na\n\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n\n\n\ntf.reshape(a,(2,2))\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)>\n\n\n- 3차원으로도 변경이 가능\n\ntf.reshape(a,(2,2,1))\n\n<tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\narray([[[1],\n        [2]],\n\n       [[3],\n        [4]]], dtype=int32)>\n\n\n- 다차원의 경우 적용\n\na = tf.constant(list(range(1,13)))\na\n\n<tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)>\n\n\n\ntf.reshape(a,(2,2,3))\n\n<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)>\n\n\n\ntf.reshape(a,(4,3))\n\n<tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)>\n\n\n\n-1 을 기입하면 남은 차원 수를 알아서 기입해줌 -1 = ? 라고 생각\n\n\ntf.reshape(a,(4,-1))\n\n<tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)>\n\n\n\nb= tf.reshape(a,(2,2,-1))\nb\n\n<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)>\n\n\n- 다시 일차원으로 되돌림\n\ntf.reshape(b,-1)\n\n<tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)>"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-14-(2주차).html#선언고급",
    "href": "post/Lecture/STBD/2022-03-14-(2주차).html#선언고급",
    "title": "02. Tensorflow-1",
    "section": "선언고급",
    "text": "선언고급\n- 리스트나, 넘파이로 만들고 output을 tensor로 변경하는 것도 좋은 방법이다.\n\nㅣ = [1,2,3,4]\ntf.constant(np.diag(ㅣ))\n\n<tf.Tensor: shape=(4, 4), dtype=int64, numpy=\narray([[1, 0, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 0],\n       [0, 0, 0, 4]])>\n\n\n- tf.ones, tf.zeros\n\ntf.zeros([3,3])\n\n<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]], dtype=float32)>\n\n\n- tf.linspace(0,1,10)\n\ntf.linspace(0,1,10)\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])>"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-14-(2주차).html#tf.concat",
    "href": "post/Lecture/STBD/2022-03-14-(2주차).html#tf.concat",
    "title": "02. Tensorflow-1",
    "section": "tf.concat",
    "text": "tf.concat\n\na = tf.constant([1,2])\nb = tf.constant([3,4])\na,b\n\n(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>,\n <tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)>)\n\n\n\na = tf.constant([[1],[2]])\nb = tf.constant([[3],[4]])\na,b\n\n(<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[1],\n        [2]], dtype=int32)>, <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[3],\n        [4]], dtype=int32)>)\n\n\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]], dtype=int32)>\n\n\n\na = tf.constant([[1],[2]])\nb = tf.constant([[3],[4]])\na,b\n\n(<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[1],\n        [2]], dtype=int32)>, <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[3],\n        [4]], dtype=int32)>)\n\n\n\ntf.concat([a,b],axis=1)\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 3],\n       [2, 4]], dtype=int32)>\n\n\n\na = tf.constant([1,2])\nb = tf.constant([3,4])\na,b\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n\n\n\na = tf.constant([[1,2]]) \nb = tf.constant([[3,4]])\na,b\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)>\n\n\n\n차원 수 증가\n\n(2,3,4,5) concat (2,3,4,5) => (4,3,4,5)\n\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb= -a\n\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(4, 3, 4, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119]]],\n\n\n       [[[   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)>\n\n\n\n(2,3,4,5) concat (2,3,4,5) => (2,6,4,5)\n\n\ntf.concat([a,b],axis=1)\n\n<tf.Tensor: shape=(2, 6, 4, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59]],\n\n        [[   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119]],\n\n        [[ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)>\n\n\n\n(2,3,4,5) concat (2,3,4,5) => (2,3,8,5)\n\n\ntf.concat([a,b],axis=2)\n\n<tf.Tensor: shape=(2, 3, 8, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19],\n         [   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39],\n         [ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59],\n         [ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79],\n         [ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99],\n         [ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119],\n         [-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)>\n\n\n\n(2,3,4,5) concat (2,3,4,5) => (2,3,4,10)\n\n\ntf.concat([a,b],axis=3)\n\n<tf.Tensor: shape=(2, 3, 4, 10), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4,    0,   -1,   -2,   -3,   -4],\n         [   5,    6,    7,    8,    9,   -5,   -6,   -7,   -8,   -9],\n         [  10,   11,   12,   13,   14,  -10,  -11,  -12,  -13,  -14],\n         [  15,   16,   17,   18,   19,  -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24,  -20,  -21,  -22,  -23,  -24],\n         [  25,   26,   27,   28,   29,  -25,  -26,  -27,  -28,  -29],\n         [  30,   31,   32,   33,   34,  -30,  -31,  -32,  -33,  -34],\n         [  35,   36,   37,   38,   39,  -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44,  -40,  -41,  -42,  -43,  -44],\n         [  45,   46,   47,   48,   49,  -45,  -46,  -47,  -48,  -49],\n         [  50,   51,   52,   53,   54,  -50,  -51,  -52,  -53,  -54],\n         [  55,   56,   57,   58,   59,  -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64,  -60,  -61,  -62,  -63,  -64],\n         [  65,   66,   67,   68,   69,  -65,  -66,  -67,  -68,  -69],\n         [  70,   71,   72,   73,   74,  -70,  -71,  -72,  -73,  -74],\n         [  75,   76,   77,   78,   79,  -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84,  -80,  -81,  -82,  -83,  -84],\n         [  85,   86,   87,   88,   89,  -85,  -86,  -87,  -88,  -89],\n         [  90,   91,   92,   93,   94,  -90,  -91,  -92,  -93,  -94],\n         [  95,   96,   97,   98,   99,  -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104, -100, -101, -102, -103, -104],\n         [ 105,  106,  107,  108,  109, -105, -106, -107, -108, -109],\n         [ 110,  111,  112,  113,  114, -110, -111, -112, -113, -114],\n         [ 115,  116,  117,  118,  119, -115, -116, -117, -118, -119]]]],\n      dtype=int32)>\n\n\n\n아래와 같은 방법도 있긴하나 난 안할래\n\n\ntf.concat([a,b],axis=-1)\n\n<tf.Tensor: shape=(2, 3, 4, 10), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4,    0,   -1,   -2,   -3,   -4],\n         [   5,    6,    7,    8,    9,   -5,   -6,   -7,   -8,   -9],\n         [  10,   11,   12,   13,   14,  -10,  -11,  -12,  -13,  -14],\n         [  15,   16,   17,   18,   19,  -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24,  -20,  -21,  -22,  -23,  -24],\n         [  25,   26,   27,   28,   29,  -25,  -26,  -27,  -28,  -29],\n         [  30,   31,   32,   33,   34,  -30,  -31,  -32,  -33,  -34],\n         [  35,   36,   37,   38,   39,  -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44,  -40,  -41,  -42,  -43,  -44],\n         [  45,   46,   47,   48,   49,  -45,  -46,  -47,  -48,  -49],\n         [  50,   51,   52,   53,   54,  -50,  -51,  -52,  -53,  -54],\n         [  55,   56,   57,   58,   59,  -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64,  -60,  -61,  -62,  -63,  -64],\n         [  65,   66,   67,   68,   69,  -65,  -66,  -67,  -68,  -69],\n         [  70,   71,   72,   73,   74,  -70,  -71,  -72,  -73,  -74],\n         [  75,   76,   77,   78,   79,  -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84,  -80,  -81,  -82,  -83,  -84],\n         [  85,   86,   87,   88,   89,  -85,  -86,  -87,  -88,  -89],\n         [  90,   91,   92,   93,   94,  -90,  -91,  -92,  -93,  -94],\n         [  95,   96,   97,   98,   99,  -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104, -100, -101, -102, -103, -104],\n         [ 105,  106,  107,  108,  109, -105, -106, -107, -108, -109],\n         [ 110,  111,  112,  113,  114, -110, -111, -112, -113, -114],\n         [ 115,  116,  117,  118,  119, -115, -116, -117, -118, -119]]]],\n      dtype=int32)>\n\n\n\n\n차원을 한번 줄여보자\n\n(4,) -> (8,)\n\n\na=tf.constant([1,2,3,4])\nb=-a\na,b\n\n(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>,\n <tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)>)\n\n\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(8,), dtype=int32, numpy=array([ 1,  2,  3,  4, -1, -2, -3, -4], dtype=int32)>\n\n\n\n(4,) -> (4,2)\n\n- 에러가 뜬다\n\ntf.concat([a,b],axis=1)\n\nInvalidArgumentError: ignored"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-14-(2주차).html#tf.stack",
    "href": "post/Lecture/STBD/2022-03-14-(2주차).html#tf.stack",
    "title": "02. Tensorflow-1",
    "section": "tf.stack",
    "text": "tf.stack\n\n(4,) -> (4,2)\n\n\na=tf.constant([1,2,3,4])\nb=-a\na,b\n\n(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>,\n <tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)>)\n\n\n\ntf.stack([a,b],axis=1)\n\n<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3],\n       [ 4, -4]], dtype=int32)>"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-14-(2주차).html#tf.einsum",
    "href": "post/Lecture/STBD/2022-03-14-(2주차).html#tf.einsum",
    "title": "02. Tensorflow-1",
    "section": "tf.einsum",
    "text": "tf.einsum"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-14-(2주차).html#tnp-사용방법-불만해결방법",
    "href": "post/Lecture/STBD/2022-03-14-(2주차).html#tnp-사용방법-불만해결방법",
    "title": "02. Tensorflow-1",
    "section": "tnp 사용방법 (불만해결방법)",
    "text": "tnp 사용방법 (불만해결방법)\n- int 와 float 을 더할 수 있음\n\ntnp.array([1,2,3]) + tnp.array([1.0,2.0,3.0]) \n\n<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])>\n\n\n\n심지어\n\n\ntnp.array(1) + tnp.array([1.0,2.0,3.0]) \n\n<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 3., 4.])>\n\n\n\ntnp.array([1,2,3]) + tf.constant([1.0,2.0,3.0]) \n\n<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])>\n\n\n\na = tnp.diag([1,2,3])\ntype(a)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\na.min(),a.max()\n\n(<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n <tf.Tensor: shape=(), dtype=int64, numpy=3>)\n\n\n\na.reshape(9,1)\n\n<tf.Tensor: shape=(9, 1), dtype=int64, numpy=\narray([[1],\n       [0],\n       [0],\n       [0],\n       [2],\n       [0],\n       [0],\n       [0],\n       [3]])>\n\n\n\n선언, 선언고급\n\nnp.random.randn(5)\n\narray([-1.79271696, -0.17190837,  1.01536417,  0.10096996,  0.6384037 ])\n\n\n\ntnp.random.randn(5)\n\n<tf.Tensor: shape=(5,), dtype=float64, numpy=array([ 0.68371875, -0.77886642, -0.78283853, -1.91862598, -0.36602414])>\n\n\n\n\n타입\n\ntype(tnp.random.randn(5))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n\ntf.contant로 만들어도 마치 넘파이인듯 쓰는 기능들\n- 묵시적 형변환이 가능해짐\n- 메소드를 쓸 수 있음.\n\n\n그렇지만 np.array는 아님\n\n여전히 값을 바꾸는 것은 허용하지 않는다.\n\n\n a = tf.constant([1,2,3])\n\n\na[0]=11\n\nTypeError: ignored\n\n\n\n\ntf.Variable\n\n선언\n\n\n타입\n\n\n인덱싱\n\n\ntf.Variable \\(\\to\\) 넘파이\n\n\ntf.Variable 도 불편하다.\n\n\n연산\n\n\n형태변환\n\n\n선언고급\n\n\ntf.concat\n\n\ntf.stack\n\n\n심지어 tf.Variable()로 만들어진 오브젝트는 tnp의 효과(은총)도 받지 못함"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-21-(3주차).html",
    "href": "post/Lecture/STBD/2022-03-21-(3주차).html",
    "title": "03. Tensorflow-2",
    "section": "",
    "text": "imports\n\nimport tensorflow as tf\nimport numpy as np\n\n\ntf.config.experimental.list_physical_devices('GPU')\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\n\n지난강의 보충\n- max, min, sum, mean\n\na= tf.constant([1.0,2.0,3.0,4.0])\na\n\n<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)>\n\n\n\ntf.reduce_mean(a)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=2.5>\n\n\n\nconcat, stack\n- 예제: (2,3,4,5) stack (2,3,4,5) -> (?,?,?,?,?)\n\na = tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5))\nb = -a \n\ncase1 (1,2,3,4,5) stack (1,2,3,4,5) –> (2,2,3,4,5) # axis=0\n\ntf.stack([a,b],axis=0)\n\n<tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]]],\n\n\n        [[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]]]],\n\n\n\n       [[[[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]],\n\n\n        [[[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)>\n\n\ncase2 (2,1,3,4,5) stack (2,1,3,4,5) –> (2,2,3,4,5) # axis=1\n\ntf.stack([a,b],axis=1)\n\n<tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]]],\n\n\n        [[[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]]],\n\n\n        [[[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)>\n\n\ncase3 (2,3,1,4,5) stack (2,3,1,4,5) –> (2,3,2,4,5) # axis=2\n\ntf.stack([a,b],axis=2)\n\n<tf.Tensor: shape=(2, 3, 2, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]]],\n\n\n        [[[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]]],\n\n\n        [[[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]]],\n\n\n        [[[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]]],\n\n\n        [[[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)>\n\n\ncase4 (2,3,4,1,5) stack (2,3,4,1,5) –> (2,3,4,2,5) # axis=3\n\ntf.stack([a,b],axis=-2)\n\n<tf.Tensor: shape=(2, 3, 4, 2, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   0,   -1,   -2,   -3,   -4]],\n\n         [[   5,    6,    7,    8,    9],\n          [  -5,   -6,   -7,   -8,   -9]],\n\n         [[  10,   11,   12,   13,   14],\n          [ -10,  -11,  -12,  -13,  -14]],\n\n         [[  15,   16,   17,   18,   19],\n          [ -15,  -16,  -17,  -18,  -19]]],\n\n\n        [[[  20,   21,   22,   23,   24],\n          [ -20,  -21,  -22,  -23,  -24]],\n\n         [[  25,   26,   27,   28,   29],\n          [ -25,  -26,  -27,  -28,  -29]],\n\n         [[  30,   31,   32,   33,   34],\n          [ -30,  -31,  -32,  -33,  -34]],\n\n         [[  35,   36,   37,   38,   39],\n          [ -35,  -36,  -37,  -38,  -39]]],\n\n\n        [[[  40,   41,   42,   43,   44],\n          [ -40,  -41,  -42,  -43,  -44]],\n\n         [[  45,   46,   47,   48,   49],\n          [ -45,  -46,  -47,  -48,  -49]],\n\n         [[  50,   51,   52,   53,   54],\n          [ -50,  -51,  -52,  -53,  -54]],\n\n         [[  55,   56,   57,   58,   59],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [ -60,  -61,  -62,  -63,  -64]],\n\n         [[  65,   66,   67,   68,   69],\n          [ -65,  -66,  -67,  -68,  -69]],\n\n         [[  70,   71,   72,   73,   74],\n          [ -70,  -71,  -72,  -73,  -74]],\n\n         [[  75,   76,   77,   78,   79],\n          [ -75,  -76,  -77,  -78,  -79]]],\n\n\n        [[[  80,   81,   82,   83,   84],\n          [ -80,  -81,  -82,  -83,  -84]],\n\n         [[  85,   86,   87,   88,   89],\n          [ -85,  -86,  -87,  -88,  -89]],\n\n         [[  90,   91,   92,   93,   94],\n          [ -90,  -91,  -92,  -93,  -94]],\n\n         [[  95,   96,   97,   98,   99],\n          [ -95,  -96,  -97,  -98,  -99]]],\n\n\n        [[[ 100,  101,  102,  103,  104],\n          [-100, -101, -102, -103, -104]],\n\n         [[ 105,  106,  107,  108,  109],\n          [-105, -106, -107, -108, -109]],\n\n         [[ 110,  111,  112,  113,  114],\n          [-110, -111, -112, -113, -114]],\n\n         [[ 115,  116,  117,  118,  119],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)>\n\n\ncase5 (2,3,4,5,1) stack (2,3,4,5,1) –> (2,3,4,5,2) # axis=4\n\ntf.stack([a,b],axis=-1)\n\n<tf.Tensor: shape=(2, 3, 4, 5, 2), dtype=int32, numpy=\narray([[[[[   0,    0],\n          [   1,   -1],\n          [   2,   -2],\n          [   3,   -3],\n          [   4,   -4]],\n\n         [[   5,   -5],\n          [   6,   -6],\n          [   7,   -7],\n          [   8,   -8],\n          [   9,   -9]],\n\n         [[  10,  -10],\n          [  11,  -11],\n          [  12,  -12],\n          [  13,  -13],\n          [  14,  -14]],\n\n         [[  15,  -15],\n          [  16,  -16],\n          [  17,  -17],\n          [  18,  -18],\n          [  19,  -19]]],\n\n\n        [[[  20,  -20],\n          [  21,  -21],\n          [  22,  -22],\n          [  23,  -23],\n          [  24,  -24]],\n\n         [[  25,  -25],\n          [  26,  -26],\n          [  27,  -27],\n          [  28,  -28],\n          [  29,  -29]],\n\n         [[  30,  -30],\n          [  31,  -31],\n          [  32,  -32],\n          [  33,  -33],\n          [  34,  -34]],\n\n         [[  35,  -35],\n          [  36,  -36],\n          [  37,  -37],\n          [  38,  -38],\n          [  39,  -39]]],\n\n\n        [[[  40,  -40],\n          [  41,  -41],\n          [  42,  -42],\n          [  43,  -43],\n          [  44,  -44]],\n\n         [[  45,  -45],\n          [  46,  -46],\n          [  47,  -47],\n          [  48,  -48],\n          [  49,  -49]],\n\n         [[  50,  -50],\n          [  51,  -51],\n          [  52,  -52],\n          [  53,  -53],\n          [  54,  -54]],\n\n         [[  55,  -55],\n          [  56,  -56],\n          [  57,  -57],\n          [  58,  -58],\n          [  59,  -59]]]],\n\n\n\n       [[[[  60,  -60],\n          [  61,  -61],\n          [  62,  -62],\n          [  63,  -63],\n          [  64,  -64]],\n\n         [[  65,  -65],\n          [  66,  -66],\n          [  67,  -67],\n          [  68,  -68],\n          [  69,  -69]],\n\n         [[  70,  -70],\n          [  71,  -71],\n          [  72,  -72],\n          [  73,  -73],\n          [  74,  -74]],\n\n         [[  75,  -75],\n          [  76,  -76],\n          [  77,  -77],\n          [  78,  -78],\n          [  79,  -79]]],\n\n\n        [[[  80,  -80],\n          [  81,  -81],\n          [  82,  -82],\n          [  83,  -83],\n          [  84,  -84]],\n\n         [[  85,  -85],\n          [  86,  -86],\n          [  87,  -87],\n          [  88,  -88],\n          [  89,  -89]],\n\n         [[  90,  -90],\n          [  91,  -91],\n          [  92,  -92],\n          [  93,  -93],\n          [  94,  -94]],\n\n         [[  95,  -95],\n          [  96,  -96],\n          [  97,  -97],\n          [  98,  -98],\n          [  99,  -99]]],\n\n\n        [[[ 100, -100],\n          [ 101, -101],\n          [ 102, -102],\n          [ 103, -103],\n          [ 104, -104]],\n\n         [[ 105, -105],\n          [ 106, -106],\n          [ 107, -107],\n          [ 108, -108],\n          [ 109, -109]],\n\n         [[ 110, -110],\n          [ 111, -111],\n          [ 112, -112],\n          [ 113, -113],\n          [ 114, -114]],\n\n         [[ 115, -115],\n          [ 116, -116],\n          [ 117, -117],\n          [ 118, -118],\n          [ 119, -119]]]]], dtype=int32)>\n\n\n- 예제: (2,3,4), (2,3,4), (2,3,4)\n\na= tf.reshape(tf.constant(range(2*3*4)),(2,3,4))\nb= -a \nc= 2*a\n\n(예시1) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (6,3,4)\n\ntf.concat([a,b,c],axis=0)\n\n<tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]],\n\n       [[  0,   2,   4,   6],\n        [  8,  10,  12,  14],\n        [ 16,  18,  20,  22]],\n\n       [[ 24,  26,  28,  30],\n        [ 32,  34,  36,  38],\n        [ 40,  42,  44,  46]]], dtype=int32)>\n\n\n(예시2) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,9,4)\n\ntf.concat([a,b,c],axis=1)\n\n<tf.Tensor: shape=(2, 9, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11],\n        [  0,   2,   4,   6],\n        [  8,  10,  12,  14],\n        [ 16,  18,  20,  22]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23],\n        [ 24,  26,  28,  30],\n        [ 32,  34,  36,  38],\n        [ 40,  42,  44,  46]]], dtype=int32)>\n\n\n(예시3) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,12)\n\ntf.concat([a,b,c],axis=-1)\n\n<tf.Tensor: shape=(2, 3, 12), dtype=int32, numpy=\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3,   0,   2,   4,   6],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7,   8,  10,  12,  14],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11,  16,  18,  20,  22]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15,  24,  26,  28,  30],\n        [ 16,  17,  18,  19, -16, -17, -18, -19,  32,  34,  36,  38],\n        [ 20,  21,  22,  23, -20, -21, -22, -23,  40,  42,  44,  46]]],\n      dtype=int32)>\n\n\n(예시4) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (3,2,3,4)\n\ntf.stack([a,b,c],axis=0)\n\n<tf.Tensor: shape=(3, 2, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  4,   5,   6,   7],\n         [  8,   9,  10,  11]],\n\n        [[ 12,  13,  14,  15],\n         [ 16,  17,  18,  19],\n         [ 20,  21,  22,  23]]],\n\n\n       [[[  0,  -1,  -2,  -3],\n         [ -4,  -5,  -6,  -7],\n         [ -8,  -9, -10, -11]],\n\n        [[-12, -13, -14, -15],\n         [-16, -17, -18, -19],\n         [-20, -21, -22, -23]]],\n\n\n       [[[  0,   2,   4,   6],\n         [  8,  10,  12,  14],\n         [ 16,  18,  20,  22]],\n\n        [[ 24,  26,  28,  30],\n         [ 32,  34,  36,  38],\n         [ 40,  42,  44,  46]]]], dtype=int32)>\n\n\n(예시5) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,3,4)\n\ntf.stack([a,b,c],axis=1)\n\n<tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  4,   5,   6,   7],\n         [  8,   9,  10,  11]],\n\n        [[  0,  -1,  -2,  -3],\n         [ -4,  -5,  -6,  -7],\n         [ -8,  -9, -10, -11]],\n\n        [[  0,   2,   4,   6],\n         [  8,  10,  12,  14],\n         [ 16,  18,  20,  22]]],\n\n\n       [[[ 12,  13,  14,  15],\n         [ 16,  17,  18,  19],\n         [ 20,  21,  22,  23]],\n\n        [[-12, -13, -14, -15],\n         [-16, -17, -18, -19],\n         [-20, -21, -22, -23]],\n\n        [[ 24,  26,  28,  30],\n         [ 32,  34,  36,  38],\n         [ 40,  42,  44,  46]]]], dtype=int32)>\n\n\n(예시6) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,3,4)\n\ntf.stack([a,b,c],axis=2)\n\n<tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  0,  -1,  -2,  -3],\n         [  0,   2,   4,   6]],\n\n        [[  4,   5,   6,   7],\n         [ -4,  -5,  -6,  -7],\n         [  8,  10,  12,  14]],\n\n        [[  8,   9,  10,  11],\n         [ -8,  -9, -10, -11],\n         [ 16,  18,  20,  22]]],\n\n\n       [[[ 12,  13,  14,  15],\n         [-12, -13, -14, -15],\n         [ 24,  26,  28,  30]],\n\n        [[ 16,  17,  18,  19],\n         [-16, -17, -18, -19],\n         [ 32,  34,  36,  38]],\n\n        [[ 20,  21,  22,  23],\n         [-20, -21, -22, -23],\n         [ 40,  42,  44,  46]]]], dtype=int32)>\n\n\n(예시7) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,4,3)\n\ntf.stack([a,b,c],axis=-1)\n\n<tf.Tensor: shape=(2, 3, 4, 3), dtype=int32, numpy=\narray([[[[  0,   0,   0],\n         [  1,  -1,   2],\n         [  2,  -2,   4],\n         [  3,  -3,   6]],\n\n        [[  4,  -4,   8],\n         [  5,  -5,  10],\n         [  6,  -6,  12],\n         [  7,  -7,  14]],\n\n        [[  8,  -8,  16],\n         [  9,  -9,  18],\n         [ 10, -10,  20],\n         [ 11, -11,  22]]],\n\n\n       [[[ 12, -12,  24],\n         [ 13, -13,  26],\n         [ 14, -14,  28],\n         [ 15, -15,  30]],\n\n        [[ 16, -16,  32],\n         [ 17, -17,  34],\n         [ 18, -18,  36],\n         [ 19, -19,  38]],\n\n        [[ 20, -20,  40],\n         [ 21, -21,  42],\n         [ 22, -22,  44],\n         [ 23, -23,  46]]]], dtype=int32)>\n\n\n- 예제: (2,3,4) (4,3,4) \\(\\to\\) (6,3,4)\n\na=tf.reshape(tf.constant(range(2*3*4)),(2,3,4))\nb=tf.reshape(-tf.constant(range(4*3*4)),(4,3,4))\n\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]],\n\n       [[-24, -25, -26, -27],\n        [-28, -29, -30, -31],\n        [-32, -33, -34, -35]],\n\n       [[-36, -37, -38, -39],\n        [-40, -41, -42, -43],\n        [-44, -45, -46, -47]]], dtype=int32)>\n\n\n\ntf.concat([a,b],axis=1)\n\nInvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat\n\n\n\ntf.concat([a,b],axis=2)\n\nInvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat\n\n\n- (2,2) @ (2,) 의 연산?\nnumpy\n\nnp.array([[1,0],[0,1]]) @ np.array([77,-88])\n\narray([ 77, -88])\n\n\n\nnp.array([77,-88]) @ np.array([[1,0],[0,1]])\n\narray([ 77, -88])\n\n\n\nnp.array([[1,0],[0,1]]) @ np.array([77,-88]).reshape(2,1)\n\narray([[ 77],\n       [-88]])\n\n\n\nnp.array([77,-88]).reshape(2,1) @ np.array([[1,0],[0,1]]) \n\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)\n\n\n\nnp.array([77,-88]).reshape(1,2) @ np.array([[1,0],[0,1]]) \n\narray([[ 77, -88]])\n\n\ntensorflow\n\nI = tf.constant([[1.0,0.0],[0.0,1.0]]) \nx = tf.constant([77.0,-88.0]) \n\n\nI @ x \n\nInvalidArgumentError: In[0] and In[1] has different ndims: [2,2] vs. [2] [Op:MatMul]\n\n\n\nx @ I\n\nInvalidArgumentError: In[0] and In[1] has different ndims: [2] vs. [2,2] [Op:MatMul]\n\n\n\nI @ tf.reshape(x,(2,1))\n\n<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[ 77.],\n       [-88.]], dtype=float32)>\n\n\n\ntf.reshape(x,(1,2)) @ I \n\n<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 77., -88.]], dtype=float32)>\n\n\n\n\n\n\ntf.Variable\n\n선언\n- tf.Variable()로 선언\n\ntf.Variable([1,2,3,4])\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n\n\n\ntf.Variable([1.0,2.0,3.0,4.0])\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)>\n\n\n- tf.constant() 선언후 변환\n\ntf.Variable(tf.constant([1,2,3,4]))\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n\n\n- np 등으로 선언후 변환\n\ntf.Variable(np.array([1,2,3,4]))\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=int64, numpy=array([1, 2, 3, 4])>\n\n\n\n\n타입\n\ntype(tf.Variable([1,2,3,4]))\n\ntensorflow.python.ops.resource_variable_ops.ResourceVariable\n\n\n\n\n인덱싱\n\na=tf.Variable([1,2,3,4])\na\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n\n\n\na[:2]\n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>\n\n\n\n\n연산가능\n\na=tf.Variable([1,2,3,4])\nb=tf.Variable([-1,-2,-3,-4])\n\n\na+b\n\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>\n\n\n\n\ntf.Variable도 쓰기 불편함\n\ntf.Variable([1,2])+tf.Variable([3.14,3.14])\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]\n\n\n\n\ntnp의 은총도 일부만 가능\n\nimport tensorflow.experimental.numpy as tnp \ntnp.experimental_enable_numpy_behavior() \n\n- 알아서 형 변환\n\ntf.Variable([1,2])+tf.Variable([3.14,3.14])\n\n<tf.Tensor: shape=(2,), dtype=float64, numpy=array([4.1400001, 5.1400001])>\n\n\n- .reshape 메소드\n\ntf.Variable([1,2,3,4]).reshape(2,2)\n\nAttributeError: 'ResourceVariable' object has no attribute 'reshape'\n\n\n\n\n대부분의 동작은 tf.constant랑 큰 차이를 모르겠음\n- tf.concat\n\na= tf.Variable([[1,2],[3,4]]) \nb= tf.Variable([[-1,-2],[-3,-4]]) \ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[ 1,  2],\n       [ 3,  4],\n       [-1, -2],\n       [-3, -4]], dtype=int32)>\n\n\n- tf.stack\n\na= tf.Variable([[1,2],[3,4]]) \nb= tf.Variable([[-1,-2],[-3,-4]]) \ntf.stack([a,b],axis=0)\n\n<tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\narray([[[ 1,  2],\n        [ 3,  4]],\n\n       [[-1, -2],\n        [-3, -4]]], dtype=int32)>\n\n\n\n\n변수값변경가능(?)\n\na= tf.Variable([1,2,3,4])\nid(a)\n\n140652736059120\n\n\n\na.assign_add([-1,-2,-3,-4])\nid(a)\n\n140652736059120\n\n\n\n\n요약\n- tf.Variable()로 만들어야 하는 뚜렷한 차이는 모르겠음.\n- 애써 tf.Variable()로 만들어도 간단한연산을 하면 그 결과는 tf.constant()로 만든 오브젝트와 동일해짐.\n\n\n\n미분\n\n모티브\n- 예제: 컴퓨터를 이용하여 \\(x=2\\)에서 \\(y=3x^2\\)의 접선의 기울기를 구해보자.\n(손풀이)\n\\[\\frac{dy}{dx}=6x\\]\n이므로 \\(x=2\\)를 대입하면 12이다.\n(컴퓨터를 이용한 풀이)\n단계1\n\nx1=2 \ny1= 3*x1**2 \n\n\nx2=2+0.000000001\ny2= 3*x2**2\n\n\n(y2-y1)/(x2-x1)\n\n12.0\n\n\n단계2\n\ndef f(x):\n    return(3*x**2)\n\n\nf(3)\n\n27\n\n\n\ndef d(f,x):\n    return (f(x+0.000000001)-f(x))/0.000000001\n\n\nd(f,2)\n\n12.000000992884452\n\n\n단계3\n\nd(lambda x: 3*x**2 ,2)\n\n12.000000992884452\n\n\n\nd(lambda x: x**2 ,0)\n\n1e-09\n\n\n단계4\n\\[f(x,y)= x^2 +3y\\]\n\ndef f(x,y):\n    return(x**2 +3*y)\n\n\nd(f,(2,3))\n\nTypeError: can only concatenate tuple (not \"float\") to tuple\n\n\n\n\ntf.GradientTape() 사용방법\n- 예제1: \\(x=2\\)에서 \\(y=3x^2\\)의 도함수값을 구하라.\n\nx=tf.Variable(2.0)\na=tf.constant(3.0)\n\n\nmytape=tf.GradientTape()\nmytape.__enter__() # 기록 시작 \ny=a*x**2 # y=ax^2 = 3x^2\nmytape.__exit__(None,None,None) # 기록 끝 \n\n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n<tf.Tensor: shape=(), dtype=float32, numpy=12.0>\n\n\n- 예제2: 조금 다른예제\n\nx=tf.Variable(2.0)\n#a=tf.constant(3.0)\n\nmytape=tf.GradientTape()\nmytape.__enter__() # 기록 시작 \na=(x/2)*3 ## a=(3/2)x \ny=a*x**2  ## y=ax^2 = (3/2)x^3\nmytape.__exit__(None,None,None) # 기록 끝 \n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n<tf.Tensor: shape=(), dtype=float32, numpy=18.0>\n\n\n\\[a=\\frac{3}{2}x\\] \\[y=ax^2=\\frac{3}{2}x^3\\]\n\\[\\frac{dy}{dx}=\\frac{3}{2} 3x^2\\]\n\n3/2*3*4\n\n18.0\n\n\n- 테이프의 개념 (\\(\\star\\))\n(상황)\n우리가 어려운 미분계산을 컴퓨터에게 부탁하는 상황임. (예를들면 \\(y=3x^2\\)) 컴퓨터에게 부탁을 하기 위해서는 연습장(=테이프)에 \\(y=3x^2\\)이라는 수식을 써서 보여줘야하는데 이때 컴퓨터에게 target이 무엇인지 그리고 무엇으로 미분하고 싶은 것인지를 명시해야함.\n\nmytape = tf.GradientTape(): tf.GradientTape()는 연습장을 만드는 명령어, 만들어진 연습장을 mytape라고 이름을 붙인다.\nmytape.__enter__(): 만들어진 공책을 연다 (=기록할수 있는 상태로 만든다)\na=x/2*3; y=a*x**2: 컴퓨터에게 전달할 수식을 쓴다\nmytape.__exit__(None,None,None): 공책을 닫는다.\nmytape.gradient(y,x): \\(y\\)를 \\(x\\)로 미분하라는 메모를 남기고 컴퓨터에게 전달한다.\n\n- 예제3: 연습장을 언제 열고 닫을지 결정하는건 중요하다.\n\nx=tf.Variable(2.0)\na=(x/2)*3 ## a=(3/2)x\n\nmytape=tf.GradientTape()\nmytape.__enter__() # 기록 시작 \ny=a*x**2  ## y=ax^2 = (3/2)x^3\nmytape.__exit__(None,None,None) # 기록 끝 \n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n<tf.Tensor: shape=(), dtype=float32, numpy=12.0>\n\n\n- 예제4: with문과 함께 쓰는 tf.GradientTape()\n\nx=tf.Variable(2.0)\na=(x/2)*3 \n\n\nwith tf.GradientTape() as mytape:\n    ## with문 시작 \n    y=a*x**2 \n    ## with문 끝 \n\n\nmytape.gradient(y,x) # y를 x로 미분하라.\n\n<tf.Tensor: shape=(), dtype=float32, numpy=12.0>\n\n\n(문법해설)\n아래와 같이 쓴다.\nwith expression as myname:\n    ## with문 시작: myname.__enter__() \n    blabla ~ \n    yadiyadi !! \n    ## with문 끝: myname.__exit__()\n\nexpression 의 실행결과 오브젝트가 생성, 생성된 오브젝트는 myname라고 이름붙임. 이 오브젝트는 .__enter__()와 .__exit__()를 숨겨진 기능으로 포함해야 한다.\nwith문이 시작되면서 myname.__enter__()이 실행된다.\n블라블라와 야디야디가 실행된다.\nwith문이 종료되면서 myname.__exit__()이 실행된다.\n\n- 예제5: 예제2를 with문과 함께 구현\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape() as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n<tf.Tensor: shape=(), dtype=float32, numpy=18.0>\n\n\n- 예제6: persistent = True\n(관찰1)\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape() as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nmytape.gradient(y,x) # 2번이상 실행해서 에러를 관측하라\n\nRuntimeError: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n\n\n(관찰2)\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape(persistent=True) as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nmytape.gradient(y,x) # 2번이상실행해도 에러가 나지않음 \n\n<tf.Tensor: shape=(), dtype=float32, numpy=18.0>\n\n\n- 예제7: watch\n(관찰1)\n\nx=tf.constant(2.0)\n\nwith tf.GradientTape(persistent=True) as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\nNone\n\n\n(관찰2)\n\nx=tf.constant(2.0)\nwith tf.GradientTape(persistent=True) as mytape:\n    mytape.watch(x) # 수동감시\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n(관찰3)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제 \n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\nNone\n\n\n(관찰4)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제\n    mytape.watch(x)\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n(관찰5)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True) as mytape: \n    mytape.watch(x)\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n- 예제9: 카페예제로 돌아오자.\n- 예제10: 카페예제의 매트릭스 버전\n- 예제11: 위의 예제에서 이론적인 \\(\\boldsymbol{\\beta}\\)의 최적값을 찾아보고 (즉 \\(\\hat{\\boldsymbol{\\beta}}\\)을 찾고) 그곳에서 loss의 미분을 구하라. 구한결과가 \\(\\begin{bmatrix}0 \\\\ 0 \\end{bmatrix}\\) 임을 확인하라."
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-28-(4주차).html",
    "href": "post/Lecture/STBD/2022-03-28-(4주차).html",
    "title": "04. 경사하강법-1",
    "section": "",
    "text": "- 예제9: 카페예제로 돌아오자. (1주차 강의)\n- 자료 생성\n\n#collapse-hide\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nx=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])\nx\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])>\n\n\n\ntnp.random.seed(43052) \ny= 10.2+ x*2.2 + tnp.random.randn(10) \ny\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([54.98269924, 60.27348365, 61.27621687, 60.53495888, 62.9770905 ,\n       66.32168996, 66.87781372, 71.0050025 , 72.63837337, 77.11143943])>"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-28-(4주차).html#경사하강법",
    "href": "post/Lecture/STBD/2022-03-28-(4주차).html#경사하강법",
    "title": "04. 경사하강법-1",
    "section": "경사하강법",
    "text": "경사하강법\n- \\(loss = (\\frac {1}{2}\\beta-1)^2\\) 을 최소화 하는 \\(\\beta\\) 를 구해보자\n- 당연히 \\(\\beta=2\\)일 떼 최솟값을 가질 것이다\n- 이것을 컴퓨터로 직접 구해보자\n\n최적화문제\n\n\n방법1: grid search\n- 단순히 베타를 개많이 만들고 loss를 최소화하는 베타를 찾자\n\n알고리즘\n\n\n구현코드\n\nbeta = tnp.linspace(-10,10,1000) \n#beta\n\n\nloss = (1/2*beta-1)**2 \n\n\ntnp.argmin(loss)\n\n<tf.Tensor: shape=(), dtype=int64, numpy=599>\n\n\n\ngrid search로 알아봤을 때 최솟값이 2에 근사하게 나온다\n\n\nbeta[599]\n\n<tf.Tensor: shape=(), dtype=float64, numpy=1.9919919919919913>\n\n\n\n\n그리드서치의 문제점\n- 좀 정확하진 않지만 표본의 수를 늘리면 2의 근접한 값을 찾는다.\n- 비판1: [-10,10]이외에 해가 존재하면? 즉 범위 밖에 존재할 수 가 있음 - 이 예제의 경우는 운좋게 [-10,10]에서 해가 존재했음 - 하지만 임의의 고정된 \\(x,y\\)에 대하여 \\(loss(\\beta)=(x\\beta-y)^2\\) 의 형태의 해가 항상 [-10,10]에서 존재한다는 보장은 없음 - 해결책: 더 넓게 많은 범위를 탐색하자? \\(\\to\\) but, 무한대의 범위에서 할 수 없음\n- 비판2: 위 해결책은 효율적이지 않음 - 알고리즘을 요약하면 결국 -10부터 10까지 작은 간격으로 조금씩 이동하며 loss를 조사하는 것이 grid search의 아이디어 - \\(\\to\\) 생각해보니까 \\(\\beta=2\\)인 순간 \\(loss=(\\frac{1}{2}\\beta-1)^2=0\\)이 되어서 이것보다 작은 최소값은 존재하지 않는다(제곱은 항상 양수이어야 하므로) - \\(\\to\\) 따라서 \\(\\beta=2\\) 이후로는 탐색할 필요가 없다\n\n\n\n방법2: gradient descent\n\n임의의 초기값을 선정하고 \\(loss\\)를 계산한다 (초깃값 셋팅)\n\n\n\\(\\beta = -5 \\to loss(-5) = (-5/2-1)^2 = 12.55\\)\n\n\n(-5/2-1)**2\n\n12.25\n\n\n\n임의의 초기값에서 좌우로 약간씩 이동해보고 \\(loss\\)를 계한한다. (미분에서 최솟값을 찾는 과정)\n\n\\(\\to \\beta = -5.01,\\, \\beta = -4.99\\)\n\n(-5.01 /2 -1)**2,(-4.99 /2 -1)**2\n\n(12.285025, 12.215025)\n\n\n\n(2)의 결과를 보고 어느쪽으로 이동하는 것이 유리한지 따져본다. 그 후 유리한 방향으로 이동한다.\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.plot(beta,loss)\n\n\n\n\n- (2) - (3)의 과정은 \\(\\beta = -5\\) 미분계수를 구한 후 미분계수가 양수이면 왼쪽으로 움직이고 음수이면 오른쪽으로 움직인다고 해석가능\n\n(2) ~ (3) 과정을 반복 후, 어느쪽으로 가도 유리한 지점이 없다면 알고리즘을 멈춘다\n\n\n알고리즘 분석\n- 알고리즘이 멈추는 지점은 \\(\\beta=2\\)이다. 왜냐하면 이경우 왼쪽으로 가도, 오른쪽으로 가도 현재 손실함수값보다 크기 때문.\n\n\n왼쪽/오른쪽중에 어디로 갈지 어떻게 판단하는 과정을 수식화?\n\n오른쪽으로 0.01간다 \\(\\to\\) 미분계수가 음수일 때\n왼쪽으로 0.01간다 \\(\\to\\) 미분계수가 양수일 때\n\n- 그렇다면\n\\(\\beta_{new} =\\beta_{old} + 0.01 \\to \\frac{d\\,loss}{d\\,\\beta_{old}}\\) 가 음수 일때\n\\(\\beta_{new} =\\beta_{old} - 0.01 \\to \\frac{d\\,loss}{d\\,\\beta_{old}}\\) 가 양수 일때\n\n\n혹시 알고리즘을 좀 개선할수 있을까?\n- 동일하게 0.01씩 이동하는게 맞는지 의문\n\nimport numpy as np\n\n\n_beta = np.linspace(-10,5)\nplt.plot(_beta,(_beta/2-1)**2)\n\n\n\n\n- \\(\\beta=-10\\) 일 경우의 접선의 기울기? \\(\\beta=-4\\) 일때 접선의 기울기?\n- \\(\\beta= -10 \\to 기울기는 -6\\)\n- \\(\\beta= -4 \\to 기울기는 -3\\)\n\n위 같은 경우 \\(\\beta = -10\\) 에서 0.01만큼 이동했다면 \\(\\beta=-4\\) 에서 0.005만큼 이동해야함\n즉, 떨어진 만큼 비례해서 조금 더 자신있게 가자는 거임\n\n\\[\\beta_{new}= \\beta_{old} -\\alpha \\left[\\frac {∂}{\\,∂\\beta}loss(\\beta)\\right],\\quad \\alpha>0\\]\n\n\n구현코드\niter1: \\(\\beta=-10\\) 출발\n\nbeta = tf.Variable(-10.0)\n\n\nwith tf.GradientTape(persistent=True) as tape: \n    loss = (beta/2-1)**2 \n\n\ntape.gradient(loss,beta)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=-6.0>\n\n\n\nalpha=0.01/6 \n\n\nbeta.assign_sub(alpha*tape.gradient(loss,beta)) ## variable 로 벼수 선언시 assign을 사용하면 초괴화가 가능\n\n<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.99>\n\n\niter2 \\(\\beta=-9.99\\)\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.99>\n\n\n\nwith tf.GradientTape(persistent=True) as tape: \n    loss = (beta/2-1)**2 \n\n\nbeta.assign_sub(alpha*tape.gradient(loss,beta))\n\n<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.980008>\n\n\nfor 문을 이용 (시도1 : 100번)\n\nbeta = tf.Variable(-10.0) \nalpha=0.01/6 \n\n\nfor k in range(100): \n    with tf.GradientTape(persistent=True) as tape: \n        loss = (beta/2-1)**2 \n    beta.assign_sub(alpha*tape.gradient(loss,beta))\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.040152>\n\n\nfor 문을 이용 (시도2 : 10,000번)\n\nbeta = tf.Variable(-10.0) \nalpha=0.01/6 \n\n\nfor k in range(10000): \n    with tf.GradientTape(persistent=True) as tape: \n        loss = (beta/2-1)**2 \n    beta.assign_sub(alpha*tape.gradient(loss,beta))\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251>\n\n\n\n사실 10,000 번까지 갈 필요도 없다 그냥 \\(\\alpha\\) 값을 크게 키우면 된다. -> 사실 이것도 분류 모델에서는 과적합 문제로 이어질 수 있다.\n\n\n\n학습률\n- \\(\\alpha\\) 값의 변화에 따라서 최적해가 어떻게 수렴하는지 시각화 해보자.\n\n[시각화 코드 예비학습]\n\nplt.plot([1,2,3],[3,4,5],\"ro\")\n\n\n\n\n\n도화지 생성\n\n\nfig = plt.figure()\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n도화지 안의 어떤 틀을 생성\n\n\nax = fig.add_subplot()\n\n\nfig\n\n\n\n\n\nfig.axes[0]\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f7948b12790>\n\n\n\ntype(fig.axes[0])\n\nmatplotlib.axes._subplots.AxesSubplot\n\n\n\nid(fig.axes[0])\n\n140158887339920\n\n\n- 네모틀(ax)의 특수기능(=메소드)중에는 plot이 있음. 이것은 또 어떤 오브젝트를 생성함\npython\n길이가 1인 튜플\na,=1\n\npnts, = ax.plot([1,2,3],[3,4,5],'or') \npnts\n\n<matplotlib.lines.Line2D at 0x7f79489e82d0>\n\n\n\nfig\n\n\n\n\n- pnts 오브젝트: x,y data를 변경해보자.\n\npnts.get_xdata(),pnts.get_ydata()\n\n(array([1, 2, 3]), array([3, 4, 5]))\n\n\n\npnts.get_ydata()\n\narray([3, 4, 5])\n\n\n\npnts.set_ydata([5,5,5])\n\n\npnts.get_ydata()\n\n[5, 5, 5]\n\n\n\nfig\n\n\n\n\n응용 숫자에 변화에 따른 animation을 구현해보자\n\nplt.rcParams[\"animation.html\"]=\"jshtml\"\nfrom matplotlib import animation \n\n\ndef animate(i): \n    if i%2 == 0:\n        pnts.set_ydata([3,4,5])\n    else: \n        pnts.set_ydata([4,4,4])\n\n\nani=animation.FuncAnimation(fig,animate,frames=10)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 위를 적용하여 최적해를 찾는 과정 즉, 수렴과정을 시각화하자\n\n\\(\\beta :-10\\to-9\\to-8\\) 이런식으로 이동한다고 하자\n그에 대한 \\(loss\\)도 저장\n\n\nbeta_lst = [-10.0,-9.00,-8.00] \nloss_lst = [(-10.0/2-1)**2,(-9.00/2-1)**2,(-8.00/2-1)**2]\n\n\nfig = plt.figure() \nax = fig.add_subplot()\n\n\n\n\n\n그 후 \\(\\beta\\) 가 이동할 경로를 그려주자\n\n\n_beta= np.linspace(-15,19)\nax.plot(_beta,(_beta/2-1)**2) \n\n\nfig\n\n\n\n\n\npnts, = ax.plot(beta_lst[0],loss_lst[0],'ro')\n\n\nanimation 구현\n\n\ndef animate(i):\n    pnts.set_xdata(beta_lst[:(i+1)])\n    pnts.set_ydata(loss_lst[:(i+1)])\n\n\nani=animation.FuncAnimation(fig,animate,frames=3) ## frame은 리스트 길이라고 생각\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n이제 목적대로 구현해보자\n\n1 초깃값 설정\n\nbeta = tf.Variable(10.0)\n\n\nbeta.numpy()\nalpha=0.01/6\n\n\nbeta_lst=[]\nbeta_lst.append(beta.numpy())\n\n\nloss_lst=[]\nloss_lst.append((beta.numpy()/2-1)**2)\n\n2 초깃값을 설정했으니 최적해를 찾아보자\n\nfor k in range(100) :\n    with tf.GradientTape(persistent=True) as tape :\n        tape.watch(beta)\n        loss = (beta/2-1)**2\n    beta.assign_sub(alpha*tape.gradient(loss,beta))\n    beta_lst.append(beta.numpy()) \n    loss_lst.append((beta.numpy()/2-1)**2) \n\n\nfig = plt.figure() # fig 는 도화지 \n\n<Figure size 432x288 with 0 Axes>\n\n\n\nax = fig.add_subplot()\nax.plot(_beta,(_beta/2-1)**2)\npnts, = ax.plot(beta_lst[0],loss_lst[0],'or')\n\n\nani=animation.FuncAnimation(fig,animate,frames=100) \nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n3 \\(\\alpha\\) 값을 살펴보니 깂이 너무 작아서 이동의 폭이 너무 짧음 \\(\\alpha \\to 0.1\\) 로 변경\n\nbeta = tf.Variable(10.0)\nbeta.numpy()\n\nalpha=0.1\n\nbeta_lst=[]\nbeta_lst.append(beta.numpy())\n\nloss_lst=[]\nloss_lst.append((beta.numpy()/2-1)**2)\n\n\nfor k in range(100) :\n    with tf.GradientTape(persistent=True) as tape :\n        tape.watch(beta)\n        loss = (beta/2-1)**2\n    beta.assign_sub(alpha*tape.gradient(loss,beta))\n    beta_lst.append(beta.numpy()) \n    loss_lst.append((beta.numpy()/2-1)**2) \n\n\nfig = plt.figure() # fig 는 도화지 \n\nax = fig.add_subplot()\nax.plot(_beta,(_beta/2-1)**2)\npnts, = ax.plot(beta_lst[0],loss_lst[0],'or')\n\n\n\n\n\nani=animation.FuncAnimation(fig,animate,frames=100) \nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n오 아까보단 빠르당\n\n4 \\(\\alpha \\to 1\\) 로 변경\n\nbeta = tf.Variable(10.0)\nbeta.numpy()\n\nalpha=1\n\nbeta_lst=[]\nbeta_lst.append(beta.numpy())\n\nloss_lst=[]\nloss_lst.append((beta.numpy()/2-1)**2)\n\n\nfor k in range(100) :\n    with tf.GradientTape(persistent=True) as tape :\n        tape.watch(beta)\n        loss = (beta/2-1)**2\n    beta.assign_sub(alpha*tape.gradient(loss,beta))\n    beta_lst.append(beta.numpy()) \n    loss_lst.append((beta.numpy()/2-1)**2) \n\n\nfig = plt.figure() # fig 는 도화지 \n\nax = fig.add_subplot()\nax.plot(_beta,(_beta/2-1)**2)\npnts, = ax.plot(beta_lst[0],loss_lst[0],'or')\n\n\n\n\n\nani=animation.FuncAnimation(fig,animate,frames=100) \nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "post/Lecture/STBD/2022-03-30-(5주차).html",
    "href": "post/Lecture/STBD/2022-03-30-(5주차).html",
    "title": "05. 경사하강법-2",
    "section": "",
    "text": "imports\n\n#!conda install -c conda-forge python-graphviz -y\n\n/bin/bash: conda: command not found\n\n\n\nimport tensorflow as tf \nimport numpy as np\nimport matplotlib.pyplot as plt \n\n\nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior() \n\n\n\n최적화의 문제\n- \\(loss=(\\frac{1}{2}\\beta-1)^2\\)\n- 기존에 했던 방법은 수식을 알고 있어야 한다는 단점이 있음\n- 그래서 확률적 경사하강법을 이용해서 최적해를 찾는 과제를 수행하였음\n- 그러나 다양한 loss 함수에서 위와 같이 간단히 최적해를 찾는 것은 매우 어려움 \\(\\to\\) 대부분의 함수는 비모수적이기 때문이다!!\n- 그래서 오늘은 그것을 쉽게 해주는 tf.keras.optimizers를 사용해서 최적해를 찾을 거얌\n\n\ntf.keras.optimizers를 이용한 최적화방법\n\n방법1: opt.apply_gradients()를 이용\n\n이전까지의 방법\n\n\nbeta = tf.Variable(-10.0)\nalpha = 0.01/6\n\n\nwith tf.GradientTape() as tape :\n    tape.watch(beta)\n    loss = (beta/2-1)**2\nslope = tape.gradient(loss,beta)\n\n\nbeta.assign_sub(slope*alpha)\n\n<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.99>\n\n\n\n이전 방법 + 새로운 방법 ( - beta.assign_sub(slope*alpha))\n\n\nopt = tf.keras.optimizers.SGD(alpha)\n\n\nopt.apply_gradients() : 베타 값과 slope의 값을 받아 최적해를 구해준다.\n즉, 위식을 통해 수식을 몰라도 최적해를 구할 수 있다.\n아래 과정은 한번에 iteration 임\n\n\nopt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope*alpha)\n\n<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>\n\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.98>\n\n\n\niteration 2\n\n\nwith tf.GradientTape() as tape :\n    tape.watch(beta)\n    loss = (beta/2-1)**2\nslope = tape.gradient(loss,beta)\n\n\nopt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope*alpha)\n\n<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=2>\n\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.9700165>\n\n\n\ninteration 1과 interation 2 의 결과가 달라진 것을 확인하였다.\nfor 문을 이용한다면?\n\n\nalpha = 0.01/6\nbeta =  tf.Variable(-10.0)\nopt = tf.keras.optimizers.SGD(alpha)\n\n\nfor epoc in range(10000) :\n  with tf.GradientTape() as tape :\n    tape.watch(beta)\n    loss = (beta/2-1)**2\n  slope = tape.gradient(loss,beta)\n  opt.apply_gradients([(slope,beta)])\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251>\n\n\n\n잠깐, 주소가 같지만 문법이 다름 근데 걍 그런갑다 하자\n\n\nopt.learning_rate, opt.lr\n\n(<tf.Variable 'SGD/learning_rate:0' shape=() dtype=float32, numpy=0.0016666667>,\n <tf.Variable 'SGD/learning_rate:0' shape=() dtype=float32, numpy=0.0016666667>)\n\n\n\nid(opt.learning_rate), id( opt.lr)\n\n(139921818271120, 139921818271120)\n\n\n\n\n방법2: opt.minimize()\n\n이 함수를 이용하면 gradienttape를 안써도 된다. 핳\n\n\nalpha = 0.01/6\nbeta =  tf.Variable(-10.0)\nopt = tf.keras.optimizers.SGD(alpha)\n\n\nloss 함수 정의\n\\[\\left(\\frac 12\\beta-1\\right)^2\\]\n\nloss_fn = lambda: (beta/2-1)**2 \n\n- iter 1\n\nopt.minimize(loss_fn,beta)\n\n<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>\n\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.99>\n\n\n- for\n\nalpha = 0.01/6\nbeta =  tf.Variable(-10.0)\nopt = tf.keras.optimizers.SGD(alpha)\nloss_fn = lambda: (beta/2-1)**2 \nfor epoc in range(10000) :\n    opt.minimize(loss_fn,beta)\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251>\n\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251>\n\n\n\n\n\n\n회귀분석 문제\n- \\({\\bf y} \\approx 2.5 + 4 {\\bf x}\\)\n\ntnp.random.seed(43052)\nN = 200\nx = tnp.linspace(0,1,N) \nepsilon = tnp.random.randn(N)*0.5\ny = 2.5 + 4*x + epsilon\ny_hat = 2.5 + 4*x\n\n\nplt.plot(x,y,'.')\nplt.plot(x,y_hat,'r--')\n\n\n\n\n\n\n이론적 풀이\n\n풀이1: 스칼라버전\n- 포인트 - \\(S_{xx}=\\sum (x-\\bar x)^2\\),\\(\\quad S_{xy}=\\sum (x-\\bar x)(y- \\bar y)\\) - \\(\\hat{\\beta}_0=\\bar y - \\hat{ \\beta_1} \\bar x\\)$,_1= $\n\nSxx = sum((x-x.mean())**2) \nSxy = sum((x-x.mean())*(y-y.mean())) \n\n\nbeta1_hat = Sxy/Sxx \nbeta0_hat = y.mean() - beta1_hat*x.mean()\n\n\nbeta0_hat,beta1_hat\n\n(<tf.Tensor: shape=(), dtype=float64, numpy=2.583667211565867>,\n <tf.Tensor: shape=(), dtype=float64, numpy=3.933034516733169>)\n\n\n\n\n풀이2: 벡터버전\n\nX=tf.stack([tf.ones(N,dtype='float64'),x],axis=1)\ny=y.reshape(N,1) \n\n\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\n\\(\\hat \\beta = (X^{T}X)^{-1} X^{T} y\\)\n\n\ntf.linalg.inv(X.T@X)@ X.T @y \n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[2.58366721],\n       [3.93303452]])>\n\n\n\n\n풀이3: 벡터버전, 손실함수의 도함수이용 (경사하강법을 이용한 풀이)\n(단, 텐서플로우의 미분기능을 사용하지 않음 ) \\(\\to\\) gradienttape X\n\nX=tf.stack([tf.ones(N,dtype='float64'),x],axis=1)\ny=y.reshape(N,1) \nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n- 초기값 설정\n\nbeta_hat = tnp.array([-5.0,10.0]).reshape(2,1)\n\n- 포인트 - \\(loss'(\\beta)=-2X'y +2X'X\\beta\\) - \\(\\beta_{new} = \\beta_{old} - \\alpha \\times loss'(\\beta_{old})\\)\n\nslope = -2*X.T@y + 2*X.T@X@beta_hat\n\n\nslope\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[-1820.07378797],\n       [ -705.77222696]])>\n\n\n\nalpha = 0.001\n\n\nstep = slope * alpha\nstep\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[-1.82007379],\n       [-0.70577223]])>\n\n\n\nfor epoc in range(1000) : \n  slope = -2*X.T@y + 2*X.T@X@beta_hat\n  beta_hat = beta_hat - alpha*slope\n\n\nbeta_hat\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[2.58366721],\n       [3.93303452]])>\n\n\n\n\n\nGradientTape를 이용\n\n풀이1: 벡터버전\n- 포인트\n## 포인트코드1: 그레디언트 테입  \nwith tf.GradientTape() as tape: \n    loss = \n## 포인트코드2: 미분 \nslope = tape.gradient(loss,beta_hat) \n## 포인트코드3: update \nbeta_hat.assign_sub(slope*alph) \n\ny=y.reshape(N,1) # N=200 \nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\n\n\nfor epoc in range(1000):   \n    with tf.GradientTape() as tape: \n        tape.watch(beta_hat)\n        yhat = X@beta_hat\n        loss = (y-yhat).T @ (y-yhat) / N \n    slope = tape.gradient(loss,beta_hat) \n    beta_hat.assign_sub(slope * alpha) \n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이2: 스칼라버전\n- 포인트\n## 포인트코드: 미분\nslope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat])\n\ny=y.reshape(-1) # N=200 \nx.shape,y.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0)\nalpha = 0.1\n\n\nfor epoc in range(1000):   \n    with tf.GradientTape() as tape: \n        yhat = beta0_hat + beta1_hat*x \n        loss = tf.reduce_sum((y-yhat)**2) / N  #loss = sum((y-yhat)**2) / N \n    slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) \n    beta0_hat.assign_sub(slope0 * alpha) \n    beta1_hat.assign_sub(slope1 * alpha) \n\n\nbeta0_hat, beta1_hat \n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.5836616>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.9330447>)\n\n\n\n\n\nGradientTape + opt.apply_gradients\n\n풀이1: 벡터버전\n- 포인트\n## 포인트코드: 업데이트\nopt.apply_gradients([(slope,beta_hat)])  ## pair의 list가 입력 \n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)  \n### tf.keras.optimizers.SGD 와 똑같은 위치에 있는 모듈이다 그냥 똑같은거다.\n\n\nfor epoc in range(1000): \n    with tf.GradientTape() as tape: \n        yhat = X@beta_hat\n        loss = (y-yhat).T @ (y-yhat) / N \n    slope = tape.gradient(loss,beta_hat)  \n    opt.apply_gradients( [(slope,beta_hat),(slope,beta_hat)] )\n\n\n(y-yhat).T @ (y-yhat) / N, beta_hat \n\n(<tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[0.25493942]])>,\n <tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\n array([[2.58366721],\n        [3.93303452]])>)\n\n\n\n\n풀이2: 스칼라버전\n- 포인트\n## 포인트코드: 업데이트 \nopt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)]) ## pair의 list가 입력 \n\ny=y.reshape(-1)\nx.shape,y.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0) \n\n\nalpha=0.1\n\n\nopt = tf.optimizers.SGD(alpha) \n\n\nfor epoc in range(1000): \n    with tf.GradientTape() as tape: \n        yhat = beta0_hat + beta1_hat*x \n        loss = tf.reduce_sum((y-yhat)**2) / N \n    slope0,slope1 = tape.gradient(loss,[beta0_hat,beㅠta1_hat])  \n    opt.apply_gradients( [(slope0,beta0_hat),(slope1,beta1_hat)] )\n\n\nbeta0_hat,beta1_hat\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.58366>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.933048>)\n\n\n\n\n\nopt.minimize\n\n풀이1: 벡터버전, 사용자정의 손실함수 with lambda\n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha) \nloss_fn = lambda: (y-X@beta_hat).T @ (y-X@beta_hat)/N\n\n\n(y-X@beta_hat).T @ (y-X@beta_hat)\n\n<tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[4811.45696758]])>\n\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat) # 미분 + update \n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이2: 스칼라버전, 사용자정의 손실함수 with lambda\n\ny=y.reshape(-1)\nx.shape,y.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0) \n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha) \nloss_fn = lambda: tf.reduce_sum((y-beta0_hat - beta1_hat*x)**2)/N\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,[beta0_hat,beta1_hat]) # 미분 + update \n\n\nbeta0_hat, beta1_hat\n\n- 포인트\n## 포인트코드: 미분 & 업데이트 = minimize \nopt.minimize(loss_fn,[beta0_hat,beta1_hat])\n\n\n풀이3: 벡터버전, 사용자정의 (짧은) 손실함수\n- 포인트\n## 포인트코드: 손실함수정의 \ndef loss_fn():\n    return ??\n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\ndef loss_fn():\n    return (y-X@beta_hat).T @ (y-X@beta_hat)/N\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat) # 미분 + update \n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이4: 벡터버전, 사용자정의 (긴) 손실함수\ndef loss_fn():\n    yhat = X@beta_hat \n    loss = (y-yhat).T @ (y-yhat) / N\n    return loss\n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\ndef loss_fn():\n    yhat = X@beta_hat \n    loss = (y-yhat).T @ (y-yhat) / N\n    return loss\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat) # 미분 + update \n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이5: 벡터버전, 사용자정의 손실함수 <- tf.losses.MSE\n- 포인트\n## 포인트코드: 미리구현되어있는 손실함수 이용 \ntf.losses.MSE(y,yhat)\n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha) \n\n\ndef loss_fn():\n    yhat= X@beta_hat\n    loss = tf.losses.MSE(y.reshape(-1),yhat.reshape(-1))\n    return loss\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat) # 미분 + update \n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이6: 벡터버전, 사용자정의 손실함수 <- tf.losses.MeaSquaredError\n- 포인트\n## 포인트코드: 클래스로부터 손실함수 오브젝트 생성 (함수를 찍어내는 클래스) \nmse_fn = tf.losses.MeanSquaredError()\nmse_fn(y,yhat)\n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha) \n\n\nmseloss_fn = tf.losses.MeanSquaredError()\n\n\nmseloss_fn(y.reshape(-1),yhat.reshape(-1))\n\n<tf.Tensor: shape=(), dtype=float64, numpy=0.25493940711021423>\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat\n    loss = mseloss_fn(y.reshape(-1),yhat.reshape(-1))\n    return loss\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat) # 미분 + update \n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n\ntf.keras.Sequential\n- \\(\\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1x_i\\) 의 서로다른 표현\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\ngv(''' \n    \"1\" -> \"beta0_hat + x*beta1_hat,    bias=False\"[label=\"* beta0_hat\"]\n    \"x\" -> \"beta0_hat + x*beta1_hat,    bias=False\"[label=\"* beta1_hat\"]\n    \"beta0_hat + x*beta1_hat,    bias=False\" -> \"yhat\"[label=\"indentity\"]\n    ''')\n\n\n\n\n\ngv('''\n\"x\" -> \"x*beta1_hat,    bias=True\"[label=\"*beta1_hat\"] ;\n\"x*beta1_hat,    bias=True\" -> \"yhat\"[label=\"indentity\"] ''')\n\n\n\n\n\ngv('''\n\"X=[1 x]\" -> \"X@beta_hat,    bias=False\"[label=\"@beta_hat\"] ;\n\"X@beta_hat,    bias=False\" -> \"yhat\"[label=\"indentity\"] ''')\n\n\n\n\n\n풀이1: 벡터버전, 사용자정의 손실함수\n- 포인트\n## 포인트코드1: 네트워크 생성 \nnet = tf.keras.Sequential()\n\n## 포인트코드2: 네트워크의 아키텍처 설계 \nnet.add(tf.keras.layers.Dense(1,input_shape=(2,),use_bias=False)) \n\n## 포인트코드3: 네트워크 컴파일 = 아키텍처 + 손실함수 + 옵티마이저\nnet.compile(opt,loss=loss_fn2)\n\n## 포인트코드4: 미분 & update \nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(units=1,input_shape=(2,),use_bias=False)) # 아키텍처 설계 = yhat을 만들계획 \n\n\ndef loss_fn2(y,yhat): ## 손실함수의 정의 \n    return (y-yhat).T @ (y-yhat) / N  \n\n\nalpha=0.1\nopt=tf.optimizers.SGD(alpha) ## 옵티마이저의 선택 \n\n\nnet.compile(opt,loss=loss_fn2) ## 컴파일 = 아키텍처 + 손실함수 + 옵티마이저 \n\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) # 미분 & update 의 반복\n\n<keras.callbacks.History at 0x7f42158d8e10>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836728],\n        [3.9330244]], dtype=float32)>]"
  },
  {
    "objectID": "post/Lecture/STBD/2022-04-11-(6주차).html",
    "href": "post/Lecture/STBD/2022-04-11-(6주차).html",
    "title": "06. layer",
    "section": "",
    "text": "imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\n\n\\(x \\to \\hat{y}\\) 가 되는 과정을 그림으로 그리기\n- 단순회귀분석의 예시 - \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i, \\quad i=1,2,\\dots,n\\)\n(표현1)\n\n#collapse-hide\ngv(''' \n    \"1\" -> \"β̂₀ + xₙ*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"xₙ\" -> \"β̂₀ + xₙ*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + xₙ*β̂₁,    bias=False\" -> \"ŷₙ\"[label=\"identity\"]\n\n    \".\" -> \"....................................\"[label=\"* β̂₀\"]\n    \"..\" -> \"....................................\"[label=\"* β̂₁\"]\n    \"....................................\" -> \"...\"[label=\" \"]\n\n    \"1 \" -> \"β̂₀ + x₂*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x₂\" -> \"β̂₀ + x₂*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x₂*β̂₁,    bias=False\" -> \"ŷ₂\"[label=\"identity\"]\n    \n    \"1  \" -> \"β̂₀ + x₁*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x₁\" -> \"β̂₀ + x₁*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x₁*β̂₁,    bias=False\" -> \"ŷ₁\"[label=\"identity\"]\n''')\n\n\n\n\n- 표현1의 소감? - 교수님이 고생해서 만든것 같음 - 그런데 그냥 다 똑같은 그림의 반복이라 사실 고생한 의미가 없음.\n(표현2)\n- 그냥 아래와 같이 그리고 “모든 \\(i=1,2,3,\\dots,n\\)에 대하여 \\(\\hat{y}_i\\)을 아래의 그림과 같이 그린다”고 하면 될것 같다.\n\n#collapse-hide\ngv(''' \n    \"1\" -> \"β̂₀ + xᵢ*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"xᵢ\" -> \"β̂₀ + xᵢ*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + xᵢ*β̂₁,    bias=False\" -> \"ŷᵢ\"[label=\"identity\"]\n\n''')\n\n\n\n\n(표현3)\n- 그런데 “모든 \\(i=1,2,3,\\dots,n\\)에 대하여 \\(\\hat{y}_i\\)을 아래의 그림과 같이 그린다” 라는 언급자체도 반복할 필요가 없을 것 같다. (어차피 당연히 그럴테니까) 그래서 단순히 아래와 같이 그려도 무방할듯 하다.\n\n#collapse-hide\ngv(''' \n    \"1\" -> \"β̂₀ + x*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x\" -> \"β̂₀ + x*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x*β̂₁,    bias=False\" -> \"ŷ\"[label=\"identity\"]\n\n''')\n\n\n\n\n(표현4)\n- 위의 모델은 아래와 같이 쓸 수 있다. (\\(\\beta_0\\)를 바이어스로 표현)\n\n#collapse-hide\ngv('''\n\"x\" -> \"x*β̂₁,    bias=True\"[label=\"*β̂₁\"] ;\n\"x*β̂₁,    bias=True\" -> \"ŷ\"[label=\"indentity\"] ''')\n\n\n\n\n\n실제로는 이 표현을 많이 사용함\n\n(표현5)\n- 벡터버전으로 표현하면 아래와 같다. 이 경우에는 \\({\\bf X}=[1,x]\\)에 포함된 1이 bias의 역할을 해주므로 bias = False 임.\n\n#collapse-hide\ngv('''\n\"X\" -> \"X@β̂,    bias=False\"[label=\"@β̂\"] ;\n\"X@β̂,    bias=False\" -> \"ŷ\"[label=\"indentity\"] ''')\n\n\n\n\n\n저는 이걸 좋아해요\n\n(표현6)\n- 딥러닝에서는 \\(\\hat{\\boldsymbol{\\beta}}\\) 대신에 \\(\\hat{{\\bf W}}\\)을 라고 표현한다.\n\n#collapse-hide\ngv('''\n\"X\" -> \"X@Ŵ,    bias=False\"[label=\"@Ŵ\"] ;\n\"X@Ŵ,    bias=False\" -> \"ŷ\"[label=\"identity\"] ''')\n\n\n\n\n- 실제로는 표현4 혹은 표현5를 외우면 된다.\n\n\nLayer의 개념\n- (표현4) 혹은 (표현5)의 그림은 레이어로 설명할 수 있다.\n- 레이어는 항상 아래와 같은 규칙을 가진다. - 첫 동그라미는 레이어의 입력이다. - 첫번째 화살표는 선형변환을 의미한다. - 두번째 동그라미는 선형변환의 결과이다. (이때 bias가 false인지 true인지에 따라서 실제 수식이 조금 다름) - 두번째 화살표는 두번째 동그라미에 어떠한 함수 \\(f\\)를 취하는 과정을 의미한다. - 세번째 동그라미는 레이어의 최종출력이다.\n- 엄청 복잡한데, 결국 레이어를 만들때 위의 그림들을 의미하도록 하려면 아래의 4개의 요소만 필요하다. 1. 레이어의 입력차원 2. 선형변환의 결과로 얻어지는 차원 3. 선형변환에서 바이어스를 쓸지? 안쓸지? 4. 함수 \\(f\\)\n- 주목: 1,2가 결정되면 자동으로 \\(\\hat{{\\bf W}}\\)의 차원이 결정된다.\n(예시) - 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=1: \\(\\hat{\\bf W}\\)는 (2,1) 매트릭스 - 레이어의 입력차원=20, 선형변환의 결과로 얻어지는 차원=5: \\(\\hat{\\bf W}\\)는 (20,5) 매트릭스 - 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=50: \\(\\hat{\\bf W}\\)는 (2,50) 매트릭스\n- 주목2: 이중에서 절대 생략불가능 것은 “2. 선형변환의 결과로 얻어지는 차원” 이다. - 레이어의 입력차원: 실제 레이어에 데이터가 들어올 때 데이터의 입력차원을 컴퓨터 스스로 체크하여 \\(\\hat{\\bf W}\\)의 차원을 결정할 수 있음. - 바이어스를 쓸지? 안쓸지? 기본적으로 쓴다고 가정한다. - 함수 \\(f\\): 기본적으로 항등함수를 가정하면 된다.\n\n\nKeras를 이용한 풀이\n- 기본뼈대: net생성 \\(\\to\\) add(layer) \\(\\to\\) compile(opt,loss) \\(\\to\\) fit(data,epochs)\n- 데이터정리\n\\[{\\bf y}\\approx 2.5 +4x\\]\n\ntnp.random.seed(43052)\nN= 200 \nx= tnp.linspace(0,1,N)\nepsilon= tnp.random.randn(N)*0.5 \ny= 2.5+4*x +epsilon\n\n\nX=tf.stack([tf.ones(N,dtype='float64'),x],axis=1)\n\n\n풀이1: 스칼라버전\n(0단계) 데이터정리\n\ny=y.reshape(N,1)\nx=x.reshape(N,1)\nx.shape,y.shape\n\n(TensorShape([200, 1]), TensorShape([200, 1]))\n\n\n(1단계) net 생성\n\nnet = tf.keras.Sequential() \n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1) ## 선형변환의 결과 차원\n# 입력차원? 데이터를 넣어보고 결정, 바이어스=디폴드값을 쓰겠음 (use_bias=true), 함수도 디폴트값을 쓰겠음 (f(x)=x)\nnet.add(layer)\n\n(3단계) net.compile(opt,loss_fn)\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) \n\n(4단계) net.fit(x,y,epochs)\n\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 \n\n<keras.callbacks.History at 0x7ff91540e790>\n\n\n(결과확인)\n\nnet.weights\n\n[<tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[3.9330251]], dtype=float32)>,\n <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([2.5836723], dtype=float32)>]\n\n\n\\[{\\bf y}\\approx 2.5 +4x\\]\n\n\n풀이2: 벡터버전\n(0단계) 데이터정리\n\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net 생성\n\nnet = tf.keras.Sequential() \n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False) \nnet.add(layer)\n\n(3단계) net.compile(opt,loss_fn)\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) \n\n(4단계) net.fit(x,y,epochs)\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 \n\n<keras.callbacks.History at 0x7ff91550e210>\n\n\n(결과확인)\n\nnet.weights\n\n[<tf.Variable 'dense_1/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836728],\n        [3.9330244]], dtype=float32)>]\n\n\n\n\n잠시문법정리\n- 잠깐 Dense layer를 만드는 코드를 정리해보자.\n\n아래는 모두 같은 코드이다.\n\n\ntf.keras.layers.Dense(1)\ntf.keras.layers.Dense(units=1)\ntf.keras.layers.Dense(units=1,activation=‘linear’) // identity 가 더 맞는것 같은데..\ntf.keras.layers.Dense(units=1,activation=‘linear’,use_bias=True)\n\n\n아래의 코드1,2는 (1)의 코드들과 살짝 다른코드이다. (코드1과 코드2는 같은코드임)\n\n\ntf.keras.layers.Dense(1,input_dim=2) # 코드1\ntf.keras.layers.Dense(1,input_shape=(2,)) # 코드2\n\n\n아래는 사용불가능한 코드이다.\n\n\ntf.keras.layers.Dense(1,input_dim=(2,)) # 코드1\ntf.keras.layers.Dense(1,input_shape=2) # 코드2\n\n- 왜 input_dim이 필요한가?\n\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Dense(1,use_bias=False)) \n\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Dense(1,use_bias=False,input_dim=2))\n\n- net1의 경우 input_dim을 명시해주지 않아 Weight를 알 수 없다\n\nnet1.weights\n\nValueError: ignored\n\n\n\nnet2.weights\n\n[<tf.Variable 'dense_3/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[-1.053657 ],\n        [ 1.3536845]], dtype=float32)>]\n\n\n- 또한 입력차원을 모르기 깨문에 summary값도 알 수 없다.\n\nnet1.summary()\n\nValueError: ignored\n\n\n\nnet2.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_3 (Dense)             (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n풀이3: 스칼라버전, 임의의 초기값을 설정\n(0단계) 데이터정리\n\ny=y.reshape(N,1)\nx=x.reshape(N,1)\nx.shape,y.shape\n\n(TensorShape([200, 1]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential() \n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,input_dim=1)\n\n\nnet.add(layer)\n\n\n초기값을 설정\n\nnet.weights\n\n[<tf.Variable 'dense_4/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[0.534932]], dtype=float32)>,\n <tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n\n\n\nnet.get_weights()\n\n[array([[0.534932]], dtype=float32), array([0.], dtype=float32)]\n\n\n\nweight, bias순으로 출력\n\n\nnet.set_weights?\n\n\nlayer_b.set_weights(layer_a.get_weights()) 와 같은방식으로 쓴다는 것이군?\n\n- 한번따라해보자.\n\n_w = net.get_weights()\n_w\n\n[array([[0.534932]], dtype=float32), array([0.], dtype=float32)]\n\n\n\n_w[0]\n\narray([[0.534932]], dtype=float32)\n\n\n\n길이가 2인 리스트이고, 각 원소는 numpy array 임\n\n\nnet.set_weights(\n    [np.array([[10.0]],dtype=np.float32), # weight, β1_hat\n     np.array([-5.0],dtype=np.float32)] # bias, β0_hat \n)\n\n\nnet.weights\n\n[<tf.Variable 'dense_4/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[10.]], dtype=float32)>,\n <tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([-5.], dtype=float32)>]\n\n\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1),tf.losses.MSE) \n\n(4단계) net.fit()\n\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7ff9152a5c50>\n\n\n결과확인\n\nnet.weights\n\n[<tf.Variable 'dense_4/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[3.933048]], dtype=float32)>,\n <tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([2.58366], dtype=float32)>]\n\n\n\n\n풀이4: 벡터버전, 임의의 초기값을 설정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False,input_dim=2) \n\n\nnet.add(layer)\n\n\n초기값을 설정하자\n\nnet.set_weights([np.array([[ -5.0],[10.0]], dtype=np.float32)])\n\n\nnet.get_weights()\n\n[array([[-5.],\n        [10.]], dtype=float32)]\n\n\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.losses.MSE) \n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7ff9151dbad0>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_5/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.58366 ],\n        [3.933048]], dtype=float32)>]\n\n\n- 사실 실전에서는 초기값을 설정할 필요가 별로 없음.\n\n\n풀이5: 벡터버전 사용자정의 손실함수\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False) \n\n\nnet.add(layer)\n\n(3단계) net.compile()\n\nloss_fn = lambda y,yhat: (y-yhat).T @ (y-yhat) / N\n\n\nnet.compile(tf.keras.optimizers.SGD(0.1), loss_fn) \n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7ff915103dd0>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_6/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836728],\n        [3.9330244]], dtype=float32)>]\n\n\n\n\n풀이6: 벡터버전, net.compile의 옵션으로 손실함수 지정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1), loss='mse') \n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7ff91502bb50>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_7/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836728],\n        [3.9330244]], dtype=float32)>]\n\n\n\n\n풀이7: 벡터버전, net.compile의 옵션으로 손실함수 지정 + 옵티마이저 지정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\n\n(3단계) net.compile()\n\nnet.compile(optimizer='sgd', loss='mse') \n#net.optimizer.lr = tf.Variable(0.1,dtype=tf.float32)\n#net.optimizer.lr = 0.1\n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=5000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7ff915117550>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_8/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5842712],\n        [3.9319096]], dtype=float32)>]\n\n\n\n\n\n여러가지 회귀모형의 적합과 학습과정의 모니터링\n\n예제1\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 x_i\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.random.randn(N) \nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*x +epsilon\n\n\nX= np.stack([np.ones(N),x],axis=1)\ny= y.reshape(N,1)\n\n\nplt.plot(x,y,'o') # 관측한 자료 \n\n\n\n\n\nbeta_hat = np.array([-3,-2]).reshape(2,1)\n\n\nyhat = X@beta_hat \n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.reshape(-1),'-') \n\n\n\n\n더 좋은 적합선을 얻기위해서!\n\nslope = (2*X.T@X@beta_hat - 2*X.T@y)/ N \nbeta_hat2 = beta_hat - 0.1*slope  \nyhat2 = X@beta_hat2\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.reshape(-1),'-') \nplt.plot(x,yhat2.reshape(-1),'-') \n\n\n\n\n초록색이 좀 더 나아보인다.\n\nbeta_hat = np.array([-3,-2]).reshape(2,1) \nbeta_hats = beta_hat # beta_hats = beta_hat.copy() 가 더 안전한 코드입니다. \nfor i in range(1,30):\n    yhat = X@beta_hat \n    slope = (2*X.T@X@beta_hat - 2*X.T@y) / N \n    beta_hat = beta_hat - 1.0*slope # 0.1은 적당, 0.3은 쪼금빠르지만 그래도 적당, 0.9는 너무 나간것같음, 1.0 은 수렴안함, 1.2 \n    beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) \n\n\nbeta_hats\n\narray([[-3.        ,  7.12238255, -1.2575366 ,  5.73166742, -0.1555309 ,\n         4.86767499,  0.51106397,  4.36611576,  0.87316777,  4.12348617,\n         1.01165173,  4.07771926,  0.97282343,  4.19586617,  0.77814101,\n         4.46653491,  0.4299822 ,  4.89562729, -0.08537358,  5.50446319,\n        -0.79684366,  6.32975688, -1.74933031,  7.42517729, -3.00603683,\n         8.86442507, -4.6523303 , 10.74592463, -6.80132547, 13.19938129],\n       [-2.        ,  8.70824998,  0.16165717,  6.93399596,  1.62435964,\n         5.72089586,  2.63858056,  4.86387722,  3.37280529,  4.22385379,\n         3.94259478,  3.70397678,  4.43004465,  3.23363047,  4.89701606,\n         2.75741782,  5.39439054,  2.22728903,  5.96886945,  1.59655409,\n         6.66836857,  0.81489407,  7.54676324, -0.17628423,  8.66856437,\n        -1.44867655, 10.11401544, -3.09256176, 11.98507323, -5.22340389]])\n\n\n\nb0hats = beta_hats[0].tolist()\nb1hats = beta_hats[1].tolist()\n\n\nnp.linalg.inv(X.T@X) @ X.T @ y\n\narray([[2.5451404 ],\n       [3.94818596]])\n\n\n\nfrom matplotlib import animation \nplt.rcParams[\"animation.html\"] = \"jshtml\" \n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n<Figure size 864x360 with 0 Axes>\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림 \nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*x) \n# ax2: 오른쪽그림 \nβ0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\nβ0=β0.reshape(-1)\nβ1=β1.reshape(-1)\nloss_fn = lambda b0,b1: np.sum((y-b0-b1*x)**2)\nloss = list(map(loss_fn, β0,β1))\nax2.scatter(β0,β1,loss,alpha=0.02) \nax2.scatter(2.5451404,3.94818596,loss_fn(2.5451404,3.94818596),s=200,marker='*') \n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*x) \n    ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\") \n\nani = animation.FuncAnimation(fig,animate,frames=30) \nani\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\n예제2\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i}\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) +epsilon\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\nX= np.stack([np.ones(N),np.exp(-x)],axis=1)\ny= y.reshape(N,1)\n\n\nbeta_hat = np.array([-3,-2]).reshape(2,1)\nbeta_hats = beta_hat.copy() # shallow copy, deep copy <--- 여름 방학 특강 \nfor i in range(1,30): \n    yhat = X@beta_hat \n    slope = (2*X.T@X@beta_hat - 2*X.T@y) /N \n    beta_hat = beta_hat - 0.05*slope \n    beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) \n\n\nbeta_hats\n\narray([[-3.        , -1.74671631, -0.82428979, -0.14453919,  0.35720029,\n         0.72834869,  1.0036803 ,  1.20869624,  1.36209751,  1.47759851,\n         1.56525696,  1.63244908,  1.68458472,  1.72563174,  1.75850062,\n         1.78532638,  1.80767543,  1.82669717,  1.84323521,  1.85790889,\n         1.8711731 ,  1.88336212,  1.89472176,  1.90543297,  1.91562909,\n         1.92540859,  1.93484428,  1.94399023,  1.9528867 ,  1.96156382],\n       [-2.        , -0.25663415,  1.01939241,  1.95275596,  2.63488171,\n         3.13281171,  3.49570765,  3.75961951,  3.95098231,  4.08918044,\n         4.18842797,  4.2591476 ,  4.30898175,  4.34353413,  4.36691339,\n         4.38213187,  4.39139801,  4.39633075,  4.39811673,  4.3976256 ,\n         4.3954946 ,  4.3921905 ,  4.38805511,  4.3833386 ,  4.37822393,\n         4.37284482,  4.36729887,  4.36165718,  4.35597148,  4.35027923]])\n\n\n\nb0hats= beta_hats[0].tolist()\nb1hats= beta_hats[1].tolist()\n\n\nnp.linalg.inv(X.T@X)@X.T@y\n\narray([[2.46307644],\n       [3.99681332]])\n\n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n<Figure size 864x360 with 0 Axes>\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림 \nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x))\n# ax2: 오른쪽그림 \nβ0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\nβ0=β0.reshape(-1)\nβ1=β1.reshape(-1)\nloss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2)\nloss = list(map(loss_fn, β0,β1))\nax2.scatter(β0,β1,loss,alpha=0.02) \nax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker='*') \n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x))\n    ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\") \n\nani = animation.FuncAnimation(fig,animate,frames=30) \nani\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\n예제3\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i} + \\beta_2 \\cos(5x_i)\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\nX=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) \ny=y.reshape(N,1)\n\n\nbeta_hat = np.array([-3,-2,-1]).reshape(3,1) \nbeta_hats = beta_hat.copy()\nfor i in range(1,30):\n    yhat = X@beta_hat \n    slope = (2*X.T@X@beta_hat -2*X.T@y) /N \n    beta_hat = beta_hat - 0.1 * slope \n    beta_hats= np.concatenate([beta_hats,beta_hat],axis=1)\n\n\nbeta_hats\n\narray([[-3.        , -0.71767532,  0.36255782,  0.89072137,  1.16423101,\n         1.31925078,  1.41819551,  1.48974454,  1.54713983,  1.59655416,\n         1.64091846,  1.68167278,  1.71956758,  1.75503084,  1.78833646,\n         1.81968188,  1.84922398,  1.877096  ,  1.90341567,  1.92828934,\n         1.95181415,  1.97407943,  1.99516755,  2.01515463,  2.0341111 ,\n         2.05210214,  2.06918818,  2.08542523,  2.10086524,  2.11555643],\n       [-2.        ,  1.16947474,  2.64116513,  3.33411605,  3.66880042,\n         3.83768856,  3.92897389,  3.98315095,  4.01888831,  4.04486085,\n         4.06516144,  4.08177665,  4.09571971,  4.10754954,  4.1176088 ,\n         4.12613352,  4.13330391,  4.13926816,  4.14415391,  4.14807403,\n         4.15112966,  4.1534121 ,  4.15500404,  4.15598045,  4.15640936,\n         4.15635249,  4.15586584,  4.15500014,  4.15380139,  4.1523112 ],\n       [-1.        , -0.95492718, -0.66119313, -0.27681968,  0.12788212,\n         0.52254445,  0.89491388,  1.24088224,  1.55993978,  1.85310654,\n         2.12199631,  2.36839745,  2.59408948,  2.8007666 ,  2.99000967,\n         3.16327964,  3.32192026,  3.46716468,  3.60014318,  3.72189116,\n         3.83335689,  3.93540864,  4.02884144,  4.11438316,  4.19270026,\n         4.26440288,  4.33004965,  4.39015202,  4.44517824,  4.49555703]])\n\n\n\nb0hats,b1hats,b2hats = beta_hats\n\n\nnp.linalg.inv(X.T@X) @ X.T @ y\n\narray([[2.46597526],\n       [4.00095138],\n       [5.04161877]])\n\n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n<Figure size 864x360 with 0 Axes>\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림 \nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x) + b2hats[0]*np.cos(5*x))\n# ax2: 오른쪽그림 \n# β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\n# β0=β0.reshape(-1)\n# β1=β1.reshape(-1)\n# loss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2)\n# loss = list(map(loss_fn, β0,β1))\n# ax2.scatter(β0,β1,loss,alpha=0.02) \n# ax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker='*') \n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x) + b2hats[i]*np.cos(5*x))\n    # ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\") \n\nani = animation.FuncAnimation(fig,animate,frames=30) \nani\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\n예제3: 케라스로 해보자!\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i} + \\beta_2 \\cos(5x_i)\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon\n\n\nX=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) \ny=y.reshape(N,1)\n\n\nnet = tf.keras.Sequential() # 1: 네트워크 생성\nnet.add(tf.keras.layers.Dense(1,use_bias=False)) # 2: add layer \nnet.compile(tf.optimizers.SGD(0.1), loss='mse') # 3: compile\nnet.fit(X,y,epochs=30, batch_size=N) # 4: fit \n\nEpoch 1/30\n1/1 [==============================] - 0s 186ms/step - loss: 82.1027\nEpoch 2/30\n1/1 [==============================] - 0s 11ms/step - loss: 23.9512\nEpoch 3/30\n1/1 [==============================] - 0s 14ms/step - loss: 10.7256\nEpoch 4/30\n1/1 [==============================] - 0s 9ms/step - loss: 7.0664\nEpoch 5/30\n1/1 [==============================] - 0s 6ms/step - loss: 5.5521\nEpoch 6/30\n1/1 [==============================] - 0s 5ms/step - loss: 4.6075\nEpoch 7/30\n1/1 [==============================] - 0s 6ms/step - loss: 3.8836\nEpoch 8/30\n1/1 [==============================] - 0s 7ms/step - loss: 3.2909\nEpoch 9/30\n1/1 [==============================] - 0s 6ms/step - loss: 2.7971\nEpoch 10/30\n1/1 [==============================] - 0s 7ms/step - loss: 2.3838\nEpoch 11/30\n1/1 [==============================] - 0s 6ms/step - loss: 2.0374\nEpoch 12/30\n1/1 [==============================] - 0s 5ms/step - loss: 1.7471\nEpoch 13/30\n1/1 [==============================] - 0s 5ms/step - loss: 1.5038\nEpoch 14/30\n1/1 [==============================] - 0s 6ms/step - loss: 1.2998\nEpoch 15/30\n1/1 [==============================] - 0s 6ms/step - loss: 1.1288\nEpoch 16/30\n1/1 [==============================] - 0s 7ms/step - loss: 0.9854\nEpoch 17/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.8652\nEpoch 18/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.7645\nEpoch 19/30\n1/1 [==============================] - 0s 7ms/step - loss: 0.6800\nEpoch 20/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.6092\nEpoch 21/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.5499\nEpoch 22/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.5001\nEpoch 23/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.4584\nEpoch 24/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.4234\nEpoch 25/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.3941\nEpoch 26/30\n1/1 [==============================] - 0s 12ms/step - loss: 0.3695\nEpoch 27/30\n1/1 [==============================] - 0s 9ms/step - loss: 0.3489\nEpoch 28/30\n1/1 [==============================] - 0s 10ms/step - loss: 0.3316\nEpoch 29/30\n1/1 [==============================] - 0s 12ms/step - loss: 0.3171\nEpoch 30/30\n1/1 [==============================] - 0s 4ms/step - loss: 0.3050\n\n\n<keras.callbacks.History at 0x7ff90f24bd10>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_9/kernel:0' shape=(3, 1) dtype=float32, numpy=\n array([[2.485702 ],\n        [3.9252913],\n        [4.6923084]], dtype=float32)>]\n\n\n\nplt.plot(x,y,'o') \nplt.plot(x,(X@net.weights).reshape(-1),'--')\n\n\n\n\n\n\n\n숙제\n\n예제2: 케라스를 이용하여 아래를 만족하는 적절한 \\(\\beta_0\\)와 \\(\\beta_1\\)을 구하라. 적합결과를 시각화하라. (애니메이션 시각화 X)\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i}\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) +epsilon"
  },
  {
    "objectID": "post/Lecture/STBD/2022-04-18-(7주차).html",
    "href": "post/Lecture/STBD/2022-04-18-(7주차).html",
    "title": "07. Adam",
    "section": "",
    "text": "import numpy as np\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "post/Lecture/STBD/2022-04-18-(7주차).html#piece-wise-linear-regression",
    "href": "post/Lecture/STBD/2022-04-18-(7주차).html#piece-wise-linear-regression",
    "title": "07. Adam",
    "section": "piece-wise linear regression",
    "text": "piece-wise linear regression\n\n이제까지는 단순선형회귀에 대해서 다루었다면 일정한 지점에서 절단점을 가지는 piecc-wise linear regreesion을 다루어보자\n\n\\[y = x + 0.3\\varepsilon \\quad x\\leq 0\\]\n\\[y = 3.5x + 0.3\\varepsilon\\]\n\nnp.random.seed(2021025)\n\nN = 100\n\nx = np.linspace(-1,1,N)\ne = np.random.normal(size=N)\n\ny = list(map(lambda x,e : x + 0.3*e if x<=0 else 3.5*x +0.3*e,x,e))\ny = np.array(y)\n\n\n시각화\n\nplt.plot(x[x<=0],y[x<=0],\".\")\nplt.plot(x[x>0],y[x>0],\".\")\nplt.legend([\"x<=0\", \"x>0\"])\n\n<matplotlib.legend.Legend at 0x7fee233e4e50>\n\n\n\n\n\n\n\n풀이 1\n\ntip : x,y가 tensor가 아니어도 신경망 적합시 잘 적합된다.\n\n\nx = x.reshape(N,1)\ny = y.reshape(N,1)\n\n\nnet = tf.keras.Sequential()\nlayer = tf.keras.layers.Dense(1)\nnet.add(layer)\n\nnet.compile(tf.optimizers.SGD(0.1),loss = \"mse\")\nnet.fit(x,y,batch_size=N,verbose=0,epochs=1000)\n\n<keras.callbacks.History at 0x7fee1f1e8b50>\n\n\n\nw = net.weights\n\n\nbeta0, beta1 = w[1],w[0]\n\n\nyhat = x*beta1+beta0\nyhat = yhat.reshape(N,)\n\n\nplt.plot(x[x<=0],y[x<=0],\".\")\nplt.plot(x[x>0],y[x>0],\".\")\nplt.plot(x,yhat,\".\")\nplt.legend([\"x<=0\", \"x>0\",\"yhat\"])\n\n<matplotlib.legend.Legend at 0x7fee1f0f8110>\n\n\n\n\n\n\n위 적합한 모형은 틀렸음\n우리가 의도한 모델은 피스와이즈 모델이지 단순선형모델이 아님 \\(\\to\\) 꺽은선 형태의 모델이 형성되어야 한다.\n위 같은 문제를 underfit 의 문제라고 함.\n\n\n\n풀이 2. 비선형 활성화 함수의 도입(Relu)\nRelu\n\\[relu(x) = max(0,x)\\]\n\n#collapse\ngv('''\n\"x\" -> \"x*w,    bias=True\"[label=\"*w\"] ;\n\"x*w,    bias=True\" -> \"y\"[label=\"relu\"] ''')\n\n\n\n\n\n즉, \\(x\\)가 0보다 작은 녀석들은 전부다 0으로 보내는 활성화함수이다.\n\n\nnet2 = tf.keras.Sequential()\n\nl1 = tf.keras.layers.Dense(1,input_shape=(1,))\n\na1 = tf.keras.layers.Activation(tf.nn.relu)\n\nnet2.add(l1)\nnet2.add(a1)\n\n\nl1의 weight(\\(\\beta_1\\))값이 1보다 크게 나와야 현재 보여주려는 문제 의도대로 풀 수 있다.\n\n\nl1.weights\n\n[<tf.Variable 'dense_3/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[0.47527015]], dtype=float32)>,\n <tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n\n\n\n현재 생성한 네트워크 상황을 확인해보자.\n\n\nu1 = l1(x)\nv1 = a1(u1)\n\n\nplt.plot(x,x,\"--\")\nplt.plot(x,u1,\"--\")\nplt.plot(x,v1,\"--\")\nplt.legend([\"x=x\",\"y=linear x\",\"relu (y)\"])\n\n<matplotlib.legend.Legend at 0x7fee1efb1990>\n\n\n\n\n\n\n오 표현하려는 relu 함수처럼 잘 표시된 것 같다.\n\n이제 모델을 컴파일 하고 적합해보자!!\n\nnet2.compile(tf.optimizers.SGD(0.1),loss=\"MSE\")\nnet2.fit(x,y,N,1000,0)\n\n<keras.callbacks.History at 0x7fee1ed30f50>\n\n\n\nyhat = net2(x)\nyhat = yhat.reshape(N,) ## 이건 굳이 안해줘도 되나 습관화하자 (목적에 맞게 차원변환)\n\n\nplt.plot(x[x>=0],y[x>=0],\".\",label=\"x>=0\")\nplt.plot(x[x<0],y[x<0],\".\",label=\"x<0\")\nplt.plot(x,yhat,\"--\",label=\"hat y\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7feea848ca10>\n\n\n\n\n\n\n그런데… 아직도 \\(x<0\\)인 부분에서는 모델 적합이 잘 되지 않은 것 같다.\nwhy? relu 함수의 특성상 \\(relu(y) = 0, (y<0)\\) 으로 전부 보냈기 때문!\n이제 선형변환된 값들이 0 이하로 떨어지는 부분들을 어떻게 하면 잘 적합시킬 수 있는지 해결하자!\n해결책 : \\(\\hat {y}\\) 가 2개가 있으면?\n\n\n\n풀이 3\n\n목표 : \\(\\hat y\\)를 2개 만들자,즉 \\(\\hat y \\to (N,2)\\)\n위의 의도대로 하려면 하나의 \\(\\bf X\\)를 받아 2개의 출력이 나와야한다!!\n\n\nnet3 = tf.keras.Sequential()\n\nl1 = tf.keras.layers.Dense(2,input_shape=(1,))\na1 = tf.keras.layers.Activation(tf.nn.relu)\n\nnet3.add(l1)\nnet3.add(a1)\n\n(현재 네트워크 상황 확인)\n\nl1.weights\n\n[<tf.Variable 'dense_50/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[-1.1642166,  0.6747991]], dtype=float32)>,\n <tf.Variable 'dense_50/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n\n\n\n## collapse\nfig,(ax1,ax2) = plt.subplots(1,2)\nfig.set_figwidth(10)\nfig.set_figheight(5)\n\nax1.plot(x,x,\"--\")\nax1.plot(x,l1(x),\"--\")\nax1.legend([\"x\",\"l1=w1*x+b\",\"l2=w2*x+b\"])\n\nax2.plot(x,x,\"--\")\nax2.plot(x,a1(l1(x)),\"--\")\nax2.legend([\"x\",\"al(l1)\",\"al(l2)\"])\n\n<matplotlib.legend.Legend at 0x7fee129f4e10>\n\n\n\n\n\n\n위의 문제점\n\n1. 우리의 의도대로 relu를 지나서 \\(\\hat y\\) 를 2차원으로 만들었다..\n2. 근데 차피 relu를 지나면 al(l1),al(l2)가 0보다 작으면 \\(\\hat y\\)는 전부 0이 나올 것임\n3. 해결책 : 노드를 추가해서 al(l1),al(l2)를 입력으로 받아 선형모형을 다시 만들자\n4. 또한 \\(\\hat y\\)의 차원의 수를 의도적으로 2차원으로 만들었으니 다시 1차원으로 변경해주자!\n\n즉, 입력차원 2, 출력차원 1로 변환해주는 노드를 추가\n\npython\ntemp  = tf.keras.Sequential()\n_l1 = tf.keras.layers.Dense(2,input_shape=(1,))\n_a1 = tf.keras.layers.Activation(tf.nn.relu)\n_l2 = tf.keras.layers.Dense(2,input_shape=(2,))\n\ntemp.add(_l1)\ntemp.add(_a1)\ntemp.add(_l2)\n\ntemp.summary()\n\nModel: \"sequential_21\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_26 (Dense)            (None, 2)                 4         \n                                                                 \n activation_15 (Activation)  (None, 2)                 0         \n                                                                 \n dense_27 (Dense)            (None, 2)                 6         \n                                                                 \n=================================================================\nTotal params: 10\nTrainable params: 10\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nl2 = tf.keras.layers.Dense(1,input_shape=(2,))\n\n\nnet3.add(l2)\n\n\nl2.weights\n\n[<tf.Variable 'dense_51/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[-0.54625785],\n        [ 0.949442  ]], dtype=float32)>,\n <tf.Variable 'dense_51/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n\n\n\nnet3.summary()\n\nModel: \"sequential_34\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_50 (Dense)            (None, 2)                 4         \n                                                                 \n activation_28 (Activation)  (None, 2)                 0         \n                                                                 \n dense_51 (Dense)            (None, 1)                 3         \n                                                                 \n=================================================================\nTotal params: 7\nTrainable params: 7\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nnet3.compile(tf.optimizers.SGD(0.1),loss=\"mse\")\nnet3.fit(x,y,N,1000,0)\n\n<keras.callbacks.History at 0x7fee12a87950>\n\n\n\nyhat = net3(x)\nyhat = yhat.reshape(N,)\n\n(네트워크의 변화과정 확인)\n\nl1_1=l1(x)[:,[0]].reshape(N,)\nl1_2=l1(x)[:,[1]].reshape(N,)\n\n\nl2.weights\n\n[<tf.Variable 'dense_51/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[-0.734917 ],\n        [ 1.8227489]], dtype=float32)>,\n <tf.Variable 'dense_51/bias:0' shape=(1,) dtype=float32, numpy=array([-0.05703843], dtype=float32)>]\n\n\n\nfig,(ax1,ax2,ax3) = plt.subplots(1,3)\nfig.set_figwidth(14b)\nfig.set_figheight(5)\n\nax1.plot(x,y,\".\")\nax1.plot(x,l1_1,\"--\")\nax1.plot(x,l1_2,\"--\")\nax1.legend([\"y\",\"l1\",\"l2\"])\nax1.set_title(\"1st linear regreesion\")\n\nax2.plot(x,y,\".\")\nax2.plot(x,a1(l1(x)),\"--\")\nax2.set_title(\"2st relu\")\nax2.legend([\"y\",\"relu(l1)\",\"relu(l2)\"])\n\nax3.plot(x,y,\".\")\nax3.plot(x,l2(a1(l1(x))),\"--\")\nax3.set_title(\"3rd linear regression\")\nax3.legend([\"y\",\"l2\"])\n\n<matplotlib.legend.Legend at 0x7fee12862550>\n\n\n\n\n\n\n표본의 수가 작아서 그런가 의도한대로 잘 안나오다가 한 10번 돌려서 나온 것 같음..\n즉. 여기까지 요약하자면\n2개의 출력을 가지는 linear \\(\\to\\) relu \\(\\to\\) 1차원의 linear\n근데 풀이 3의 실패하는 경우도 존재함.\n\n\nnp.random.seed(4)\nN = 100\n\nx = np.linspace(-1,1,N)\ne = np.random.normal(size=N)\n\ny = list(map(lambda x,e : x + 0.3*e if x<=0 else 3.5*x +0.3*e,x,e))\ny = np.array(y)\n\nx= x.reshape(N,1)\ny= y.reshape(N,1)\n\n\ntf.random.set_seed(2)\n\nnet3 = tf.keras.Sequential()\n####\nnet3.add(tf.keras.layers.Dense(2))\nnet3.add(tf.keras.layers.Activation(\"relu\"))\nnet3.add(tf.keras.layers.Dense(1))\n####\nnet3.compile(optimizer = tf.optimizers.SGD(0.1),loss=\"mse\")\n\nnet3.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7fee1d2ff490>\n\n\n\nl2.weights\n\n[<tf.Variable 'dense_51/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[-0.734917 ],\n        [ 1.8227489]], dtype=float32)>,\n <tf.Variable 'dense_51/bias:0' shape=(1,) dtype=float32, numpy=array([-0.05703843], dtype=float32)>]\n\n\n\n그냥 이거는 교수님 코드 따라서 친건데 밑에 그래프하고 똑같음\n\n\n#collapse\nfig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4) \nfig.set_figwidth(16) \nax1.plot(x,y,'.')\nax1.plot(x,l1(x)[:,0],'--r')\nax1.plot(x,l1(x)[:,1],'--b')\nax2.plot(x,y,'.')\nax2.plot(x,a1(l1(x))[:,0],'--r')\nax2.plot(x,a1(l1(x))[:,1],'--b')\nax3.plot(x,y,'.')\nax3.plot(x,a1(l1(x))[:,0]*(-0.734917),'--r')\nax3.plot(x,a1(l1(x))[:,1]*(1.8227489)+(-0.05703843),'--b')\nax4.plot(x,y,'.')\nax4.plot(x,a1(l1(x))[:,0]*(-0.734917)+a1(l1(x))[:,1]*(1.8227489)+(-0.05703843),'--')\n\n\n\n\n\n#collapse\nfig,(ax1,ax2,ax3) = plt.subplots(1,3)\n\nfig.set_figwidth(14)\nfig.set_figheight(5)\n\nax1.plot(x,y,\".\")\nax1.plot(x,l1_1,\"--\")\nax1.plot(x,l1_2,\"--\")\nax1.legend([\"y\",\"l1\",\"l2\"])\nax1.set_title(\"1st linear regreesion\")\n\nax2.plot(x,y,\".\")\nax2.plot(x,a1(l1(x)),\"--\")\nax2.set_title(\"2st relu\")\nax2.legend([\"y\",\"relu(l1)\",\"relu(l2)\"])\n\nax3.plot(x,y,\".\")\nax3.plot(x,l2(a1(l1(x))),\"--\")\nax3.set_title(\"3rd linear regression\")\nax3.legend([\"y\",\"l2\"])\n\n<matplotlib.legend.Legend at 0x7fee1c33b250>\n\n\n\n\n\n\n사실 위 경우는 2개의 linear가 둘 다 일을 잘하고 있는 것임\n근데 가끔 둘 중에 한 개가 일을 제대로 하지 못하는 경우가 발생 (예를 들어 선형변환된 선중에 하나가 계속 0값을 가지면?)\n즉, 현재 학습한 에포크에서 우리가 만든 모델이 최적상태이다. \\(\\to\\) 완전한 예측은 난 못해 이런느낌\n이를 global minimum(로스를 최소하는)을 찾지 못하고 local minimum(현재 조건에서 최적화한)에 빠졌다고 한다.\n7주차 공부한 피스와이즈는 DNN 이라고 할 수 있다."
  },
  {
    "objectID": "post/Lecture/STBD/2022-04-18-(7주차).html#logistic",
    "href": "post/Lecture/STBD/2022-04-18-(7주차).html#logistic",
    "title": "07. Adam",
    "section": "logistic",
    "text": "logistic\n\n결론부터 말하자면 로지스틱은 MSE대신 BCE(Binary cross entropy)를 손실함수로 사용한다.\n왜? 로지스틱의 손실함수 BCE인 경우 convex(2차함수모양)하기 때문에 글로벌 미니멈을 잘 찾을 수 있음!!\n\n\n예제\n\nN = 2000\n\nx = np.linspace(-1,1,N).reshape(N,1)\n\n초기 가중치 설정 및 베르누이 분포\n\nw0 = -1\nw1 = 5\n\nu = w0+x*w1\n\nv = tf.nn.sigmoid(u)\ny = tf.constant(np.random.binomial(1,v))\n\n\nplt.plot(x,y,\".\",alpha=0.2)\nplt.plot(x,v,\"--\")\nplt.legend([\"y\",\"p(y=1)\"])\n\n<matplotlib.legend.Legend at 0x7fee1851fe90>\n\n\n\n\n\n\n케라스를 이용하여 구현\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1,input_shape=(1,),activation=\"sigmoid\"))\nnet.compile(tf.optimizers.SGD(0.1),loss='binary_crossentropy' )\n\n\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7fee12459b90>\n\n\n\nfig, (ax1,ax2) = plt.subplots(1,2)\nfig.set_figwidth(12)\n\nax1.plot(x,y,\".\",alpha=0.2)\nax1.plot(x,v,\"--\")\nax1.legend([\"y\",\"p(y=1)\"])\nax1.set_title(\"not keras\")\n\nax2.plot(x,y,\".\",alpha=0.2)\nax2.plot(x,net(x),\"--\")\nax2.legend([\"y\",\"p(y=1)\"])\nax2.set_title(\"with keras\")\n\nText(0.5, 1.0, 'with keras')\n\n\n\n\n\n\n\nMSE vs BCE\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Dense(1,input_shape=(1,),activation=\"sigmoid\"))\nnet2.compile(tf.optimizers.SGD(0.1),loss='mse' )\n\n\nnet2.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7fee11a3e2d0>\n\n\n\nplt.plot(x,y,\".\",alpha=0.2)\nplt.plot(x,net(x),\"--\")\nplt.plot(x,net2(x),\"--\")\nplt.legend([\"y\",\"BCE\",\"MSE\"])\nplt.title(\"BCE vs MSE\")\n\nText(0.5, 1.0, 'BCE vs MSE')\n\n\n\n\n\n\n로지스틱 회귀모형의 특성상 \\(x\\)는 0을 기점으로 약간 지그재그에 가까울수록 좋은 모형인데\nBCE가 확실히 MSE에 비해서 지그재그에 가까워 보인다!\n즉 BCE는 동일한 조건(epoch,가중치)에서 BCE가 MSE보다 적합이 뛰어나다.\n\n\nMSE, BCE loss 비교\n\nmseloss_fn = lambda y,yhat: tf.reduce_mean((y-yhat)**2)\nbceloss_fn = lambda y,yhat: -tf.reduce_mean(y*tnp.log(yhat) + (1-y)*tnp.log(1-yhat))\n\n\ndef loss_fn1(w0,w1):\n    u = w0+w1*x \n    yhat = np.exp(u)/(np.exp(u)+1)\n    return mseloss_fn(y,yhat) \n\ndef loss_fn2(w0,w1):\n    u = w0+w1*x \n    yhat = np.exp(u)/(np.exp(u)+1)\n    return bceloss_fn(y,yhat)\n\n\nw0, w1 = np.meshgrid(np.arange(-10,3,0.2), np.arange(-1,10,0.2), indexing='ij')\nw0, w1 = w0.reshape(-1), w1.reshape(-1)\n\n\nloss1 = list(map(loss_fn1,w0,w1))\nloss2 = list(map(loss_fn2,w0,w1))\n\n\nw0.shape,w1.shape\n\n((3575,), (3575,))\n\n\n\nfig = plt.figure()\nfig.set_figwidth(9)\nfig.set_figheight(9)\nax1=fig.add_subplot(1,2,1,projection='3d')\nax2=fig.add_subplot(1,2,2,projection='3d')\nax1.elev=15\nax2.elev=15\nax1.azim=75\nax2.azim=75\nax1.scatter(w0,w1,loss1,s=0.1)\nax1.set_title(\"MSE Loss\")\nax2.scatter(w0,w1,loss2,s=0.1) \nax2.set_title(\"BCE Loss\")\n\nText(0.5, 0.92, 'BCE Loss')\n\n\n\n\n\n\n로스를 확인한 결과 오른쪽 그림이 더 최적해에 잘 수렴할 것 같음\n이번엔 동일한 기준을 설정해놓고 확인해보자.\n\n- 파라미터 : (w0,w1) = (-3.0,1.0), epoch 15 마다 관찰\n\nx.shape\n\n(2000, 1)\n\n\n\ntf.ones(N,dtype=tf.float64).reshape(N,1)\n\n<tf.Tensor: shape=(2000, 1), dtype=float64, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       ...,\n       [1.],\n       [1.],\n       [1.]])>\n\n\n\nX = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1)\n\n\nX.shape\n\nTensorShape([2000, 2])\n\n\n\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet1.compile(tf.keras.optimizers.SGD(0.1),loss=\"mse\")\nnet1.fit(X,y,N,1,0)\n\n<keras.callbacks.History at 0x7fee0a4f27d0>\n\n\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet2.compile(tf.keras.optimizers.SGD(0.1),loss=\"binary_crossentropy\")\nnet2.fit(X,y,N,1,0)\n\n<keras.callbacks.History at 0x7fee0a47add0>\n\n\n\n에포크 한번 안덜리면 set_weights못함…\n\n\nnet1.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\nnet2.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-3.],\n         [-1.]], dtype=float32)], [array([[-3.],\n         [-1.]], dtype=float32)])\n\n\n이제 15 epoch마다 기록해보자\n\nmse_hat = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\nbce_hat = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\n\n\ntf.concat([mse_hat,bce_hat],axis=1)\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-3., -3.],\n       [-1., -1.]], dtype=float32)>\n\n\n\nnet1.weights[0]\n\n<tf.Variable 'dense_116/kernel:0' shape=(2, 1) dtype=float32, numpy=\narray([[-1.4075234 ],\n       [ 0.26052466]], dtype=float32)>\n\n\n\nfor i in range(29) :\n    net1.fit(X,y,N,15,0)\n    net2.fit(X,y,N,15,0)\n    mse_hat = tf.concat([mse_hat,net1.weights[0]],axis=1)\n    bce_hat = tf.concat([bce_hat,net2.weights[0]],axis=1)\n\n\n\n시각화\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"SGD, Winit=(-3,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r'); \nline3, = ax3.plot(x,1/(1+np.exp(-X@mse_hat[:,0])),'--b')\nax3.set_title(\"MSE\")\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@bce_hat[:,0])),'--b')\nax4.set_title(\"BCE\")\ndef animate(i):\n    _w0_mse,_w1_mse = mse_hat[:,i]\n    _w0_bce,_w1_bce = bce_hat[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@mse_hat[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@bce_hat[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\nMSE의 경우 초기 기울기가 크지 않아 loss가 0으로 가는 속도가 매우 느리다.\n즉 동일한 기준에서 BCE loss가 성능이 더좋다!!\n\n\n\nAdam 옵티마이저\n\nnet_mse = tf.keras.Sequential()\nnet_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn) \nnet_mse.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 438ms/step - loss: 0.3187\n\n\n<keras.callbacks.History at 0x7fee07c90f90>\n\n\n\nnet_bce = tf.keras.Sequential()\nnet_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn) \nnet_bce.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 495ms/step - loss: 0.8539\n\n\n<keras.callbacks.History at 0x7fee07a886d0>\n\n\n\nnet_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\nnet_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nWhat_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\nWhat_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\n\n\nfor k in range(29): \n    net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) \n    What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1)\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"Adam, Winit=(-3,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r'); \nline3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),'--b')\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),'--b')\n\ndef animate(i):\n    _w0_mse,_w1_mse = What_mse[:,i]\n    _w0_bce,_w1_bce = What_bce[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\nAdam의 경우 손실함수 미분시 자동으로 scale을 조정해주고 계산된 기울기 값을 누적시켜 반영하기 때문에 잘 적합됨을 보인다.\n그렇다고 손실함수 상관없이 Adam을 무조건으로 선호하면 안된다.\n아무리 Adam이라고 해도 초기값 설정이 좋지 않으면 로컬미니멈에서 빠져나오기 힘들다.\n따라서 경우에 맞게 일차적으로 손실함수를 결정한 후 그 후에 옵티마이저를 결정하는 것이 옳은 판단!!"
  },
  {
    "objectID": "post/Lecture/STBD/2022-04-27-중간고사 해설.html",
    "href": "post/Lecture/STBD/2022-04-27-중간고사 해설.html",
    "title": "08. Mid term",
    "section": "",
    "text": "imports\n\nimport numpy as np\nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt \n\n\n\n1. 경사하강법과 tf.GradientTape()의 사용방법 (30점)\n- 문제 의도 : 손실함수와 로그우도함수, LSE와 MLE의 개념\n(1) 아래는 \\(X_i \\overset{iid}{\\sim} N(3,2^2)\\) 를 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tnp.random.randn(10000)*2+3\nx\n\n<tf.Tensor: shape=(10000,), dtype=float64, numpy=\narray([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n        5.01072291, -1.13050477])>\n\n\n함수 \\(L(\\mu,\\sigma)\\)을 최대화하는 \\((\\mu,\\sigma)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\mu\\)의 초기값은 2로 \\(\\sigma\\)의 초기값은 3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x_i-\\mu}{\\sigma})^2}\\]\n(풀이)\n\nsigma = tf.Variable(3.0) \nmu = tf.Variable(2.0)\n\n\nwith tf.GradientTape() as tape: \n    pdf = 1/sigma * tnp.exp(-0.5*((x-mu)/sigma)**2)\n    logL = tf.reduce_sum(tnp.log(pdf) ) \ntape.gradient(logL,[mu,sigma]) \n\n[<tf.Tensor: shape=(), dtype=float32, numpy=1129.3353>,\n <tf.Tensor: shape=(), dtype=float32, numpy=-1488.3431>]\n\n\n\nfor i in range(1000):\n    with tf.GradientTape() as tape: \n        pdf = 1/sigma * tnp.exp(-0.5*((x-mu)/sigma)**2)\n        logL = tf.reduce_sum(tnp.log(pdf) ) \n    slope1, slope2 = tape.gradient(logL,[mu,sigma]) \n    mu.assign_add(slope1* 0.1/10000) # N=10000 \n    sigma.assign_add(slope2* 0.1/10000) \n\n\nmu,sigma\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0163972>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9870595>)\n\n\n(2) 아래는 \\(X_i \\overset{iid}{\\sim} Ber(0.8)\\)을 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tf.constant(np.random.binomial(1,0.8,(10000,)))\nx\n\n<tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 1, 1, 1])>\n\n\n함수 \\(L(p)\\)을 최대화하는 \\(p\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(p\\)의 초기값은 0.3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=p^{x_i}(1-p)^{1-x_i}\\]\n(풀이)\n\np=tf.Variable(0.3) \nfor i in range(1000):\n    with tf.GradientTape() as tape: \n        pdf = p**x * (1-p)**(1-x) \n        logL = tf.reduce_sum(tnp.log(pdf)) \n    slope = tape.gradient(logL,p) \n    p.assign_add(slope* 0.1/10000) # N=10000 \n\n\np\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.802>\n\n\n(3) 아래의 모형에 따라서 \\(\\{Y_i\\}_{i=1}^{10000}\\)를 생성하는 코드를 작성하라. - \\(Y_i \\overset{iid}{\\sim} N(\\mu_i,1)\\) - \\(\\mu_i = \\beta_0 + \\beta_1 x_i = 0.5 + 2 x_i\\) , where \\(x_i = \\frac{i}{10000}\\)\n함수 \\(L(\\beta_0,\\beta_1)\\)을 최대화하는 \\((\\beta_0,\\beta_1)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\beta_0,\\beta_1\\)의 초기값은 모두 1로 설정할 것)\n\\[L(\\beta_0,\\beta_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(y_i)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y_i-\\mu_i)^2}, \\quad \\mu_i=\\beta_0+\\beta_1 x_i\\]\n(풀이)\n\nx= tf.constant(np.arange(1,10001)/10000)\ny= tnp.random.randn(10000) + (0.5 + 2*x) \n\n\nbeta0= tf.Variable(1.0)\nbeta1= tf.Variable(1.0) \nfor i in range(2000):\n    with tf.GradientTape() as tape: \n        mu = beta0 + beta1*x \n        pdf = tnp.exp(-0.5*(y-mu)**2)\n        logL = tf.reduce_sum(tnp.log(pdf)) \n    slope1, slope2 = tape.gradient(logL,[beta0,beta1]) \n    beta0.assign_add(slope1* 0.1/10000) # N=10000 \n    beta1.assign_add(slope2* 0.1/10000) \n\n\nbeta0, beta1\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.5553082>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.8987025>)\n\n\n\n\n2. 회귀분석의 이론적해와 tf.keras.optimizer 이용방법 (20점)\n- 문제 의도 : 효율적으로 학습률을 찾는 방법, 미니 배치의 개념 + 확률적 경사하강법\n아래와 같은 선형모형을 고려하자.\n\\[y_i = \\beta_0 + \\beta_1 x_i +\\epsilon_i.\\]\n이때 오차항은 정규분포로 가정한다. 즉 \\(\\epsilon_i \\overset{iid}{\\sim} N(0,\\sigma^2)\\)라고 가정한다.\n관측데이터가 아래와 같을때 아래의 물음에 답하라.\n\n#x= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])\n\nX= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4],\n              [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]])\ny= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , \n              63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286]).reshape(10,1)\n\n(1) MSE loss를 최소화 하는 \\(\\beta_0,\\beta_1\\)의 해석해를 구하라.\n(풀이)\n\ntf.linalg.inv(X.T @ X ) @ X.T @ y\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[9.94457323],\n       [2.21570461]])>\n\n\n(2) 경사하강법과 MSE loss의 도함수를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것.\n(풀이)\n\nbeta= tnp.array([5,10]).reshape(2,1) \n\n\nfor i in range(50000): \n    beta = beta - 0.0015 * (-2*X.T @y + 2*X.T@X@beta)/10 \n\n\nbeta\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[9.28579424],\n       [2.24168098]])>\n\n\n(3) tf.keras.optimizers의 apply_gradients()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n(풀이)\n\nbeta = tf.Variable(tnp.array([5.0,10.0]).reshape(2,1)) \nopt = tf.optimizers.SGD(0.0015) \nfor i in range(50000): \n    with tf.GradientTape() as tape: \n        loss = (y-X@beta).T @ (y-X@beta) / 10 \n    slope = tape.gradient(loss,beta) \n    opt.apply_gradients([(slope,beta)]) \n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[9.28579425],\n       [2.24168098]])>\n\n\n(4) tf.keras.optimizers의 minimize()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n(풀이)\n\nbeta = tf.Variable(tnp.array([5.0,10.0]).reshape(2,1)) \nopt = tf.optimizers.SGD(0.0015) \nloss_fn = lambda: (y-X@beta).T @ (y-X@beta) / 10 \nfor i in range(50000): \n    opt.minimize(loss_fn,beta)  \n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[9.28579425],\n       [2.24168098]])>\n\n\n\n\n3. keras를 이용한 풀이 (30점)\n- 문제 의도 : 복잡한 모형과 오버피팅의 개념\n(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i, \\quad i=0,1,\\dots, 999\\]\n여기에서 \\(t_i=\\frac{2\\pi i}{1000}\\) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.array(range(1000))* np.pi/1000\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.2)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2,3,1,0,0,0.5 이다)\n(풀이)\n\ny = y.reshape(1000,1)\nx1 = np.cos(t) \nx2 = np.cos(2*t)\nx3 = np.cos(3*t)\nx4 = np.cos(4*t)\nx5 = np.cos(5*t)\nX = tf.stack([x1,x2,x3,x4,x5],axis=1)\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(loss='mse',optimizer='sgd') \nnet.fit(X,y,batch_size=1000, epochs = 1000, verbose=0) \n\n<keras.callbacks.History at 0x7fb471478050>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense/kernel:0' shape=(5, 1) dtype=float32, numpy=\n array([[ 3.0008404e+00],\n        [ 1.0067019e+00],\n        [ 1.8562071e-03],\n        [-3.8460975e-03],\n        [ 4.9710521e-01]], dtype=float32)>,\n <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([-2.0122595], dtype=float32)>]\n\n\n(2) 아래와 같은 모형을 고려하자.\n\\[y_i \\sim Ber(\\pi_i), ~ \\text{where} ~ \\pi_i=\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\]\n위의 모형에서 관측한 데이터는 아래와 같다.\n\ntf.random.set_seed(43052)\nx = tnp.linspace(-1,1,2000) \ny = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64) \nplt.plot(x,y,'.',alpha=0.05)\n\n\n\n\ntf.keras를 이용하여 \\(w_0,w_1\\)을 추정하라. (참고: \\(w_0, w_1\\)에 대한 참값은 -1과 5이다.)\n(풀이)\n\nx= x.reshape(2000,1) \ny= y.reshape(2000,1) \n\n\nnet= tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='sgd', loss= tf.losses.binary_crossentropy) \nnet.fit(x,y,epochs=10000,batch_size=2000, verbose=0)\n\n<keras.callbacks.History at 0x7fb4719abb10>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_1/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[4.232856]], dtype=float32)>,\n <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([-0.90837014], dtype=float32)>]\n\n\n\nplt.plot(y,'.')\nplt.plot(net(x),'--')\n\n\n\n\n\n\n4. Piecewise-linear regression (15점)\n- 분석결과를 뜯어보는 방법\n아래의 모형을 고려하자.\nmodel: \\(y_i=\\begin{cases} x_i +0.3\\epsilon_i & x\\leq 0 \\\\ 3.5x_i +0.3\\epsilon_i & x>0 \\end{cases}\\)\n아래는 위의 모형에서 생성한 샘플이다.\n\n## data \nnp.random.seed(43052)\nN=100\nx= np.linspace(-1,1,N).reshape(N,1)\ny= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x<0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1)\n\n(1) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n$ = _0+_1x $\n\n\ntf.random.set_seed(43054) \nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감\n\n<keras.callbacks.History at 0x7fb4712b94d0>\n\n\n케라스에 의해 추정된 \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\)을 구하라.\n\nnet.weights\n\n[<tf.Variable 'dense_2/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[2.2616348]], dtype=float32)>,\n <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.6069048], dtype=float32)>]\n\n\n(풀이)\n\n\\(\\hat{\\beta}_0= 0.6069048\\)\n\\(\\hat{\\beta}_1= 2.2616348\\)\n\n(2) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n\\(\\boldsymbol{u}= x\\boldsymbol{W}^{(1)}+\\boldsymbol{b}^{(1)}\\)\n\\(\\boldsymbol{v}= \\text{relu}(u)\\)\n\\(yhat= \\boldsymbol{v}\\boldsymbol{W}^{(2)}+b^{(2)}\\)\n\n\ntf.random.set_seed(43056) \n## 1단계\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(2))\nnet.add(tf.keras.layers.Activation('relu')) \nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7f14b3dc7490>\n\n\n\\({\\boldsymbol u}\\)를 이용하여 \\({\\boldsymbol v}\\)를 만드는 코드와 \\({\\boldsymbol v}\\)를 이용하여 \\(yhat\\)를 만드는 코드를 작성하라.\n(풀이)\n\nu=net.layers[0](x)\nv=net.layers[1](u) \nyhat=net.layers[2](v) \n\n(3) 아래는 (1)-(2)번 모형에 대한 discussion이다. 올바른 것을 모두 골라라.\n(곤이) (2) 모형은 활성화함수로 relu를 사용하였다.\n(철용) (1) 모형에서 추정해야할 파라메터의 수는 2개이다.\n(아귀) (2) 모형이 (1) 모형보다 복잡한 모형이다.\n(짝귀) (1) 의 모형은 오버피팅의 위험이 있다.\n\n\n5. 다음을 잘 읽고 참과 거짓을 판단하라. (5점)\n(1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제나 전역최소해를 찾을 수 있다.\n(2) tf.GradeintTape()는 경사하강법을 이용하여 최적점을 찾아주는 tool이다.\n\n그냥 미분해주는 계산기이다.\n\n(3) 학습률이 크다는 것은 파라메터는 1회 업데이트 하는 양이 크다는 것을 의미한다.\n(4) 학습률이 크면 학습파라메터의 수렴속도가 빨라지지만 때때로 과적합에 빠질 수도 있다.\n\n과적합은 모형의 복잡도와 관려있는 것이지 학습률과는 연관이 없다\n\n(5) 단순회귀분석에서 MSE loss를 최소화 하는 해는 경사하강법을 이용하지 않아도 해석적으로 구할 수 있다."
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-01-(9주차).html",
    "href": "post/Lecture/STBD/2022-05-01-(9주차).html",
    "title": "09. MLE",
    "section": "",
    "text": "imports\n\nimport numpy as np\nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt \n\n\n\n우도함수와 최대우도추정량\n\n베이스 : 우도함수는 특정 모수가 주어졌을 때 추출된 샘플들이 얻어질 확률이지 해당 모수의 값에대한 확률을 의미하지 않는다.\n\n(예제)\n\\(X_i \\overset{iid}{\\sim} Ber(p)\\)에서 얻은 샘플이 아래와 같다고 하자.\n\nx=[0,1,0,1] \nx\n\n[0, 1, 0, 1]\n\n\n\\(p\\)는 얼마라고 볼 수 있는가? –> 0.5\n왜?? \\(p\\)가 0.5라고 주장할 수 있는 이론적 근거, 혹은 논리체계가 무엇인가?\n- suppose: \\(p=0.1\\) 이라고 하자.\n그렇다면 \\((x_1,x_2,x_3,x_4)=(0,1,0,1)\\)와 같은 샘플이 얻어질 확률이 아래와 같다.\n\n0.9 * 0.1 * 0.9 * 0.1 ## 우도함수 계산값\n\n0.008100000000000001\n\n\n- suppose: \\(p=0.2\\) 이라고 하자.\n그렇다면 \\((x_1,x_2,x_3,x_4)=(0,1,0,1)\\)와 같은 샘플이 얻어질 확률이 아래와 같다.\n\n0.8 * 0.2 * 0.8 * 0.2 ## 우도함수 계산값\n\n0.025600000000000008\n\n\n- 질문1: \\(p=0.1\\)인것 같냐? 아니면 \\(p=0.2\\)인것 같냐? -> 두 가지 경우 중 \\(p=0.2\\) 가 답인 것 같다. - 왜?? \\(p=0.2\\)일 확률이 더 크다! \\(\\to\\) 이렇게 말하기 어려움\n\n왜 어렵냐?? \\(\\to\\) 확률이 더 크다! 확률 이라는 단어를 함부로 쓸 수가 없다\n\n\n(여기서 잠깐 중요한것) 확률이라는 말을 함부로 쓸 수 없다.\n- 0.0256은 “\\(p=0.2\\)일 경우 샘플 (0,1,0,1)이 얻어질 확률”이지 “\\(p=0.2\\)일 확률”은 아니다.\\((\\star\\star\\star)\\)\n“\\(p=0.2\\)인 확률” 이라는 개념이 성립하려면 아래코드에서 sum([(1-p)*p*(1-p)*p for p in _plist])이 1보다는 작아야 한다. (그런데 1보다 크다)\n즉, 아래와 같이 p=0.499일 때 샘플 (0,1,0,1)이 얻어질 확률\n\n(1-0.499)*0.499*(1-0.499)*0.499\n\n0.06249950000099999\n\n\n\n_plist = np.linspace(0.499,0.501,1000) \n_prob=[(1-p)*p*(1-p)*p for p in _plist]\n\n\n_prob[:1] # p 가 0.499일확률\n_prob[2] # p가 0.499 + epsilon\n\n0.06249950399697206\n\n\n아래와 같이 p의 합이 1보다 크기 때문에 \\(p=0.1\\,\\, or \\,\\, p=0.2\\)일 확률이다 라고 정의할수 없다.\n\nsum(_prob)\n\n62.49983299986714\n\n\n따라서 우도함수(가능도)는 주어진 확률값에 대한 임의의 샘플들이 얻어질 확률이라고 정의하자.\n- 확률이라는 말을 쓸 수 없지만 확률의 느낌은 있음 -> 가능도라는 말을 쓰자. - 0.0256 \\(=\\) \\(p\\)가 0.2일 경우 샘플 (0,1,0,1)이 얻어질 확률 \\(\\to\\) \\(p\\)가 0.2일 가능도\n\n- 다시 질문1로 돌아가자! - 질문1: \\(p=0.1\\)인 것 같냐? 아니면 \\(p=0.2\\)인 것 같냐? -> 답 \\(p=0.2\\) -> 왜? \\(p=0.2\\)인 가능도가 더 크니까! - 질문2: \\(p=0.2\\)인 것 같냐? 아니면 \\(p=0.3\\)인 것 같냐? -> 답 \\(p=0.3\\) -> 왜? \\(p=0.3\\)인 가능도가 더 크니까!\n- 궁극의 질문: \\(p\\)가 뭐일 것 같아? - \\(p\\)가 입력으로 들어가면 가능도가 계산되는 함수를 만들자. - 그 함수를 최대화하는 \\(p\\)를 찾자. - 그 \\(p\\)가 궁극의 질문에 대한 대답이 된다.\n- 잠깐 용어정리 - 가능도함수 \\(=\\) 우도함수 \\(=\\) likelihood function \\(:=\\) \\(L(p)\\) - \\(p\\)의 maximum likelihood estimator \\(=\\) p의 MLE \\(=\\) \\(\\hat{p}^{mle}\\) \\(=\\) \\(\\text{argmax}_p L(p)\\) \\(=\\) \\(\\hat{p}\\)\n\n\n중간고사 1번\n(1) \\(N(\\mu,\\sigma)\\)에서 얻은 샘플이 아래와 같다고 할때 \\(\\mu,\\sigma\\)의 MLE를 구하여라.\n<tf.Tensor: shape=(10000,), dtype=float64, numpy=\narray([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n        5.01072291, -1.13050477])>\n(2) \\(Ber(p)\\)에서 얻은 샘플이 아래와 같다고 할 때 \\(p\\)의 MLE를 구하여라.\n<tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 0, 0, 1])>\n(3) \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\), \\(\\epsilon_i \\overset{iid}{\\sim} N(0,1)\\) 일때 \\((\\beta_0,\\beta_1)\\)의 MLE를 구하여라. (회귀모형)\n(풀이) 가능도함수\n\\[L(\\beta_0,\\beta_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(y_i)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y_i-\\mu_i)^2}, \\quad \\mu_i=\\beta_0+\\beta_1 x_i\\]\n를 최대화하는 \\(\\beta_0,\\beta_1\\)을 구하면된다. 그런데 이것은 아래를 최소화하는 \\(\\beta_0,\\beta_1\\)을 구하는 것과 같다.\n\\[-\\log L(\\beta_0,\\beta_1) = \\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2\\]\n위의 식은 SSE와 같다. 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 MSE를 최소화하는 \\(\\beta_0,\\beta_1\\)을 구하면 된다.\n중간고사 1-(3)의 다른 풀이\nstep1: 생성\n\nx= tf.constant(np.arange(1,10001)/10000)\ny= tnp.random.randn(10000) + (0.5 + 2*x) \n\nstep2: minimize MSEloss (원래는 maximize log-likelihood)\n\nmaximize likelihood였던 문제를 minimize MSEloss로 바꾸어도 되는근거? 주어진 함수(=가능도함수)를 최대화하는 \\(\\beta_0,\\beta_1\\)은 MSE를 최소화하는 \\(\\beta_0,\\beta_1\\)과 동일하므로\n\n\nbeta0= tf.Variable(1.0)\nbeta1= tf.Variable(1.0) \nfor i in range(2000):\n    with tf.GradientTape() as tape: \n        #minus_log_likelihood = tf.reduce_sum((y-beta0-beta1*x)**2)\n        loss =  tf.reduce_sum((y-beta0-beta1*x)**2)\n    slope1, slope2 = tape.gradient(loss,[beta0,beta1]) \n    beta0.assign_sub(slope1* 0.1/10000) # N=10000 \n    beta1.assign_sub(slope2* 0.1/10000) \n\n\nbeta0,beta1\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.4626273>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0632904>)\n\n\n- 문제를 풀면서 생각해보니 손실함수는 -로그가능도함수로 선택하면 될 것 같다? - 손실함수를 선택하는 기준이 -로그가능도함수만 존재하는 것은 아니나 대부분 그러하긴함\n(4) 출제하지 못한 중간고사 문제\n아래의 모형을 생각하자. - \\(Y_i \\overset{iid}{\\sim} Ber(\\pi_i)\\) - \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}=\\frac{\\exp(-1+5x_i)}{1+\\exp(-1+5x_i)}\\)\n아래는 위의 모형에서 얻은 샘플이다.\n\nx = tnp.linspace(-1,1,2000)\npi = tnp.exp(-1+5*x) / (1+tnp.exp(-1+5*x))\ny = np.random.binomial(1,pi)\ny = tf.constant(y)\n\n함수 \\(L(w_0,w_1)\\)을 최대화하는 \\((w_0,w_1)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\((w_0,w_1)\\)의 초기값은 모두 0.1로 설정할 것)\n\\[L(w_0,w_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(x_i)={\\pi_i}^{y_i}(1-\\pi_i)^{1-y_i},\\quad \\pi_i=\\text{sigmoid}(w_0+w_1x_i)\\]\n(풀이1)\n\nw0hat = tf.Variable(1.0) \nw1hat = tf.Variable(1.0) \n\n\nfor i in range(1000): \n    with tf.GradientTape() as tape: \n        pihat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x))\n        pdf = pihat**y * (1-pihat)**(1-y) \n        logL = tf.reduce_mean(tnp.log(pdf)) \n    slope1,slope2 = tape.gradient(logL,[w0hat,w1hat])\n    w0hat.assign_add(slope1*0.1) \n    w1hat.assign_add(slope2*0.1) \n\n\nw0hat,w1hat\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.8487661>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.1949835>)\n\n\n(해석) - 로지스틱에서 가능도함수와 BCEloss의 관계\n\\(L(w_0,w_1)\\)를 최대화하는 \\(w_0,w_1\\)은 아래를 최소화하는 \\(w_0,w_1\\)와 같다.\n\\[-\\log L(w_0,w_1) = - \\sum_{i=1}^{n}\\big(y_i \\log(\\pi_i) + (1-y_i)\\log(1-\\pi_i)\\big)\\]\n이것은 최적의 \\(w_0,w_1\\)을 \\(\\hat{w}_0,\\hat{w}_1\\)이라고 하면 \\(\\hat{\\pi}_i=\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\hat{y}_i\\)이 되고 따라서 위의 식은 \\(n\\times\\)BCEloss의 형태임을 쉽게 알 수 있다.\n결국 로지스틱 모형에서 \\((w_0,w_1)\\)의 MLE를 구하기 위해서는 BCEloss를 최소화하는 \\((w_0,w_1)\\)을 구하면 된다!\n(풀이2)\n\nw0hat = tf.Variable(1.0) \nw1hat = tf.Variable(1.0) \n\n\nfor i in range(1000): \n    with tf.GradientTape() as tape: \n        yhat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x))\n        loss = tf.losses.binary_crossentropy(y,yhat)\n    slope1,slope2 = tape.gradient(loss,[w0hat,w1hat])\n    w0hat.assign_sub(slope1*0.1) \n    w1hat.assign_sub(slope2*0.1) \n\n\nw0hat,w1hat\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.8487661>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.1949835>)\n\n\n\n\n손실함수의 설계 (선택)\n- 회귀분석이든 로지스틱이든 손실함수는 minus_log_likelihood 로 선택한다. - 그런데 (오차항이 정규분포인) 회귀분석 일때는 minus_log_likelihood 가 MSEloss가 되고 - 로지스틱일때는 minus_log_likelihood 가 BCEloss가 된다\n- minus_log_likelihood가 손실함수를 선택하는 유일한 기준은 아니다. <— 참고만하세요, 이 수업에서는 안중요합니다. - 오차항이 대칭이고 서로독립이며 등분산 가정을 만족하는 어떠한 분포에서의 회귀모형이 있다고 하자. 이 회귀모형에서 \\(\\hat{\\beta}\\)은 여전히 MSEloss를 최소화하는 \\(\\beta\\)를 구함으로써 얻을 수 있다.\n\n그러나 샘플 추출시 임의의 분포 가정을 가정하기 때문에 분포를 모를 경우 minus_log_likelihood를 구할 수 없다. 그러면 왜 이것을 썻는가? 위 같은 분포 가정을 하여기 때문이다!!\n이 경우 MSEloss를 쓰는 이론적근거? \\(\\hat{\\beta}\\)이 BLUE(Best Linear Unbiased Estimator)가 되기 때문임 (가우스-마코프정리)"
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-02-lecture.html",
    "href": "post/Lecture/STBD/2022-05-02-lecture.html",
    "title": "10. 확률적 경사하강법",
    "section": "",
    "text": "import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow.experimental.numpy as tnp\n\n\ntf.config.experimental.list_physical_devices()\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')"
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-02-lecture.html#중간고사-관련-잡담",
    "href": "post/Lecture/STBD/2022-05-02-lecture.html#중간고사-관련-잡담",
    "title": "10. 확률적 경사하강법",
    "section": "중간고사 관련 잡담",
    "text": "중간고사 관련 잡담\n\n중간고사 3번문제\n- 특이한 모형 : 오버핏이 일어날 수 없는 모형이다.\n- 회귀분석은 과적합이 안된다! \\(\\to\\) 알아서 \\(n\\)이 커질수록 유의미하지 않은 변수들을 거슬러줌 * 모형이 스스로 변수에 coffecient에 대한 유의성 검정을 수행 따라서 과적합 이슈가 없다 * keypoint : 무조건 변수가 많다고 과적합이 일어나지 않음 * 빨강, 노랑, 파랑으로 모든 색깔을 표현 가능(\\(\\star\\star\\star\\))\n\n\n중간고사 1-(3)번 문제"
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-02-lecture.html#경사하강법과-확률적경사하강법",
    "href": "post/Lecture/STBD/2022-05-02-lecture.html#경사하강법과-확률적경사하강법",
    "title": "10. 확률적 경사하강법",
    "section": "경사하강법과 확률적경사하강법",
    "text": "경사하강법과 확률적경사하강법\n\n확률적 경사하강법을 오늘부터 배울 거임!!\n\n\nver1: 모든 샘플을 사용하여 slope계산 (Gradient decent)\n\n기본까지 했던 방법\n10개의 샘플을 관측!\n\n(epoch1) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\n\n\nver2: 하나의 샘플만 사용하여 slope계산(stochastic gradient descent)\n\n(epoch이 3번이면 for문이 30번돌아감)\n\n(epoch1) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n\n총 쏘는거 생각 : ver2는 동일한 총알이 주어졌을 때 그냥 막 쏘는거임\n동일 iteration 대비 효율이 좋은 것은 ver1이다.\n\n왜냐 ver1의경우 iteration == epoch이기 때문!\n\n동일 epoch 대비 효율이 좋은 것도 ver1이다.\n정확도 또한 ver1이 더 높을 것이다. 그러나 이터레이셧 횟수는 ver2가 더 많을 것이다.\nver1은 모든 샘플을 고려해 한번에 기울기를 계산(질로 승부), ver2는 양으로 승부하는 것이라고 생각한다.\n\n\n\nver3: \\(m(\\leq n)\\)개의 샘플만 사용하여 slope계산\n\\(m=3\\)이라고 하자.\n(epoch1) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\n\nVer 3의 겨우 파라미터 업데이트시 한번도 사용이 안되는 샘플이 존재할 수 가 있다.\n\n\n\n용어의 정리\n\n옛날 (좀 더 엄밀)\n- ver1: gradient descent, batch gradient descent\n- ver2: stochastic gradient descent (확률적 경사 하강법)\n- ver3: mini-batch gradient descent, mini-batch stochastic gradient descent\n\n\n요즘\n- ver1: gradient descent\n- ver2: stochastic gradient descent with batch size = 1\n- ver3: stochastic gradient descent (확률적 경사하강법) - https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고.\nnote: 이렇게 많이 쓰는 이유? ver1,2는 사실상 없는 방법이므로\n\n\n\nver1,2,3 이외에 좀 더 지저분한 것들이 있다.\n- ver2,3에서 샘플을 셔플할 수도 있다.\n- ver3에서 일부 샘플이 학습에 참여 안하는 버전도 있다.\n- 개인적 생각: 크게3개정도만 알면 괜찮고 나머지는 그렇게 유의미하지 않아보인다.\n\n\nDiscussion\n- 핵심개념 - 메모리사용량: ver1 > ver3 > ver2 (한번에 한개의 파라미터를 업데이트 할 때!!) - 계산속도: ver1 > ver3 > ver2 (한번에 한개의 파라미터를 업데이트 할 때!!) - local-min에 갇힘: ver1> ver3 > ver2 (이건 알고리즘 구성의 차이)\n* ver1 은 local-min을 잘 찾는다 * ver2, ver3은 운좋게 local-min을 탈출한다\n- 본질: GPU 메모리가 한정되어 있어서 ver1을 쓰지는 못한다. GPU 메모리를 가장 적게쓰는것은 ver2인데 이것은 너무 불안정하다.\n- 틀리진 않지만 어색한 블로그 정리 내용들 - 경사하강법은 종종 국소최소점에 갇히는 문제가 있다. 이를 해결하기 위해서 등장한 방법이 확률적 경사하강법이다.(X) * 틀린말은 아니나 그것을 의도하고 만든 것은 아님, 가끔 그럴 때도 있는 것이지 확률적 경사하강법을 쓴다고 local_minimum에 빠지지 않는 것은 아니다. - 경사하강법은 계산시간이 오래걸린다(O). 계산을 빠르게 하기 위해서 등장한 방법이 확률적 경사하강법이다.(O) * 1회 업데이트는 빠르게 계산하나, 하지만 그것이 최적의 \\(\\beta\\)를 빠르게 얻을 수 있는 것은 아님\n\n결론\n\n확률적 경사하강법을 쓰는 이유는 메모리적 문제이다. 메모리를 아끼기 위해 우리는 tf.Variable을 이용한다. 그런데 Gradient를 계산하려면 loss를 계산해야 하고 샘플 (y,yhat)을 전부 메모리상에 올려야한다.\n위 같은 경우 GPU가 샘플의 일부만 올라갈 수 있다면 미니배치방법인 확률적 경사하강법을 사용해야기 때문에 확률적경사하강법을 우리가 사용하는 이유다."
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-02-lecture.html#fashion_mnist-모듈",
    "href": "post/Lecture/STBD/2022-05-02-lecture.html#fashion_mnist-모듈",
    "title": "10. 확률적 경사하강법",
    "section": "fashion_mnist 모듈",
    "text": "fashion_mnist 모듈\n\ntf.keras.datasets.fashion_mnist.load_data()\n\ntype(tf.keras.datasets.fashion_mnist)\n\nmodule\n\n\n\n\n데이터생성 및 탐색\n- tf.keras.datasets.fashion_mnist.load_data()를 이용한 데이터 생성\n\ntype(tf.keras.datasets.fashion_mnist.load_data)\n\nfunction\n\n\n\n__call__ : 오브젝트가 숨겨져있음 \\(\\to\\) 괄호를 열고 닫으면 오브젝트가 생성됨\nenter 와 exit가 없으면 with를 같이 쓸 수 없음\n\n\ndir(tf.keras.datasets.fashion_mnist.load_data)\n\n['__annotations__',\n '__call__',\n '__class__',\n '__closure__',\n '__code__',\n '__defaults__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__get__',\n '__getattribute__',\n '__globals__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__kwdefaults__',\n '__le__',\n '__lt__',\n '__module__',\n '__name__',\n '__ne__',\n '__new__',\n '__qualname__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '_keras_api_names',\n '_keras_api_names_v1']\n\n\n\ntf.keras.datasets.fashion_mnist.load_data??\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n32768/29515 [=================================] - 0s 0us/step\n40960/29515 [=========================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26427392/26421880 [==============================] - 0s 0us/step\n26435584/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n16384/5148 [===============================================================================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4423680/4422102 [==============================] - 0s 0us/step\n4431872/4422102 [==============================] - 0s 0us/step\n\n\n\nx_train.shape, y_train.shape\n\n((60000, 28, 28), (60000,))\n\n\n\nx_test.shape, y_test.shape\n\n((10000, 28, 28), (10000,))\n\n\n\n코드를 바로 뜯어보고 구글링은 최후 수단으로 하자\n\n\n\n데이터구조\n\n#x_train[0] ## 첫번째 관측치\n\n\nx_train[0].shape\n\n(28, 28)\n\n\n\nplt.imshow(x_train[0])\n\n<matplotlib.image.AxesImage at 0x7f44628357d0>\n\n\n\n\n\n\nnp.unique(y_train)\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)\n\n\n\nnp.where(y_train==9)\n\n(array([    0,    11,    15, ..., 59932, 59970, 59978]),)\n\n\n\n오 왠지 11번째 데이터도 신발일 것 같에\n\n\nplt.imshow(x_train[11])\n\n<matplotlib.image.AxesImage at 0x7f446239ba50>\n\n\n\n\n\n- \\(\\bf{X} : (n,28,28)\\)\n- \\(y\\) : 각 이미지의 라벨 데이터"
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-02-lecture.html#예제1",
    "href": "post/Lecture/STBD/2022-05-02-lecture.html#예제1",
    "title": "10. 확률적 경사하강법",
    "section": "예제1",
    "text": "예제1\n\n데이터 정리\n- \\(y=0,1\\)에 대응하는 이미지만 정리하자. (우리가 배운건 로지스틱이니깐)\n\ny=y_train[(y_train == 0) | (y_train == 1)].reshape(-1,1)\nyy= y_test[(y_test == 0) | (y_test == 1)].reshape(-1,1)\n\n\nX=x_train[(y_train == 0) | (y_train == 1)]\nXX= x_test[(y_test == 0) | (y_test== 1)]\n\n\nX.shape\n\n(12000, 28, 28)\n\n\n\ny.shape\n\n(12000, 1)\n\n\n\n28*28\n\n784\n\n\n\nX=x_train[(y_train == 0) | (y_train == 1)].reshape(-1,784)\nXX= x_test[(y_test == 0) | (y_test== 1)].reshape(-1,784)\n\n\nX.shape,XX.shape\n\n((12000, 784), (2000, 784))\n\n\n\n\n풀이1: 은닉층을 포함한 신경망 // epochs=100\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \n    \"x784\" -> \"node1\"\n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node30\"\n    \"x2\" -> \"node30\"\n    \"..\" -> \"node30\"\n    \"x784\" -> \"node30\"\n\n\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"y\"\n    \"node2\" -> \"y\"\n    \"...\" -> \"y\"\n    \"node30\" -> \"y\"\n    label = \"Layer 2: sigmoid\"\n}\n''')\n\n\n\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='sgd',loss=tf.losses.binary_crossentropy)\nnet.fit(X,y,epochs=100,batch_size=12000,verbose=0) \n\n<keras.callbacks.History at 0x7f44601b7990>\n\n\n\nloss는 미분하기위한 정보지 성능에 대한 정보는 아니다.\ntrain\n\n\nnp.mean((net(X)>0.5) == y.reshape(12000,1))\n\n0.5000833333333333\n\n\n\ntest\n\n\nnp.mean((net(XX)>0.5) == yy.reshape(2000,1))\n\n0.5\n\n\n\nlocal_min에 빠져서 못 나오는 것임(verbose=1로하고 loss를 살펴볼 줄 알아야한다)\n초기값 문제가 아닌 옵티마이저의 문제임\\((\\star\\star\\star)\\)\n\n\n\n풀이2: 옵티마이저 개선\n\ntf.random.set_seed(43051)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy)\nnet.fit(X,y,epochs=100,batch_size=12000,verbose=0) \n\n<keras.callbacks.History at 0x7f445e8ef250>\n\n\n\nnp.mean((net(X)>0.5) == y.reshape(12000,1))\n\n0.9919166666666667\n\n\n\nnp.mean((net(XX)>0.5) == yy.reshape(2000,1))\n\n0.9855\n\n\n\n\n풀이3: 컴파일시 metrics=[‘accuracy’] 추가\n\ntf.random.set_seed(43055)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy,metrics=['accuracy'])\nnet.fit(X,y,epochs=100,batch_size=12000) \n\nEpoch 1/100\n1/1 [==============================] - 0s 486ms/step - loss: 100.9425 - accuracy: 0.4988\nEpoch 2/100\n1/1 [==============================] - 0s 82ms/step - loss: 44.4441 - accuracy: 0.3741\nEpoch 3/100\n1/1 [==============================] - 0s 75ms/step - loss: 29.2322 - accuracy: 0.4321\nEpoch 4/100\n1/1 [==============================] - 0s 109ms/step - loss: 22.6921 - accuracy: 0.5399\nEpoch 5/100\n1/1 [==============================] - 0s 69ms/step - loss: 8.7741 - accuracy: 0.7321\nEpoch 6/100\n1/1 [==============================] - 0s 63ms/step - loss: 4.6409 - accuracy: 0.8516\nEpoch 7/100\n1/1 [==============================] - 0s 65ms/step - loss: 5.2642 - accuracy: 0.8711\nEpoch 8/100\n1/1 [==============================] - 0s 69ms/step - loss: 6.1993 - accuracy: 0.8771\nEpoch 9/100\n1/1 [==============================] - 0s 66ms/step - loss: 6.5543 - accuracy: 0.8845\nEpoch 10/100\n1/1 [==============================] - 0s 68ms/step - loss: 6.3454 - accuracy: 0.8953\nEpoch 11/100\n1/1 [==============================] - 0s 66ms/step - loss: 5.7887 - accuracy: 0.9062\nEpoch 12/100\n1/1 [==============================] - 0s 67ms/step - loss: 5.1074 - accuracy: 0.9168\nEpoch 13/100\n1/1 [==============================] - 0s 69ms/step - loss: 4.4821 - accuracy: 0.9276\nEpoch 14/100\n1/1 [==============================] - 0s 71ms/step - loss: 3.9864 - accuracy: 0.9359\nEpoch 15/100\n1/1 [==============================] - 0s 64ms/step - loss: 3.6388 - accuracy: 0.9402\nEpoch 16/100\n1/1 [==============================] - 0s 71ms/step - loss: 3.4077 - accuracy: 0.9412\nEpoch 17/100\n1/1 [==============================] - 0s 65ms/step - loss: 3.2687 - accuracy: 0.9415\nEpoch 18/100\n1/1 [==============================] - 0s 72ms/step - loss: 3.1829 - accuracy: 0.9404\nEpoch 19/100\n1/1 [==============================] - 0s 63ms/step - loss: 3.1196 - accuracy: 0.9388\nEpoch 20/100\n1/1 [==============================] - 0s 71ms/step - loss: 3.0526 - accuracy: 0.9380\nEpoch 21/100\n1/1 [==============================] - 0s 65ms/step - loss: 2.9635 - accuracy: 0.9371\nEpoch 22/100\n1/1 [==============================] - 0s 80ms/step - loss: 2.8397 - accuracy: 0.9376\nEpoch 23/100\n1/1 [==============================] - 0s 66ms/step - loss: 2.6812 - accuracy: 0.9389\nEpoch 24/100\n1/1 [==============================] - 0s 65ms/step - loss: 2.4916 - accuracy: 0.9396\nEpoch 25/100\n1/1 [==============================] - 0s 81ms/step - loss: 2.2804 - accuracy: 0.9408\nEpoch 26/100\n1/1 [==============================] - 0s 65ms/step - loss: 2.0630 - accuracy: 0.9433\nEpoch 27/100\n1/1 [==============================] - 0s 74ms/step - loss: 1.8600 - accuracy: 0.9470\nEpoch 28/100\n1/1 [==============================] - 0s 70ms/step - loss: 1.6744 - accuracy: 0.9488\nEpoch 29/100\n1/1 [==============================] - 0s 68ms/step - loss: 1.5003 - accuracy: 0.9510\nEpoch 30/100\n1/1 [==============================] - 0s 65ms/step - loss: 1.3529 - accuracy: 0.9531\nEpoch 31/100\n1/1 [==============================] - 0s 74ms/step - loss: 1.2575 - accuracy: 0.9542\nEpoch 32/100\n1/1 [==============================] - 0s 68ms/step - loss: 1.1763 - accuracy: 0.9553\nEpoch 33/100\n1/1 [==============================] - 0s 69ms/step - loss: 1.0853 - accuracy: 0.9567\nEpoch 34/100\n1/1 [==============================] - 0s 78ms/step - loss: 0.9978 - accuracy: 0.9587\nEpoch 35/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.9337 - accuracy: 0.9603\nEpoch 36/100\n1/1 [==============================] - 0s 65ms/step - loss: 0.8893 - accuracy: 0.9617\nEpoch 37/100\n1/1 [==============================] - 0s 69ms/step - loss: 0.8503 - accuracy: 0.9627\nEpoch 38/100\n1/1 [==============================] - 0s 66ms/step - loss: 0.8154 - accuracy: 0.9632\nEpoch 39/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.7843 - accuracy: 0.9642\nEpoch 40/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.7548 - accuracy: 0.9654\nEpoch 41/100\n1/1 [==============================] - 0s 65ms/step - loss: 0.7288 - accuracy: 0.9663\nEpoch 42/100\n1/1 [==============================] - 0s 70ms/step - loss: 0.7061 - accuracy: 0.9674\nEpoch 43/100\n1/1 [==============================] - 0s 65ms/step - loss: 0.6844 - accuracy: 0.9687\nEpoch 44/100\n1/1 [==============================] - 0s 66ms/step - loss: 0.6640 - accuracy: 0.9693\nEpoch 45/100\n1/1 [==============================] - 0s 77ms/step - loss: 0.6427 - accuracy: 0.9710\nEpoch 46/100\n1/1 [==============================] - 0s 75ms/step - loss: 0.6187 - accuracy: 0.9716\nEpoch 47/100\n1/1 [==============================] - 0s 65ms/step - loss: 0.5933 - accuracy: 0.9723\nEpoch 48/100\n1/1 [==============================] - 0s 75ms/step - loss: 0.5693 - accuracy: 0.9730\nEpoch 49/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.5471 - accuracy: 0.9733\nEpoch 50/100\n1/1 [==============================] - 0s 70ms/step - loss: 0.5253 - accuracy: 0.9737\nEpoch 51/100\n1/1 [==============================] - 0s 65ms/step - loss: 0.5031 - accuracy: 0.9744\nEpoch 52/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.4805 - accuracy: 0.9750\nEpoch 53/100\n1/1 [==============================] - 0s 66ms/step - loss: 0.4572 - accuracy: 0.9767\nEpoch 54/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.4368 - accuracy: 0.9775\nEpoch 55/100\n1/1 [==============================] - 0s 73ms/step - loss: 0.4180 - accuracy: 0.9778\nEpoch 56/100\n1/1 [==============================] - 0s 69ms/step - loss: 0.3991 - accuracy: 0.9783\nEpoch 57/100\n1/1 [==============================] - 0s 62ms/step - loss: 0.3828 - accuracy: 0.9791\nEpoch 58/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.3701 - accuracy: 0.9793\nEpoch 59/100\n1/1 [==============================] - 0s 78ms/step - loss: 0.3568 - accuracy: 0.9793\nEpoch 60/100\n1/1 [==============================] - 0s 66ms/step - loss: 0.3426 - accuracy: 0.9805\nEpoch 61/100\n1/1 [==============================] - 0s 78ms/step - loss: 0.3286 - accuracy: 0.9813\nEpoch 62/100\n1/1 [==============================] - 0s 70ms/step - loss: 0.3165 - accuracy: 0.9822\nEpoch 63/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.3051 - accuracy: 0.9827\nEpoch 64/100\n1/1 [==============================] - 0s 73ms/step - loss: 0.2929 - accuracy: 0.9827\nEpoch 65/100\n1/1 [==============================] - 0s 69ms/step - loss: 0.2836 - accuracy: 0.9827\nEpoch 66/100\n1/1 [==============================] - 0s 70ms/step - loss: 0.2753 - accuracy: 0.9830\nEpoch 67/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.2664 - accuracy: 0.9835\nEpoch 68/100\n1/1 [==============================] - 0s 65ms/step - loss: 0.2574 - accuracy: 0.9843\nEpoch 69/100\n1/1 [==============================] - 0s 66ms/step - loss: 0.2493 - accuracy: 0.9845\nEpoch 70/100\n1/1 [==============================] - 0s 69ms/step - loss: 0.2411 - accuracy: 0.9852\nEpoch 71/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.2335 - accuracy: 0.9852\nEpoch 72/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.2267 - accuracy: 0.9852\nEpoch 73/100\n1/1 [==============================] - 0s 75ms/step - loss: 0.2198 - accuracy: 0.9858\nEpoch 74/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.2137 - accuracy: 0.9866\nEpoch 75/100\n1/1 [==============================] - 0s 71ms/step - loss: 0.2075 - accuracy: 0.9866\nEpoch 76/100\n1/1 [==============================] - 0s 71ms/step - loss: 0.2013 - accuracy: 0.9873\nEpoch 77/100\n1/1 [==============================] - 0s 69ms/step - loss: 0.1954 - accuracy: 0.9875\nEpoch 78/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.1903 - accuracy: 0.9879\nEpoch 79/100\n1/1 [==============================] - 0s 69ms/step - loss: 0.1850 - accuracy: 0.9883\nEpoch 80/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.1801 - accuracy: 0.9882\nEpoch 81/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.1751 - accuracy: 0.9886\nEpoch 82/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.1708 - accuracy: 0.9893\nEpoch 83/100\n1/1 [==============================] - 0s 65ms/step - loss: 0.1671 - accuracy: 0.9893\nEpoch 84/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.1629 - accuracy: 0.9893\nEpoch 85/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.1592 - accuracy: 0.9898\nEpoch 86/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.1553 - accuracy: 0.9898\nEpoch 87/100\n1/1 [==============================] - 0s 78ms/step - loss: 0.1514 - accuracy: 0.9898\nEpoch 88/100\n1/1 [==============================] - 0s 71ms/step - loss: 0.1479 - accuracy: 0.9900\nEpoch 89/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.1441 - accuracy: 0.9900\nEpoch 90/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.1409 - accuracy: 0.9899\nEpoch 91/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.1373 - accuracy: 0.9902\nEpoch 92/100\n1/1 [==============================] - 0s 70ms/step - loss: 0.1340 - accuracy: 0.9902\nEpoch 93/100\n1/1 [==============================] - 0s 71ms/step - loss: 0.1305 - accuracy: 0.9904\nEpoch 94/100\n1/1 [==============================] - 0s 66ms/step - loss: 0.1275 - accuracy: 0.9909\nEpoch 95/100\n1/1 [==============================] - 0s 71ms/step - loss: 0.1242 - accuracy: 0.9908\nEpoch 96/100\n1/1 [==============================] - 0s 69ms/step - loss: 0.1213 - accuracy: 0.9908\nEpoch 97/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.1184 - accuracy: 0.9910\nEpoch 98/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.1157 - accuracy: 0.9911\nEpoch 99/100\n1/1 [==============================] - 0s 71ms/step - loss: 0.1132 - accuracy: 0.9912\nEpoch 100/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.1110 - accuracy: 0.9917\n\n\n<keras.callbacks.History at 0x7f44600a41d0>\n\n\n\nnet.evaluate(X,y)\n\n375/375 [==============================] - 1s 2ms/step - loss: 0.1086 - accuracy: 0.9918\n\n\n[0.10858089476823807, 0.9917500019073486]\n\n\n\nnet.evaluate(XX,yy)\n\n63/63 [==============================] - 0s 2ms/step - loss: 0.2933 - accuracy: 0.9800\n\n\n[0.2932808995246887, 0.9800000190734863]\n\n\n\n\n풀이4: 확률적경사하강법 이용 // epochs=10 (Ver2)\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy,metrics=['accuracy'])\nnet.fit(X,y,epochs=10,batch_size=120) \n\nEpoch 1/10\n100/100 [==============================] - 1s 3ms/step - loss: 5.6484 - accuracy: 0.9418\nEpoch 2/10\n100/100 [==============================] - 0s 4ms/step - loss: 0.5078 - accuracy: 0.9793\nEpoch 3/10\n100/100 [==============================] - 0s 3ms/step - loss: 0.3784 - accuracy: 0.9818\nEpoch 4/10\n100/100 [==============================] - 0s 3ms/step - loss: 0.3390 - accuracy: 0.9828\nEpoch 5/10\n100/100 [==============================] - 0s 4ms/step - loss: 0.2474 - accuracy: 0.9857\nEpoch 6/10\n100/100 [==============================] - 0s 3ms/step - loss: 0.2116 - accuracy: 0.9870\nEpoch 7/10\n100/100 [==============================] - 0s 3ms/step - loss: 0.1743 - accuracy: 0.9889\nEpoch 8/10\n100/100 [==============================] - 0s 3ms/step - loss: 0.1374 - accuracy: 0.9899\nEpoch 9/10\n100/100 [==============================] - 0s 3ms/step - loss: 0.1570 - accuracy: 0.9891\nEpoch 10/10\n100/100 [==============================] - 0s 4ms/step - loss: 0.1097 - accuracy: 0.9915\n\n\n<keras.callbacks.History at 0x7f445ff81490>\n\n\n\nnet.evaluate(X,y)\n\n375/375 [==============================] - 1s 2ms/step - loss: 0.0889 - accuracy: 0.9933\n\n\n[0.08887288719415665, 0.9932500123977661]\n\n\n\nnet.evaluate(XX,yy)\n\n63/63 [==============================] - 0s 2ms/step - loss: 0.2973 - accuracy: 0.9845\n\n\n[0.2972556948661804, 0.984499990940094]\n\n\n\n풀이 4는 정확도가 초기값이 너무 좋음\n왜 업데이트를 한번의 에폭에서 현재 100번을 수행했기 때문!!\n배치사이즈는 보통 메모리에 맞추어 올린다"
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-08-lecture.html",
    "href": "post/Lecture/STBD/2022-05-08-lecture.html",
    "title": "11. softmax",
    "section": "",
    "text": "import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow.experimental.numpy as tnp\n\n\ntf.config.experimental.list_physical_devices()\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')"
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-08-lecture.html#평가지표",
    "href": "post/Lecture/STBD/2022-05-08-lecture.html#평가지표",
    "title": "11. softmax",
    "section": "평가지표",
    "text": "평가지표\n\n다양한 평가지표들\n- 의문: 왜 다양한 평가지표가 필요한가? (accuray면 끝나는거 아닌가? 더 이상 뭐가 필요해?)\n- 여러가지 평가지표들: https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values - 이걸 다 암기하는건 불가능함. - 몇 개만 뽑아서 암기하고 왜 쓰는지만 생각해보고 넘어가자!\n\n\nconfusion matrix의 이해\n- 표1\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\nTP\nFN\n\n\n안나감(실제)\nFP\nTN\n\n\n\n- 표2 (책에없음)\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\n$(y,)= $ (O,O)\n$(y,)= $(O,X)\n\n\n안나감(실제)\n$(y,)= $(X,O)\n$(y,)= $(X,X)\n\n\n\n- 표3 (책에없음)\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\nTP, \\(\\# O/O\\)\nFN, \\(\\#O/X\\)\n\n\n안나감(실제)\nFP, \\(\\#X/O\\)\nTN, \\(\\#X/X\\)\n\n\n\n\n암기법, (1) 두번째 글자를 그대로 쓴다 (2) 첫글자가 T이면 분류를 제대로한것, 첫글자가 F이면 분류를 잘못한것\n\n- 표4 (위키등에 있음)\n\n\n\n\n\n\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n\n퇴사(실제)\nTP, \\(\\# O/O\\)\nFN, \\(\\# O/X\\)\nSensitivity(민감도)=Recall(재현율)=\\(\\frac{TP}{TP+FN}\\)=\\(\\frac{\\#O/O}{\\# O/O+ \\#O/X}\\)\n\n\n안나감(실제)\nFP, \\(\\# X/O\\)\nTN, \\(\\# X/X\\)\n\n\n\n\nPrecision(프리시즌)=\\(\\frac{TP}{TP+FP}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# X/O}\\)\n\nAccuracy(애큐러시)=\\(\\frac{TP+TN}{total}\\)=\\(\\frac{\\#O/O+\\# X/X}{total}\\)\n\n\n\n\n\n상황극\n- 최규빈은 입사하여 “퇴사자 예측시스템”의 개발에 들어갔다.\n- 자료의 특성상 대부분의 사람이 퇴사하지 않고 회사에 잘 다닌다. 즉 1000명이 있으면 10명정도 퇴사한다.\n\n\nAccuracy\n- 정의: Accuracy(애큐러시)=\\(\\frac{TP+TN}{total}\\)=\\(\\frac{\\#O/O+ \\#X/X}{total}\\) - 한국말로는 정확도, 정분류율이라고 한다. - 한국말이 헷갈리므로 그냥 영어를 외우는게 좋다. (어차피 Keras에서 옵션도 영어로 넣음)\n- (상확극 시점1) 왜 애큐러시는 불충분한가? - 회사: 퇴사자예측프로그램 개발해 - 최규빈: 귀찮은데 다 나간다고 하자! -> 99퍼의 accuracy\n\n모델에 사용한 파라메터 = 0. 그런데 애큐러시 = 99! 이거 엄청 좋은 모형이다?\n\n\n\nSensitivity(민감도), Recall(재현율), True Positive Rate(TPR)\n- 정의: Sensitivity(민감도)=Recall(재현율)=\\(\\frac{TP}{TP+FN}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# O/X}\\) - 분모: 실제 O인 관측치 수 - 분자: 실제 O를 O라고 예측한 관측치 수 - 뜻: 실제 O를 O라고 예측한 비율\n- (상황극 시점2) recall을 봐야하는 이유 - 인사팀: 실제 퇴사자를 퇴사자로 예측해야 의미가 있음! 우리는 퇴사할것 같은 10명을 찍어달란 의미였어요! (그래야 면담을 하든 할거아냐!) - 최규빈: 가볍고(=파라메터 적고) 잘 맞추는 모형 만들어 달라면서요?\n\n인사팀: (고민중..) 사실 생각해보니까 이 경우는 애큐러시는 의미가 없네. 실제 나간 사람 중 최규빈이 나간다고 한 사람이 몇인지 카운트 하는게 더 의미가 있겠다. 우리는 앞으로 리컬(혹은 민감도)를 보겠다!\n\n\n예시1: 실제로 퇴사한 10명중 최규빈이 나간다고 찍은 사람이 5명이면 리컬이 50%\n\n\n예시2: 최규빈이 아무도 나가지 않는다고 예측해버린다? 실제 10명중에서 최규빈이 나간다고 적중시킨사람은 0명이므로 이 경우 리컬은 0%\n\n\n결론: 우리가 필요한건 recall이니까 앞으로 recall을 가져와! accuracy는 큰 의미없어. (그래도 명색이 모델인데 accuracy가 90은 되면 좋겠다)\n\n\n\nPrecision\n- 정의: Precision(프리시즌)=\\(\\frac{TP}{TP+FP}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# X/O}\\) - 분모: O라고 예측한 관측치 - 분자: O라고 예측한 관측치중 진짜 O인 관측치 - 뜻: O라고 예측한 관측치중 진짜 O인 비율\n- (상황극 시점3) recall 만으로 불충분한 이유\n\n최규빈: 에휴.. 귀찮은데 그냥 좀만 수틀리면 다 나갈것 같다고 해야겠다. -> 한 100명 나간다고 했음 -> 실제로 최규빈이 찍은 100명중에 10명이 다 나감!\n\n\n이 경우 애큐러시는 91%, 리컬은 100% (퇴사자 10명을 일단은 다 맞췄으므로).\n\n\n인사팀: (화가 많이 남) 멀쩡한 사람까지 다 퇴사할 것 같다고 하면 어떡해요? 최규빈 연구원이 나간다고 한 100명중에 실제로 10명만 나갔어요.\n인사팀: 마치 총으로 과녁중앙에 맞춰 달라고 했더니 기관총을 가져와서 한번 긁은것이랑 뭐가 달라요? 맞추는게 문제가 아니고 precision이 너무 낮아요.\n최규빈: accuracy 90% 이상, recall은 높을수록 좋다는게 주문 아니었나요?\n인사팀: (고민중..) 앞으로는 recall과 함께 precision도 같이 제출하세요. precision은 당신이 나간다고 한 사람중에 실제 나간사람의 비율을 의미해요. 이 경우는 \\(\\frac{10}{100}\\)이니까 precision이 10%입니다. (속마음: recall 올리겠다고 무작정 너무 많이 예측하지 말란 말이야!)\n\n\n\nF1 score\n- 정의: recall과 precision의 조화평균\n- (상황극 시점4) recall, precision을 모두 고려\n\n최규빈: recall/precision을 같이 내는건 좋은데요, 둘은 trade off의 관계에 있습니다. 물론 둘다 올리는 모형이 있다면 좋지만 그게 쉽지는 않아요. 보통은 precision을 올리려면 recall이 희생되는 면이 있고요, recall을 올리려고 하면 precision이 다소 떨어집니다.\n최규빈: 평가기준이 애매하다는 의미입니다. 모형1,2가 있는데 모형1은 모형2보다 precision이 약간 좋고 대신 recall이 떨어진다면 모형1이 좋은것입니까? 아니면 모형2가 좋은것입니까?\n인사팀: 그렇다면 둘을 평균내서 F1score를 계산해서 제출해주세요.\n\n\n\nSpecificity(특이도), False Positive Rate(FPR)\n- 정의:\n\nSpecificity(특이도)=\\(\\frac{TN}{FP+TN}\\)=\\(\\frac{\\# X/X}{\\# X/O+\\# X/X}\\)\nFalse Positive Rate (FPR) = 1-Specificity(특이도) = \\(\\frac{FP}{FP+TN}\\)=\\(\\frac{\\# X/O}{\\# X/O+\\# X/X}\\)\n\n- 의미: FPR = 오해해서 미안해, recall(=TPR)을 올리려고 보니 어쩔 수 없었어 ㅠㅠ - specificity는 안나간 사람을 안나갔다고 찾아낸 비율인데 별로 안중요하다. - FPR은 recall을 올리기 위해서 “실제로는 회사 잘 다니고 있는 사람 중 최규빈이 나갈것 같다고 찍은 사람들” 의 비율이다.\n\n즉 생사람잡은 비율.. 오해해서 미안한 사람의 비율..\n\n\n\nROC curve\n- 정의: \\(x\\)축=FPR, \\(y\\)축=TPR 을 그린 커브\n- 의미: - 결국 “오해해서 미안해 vs recall”을 그린 곡선이 ROC커브이다. - 생각해보면 오해하는 사람이 많을수록 당연히 recall은 올라간다. 따라서 우상향하는 곡선이다. - 오해한 사람이 매우 적은데 recall이 우수하면 매우 좋은 모형이다. 그래서 초반부터 ROC값이 급격하게 올라가면 좋은 모형이다."
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-08-lecture.html#fashion_mnist-revisit",
    "href": "post/Lecture/STBD/2022-05-08-lecture.html#fashion_mnist-revisit",
    "title": "11. softmax",
    "section": "fashion_mnist (revisit)",
    "text": "fashion_mnist (revisit)\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nx_train.shape\n\n(60000, 28, 28)\n\n\n이미지의 차원이 단지 (28,28)이라는 것은 흑백이미지라는 뜻이다.\n\nplt.imshow(x_train[0]) \n\n<matplotlib.image.AxesImage at 0x7f2ad4a04990>\n\n\n\n\n\n\n아닌데요?! 칼라인데요?! -> 흑백이다. 그냥 밝을수록 노란색, 어두울수록 남색으로 표현한것 뿐임 (colormap이 viridis일 뿐임)\n\n일반적으로 분석할 이미지는 칼라를 의미하는 채널도 포함할테니 아래와 같이 자료형을 정리하는게 일반적으로 이미지 자료를 분석하는 정석적인 처리방법이다.\n\nX = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nX.shape\n\nTensorShape([60000, 28, 28, 1])\n\n\nkeras에서 이미지자료는 (관측치수,픽셀,픽셀,채널)과 같은 형식을 가진다\n예를들어 256*256 size인 칼라이미지(채널수=3)가 10개 있다면 X.shape은 (10,256,256,3)이다."
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-08-lecture.html#x의-차원이-관측치수픽셀픽셀채널일-경우-dnn-쓰기",
    "href": "post/Lecture/STBD/2022-05-08-lecture.html#x의-차원이-관측치수픽셀픽셀채널일-경우-dnn-쓰기",
    "title": "11. softmax",
    "section": "X의 차원이 (관측치수,픽셀,픽셀,채널)일 경우 DNN 쓰기",
    "text": "X의 차원이 (관측치수,픽셀,픽셀,채널)일 경우 DNN 쓰기\n\n(예제1) X -> Dense(30,relu) -> Dense(10,softmax):=> y\n- 이러한 아키텍처를 돌리기 위해서는 X의 shape을 미리 바꿔야 했었다. 혹시 바꾸지 않는 방법도 있을까?\n\nflttn = tf.keras.layers.Flatten()\n\n\nX.shape\n\nTensorShape([60000, 28, 28, 1])\n\n\n\nflttn(X).shape\n\nTensorShape([60000, 784])\n\n\n펴진다? 즉 X.reshape(-1,784)와 같은 기능!\n- 근데 이거 레이어다? 즉 네트워크에 add 할 수 있다는 의미!\n\ntf.random.set_seed(43052)\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Flatten())\nnet1.add(tf.keras.layers.Dense(30,activation='relu'))\nnet1.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet1.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet1.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 2.5431 - accuracy: 0.4038\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.2042 - accuracy: 0.5173\nEpoch 3/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.0222 - accuracy: 0.5878\nEpoch 4/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.9312 - accuracy: 0.6192\nEpoch 5/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.8973 - accuracy: 0.6270\n\n\n<keras.callbacks.History at 0x7f2ad4ad0850>\n\n\n\nnet1.layers\n\n[<keras.layers.core.flatten.Flatten at 0x7f2ad4a3e5d0>,\n <keras.layers.core.dense.Dense at 0x7f2ad4a3e9d0>,\n <keras.layers.core.dense.Dense at 0x7f2ad4c22790>]\n\n\n\nnet1.layers[0](X) \n\n<tf.Tensor: shape=(60000, 784), dtype=float32, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>\n\n\n\nnet1.layers[1](net1.layers[0](X)) # 출력이 30이니까~ + 렐루를 거쳐서 0또는 양수인 모습!\n\n<tf.Tensor: shape=(60000, 30), dtype=float32, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>\n\n\n\nnet1.layers[2](net1.layers[1](net1.layers[0](X))) # 최종출력 10차원, 각각은 확률을 의미하게 된다. \n\n<tf.Tensor: shape=(60000, 10), dtype=float32, numpy=\narray([[0.0000000e+00, 2.8076959e-37, 0.0000000e+00, ..., 3.8582063e-04,\n        0.0000000e+00, 9.9960333e-01],\n       [2.0941226e-01, 1.4367345e-03, 2.3821327e-01, ..., 5.0225304e-03,\n        2.5805481e-02, 3.0450234e-03],\n       [3.0291098e-01, 1.3899502e-02, 2.2462834e-02, ..., 2.5782569e-15,\n        2.1044535e-04, 2.6176517e-10],\n       ...,\n       [6.8422541e-02, 2.0148051e-01, 1.2838944e-05, ..., 0.0000000e+00,\n        5.6517130e-10, 8.8627344e-27],\n       [2.0941226e-01, 1.4367345e-03, 2.3821327e-01, ..., 5.0225304e-03,\n        2.5805481e-02, 3.0450234e-03],\n       [1.0189731e-34, 0.0000000e+00, 0.0000000e+00, ..., 1.7085880e-05,\n        2.8456826e-20, 5.3099287e-05]], dtype=float32)>\n\n\n`-`` (참고) metrics=[‘accuracy’] 대신에 이렇게 해도된다~\n\ntf.random.set_seed(43052)\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Flatten())\nnet1.add(tf.keras.layers.Dense(30,activation='relu'))\nnet1.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet1.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=[tf.metrics.CategoricalAccuracy()])\nnet1.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 5s 2ms/step - loss: 2.5431 - categorical_accuracy: 0.4038\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.2042 - categorical_accuracy: 0.5173\nEpoch 3/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.0222 - categorical_accuracy: 0.5878\nEpoch 4/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.9312 - categorical_accuracy: 0.6192\nEpoch 5/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.8973 - categorical_accuracy: 0.6270\n\n\n<keras.callbacks.History at 0x7f2ad3b95310>\n\n\n\nid(tf.metrics.CategoricalAccuracy), id(tf.keras.metrics.CategoricalAccuracy)\n\n(115791520, 115791520)\n\n\n\n주소가 똑같다!!\n주의사항: tf.metrics.Accuracy() 말고 tf.metrics.CategoricalAccuracy() 를 써야함\n(참고2) 메트릭을 추가할수도 있다\n\n\ntf.random.set_seed(43052)\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Flatten())\nnet1.add(tf.keras.layers.Dense(30,activation='relu'))\nnet1.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet1.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=[tf.metrics.CategoricalAccuracy(),tf.metrics.Recall()])\nnet1.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 49s 2ms/step - loss: 2.5431 - categorical_accuracy: 0.4038 - recall: 0.2949\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.2042 - categorical_accuracy: 0.5173 - recall: 0.4078\nEpoch 3/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.0222 - categorical_accuracy: 0.5878 - recall: 0.4774\nEpoch 4/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.9312 - categorical_accuracy: 0.6192 - recall: 0.5059\nEpoch 5/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.8973 - categorical_accuracy: 0.6270 - recall: 0.5128\n\n\n<keras.callbacks.History at 0x7f2acdaafd90>\n\n\nrecall을 추가하면 test set의 성능평가에도 리콜을 볼 수 있다.\n\nnet1.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 2ms/step - loss: 0.9399 - categorical_accuracy: 0.6308 - recall: 0.5122\n\n\n[0.9398639798164368, 0.6308000087738037, 0.5121999979019165]\n\n\n\n\n(예제2) X -> Dense(500,relu) -> Dense(500,relu) -> Dense(10,softmax):=>y\n- 다른 모형으로 적합해보기\n\ntf.random.set_seed(43052)\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Flatten())\nnet2.add(tf.keras.layers.Dense(500,activation='relu'))\nnet2.add(tf.keras.layers.Dense(500,activation='relu'))\nnet2.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet2.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet2.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 15s 8ms/step - loss: 2.2713 - accuracy: 0.7542\nEpoch 2/5\n1875/1875 [==============================] - 14s 7ms/step - loss: 0.6264 - accuracy: 0.7955\nEpoch 3/5\n1875/1875 [==============================] - 14s 7ms/step - loss: 0.5365 - accuracy: 0.8209\nEpoch 4/5\n1875/1875 [==============================] - 14s 7ms/step - loss: 0.4477 - accuracy: 0.8425\nEpoch 5/5\n1875/1875 [==============================] - 14s 7ms/step - loss: 0.4127 - accuracy: 0.8548\n\n\n<keras.callbacks.History at 0x7f2ad49ae390>\n\n\n\nnet2.fit(XX,yy)\n\n313/313 [==============================] - 3s 9ms/step - loss: 0.4584 - accuracy: 0.8366\n\n\n<keras.callbacks.History at 0x7f2ad50e3ad0>\n\n\n- 위 모형은 epoch을 늘려도 큰 변화가 없다.\n\n\n(예제3) 아주 복잡한 DNN\n\ntf.random.set_seed(43052)\nnet3 = tf.keras.Sequential()\nnet3.add(tf.keras.layers.Flatten())\nnet3.add(tf.keras.layers.Dense(500,activation='relu'))\nnet3.add(tf.keras.layers.Dense(500,activation='relu'))\nnet3.add(tf.keras.layers.Dense(500,activation='relu'))\nnet3.add(tf.keras.layers.Dense(500,activation='relu'))\nnet3.add(tf.keras.layers.Dense(500,activation='relu'))\nnet3.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet3.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet3.fit(X,y,epochs=10)\n\nEpoch 1/10\n1875/1875 [==============================] - 31s 16ms/step - loss: 0.9820 - accuracy: 0.7955\nEpoch 2/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.4518 - accuracy: 0.8384\nEpoch 3/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.4163 - accuracy: 0.8536\nEpoch 4/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.3869 - accuracy: 0.8636\nEpoch 5/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.3759 - accuracy: 0.8677\nEpoch 6/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.3616 - accuracy: 0.8737\nEpoch 7/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.3443 - accuracy: 0.8794\nEpoch 8/10\n1875/1875 [==============================] - 29s 16ms/step - loss: 0.3355 - accuracy: 0.8816\nEpoch 9/10\n1875/1875 [==============================] - 29s 16ms/step - loss: 0.3335 - accuracy: 0.8831\nEpoch 10/10\n1875/1875 [==============================] - 36s 19ms/step - loss: 0.3182 - accuracy: 0.8866\n\n\n<keras.callbacks.History at 0x7f2ad50be210>\n\n\n\nnet3.evaluate(XX,yy)\n\n313/313 [==============================] - 2s 5ms/step - loss: 0.3748 - accuracy: 0.8718\n\n\n[0.37482383847236633, 0.8718000054359436]"
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-08-lecture.html#발상의-전환",
    "href": "post/Lecture/STBD/2022-05-08-lecture.html#발상의-전환",
    "title": "11. softmax",
    "section": "발상의 전환",
    "text": "발상의 전환\n\n현재 신경망을 복잡하게 해봤지만 늘어난 파라미터 대비 성능개선이 크지 않다.\n다른 방법을 이용해보자.\nMaxPooling2D, MaxPool2D를 이용\n\n\nid(tf.keras.layers.MaxPooling2D), id(tf.keras.layers.MaxPool2D)\n\n(114701472, 114701472)\n\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nmp = tf.keras.layers.MaxPool2D() # pool size의 디폴트는 (2,2)\n\n-테스트1: (2,2) 이미지\n임의의 흑백이미지 생성\n\nXXX = tnp.arange(1*2*2*1).reshape(1,2,2,1)\nXXX\n\n<tf.Tensor: shape=(1, 2, 2, 1), dtype=int64, numpy=\narray([[[[0],\n         [1]],\n\n        [[2],\n         [3]]]])>\n\n\n\nXXX.reshape(1,2,2) # 채널때문에 살짝 헷갈리지만 실제로는 이렇게 생긴 이미지! \n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[0, 1],\n        [2, 3]]])>\n\n\n\nmp(XXX) # 4개중에 제일 큰 값이 나오는 것 같다.\n\n<tf.Tensor: shape=(1, 1, 1, 1), dtype=int64, numpy=array([[[[3]]]])>\n\n\n-테스트2: (4,4) 이미지로 변경\n\nXXX = tnp.arange(1*4*4*1).reshape(1,4,4,1)\nXXX,XXX.reshape(1,4,4)\n\n(<tf.Tensor: shape=(1, 4, 4, 1), dtype=int64, numpy=\n array([[[[ 0],\n          [ 1],\n          [ 2],\n          [ 3]],\n \n         [[ 4],\n          [ 5],\n          [ 6],\n          [ 7]],\n \n         [[ 8],\n          [ 9],\n          [10],\n          [11]],\n \n         [[12],\n          [13],\n          [14],\n          [15]]]])>, <tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy=\n array([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11],\n         [12, 13, 14, 15]]])>)\n\n\n\nmp(XXX),mp(XXX).reshape(1,2,2) ## 4개 의 값중 가장 큰 값을 반환\n\n(<tf.Tensor: shape=(1, 2, 2, 1), dtype=int64, numpy=\n array([[[[ 5],\n          [ 7]],\n \n         [[13],\n          [15]]]])>, <tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\n array([[[ 5,  7],\n         [13, 15]]])>)\n\n\n- 테스트3: (6,6) 이미지 + pool_size = (3,3)\n\nXXX = tnp.arange(1*6*6*1).reshape(1,6,6,1)\nXXX.reshape(1,6,6)\n\n<tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]]])>\n\n\n\nmp(XXX).reshape(1,3,3) # 왜 (2,2)씩...? \n\n<tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy=\narray([[[ 7,  9, 11],\n        [19, 21, 23],\n        [31, 33, 35]]])>\n\n\n\nmp3 = tf.keras.layers.MaxPool2D(pool_size=(3,3))\nmp3(XXX).reshape(1,2,2)\n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[14, 17],\n        [32, 35]]])>\n\n\n\n모형적합\n\nnet4 = tf.keras.Sequential()\nnet4.add(tf.keras.layers.Con)"
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-11-(11주차).html",
    "href": "post/Lecture/STBD/2022-05-11-(11주차).html",
    "title": "12. Max pooling, CNN",
    "section": "",
    "text": "import tensorflow as tf \nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-11-(11주차).html#maxpooling",
    "href": "post/Lecture/STBD/2022-05-11-(11주차).html#maxpooling",
    "title": "12. Max pooling, CNN",
    "section": "maxpooling",
    "text": "maxpooling\n\n테스트 1\n\nm = tf.keras.layers.MaxPool2D()\n\n\nXXX = tnp.arange(1*4*4*1)\nXXX\n\n<tf.Tensor: shape=(16,), dtype=int64, numpy=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])>\n\n\n\nXXX = XXX.reshape(1,4,4,1) ## 컬러이미지\nXXX\n\n<tf.Tensor: shape=(1, 4, 4, 1), dtype=int64, numpy=\narray([[[[ 0],\n         [ 1],\n         [ 2],\n         [ 3]],\n\n        [[ 4],\n         [ 5],\n         [ 6],\n         [ 7]],\n\n        [[ 8],\n         [ 9],\n         [10],\n         [11]],\n\n        [[12],\n         [13],\n         [14],\n         [15]]]])>\n\n\n\nXXX.reshape(1,4,4) ## 흑백이미지\n\n<tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15]]])>\n\n\n\nm(XXX)\n\n<tf.Tensor: shape=(1, 2, 2, 1), dtype=int64, numpy=\narray([[[[ 5],\n         [ 7]],\n\n        [[13],\n         [15]]]])>\n\n\n\nm(XXX).reshape(1,2,2)\n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[ 5,  7],\n        [13, 15]]])>\n\n\n\n눈치껏 XXX.reshape(1,4,4)에서 각각의 2 x 2에서 가장 큰 값을 찾아주는 것으로 생각하자.\n즉, 2 x 2의 window를 만들고 그 중 가장 기록 후 윈도우를 움직이면서 이 과정을 반복한다.\n\n\n\n테스트 2\n\nXXX = tnp.arange(1*6*6*1).reshape(1,6,6,1)\nXXX.reshape(1,6,6)\n\n<tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]]])>\n\n\n\nm(XXX).reshape(1,3,3)\n\n<tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy=\narray([[[ 7,  9, 11],\n        [19, 21, 23],\n        [31, 33, 35]]])>\n\n\n\n윈도우 크기 2 x 2는 디폴트 값이다!!\n\n\n\n테스트 3\n\nm=tf.keras.layers.MaxPool2D(pool_size=(3,3))\n\n\nXXX = tnp.arange(1*6*6*1).reshape(1,6,6,1)\nXXX.reshape(1,6,6)\n\n<tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]]])>\n\n\n\nm(XXX).reshape(1,2,2)\n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[14, 17],\n        [32, 35]]])>\n\n\n\n\n테스트 4\n\nm=tf.keras.layers.MaxPool2D(pool_size=(2,2))\n\n\nXXX = tnp.arange(1*5*5*1).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n<tf.Tensor: shape=(1, 5, 5), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19],\n        [20, 21, 22, 23, 24]]])>\n\n\n\n차원이 안맞는 부분은 버리고 계산한다.\n\n\nm(XXX).reshape(1,2,2) ## 어? 뒤에 나머지 5행과 5열은 버리고 계산한다....\n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[ 6,  8],\n        [16, 18]]])>\n\n\n\n# ?tf.keras.layers.MaxPool2D\n\n\npadding 옵션을 same으로 주면 남는 차원을 버리지 않음\n\n\nm=tf.keras.layers.MaxPool2D(pool_size=(2,2),padding=\"same\")\n\n\nm(XXX).reshape(1,3,3) \n\n<tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy=\narray([[[ 6,  8,  9],\n        [16, 18, 19],\n        [21, 23, 24]]])>\n\n\n\n\n테스트 5\n\n관측치가 2개 채널(색)은 흑백\n\n\nm=tf.keras.layers.MaxPool2D(pool_size=(2,2))\n\n\nXXX = tnp.arange(2*4*4*1).reshape(2,4,4,1)\nXXX.reshape(2,4,4)\n\n<tf.Tensor: shape=(2, 4, 4), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15]],\n\n       [[16, 17, 18, 19],\n        [20, 21, 22, 23],\n        [24, 25, 26, 27],\n        [28, 29, 30, 31]]])>\n\n\n\nm(XXX).reshape(2,2,2) \n\n<tf.Tensor: shape=(2, 2, 2), dtype=int64, numpy=\narray([[[ 5,  7],\n        [13, 15]],\n\n       [[21, 23],\n        [29, 31]]])>\n\n\n\n\n테스트 6\n\nXXX = tnp.arange(1*4*4*3).reshape(1,4,4,3)\n\n\nXXX[:,:,:,0] ## 첫번째 채널\n\n<tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy=\narray([[[ 0,  3,  6,  9],\n        [12, 15, 18, 21],\n        [24, 27, 30, 33],\n        [36, 39, 42, 45]]])>\n\n\n\nm(XXX)\n\n<tf.Tensor: shape=(1, 2, 2, 3), dtype=int64, numpy=\narray([[[[15, 16, 17],\n         [21, 22, 23]],\n\n        [[39, 40, 41],\n         [45, 46, 47]]]])>\n\n\n\nm(XXX)[:,:,:,0] ## 첫번째 채널\n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[15, 21],\n        [39, 45]]])>"
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-11-(11주차).html#convolution-2d",
    "href": "post/Lecture/STBD/2022-05-11-(11주차).html#convolution-2d",
    "title": "12. Max pooling, CNN",
    "section": "Convolution 2D",
    "text": "Convolution 2D\n- 테스트 1\n- 레이어 생성\n\ncv = tf.keras.layers.Conv2D(1, kernel_size = (2,2))\n\n- XXX 생성\n\nXXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1) ### 4 x 4 픽셀을 가진 이미지 생성\nXXX.reshape(1,4,4) \n\n<tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]]])>\n\n\n\n아래가 에러가 나는 이유\n\n데이터 타입이 int가 아닌 float이어야한다.\n또한 차원을 XXX.reshape(1,4,4)로 입력하면 에러가 난다.\n\n\n\ncv(XXX).reshape(1,3,3)\n\n<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[-4.290876 , -4.5067925, -4.722709 ],\n        [-5.154542 , -5.3704586, -5.5863748],\n        [-6.0182076, -6.234124 , -6.45004  ]]], dtype=float32)>\n\n\n\n사칙연산, max 이런 형태로 데이터가 변환되서 출력 되는 것이 아닌 것 같음\n또한, 레이어의 가중치가 랜덤으로 배정되는 것 같음\n\n- 코드 정리 + 시드통일\n\ntf.random.set_seed(43052)\ncv = tf.keras.layers.Conv2D(1, kernel_size = (2,2))\nXXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1)\nprint(XXX.reshape(1,4,4)) \nprint(cv(XXX).reshape(1,3,3))\n\ntf.Tensor(\n[[[ 0.  1.  2.  3.]\n  [ 4.  5.  6.  7.]\n  [ 8.  9. 10. 11.]\n  [12. 13. 14. 15.]]], shape=(1, 4, 4), dtype=float64)\ntf.Tensor(\n[[[ -4.125754   -5.312817   -6.4998803]\n  [ -8.874006  -10.0610695 -11.248133 ]\n  [-13.622259  -14.809322  -15.996386 ]]], shape=(1, 3, 3), dtype=float32)\n\n\n\ndense layer를 생각해보면 가중치가 랜덤으로 배정되기 때문에 시드를 고정해주었다.\nweight를 한번 찍어보자\n\n\ncv.weights\n\n[<tf.Variable 'conv2d_7/kernel:0' shape=(2, 2, 1, 1) dtype=float32, numpy=\n array([[[[-0.13014299]],\n \n         [[-0.23927206]]],\n \n \n        [[[-0.20175874]],\n \n         [[-0.6158894 ]]]], dtype=float32)>,\n <tf.Variable 'conv2d_7/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n\n\n\ntype(cv.weights) ## 길이가 2인 리스트가 출력\n\nlist\n\n\n\ncv.weights[0]\n\n<tf.Variable 'conv2d_7/kernel:0' shape=(2, 2, 1, 1) dtype=float32, numpy=\narray([[[[-0.13014299]],\n\n        [[-0.23927206]]],\n\n\n       [[[-0.20175874]],\n\n        [[-0.6158894 ]]]], dtype=float32)>\n\n\n\nprint(XXX.reshape(1,4,4)) \ntf.reshape(cv.weights[0],(2,2))\n\ntf.Tensor(\n[[[ 0.  1.  2.  3.]\n  [ 4.  5.  6.  7.]\n  [ 8.  9. 10. 11.]\n  [12. 13. 14. 15.]]], shape=(1, 4, 4), dtype=float64)\n\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.13014299, -0.23927206],\n       [-0.20175874, -0.6158894 ]], dtype=float32)>\n\n\n- 연산과정\n\n0*-0.13014299 + 1*-0.23927206 + 4*-0.20175874 + 5*-0.6158894 +0 ## 0 은 bias\n\n-4.1257540200000005\n\n\n\nprint(cv(XXX).reshape(1,3,3))\n\ntf.Tensor(\n[[[ -4.125754   -5.312817   -6.4998803]\n  [ -8.874006  -10.0610695 -11.248133 ]\n  [-13.622259  -14.809322  -15.996386 ]]], shape=(1, 3, 3), dtype=float32)\n\n\n- weight를 변경해보자\n\ncv.get_weights()[0].shape\n\n(2, 2, 1, 1)\n\n\n\nw = tnp.array([1/4,1/4,1/4,1/4],dtype=tf.float32).reshape(2,2,1,1)\nb = tnp.array([3],dtype=tf.float32)\n\n\ncv.set_weights([w,b])\n\n\nXXX.reshape(1,4,4)\n\n<tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]]])>\n\n\nw는 0,1,4,5를 평균내기위한 weight 값으로 설정한 것이다.\n\ncv(XXX).reshape(1,3,3)\n\n<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[ 5.5,  6.5,  7.5],\n        [ 9.5, 10.5, 11.5],\n        [13.5, 14.5, 15.5]]], dtype=float32)>\n\n\n\ntnp.mean([0,1,4,5])+3,tnp.mean([1,2,5,6])+3,tnp.mean([2,3,6,7])+3\n\n(<tf.Tensor: shape=(), dtype=float64, numpy=5.5>,\n <tf.Tensor: shape=(), dtype=float64, numpy=6.5>,\n <tf.Tensor: shape=(), dtype=float64, numpy=7.5>)\n\n\n\nConv 2D 요약\n\nsize = (2,2)인 윈도우를 만든다.\nXXX에 윈도우를 통과시켜서 (2,2)크기의 sub XXX를 얻음, sub XXX의 각 원소에 conv2d.weights[0]의 각 원소를 element-wise하게 곱한다.\n(2)의 결과를 모두 더한다.\n위 과정을 window를 이동시키면서 반복!\n\n- 테스트 2\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,(3,3))\nXXX = tnp.arange(1*5*5*1,dtype=tf.float64).reshape(1,5,5,1)\n\n\nXXX.reshape(1,5,5)\n\n<tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.,  4.],\n        [ 5.,  6.,  7.,  8.,  9.],\n        [10., 11., 12., 13., 14.],\n        [15., 16., 17., 18., 19.],\n        [20., 21., 22., 23., 24.]]])>\n\n\n\ntf.reshape(cnv.weights[0],(3,3))\n\n<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.08676198, -0.1595147 , -0.13450584],\n       [-0.4105929 , -0.38366908,  0.07744962],\n       [-0.09255642,  0.4915564 ,  0.20828158]], dtype=float32)>\n\n\n\ncnv(XXX).reshape(1,3,3)\n\n<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[ 2.7395768 ,  2.2492635 ,  1.7589504 ],\n        [ 0.28801066, -0.20230258, -0.6926158 ],\n        [-2.1635566 , -2.6538715 , -3.1441827 ]]], dtype=float32)>\n\n\n\ntf.reduce_sum(XXX.reshape(1,5,5)[0,:3,:3] * tf.reshape(cnv.weights[0],(3,3)))\n\n<tf.Tensor: shape=(), dtype=float64, numpy=2.739577144384384>\n\n\n- 테스트 3\n\nXXX = tf.constant([[3,3,2,1,0],[0,0,1,3,1],[3,1,2,2,3],[2,0,0,2,2],[2,0,0,0,1]],dtype=tf.float64).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n<tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy=\narray([[[3., 3., 2., 1., 0.],\n        [0., 0., 1., 3., 1.],\n        [3., 1., 2., 2., 3.],\n        [2., 0., 0., 2., 2.],\n        [2., 0., 0., 0., 1.]]])>\n\n\n\n_w = tf.constant([[0,1,2],[2,2,0],[0,1,2]],dtype = tf.float64)\n_b = tf.constant([0],dtype=tf.float64)\n\n\ncnv= tf.keras.layers.Conv2D(1, kernel_size = (3,3))\n\n\ncnv.set_weights([_w,_b])\n\nValueError: ignored\n\n\n- 테스트 4\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,kernel_size=(2,2))\nXXX \n\n\n\n- 테스트 3,4 다시…\n- 테스트 5\n\n관측치는 고정이고 출력채널이 바뀔 때\n\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,kernel_size=(2,2)) ## 출력채널이 1인 경우 값이 1개 나옴 element wise 정확히 알아야한다\nXXX = tnp.arange(1*2*2*1,dtype=tf.float64).reshape(1,2,2,1)\n\n\nprint(XXX.reshape(1,2,2))\n\ntf.Tensor(\n[[[0. 1.]\n  [2. 3.]]], shape=(1, 2, 2), dtype=float64)\n\n\n\ncnv(XXX)\n\n<tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=\narray([[[[ 1.048703  , -1.0948269 , -0.04476056, -1.7289025 ]]]],\n      dtype=float32)>\n\n\n\ncnv.weights[0] # (2 x 2)커널의크기 // 1은 XXX의 채널 수 // 4는 con(XXX)의 채널수\n\n<tf.Variable 'conv2d_28/kernel:0' shape=(2, 2, 1, 4) dtype=float32, numpy=\narray([[[[-0.08230966, -0.15132892, -0.12760344, -0.38952267]],\n\n        [[-0.36398047,  0.07347518, -0.08780673,  0.46633136]]],\n\n\n       [[[ 0.19759327, -0.46042526, -0.15406173, -0.34838456]],\n\n        [[ 0.33916563, -0.08248386,  0.11705655, -0.49948823]]]],\n      dtype=float32)>\n\n\n\ncnv.weights[0][...,0].reshape(2,2) ## conv(XXX)의 첫 번째 채널의 weight 값\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.08230966, -0.36398047],\n       [ 0.19759327,  0.33916563]], dtype=float32)>\n\n\n\ntf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2)) ## 이는 Conv 2D를 거쳐 나오는 첫번째 출력 값\n\n<tf.Tensor: shape=(), dtype=float64, numpy=1.0487029552459717>\n\n\n\n채널을 뻥튀기한다 \\(\\to\\) 아무의미없다 그냥 늘리는 것이다 $ $ 그냥 파라미터를 많이 써서 뻥튀기하는 것이다.\n원래 채널은 3개(색깔)지만 그냥 모델을 복잡하게 해서 뻥튀기를 하는 것이다.\n\n- 테스트 6\n\nX의 채널이 여러개고, Conv의 채널도 여러개 일때\n\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,kernel_size=(2,2))\nXXX = tnp.array([1]*1*2*2*1,dtype=tf.float64).reshape(1,2,2,1)\n\n\nprint(XXX.reshape(1,2,2))\n\ntf.Tensor(\n[[[1. 1.]\n  [1. 1.]]], shape=(1, 2, 2), dtype=float64)\n\n\n\n위와같이 XXX를 설정하면 cnv(XXX)의 결과는 단지 cnv의 weigth들의 합이 된다.\n\n\ncnv(XXX)\n\n<tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=\narray([[[[ 0.09046876, -0.6207628 , -0.25241536, -0.7710641 ]]]],\n      dtype=float32)>\n\n\n\ncnv.weights[0] # (2 x 2)커널의크기 // 1은 XXX의 채널 수 // 4는 con(XXX)의 채널수\n\n<tf.Variable 'conv2d_29/kernel:0' shape=(2, 2, 1, 4) dtype=float32, numpy=\narray([[[[-0.08230966, -0.15132892, -0.12760344, -0.38952267]],\n\n        [[-0.36398047,  0.07347518, -0.08780673,  0.46633136]]],\n\n\n       [[[ 0.19759327, -0.46042526, -0.15406173, -0.34838456]],\n\n        [[ 0.33916563, -0.08248386,  0.11705655, -0.49948823]]]],\n      dtype=float32)>\n\n\n\ntf.reduce_sum(cnv.weights[0][...,0]) == tf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2))\n\n<tf.Tensor: shape=(), dtype=bool, numpy=True>\n\n\n\n계산결과를 확인하기 쉽게 만든 약간의 트릭\n\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,kernel_size=(2,2))\nXXX = tnp.array([1]*1*2*2*3,dtype=tf.float64).reshape(1,2,2,3) ## X의 채널 차원 수를 늘림\n\n\ncnv(XXX)\n\n<tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=\narray([[[[ 0.3297621, -0.4498347, -1.0487393, -1.580095 ]]]],\n      dtype=float32)>\n\n\n\ncnv.weights ## (2 x 2)는 커널 사이즈 // 3은 XXX의 채널 // 4는 conv(xxx) 출력 채널\n\n[<tf.Variable 'conv2d_34/kernel:0' shape=(2, 2, 3, 4) dtype=float32, numpy=\n array([[[[-0.06956434, -0.12789628, -0.10784459, -0.32920673],\n          [-0.30761963,  0.06209785, -0.07421023,  0.3941219 ],\n          [ 0.16699678, -0.38913035, -0.13020593, -0.29443866]],\n \n         [[ 0.28664726, -0.0697116 ,  0.09893084, -0.4221446 ],\n          [-0.23161241, -0.16410837, -0.36420006,  0.12424195],\n          [-0.14245945,  0.36286396, -0.10751781,  0.1733647 ]]],\n \n \n        [[[ 0.02764335,  0.15547717, -0.42024496, -0.31893867],\n          [ 0.22414821,  0.3619454 , -0.00282967, -0.3503708 ],\n          [ 0.4610079 , -0.17417148,  0.00401336, -0.29777044]],\n \n         [[-0.1620284 , -0.42066965, -0.01578814, -0.4240524 ],\n          [ 0.37925082,  0.24236053,  0.3949356 , -0.20996472],\n          [-0.30264795, -0.28889188, -0.3237777 ,  0.37506342]]]],\n       dtype=float32)>,\n <tf.Variable 'conv2d_34/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n\n\n\ncnv.weights[0][...,0] ### cnv(XXX)의 첫번째 채널을 얻기위해 사용되는 weight값\n\n<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\narray([[[-0.06956434, -0.30761963,  0.16699678],\n        [ 0.28664726, -0.23161241, -0.14245945]],\n\n       [[ 0.02764335,  0.22414821,  0.4610079 ],\n        [-0.1620284 ,  0.37925082, -0.30264795]]], dtype=float32)>\n\n\n\ntf.reduce_sum(cnv.weights[0][...,0]) ## 오 cnv(XXX)의 첫번째 값과 같다. 즉 첫번째 채널의 결과 ## 이거 시험에 안나오니 정리만!!\n## 채널이 뻥튀기 될 때 뻥튀기 된 채널이 아무의미가 없고 유의미한 피쳐가 있을 것이라고 기대하는 것이다 즉 그중에 강아지인지, 고양이지인지 그중에 하나만 걸려라의 느낌의 뻥튀기\n\n<tf.Tensor: shape=(), dtype=float32, numpy=0.32976213>\n\n\n\nprint(tf.reduce_sum(cnv.weights[0][...,1])) ## 두 번째 cnv(XXX)\nprint(tf.reduce_sum(cnv.weights[0][...,2])) ## 두 번째 cnv(XXX)\nprint(tf.reduce_sum(cnv.weights[0][...,3])) ## 두 번째 cnv(XXX)\n\ntf.Tensor(-0.44983464, shape=(), dtype=float32)\ntf.Tensor(-1.0487392, shape=(), dtype=float32)\ntf.Tensor(-1.5800952, shape=(), dtype=float32)\n\n\n\nw_red = cnv.weights[0][...,0][...,0] ## X의 입력 채널\nw_green = cnv.weights[0][...,0][...,1] ## X의 입력 채널\nw_blue = cnv.weights[0][...,0][...,2] ## X의 입력 채널\n\n\nw_red\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.06956434,  0.28664726],\n       [ 0.02764335, -0.1620284 ]], dtype=float32)>\n\n\n\nXXX[...,0]*w_red  +  XXX[...,1] * w_green  + XXX[...,2] * w_blue \n\n<tf.Tensor: shape=(1, 2, 2), dtype=float64, numpy=\narray([[[-0.2101872 , -0.08742461],\n        [ 0.71279946, -0.08542553]]])>\n\n\n\nXXX[...,0] ## X_red\n\n<tf.Tensor: shape=(1, 2, 2), dtype=float64, numpy=\narray([[[1., 1.],\n        [1., 1.]]])>\n\n\n\n만약 layer의 acitvation을 relu로 잡으면 음의값은 전부 0으로 바뀐다."
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-11-(11주차).html#텐서플로우를-공부하기-좋은-교재-혹은-참고자료",
    "href": "post/Lecture/STBD/2022-05-11-(11주차).html#텐서플로우를-공부하기-좋은-교재-혹은-참고자료",
    "title": "12. Max pooling, CNN",
    "section": "텐서플로우를 공부하기 좋은 교재 혹은 참고자료",
    "text": "텐서플로우를 공부하기 좋은 교재 혹은 참고자료\n\n텐서플로우 교재\n- 교재1: http://www.kyobobook.co.kr/product/detailViewEng.laf?mallGb=ENG&ejkGb=ENG&barcode=9781838823412 - 장점: 텐서플로우 2.0을 다룬 교재, 기본적 내용을 간략히 소개. 다양한 분야를 커버. - 단점: 내용이 조금 산만함 (잘 안읽힘). 코드가 예쁘지 않음. 글을 진짜 못씀.\n- 교재2: https://www.amazon.com/Advanced-Deep-Learning-TensorFlow-Keras/dp/1838821651/ref=pd_sbs_sccl_2_2/139-5369471-9501726?pd_rd_w=PLPU2&pf_rd_p=3676f086-9496-4fd7-8490-77cf7f43f846&pf_rd_r=019BZNCP8B6RHXPKXF0T&pd_rd_r=2dea359c-c5d4-4404-86d8-cb1f5f934607&pd_rd_wg=98kmk&pd_rd_i=1838821651&psc=1\n\n장점: 코드가 예쁘다. 예제가 잘 구성됨. 참신하다. 내용에 깊이가 있다.\n단점: 입문용 내용이 아예 없음\n\n- 교재3: 텐서플로 딥러닝 프로그래밍 (김동근/가메출판사) - 장점: 한글교재, 그래도 교재구성에 흐름이 있다. 공식문서의 가이드내용도 포함 (대부분은 튜토리얼 수준만 포함) - 단점: 공식문서내용 그냥 거의 그대로 베낌.\n\n\n공식홈페이지\n- 튜토리얼: https://www.tensorflow.org/tutorials?hl=ko - 탭눌러서 초보자~고급 모두 읽어보면 좋다. - 간단한 모형실습들 제공. - 대부분의 교재는 튜토리얼의 내용을 베껴서 출판한다. (외국교재도 포함!)\n- 가이드: https://www.tensorflow.org/guide?hl=ko - 왜 파이토치가 아니고 텐서플로우를 써야하는가? 에 대한 대답들 - 모형에 대한 설명보다 프로그램 자체에 대한 이해도를 높여준다.\n- API: https://www.tensorflow.org/versions?hl=ko - tf의 다양한 모듈구조를 확인 - 파이썬에서 도움말 치면 나오는 것들 + \\(\\alpha\\) - 교재에서 확인불가능한 아주 디테일한 부분까지 확인가능\n\ntf.GradientTape?\n\n\nInit signature: tf.GradientTape(persistent=False, watch_accessed_variables=True)\nDocstring:     \nRecord operations for automatic differentiation.\nOperations are recorded if they are executed within this context manager and\nat least one of their inputs is being \"watched\".\nTrainable variables (created by `tf.Variable` or `tf.compat.v1.get_variable`,\nwhere `trainable=True` is default in both cases) are automatically watched.\nTensors can be manually watched by invoking the `watch` method on this context\nmanager.\nFor example, consider the function `y = x * x`. The gradient at `x = 3.0` can\nbe computed as:\n>>> x = tf.constant(3.0)\n>>> with tf.GradientTape() as g:\n...   g.watch(x)\n...   y = x * x\n>>> dy_dx = g.gradient(y, x)\n>>> print(dy_dx)\ntf.Tensor(6.0, shape=(), dtype=float32)\nGradientTapes can be nested to compute higher-order derivatives. For example,\n>>> x = tf.constant(5.0)\n>>> with tf.GradientTape() as g:\n...   g.watch(x)\n...   with tf.GradientTape() as gg:\n...     gg.watch(x)\n...     y = x * x\n...   dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x\n>>> d2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2\n>>> print(dy_dx)\ntf.Tensor(10.0, shape=(), dtype=float32)\n>>> print(d2y_dx2)\ntf.Tensor(2.0, shape=(), dtype=float32)\nBy default, the resources held by a GradientTape are released as soon as\nGradientTape.gradient() method is called. To compute multiple gradients over\nthe same computation, create a persistent gradient tape. This allows multiple\ncalls to the gradient() method as resources are released when the tape object\nis garbage collected. For example:\n>>> x = tf.constant(3.0)\n>>> with tf.GradientTape(persistent=True) as g:\n...   g.watch(x)\n...   y = x * x\n...   z = y * y\n>>> dz_dx = g.gradient(z, x)  # (4*x^3 at x = 3)\n>>> print(dz_dx)\ntf.Tensor(108.0, shape=(), dtype=float32)\n>>> dy_dx = g.gradient(y, x)\n>>> print(dy_dx)\ntf.Tensor(6.0, shape=(), dtype=float32)\nBy default GradientTape will automatically watch any trainable variables that\nare accessed inside the context. If you want fine grained control over which\nvariables are watched you can disable automatic tracking by passing\n`watch_accessed_variables=False` to the tape constructor:\n>>> x = tf.Variable(2.0)\n>>> w = tf.Variable(5.0)\n>>> with tf.GradientTape(\n...     watch_accessed_variables=False, persistent=True) as tape:\n...   tape.watch(x)\n...   y = x ** 2  # Gradients will be available for `x`.\n...   z = w ** 3  # No gradients will be available as `w` isn't being watched.\n>>> dy_dx = tape.gradient(y, x)\n>>> print(dy_dx)\ntf.Tensor(4.0, shape=(), dtype=float32)\n>>> # No gradients will be available as `w` isn't being watched.\n>>> dz_dy = tape.gradient(z, w)\n>>> print(dz_dy)\nNone\nNote that when using models you should ensure that your variables exist when\nusing `watch_accessed_variables=False`. Otherwise it's quite easy to make your\nfirst iteration not have any gradients:\n```python\na = tf.keras.layers.Dense(32)\nb = tf.keras.layers.Dense(32)\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\n  tape.watch(a.variables)  # Since `a.build` has not been called at this point\n                           # `a.variables` will return an empty list and the\n                           # tape will not be watching anything.\n  result = b(a(inputs))\n  tape.gradient(result, a.variables)  # The result of this computation will be\n                                      # a list of `None`s since a's variables\n                                      # are not being watched.\nNote that only tensors with real or complex dtypes are differentiable. Init docstring: Creates a new GradientTape. Args: persistent: Boolean controlling whether a persistent gradient tape is created. False by default, which means at most one call can be made to the gradient() method on this object. watch_accessed_variables: Boolean controlling whether the tape will automatically watch any (trainable) variables accessed while the tape is active. Defaults to True meaning gradients can be requested from any result computed in the tape derived from reading a trainable Variable. If False users must explicitly watch any Variables they want to request gradients from. File: ~/anaconda3/envs/tfgpu/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py Type: type Subclasses: LossScaleGradientTape\n\n```\n\n\n\n\n\n텐서플로우가 아닌 그냥 딥러닝 이론교재\n- Deep Learning (이안굿펠로우): https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=171345378 - 장점: 그나마 잘 쓰여진 전통있는 교재, 교수님들 책꽃이에 하나씩 꼽혀있었음. 방대한 내용다룸. 깊이있음. 틀린것 없음. 무료 - 단점: 번역이 쓰레기임. 내용이 너무 어려움. (이해를 하라고 쓴 설명이 아님) - 무료제공: https://www.deeplearningbook.org/\n- 기계학습 (오일석) - 장점: 이해가 잘된다. 꽤 넓은 분야를 다룬다. 비교적 간단한 예제로 개념을 설명한다.\n\n\nCNN의 시작: 알렉스넷\n- 만화: https://wedatalab.tistory.com/71\n\n\n2d convolution"
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-23-(12주차).html#imports",
    "href": "post/Lecture/STBD/2022-05-23-(12주차).html#imports",
    "title": "13. CNN-2",
    "section": "imports",
    "text": "imports\n\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-23-(12주차).html#cnn",
    "href": "post/Lecture/STBD/2022-05-23-(12주차).html#cnn",
    "title": "13. CNN-2",
    "section": "CNN",
    "text": "CNN\n\nCONV의 역할\n- 2D 에서 이미지 처리파트 * CONV - MAXPOOL - CONV - MAXPOOL\n- DNN 파트 * Flatten - Dense\n- 데이터생성 (그냥 흑백대비 데이터)\n\n_X1 = tnp.ones([50,25])*10 \n_X1\n\n<tf.Tensor: shape=(50, 25), dtype=float64, numpy=\narray([[10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       ...,\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.]])>\n\n\n\n_X2 = tnp.zeros([50,25])*10 \n_X2\n\n<tf.Tensor: shape=(50, 25), dtype=float64, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])>\n\n\n\ntf.concat([_X1,_X2],axis=1)\n\n<tf.Tensor: shape=(50, 50), dtype=float64, numpy=\narray([[10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       ...,\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.]])>\n\n\n\n_noise = tnp.random.randn(50*50).reshape(50,50)\n_noise\n\n<tf.Tensor: shape=(50, 50), dtype=float64, numpy=\narray([[ 0.37129319, -0.22218732,  2.47718277, ...,  0.49074486,\n         0.83356932,  0.45569771],\n       [-0.71583146, -0.11528848, -1.54078337, ..., -1.10061299,\n         0.38703597,  0.10322716],\n       [ 1.52172142,  1.81161957, -0.16138102, ..., -1.27557782,\n        -0.39697905,  1.96703026],\n       ...,\n       [ 1.34529249, -0.33572408, -1.13030603, ...,  0.84900934,\n        -1.70690337, -0.55816   ],\n       [-0.78912273, -1.80655893,  0.16310892, ...,  0.8075891 ,\n        -2.06762249,  0.84053733],\n       [-1.06669861, -0.04952165,  0.40685118, ..., -0.100907  ,\n         1.56423262, -1.63625911]])>\n\n\n\nXXX = tf.concat([_X1,_X2],axis=1) + _noise\n\n\nXXX=XXX.reshape(1,50,50,1)\n\n\nplt.imshow(XXX.reshape(50,50),cmap='gray')\n\n<matplotlib.image.AxesImage at 0x7fbc5d008750>\n\n\n\n\n\n- conv layer 생성\n\nconv = tf.keras.layers.Conv2D(2,(2,2)) \n\n\nconv.weights # 처음에는 가중치가 없음 \n\n[]\n\n\n\nconv(XXX) # 가중치를 만들기 위해서 XXX를 conv에 한번 통과시킴\nconv.weights # 이제 가중치가 생김\n\n[<tf.Variable 'conv2d/kernel:0' shape=(2, 2, 1, 2) dtype=float32, numpy=\n array([[[[ 0.4503097 , -0.6546046 ]],\n \n         [[ 0.53834134,  0.05130672]]],\n \n \n        [[[-0.34205168,  0.12724537]],\n \n         [[-0.39912695,  0.36518508]]]], dtype=float32)>,\n <tf.Variable 'conv2d/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n\n\n- 가중치의 값을 확인해보자.\n\nconv.weights[0] # kernel에 해당하는것 \n\n<tf.Variable 'conv2d/kernel:0' shape=(2, 2, 1, 2) dtype=float32, numpy=\narray([[[[ 0.4503097 , -0.6546046 ]],\n\n        [[ 0.53834134,  0.05130672]]],\n\n\n       [[[-0.34205168,  0.12724537]],\n\n        [[-0.39912695,  0.36518508]]]], dtype=float32)>\n\n\n\nconv.weights[1] # bias에 해당하는것 \n\n<tf.Variable 'conv2d/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>\n\n\n- 필터값을 원하는 것으로 변경해보자.\n\nw0 = [[0.25,0.25],[0.25,0.25]] # 잡티를 제거하는 효과를 준다. \nw1 = [[-1.0,1.0],[-1.0,1.0]] # 경계를 찾기 좋아보이는 필터이다. (엣지검출)\n\n\nnp.array(w1)\n\narray([[-1.,  1.],\n       [-1.,  1.]])\n\n\n\nw=np.concatenate([np.array(w0).reshape(2,2,1,1),np.array(w1).reshape(2,2,1,1)],axis=-1)\nw\n\narray([[[[ 0.25, -1.  ]],\n\n        [[ 0.25,  1.  ]]],\n\n\n       [[[ 0.25, -1.  ]],\n\n        [[ 0.25,  1.  ]]]])\n\n\n\nb= np.array([0.0,0.0])\nb\n\narray([0., 0.])\n\n\n\nconv.set_weights([w,b])\nconv.get_weights()\n\n[array([[[[ 0.25, -1.  ]],\n \n         [[ 0.25,  1.  ]]],\n \n \n        [[[ 0.25, -1.  ]],\n \n         [[ 0.25,  1.  ]]]], dtype=float32), array([0., 0.], dtype=float32)]\n\n\n\n첫번째는 평균을 구하는 필터\n두번째는 엣지를 검출하는 필터\n\n- 필터를 넣은 결과를 확인\n\nXXX0=conv(XXX)[...,0] # 채널0\nXXX0\n\n<tf.Tensor: shape=(1, 49, 49), dtype=float32, numpy=\narray([[[ 9.829496  , 10.149731  , 10.115938  , ..., -0.4450039 ,\n          0.1526843 ,  0.44488257],\n        [10.625555  ,  9.998542  ,  9.816621  , ..., -1.199796  ,\n         -0.5965334 ,  0.5150786 ],\n        [10.125448  ,  9.55331   ,  9.920209  , ..., -0.09245254,\n         -0.50081515, -0.37436587],\n        ...,\n        [10.242724  , 10.06068   ,  9.8106785 , ...,  0.59299064,\n          0.25008035, -0.6558946 ],\n        [ 9.603472  ,  9.22263   ,  9.8764715 , ...,  0.17234075,\n         -0.52948177, -0.8730371 ],\n        [ 9.072025  ,  9.678471  ,  9.743685  , ..., -0.26665327,\n          0.05082306, -0.3247779 ]]], dtype=float32)>\n\n\n\nXXX1=conv(XXX)[...,1] # 채널1\nXXX1\n\n<tf.Tensor: shape=(1, 49, 49), dtype=float32, numpy=\narray([[[ 7.0648193e-03,  1.2738743e+00, -1.4090481e+00, ...,\n          5.6027925e-01,  1.8304735e+00, -6.6168052e-01],\n        [ 8.9044189e-01, -3.3984947e+00,  2.6708107e+00, ...,\n          4.6802759e-02,  2.3662477e+00,  2.0802004e+00],\n        [ 1.2678986e+00, -3.5564499e+00,  5.0240440e+00, ...,\n         -5.1247388e-01, -1.1209767e+00,  1.6267738e+00],\n        ...,\n        [-8.2698441e-01,  9.8808289e-02, -1.0988159e+00, ...,\n          2.4145689e+00, -3.7862101e+00,  1.6231036e-01],\n        [-2.6984520e+00,  1.1750851e+00,  1.4402809e+00, ...,\n          2.6238339e+00, -5.4311242e+00,  4.0569029e+00],\n        [-2.5749207e-04,  2.4260387e+00, -2.1651821e+00, ...,\n          2.4799771e+00, -1.2100719e+00, -2.9233193e-01]]], dtype=float32)>\n\n\n- 각 채널을 시각화\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2)\n\n\n\n\n\nax1.imshow(XXX.reshape(50,50),cmap='gray')\n\n<matplotlib.image.AxesImage at 0x7fbc5bca1910>\n\n\n\nax3.imshow(XXX0.reshape(49,49),cmap='gray')\n\n<matplotlib.image.AxesImage at 0x7fbc5bbfb1d0>\n\n\n\nax4.imshow(XXX1.reshape(49,49),cmap='gray')\n\n<matplotlib.image.AxesImage at 0x7fbc5bbca450>\n\n\n\nfig\n\n\n\n\n\n2사분면: 원래이미지\n3사분면: 원래이미지 -> 평균을 의미하는 conv적용\n4사분면: 원래이미지 -> 엣지를 검출하는 conv적용\n\n- conv(XXX)의 각 채널에 한번더 conv를 통과시켜보자\n\nconv(XXX0.reshape(1,49,49,1))[...,0] ### XXX0 -> 평균필터 <=> XXX -> 평균필터 -> 평균필터 \nconv(XXX0.reshape(1,49,49,1))[...,1] ### XXX0 -> 엣지필터 <=> XXX -> 평균필터 -> 엣지필터 \nconv(XXX1.reshape(1,49,49,1))[...,0] ### XXX1 -> 평균필터 <=> XXX -> 엣지필터 -> 평균필터 \nconv(XXX1.reshape(1,49,49,1))[...,1] ### XXX1 -> 엣지필터 <=> XXX -> 엣지필터 -> 엣지필터 \n\n<tf.Tensor: shape=(1, 48, 48), dtype=float32, numpy=\narray([[[ -3.0221272 ,   3.386383  ,  -4.2654552 , ...,  -1.2773508 ,\n           3.5896392 ,  -2.778201  ],\n        [ -9.113285  ,  14.649799  , -11.062517  , ...,  -2.4062634 ,\n           1.710942  ,   2.4617033 ],\n        [ -5.9759903 ,  10.901724  ,  -7.9709587 , ...,  -2.7940087 ,\n          -0.18495584,   1.7879481 ],\n        ...,\n        [  2.469574  ,  -3.258545  ,   4.4745617 , ...,   6.023678  ,\n          -9.406242  ,   3.7584436 ],\n        [  4.7993298 ,  -0.93242836,  -5.0819206 , ...,   7.839739  ,\n         -14.255737  ,  13.436548  ],\n        [  6.2998333 ,  -4.326025  ,  -2.8787212 , ...,   9.661198  ,\n         -11.7450075 ,  10.405767  ]]], dtype=float32)>\n\n\n\nfig,ax =plt.subplots(3,4)\n\n\n\n\n\nax[0][0].imshow(XXX.reshape(50,50),cmap='gray') # 원래이미지\n\n<matplotlib.image.AxesImage at 0x7fbc5b8ab210>\n\n\n\nax[1][0].imshow(XXX0.reshape(49,49),cmap='gray') # 원래이미지 -> 평균필터 \nax[1][1].imshow(XXX1.reshape(49,49),cmap='gray') # 원래이미지 -> 엣지필터\n\n<matplotlib.image.AxesImage at 0x7fbc5b87edd0>\n\n\n\nax[2][0].imshow(conv(XXX0.reshape(1,49,49,1))[...,0].reshape(48,48),cmap='gray') # 원래이미지 -> 평균필터 -> 평균필터 \nax[2][1].imshow(conv(XXX0.reshape(1,49,49,1))[...,1].reshape(48,48),cmap='gray') # 원래이미지 -> 평균필터 -> 엣지필터\nax[2][2].imshow(conv(XXX1.reshape(1,49,49,1))[...,0].reshape(48,48),cmap='gray') # 원래이미지 -> 엣지필터 -> 평균필터 \nax[2][3].imshow(conv(XXX1.reshape(1,49,49,1))[...,1].reshape(48,48),cmap='gray') # 원래이미지 -> 엣지필터 -> 엣지필터\n\n<matplotlib.image.AxesImage at 0x7fbc5b8d6250>\n\n\n\nfig.set_figheight(8)\nfig.set_figwidth(16)\nfig.tight_layout()\nfig\n\n\n\n\n- 요약 - conv의 weight에 따라서 엣지를 검출하는 필터가 만들어지기도 하고 스무딩의 역할을 하는 필터가 만들어지기도 한다. 그리고 우리는 의미를 알 수 없지만 어떠한 역할을 하는 필터가 만들어질 것이다. - 이것들을 조합하다보면 우연히 이미지를 분류하기에 유리한 특징을 뽑아내는 weight가 맞춰질 수도 있겠다. - 컨볼루션 레이어의 역할은 이미지의 특징을 추출한다. - 채널수를 많이 만들고 다양한 웨이트 조합을 실험하다보면 보다 복잡한 이미지의 특징을 추출할 수 도있다.\n\n우리가 설정한 에지는 수직선 에지를 찾는데 유리하다.\n\n\nplt.imshow(XXX.reshape(50,50))\n\n<matplotlib.image.AxesImage at 0x7fbc5765a510>\n\n\n\n\n\n\n아래와 같은 이미지에서 설정한 에지에서 적절한 경계를 찾는것이 힘들다\n\n\nplt.imshow(XXX.reshape(50,50).T)\n\n<matplotlib.image.AxesImage at 0x7fbc575b47d0>\n\n\n\n\n\n- 참고: 스트라이드, 패딩 - 스트라이드: 컨볼루션을 수행할 때 윈도우가 1칸씩 이동하는 것이 아니라 2~3칸씩 이동함 - 패딩: 이미지의 가장자리에 정당한 값을 넣어서 (예를들어 0) 컨볼루션을 수행. 따라서 컨볼루션 연산 이후에도 이미지의 크기가 줄어들지 않도록 방지한다.\n\n\nMAXPOOL\n- 기본적역할: 이미지의 크기를 줄이는 것 - 이미지의 크기를 줄여야하는 이유? 어차피 최종적으로 출력값을 클래스 개수 (ex. 10차원)으로 줄어야하므로 - 이미지의 크기를 줄이면서도 동시에 아주 크리티컬한 특징은 손실없이 유지하고 싶다~\n- 점점 작은 이미지가 되면서 중요한 특징들은 살아남지만 그렇지 않으면 죽는다. (캐리커쳐 느낌)\n- 평균이 아니라 max를 쓴 이유는? 그냥 평균보다 나을것이라고 생각했음.. - 그런데 사실은 꼭 그렇지만은 않아서 최근에는 꼭 맥스풀링을 고집하진 않는 추세 (평균풀링도 많이씀)\n\n\nCNN 아키텍처의 표현방법\n- 아래와 같이 아키텍처의 다이어그램형태로 표현하고 굳이 노드별로 이미지를 그리진 않음\n\n- 물론 아래와 같이 그리는 경우도 있음\n\n\n\nDiscusstion about CNN\n- 격자형태로 배열된 자료를 처리하는데 특화된 신경망이다. - 시계열 (1차원격자), 이미지 (2차원격자)\n- 실제응용에서 엄청난 성공을 거두었다.\n- 이름의 유래는 컨볼루션이라는 수학적 연산을 사용했기 때문 - 컨볼루션은 조금 특별한 선형변환이다.\n- 신경과학의 원리가 심층학습에 영향을 미친 사례이다.\n\n\nCNN의 모티브\n- 희소성 + 매개변수의 공유 - 다소 철학적인 모티브임 - 희소성: 이미지를 분석하여 특징을 뽑아낼때 부분부분의 특징만 뽑으면 된다는 의미 - 매개변수의 공유: 한 채널에는 하나의 역할을 하는 커널을 설계하면 된다는 의미 (스무딩이든 엣징이든). 즉 어떤지역은 스무딩, 어떤지역은 엣징을 할 필요가 없이 한채널에서는 엣징만, 다른채널에서는 스무딩만 수행한뒤 여러채널을 조합해서 이해하면 된다.\n- 매개변수 공유효과로 인해서 파라메터가 확 줄어든다.\n(예시) (1,6,6,1) -> (1,5,5,2) - MLP방식이면 (36,50) 의 차원을 가진 매트릭스가 필요함 => 1800개의 매개변수 필요 - CNN은 8개의 매개변수 필요\n\n\nCNN 신경망의 기본구조\n- 기본유닛 - conv - activation - pooling - conv - conv - activation - pooling"
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-23-(12주차).html#모형의-성능을-올리기-위한-노력들",
    "href": "post/Lecture/STBD/2022-05-23-(12주차).html#모형의-성능을-올리기-위한-노력들",
    "title": "13. CNN-2",
    "section": "모형의 성능을 올리기 위한 노력들",
    "text": "모형의 성능을 올리기 위한 노력들\n\ndropout\n- 아래의 예제를 복습하자.\n\nnp.random.seed(43052)\nx = np.linspace(0,1,100).reshape(100,1)\ny = np.random.normal(loc=0,scale=0.01,size=(100,1))\nplt.plot(x,y)\n\n\n\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x,y,epochs=5000,verbose=0,batch_size=100)\n\n2022-05-23 19:33:01.211991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n<keras.callbacks.History at 0x7f1b9528feb0>\n\n\n\nplt.plot(x,y)\nplt.plot(x,net(x),'--')\n\n\n\n\n- train/test로 나누어서 생각해보자.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80)\n\n<keras.callbacks.History at 0x7f1b881f9840>\n\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\n\n\n\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\nplt.plot(x[80:],net(x[80:]),'--')\n\n\n\n\n\ntrain에서 추세를 따라가는게 좋은게 아니다 \\(\\to\\) 그냥 직선으로 핏하는거 이외에는 다 오버핏이다.\n\n- 매 에폭마다 적당히 80%의 노드들을 빼고 학습하자 \\(\\to\\) 너무 잘 학습되는 문제는 생기지 않을 것이다 (과적합이 방지될것이다?)\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dropout(0.8))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80)\n\n<keras.callbacks.History at 0x7f1b80381a50>\n\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\nplt.plot(x[80:],net(x[80:]),'--')\n\n\n\n\n- 드랍아웃에 대한 summary - 직관: 특정노드를 랜덤으로 off시키면 학습이 방해되어 오히려 과적합이 방지되는 효과가 있다 (그렇지만 진짜 중요한 특징이라면 랜덤으로 off 되더라도 어느정도는 학습될 듯) - note: 드랍아웃을 쓰면 오버핏이 줄어드는건 맞지만 완전히 없어지는건 아니다. - note: 오버핏을 줄이는 유일한 방법이 드랍아웃만 있는것도 아니며, 드랍아웃이 오버핏을 줄이는 가장 효과적인 방법도 아니다 (최근에는 dropout보다 batch nomalization을 사용하는 추세임)\n\n\ntrain / val / test\n- data\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n32768/29515 [=================================] - 0s 0us/step\n40960/29515 [=========================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26427392/26421880 [==============================] - 0s 0us/step\n26435584/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n16384/5148 [===============================================================================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4423680/4422102 [==============================] - 0s 0us/step\n4431872/4422102 [==============================] - 0s 0us/step\n\n\n\nX= x_train.reshape(-1,28,28,1)/255 ## 입력이 0~255 -> 0~1로 표준화 시키는 효과 + float으로 자료형이 바뀜 \ny = tf.keras.utils.to_categorical(y_train)\nXX = x_test.reshape(-1,28,28,1)/255\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\nimport tensorflow as tf\n\n\ntf.config.experimental.list_physical_devices('GPU')\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\n#collapse_output\ncb1 = tf.keras.callbacks.TensorBoard()\nnet.fit(X,y,epochs=5,batch_size=200,validation_split=0.2,callbacks=cb1,verbose=1) \n\nEpoch 1/5\n240/240 [==============================] - 1s 3ms/step - loss: 0.0677 - accuracy: 0.9765 - val_loss: 0.6648 - val_accuracy: 0.8743\nEpoch 2/5\n240/240 [==============================] - 1s 3ms/step - loss: 0.0685 - accuracy: 0.9769 - val_loss: 0.6766 - val_accuracy: 0.8745\nEpoch 3/5\n240/240 [==============================] - 1s 3ms/step - loss: 0.0702 - accuracy: 0.9748 - val_loss: 0.6618 - val_accuracy: 0.8766\nEpoch 4/5\n240/240 [==============================] - 1s 3ms/step - loss: 0.0647 - accuracy: 0.9778 - val_loss: 0.6751 - val_accuracy: 0.8758\nEpoch 5/5\n240/240 [==============================] - 1s 3ms/step - loss: 0.0659 - accuracy: 0.9775 - val_loss: 0.6805 - val_accuracy: 0.8746\n\n\n<keras.callbacks.History at 0x7fdf70127910>\n\n\n- 텐서보드 여는 방법1\n\n%reload_ext tensorboard\n# 주피터노트북 (혹은 주피터랩)에서 텐서보드를 임베딩하여 넣을 수 있도록 도와주는 매직펑션\n\n\n#\n# !rm -rf logs\n# !kill 313799\n\n\n#\n# %tensorboard --logdir logs --host 0.0.0.0\n%tensorboard --logdir logs # <-- 실습에서는 이렇게 하면됩니다. \n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n프로세스 죽이기\n\n!kill 1424\n\n\n\n폴더안에 파일 지우기\n\n!rm -rf logs\n\n(참고사항) 파이썬 3.10의 경우 아래의 수정이 필요\n?/python3.10/site-packages/tensorboard/_vendor/html5lib/_trie/_base.py 을 열고\nfrom collections import Mapping ### 수정전\nfrom collections.abc import Mapping ### 수정후 \n와 같이 수정한다.\n\n왜냐하면 파이썬 3.10부터 from collections import Mapping 가 동작하지 않고 from collections.abc import Mapping 가 동작하도록 문법이 바뀜\n\n- 텐서보드를 실행하는 방법2\n\n#\n# !tensorboard --logdir logs --host 0.0.0.0\n# !tensorboard --logdir logs # <-- 실습에서는 이렇게 하면됩니다. \n\n\n\n\n조기종료\n- 텐서보드를 살펴보니 특정에폭 이후에는 오히려 과적합이 진행되는 듯 하다 (학습할수록 손해인듯 하다) \\(\\to\\) 그 특정에폭까지만 학습해보자\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 과적합좀 시키려고 \nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가 \nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 4ms/step - loss: 0.5483 - accuracy: 0.8134 - val_loss: 0.4027 - val_accuracy: 0.8546\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.3568 - accuracy: 0.8671 - val_loss: 0.3531 - val_accuracy: 0.8712\nEpoch 3/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.3210 - accuracy: 0.8799 - val_loss: 0.3477 - val_accuracy: 0.8733\nEpoch 4/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2971 - accuracy: 0.8876 - val_loss: 0.3502 - val_accuracy: 0.8776\n\n\n<keras.callbacks.History at 0x7f1b80086650>\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2791 - accuracy: 0.8935 - val_loss: 0.3224 - val_accuracy: 0.8820\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2619 - accuracy: 0.8999 - val_loss: 0.3498 - val_accuracy: 0.8779\n\n\n<keras.callbacks.History at 0x7f1b24290a90>\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2491 - accuracy: 0.9043 - val_loss: 0.3641 - val_accuracy: 0.8711\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2328 - accuracy: 0.9110 - val_loss: 0.3282 - val_accuracy: 0.8848\nEpoch 3/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2254 - accuracy: 0.9151 - val_loss: 0.3280 - val_accuracy: 0.8843\nEpoch 4/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2144 - accuracy: 0.9177 - val_loss: 0.3191 - val_accuracy: 0.8925\nEpoch 5/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2074 - accuracy: 0.9223 - val_loss: 0.3152 - val_accuracy: 0.8949\nEpoch 6/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1952 - accuracy: 0.9250 - val_loss: 0.3322 - val_accuracy: 0.8863\n\n\n<keras.callbacks.History at 0x7f1b242c1660>\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1908 - accuracy: 0.9257 - val_loss: 0.3513 - val_accuracy: 0.8836\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1799 - accuracy: 0.9304 - val_loss: 0.3376 - val_accuracy: 0.8901\nEpoch 3/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1712 - accuracy: 0.9346 - val_loss: 0.3568 - val_accuracy: 0.8894\n\n\n<keras.callbacks.History at 0x7f1b24302230>\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1591 - accuracy: 0.9367 - val_loss: 0.3995 - val_accuracy: 0.8780\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1552 - accuracy: 0.9398 - val_loss: 0.3469 - val_accuracy: 0.8917\nEpoch 3/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1481 - accuracy: 0.9423 - val_loss: 0.3726 - val_accuracy: 0.8853\n\n\n<keras.callbacks.History at 0x7f1b24136e00>\n\n\n- 몇 번 좀 참았다가 멈추면 좋겠다.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 과적합좀 시키려고 \nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가 \nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=5) # 좀더 참다가 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 4ms/step - loss: 0.5475 - accuracy: 0.8139 - val_loss: 0.4219 - val_accuracy: 0.8453\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.3575 - accuracy: 0.8676 - val_loss: 0.3647 - val_accuracy: 0.8712\nEpoch 3/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.3219 - accuracy: 0.8792 - val_loss: 0.3559 - val_accuracy: 0.8710\nEpoch 4/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2990 - accuracy: 0.8883 - val_loss: 0.3448 - val_accuracy: 0.8808\nEpoch 5/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2759 - accuracy: 0.8966 - val_loss: 0.3337 - val_accuracy: 0.8792\nEpoch 6/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2621 - accuracy: 0.9004 - val_loss: 0.3220 - val_accuracy: 0.8841\nEpoch 7/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2478 - accuracy: 0.9074 - val_loss: 0.3302 - val_accuracy: 0.8858\nEpoch 8/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2342 - accuracy: 0.9110 - val_loss: 0.3150 - val_accuracy: 0.8904\nEpoch 9/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2261 - accuracy: 0.9144 - val_loss: 0.3117 - val_accuracy: 0.8932\nEpoch 10/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2116 - accuracy: 0.9200 - val_loss: 0.3345 - val_accuracy: 0.8888\nEpoch 11/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2081 - accuracy: 0.9207 - val_loss: 0.3344 - val_accuracy: 0.8867\nEpoch 12/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1956 - accuracy: 0.9255 - val_loss: 0.3158 - val_accuracy: 0.8975\nEpoch 13/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1863 - accuracy: 0.9275 - val_loss: 0.3302 - val_accuracy: 0.8934\nEpoch 14/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1764 - accuracy: 0.9324 - val_loss: 0.3717 - val_accuracy: 0.8859\n\n\n<keras.callbacks.History at 0x7f1b24301960>\n\n\n- 텐서보드로 그려보자?\n\n#\n# %tensorboard --logdir logs --host 0.0.0.0 \n# 아무것도 안나온다 -> 왜? cb1을 써야 텐서보드가 나옴\n\n- 조기종료와 텐서보드를 같이 쓰려면?\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu')) \nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\ncb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=7) # 좀더 참다가 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=[cb1,cb2]) \n\nEpoch 1/200\n240/240 [==============================] - 0s 1ms/step - loss: 0.7184 - accuracy: 0.7581 - val_loss: 0.5077 - val_accuracy: 0.8276\nEpoch 2/200\n240/240 [==============================] - 0s 890us/step - loss: 0.4752 - accuracy: 0.8386 - val_loss: 0.4793 - val_accuracy: 0.8342\nEpoch 3/200\n240/240 [==============================] - 0s 899us/step - loss: 0.4304 - accuracy: 0.8517 - val_loss: 0.4386 - val_accuracy: 0.8497\nEpoch 4/200\n240/240 [==============================] - 0s 880us/step - loss: 0.4048 - accuracy: 0.8582 - val_loss: 0.4029 - val_accuracy: 0.8603\nEpoch 5/200\n240/240 [==============================] - 0s 923us/step - loss: 0.3832 - accuracy: 0.8669 - val_loss: 0.3932 - val_accuracy: 0.8619\nEpoch 6/200\n240/240 [==============================] - 0s 934us/step - loss: 0.3697 - accuracy: 0.8705 - val_loss: 0.3842 - val_accuracy: 0.8657\nEpoch 7/200\n240/240 [==============================] - 0s 900us/step - loss: 0.3569 - accuracy: 0.8759 - val_loss: 0.3844 - val_accuracy: 0.8668\nEpoch 8/200\n240/240 [==============================] - 0s 889us/step - loss: 0.3482 - accuracy: 0.8774 - val_loss: 0.3679 - val_accuracy: 0.8708\nEpoch 9/200\n240/240 [==============================] - 0s 912us/step - loss: 0.3387 - accuracy: 0.8799 - val_loss: 0.3602 - val_accuracy: 0.8719\nEpoch 10/200\n240/240 [==============================] - 0s 923us/step - loss: 0.3299 - accuracy: 0.8820 - val_loss: 0.3610 - val_accuracy: 0.8748\nEpoch 11/200\n240/240 [==============================] - 0s 853us/step - loss: 0.3229 - accuracy: 0.8858 - val_loss: 0.3574 - val_accuracy: 0.8717\nEpoch 12/200\n240/240 [==============================] - 0s 904us/step - loss: 0.3157 - accuracy: 0.8873 - val_loss: 0.3572 - val_accuracy: 0.8743\nEpoch 13/200\n240/240 [==============================] - 0s 890us/step - loss: 0.3106 - accuracy: 0.8899 - val_loss: 0.3545 - val_accuracy: 0.8761\nEpoch 14/200\n240/240 [==============================] - 0s 911us/step - loss: 0.3046 - accuracy: 0.8914 - val_loss: 0.3493 - val_accuracy: 0.8759\nEpoch 15/200\n240/240 [==============================] - 0s 921us/step - loss: 0.3011 - accuracy: 0.8928 - val_loss: 0.3483 - val_accuracy: 0.8776\nEpoch 16/200\n240/240 [==============================] - 0s 937us/step - loss: 0.2988 - accuracy: 0.8935 - val_loss: 0.3733 - val_accuracy: 0.8716\nEpoch 17/200\n240/240 [==============================] - 0s 892us/step - loss: 0.2925 - accuracy: 0.8947 - val_loss: 0.3481 - val_accuracy: 0.8768\nEpoch 18/200\n240/240 [==============================] - 0s 933us/step - loss: 0.2880 - accuracy: 0.8951 - val_loss: 0.3396 - val_accuracy: 0.8801\nEpoch 19/200\n240/240 [==============================] - 0s 957us/step - loss: 0.2827 - accuracy: 0.8982 - val_loss: 0.3439 - val_accuracy: 0.8798\nEpoch 20/200\n240/240 [==============================] - 0s 881us/step - loss: 0.2791 - accuracy: 0.8986 - val_loss: 0.3489 - val_accuracy: 0.8779\nEpoch 21/200\n240/240 [==============================] - 0s 886us/step - loss: 0.2765 - accuracy: 0.9007 - val_loss: 0.3350 - val_accuracy: 0.8823\nEpoch 22/200\n240/240 [==============================] - 0s 912us/step - loss: 0.2709 - accuracy: 0.9016 - val_loss: 0.3350 - val_accuracy: 0.8812\nEpoch 23/200\n240/240 [==============================] - 0s 908us/step - loss: 0.2688 - accuracy: 0.9029 - val_loss: 0.3374 - val_accuracy: 0.8820\nEpoch 24/200\n240/240 [==============================] - 0s 930us/step - loss: 0.2658 - accuracy: 0.9041 - val_loss: 0.3445 - val_accuracy: 0.8805\nEpoch 25/200\n240/240 [==============================] - 0s 872us/step - loss: 0.2607 - accuracy: 0.9058 - val_loss: 0.3383 - val_accuracy: 0.8822\nEpoch 26/200\n240/240 [==============================] - 0s 928us/step - loss: 0.2607 - accuracy: 0.9056 - val_loss: 0.3415 - val_accuracy: 0.8811\nEpoch 27/200\n240/240 [==============================] - 0s 927us/step - loss: 0.2576 - accuracy: 0.9068 - val_loss: 0.3402 - val_accuracy: 0.8814\nEpoch 28/200\n240/240 [==============================] - 0s 905us/step - loss: 0.2525 - accuracy: 0.9098 - val_loss: 0.3469 - val_accuracy: 0.8802\n\n\n<keras.callbacks.History at 0x7f1b24217a00>\n\n\n\n# \n# 조기종료가 구현된 그림이 출력\n# %tensorboard --logdir logs --host 0.0.0.0 \n\n\n\n하이퍼파라메터 선택\n- 하이퍼파라메터 설정\n\nfrom tensorboard.plugins.hparams import api as hp\n\n\na=net.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 859us/step - loss: 0.3803 - accuracy: 0.8704\n\n\n\n!rm -rf logs\nfor u in [50,5000]: \n    for d in [0.0,0.5]: \n        for o in ['adam','sgd']:\n            logdir = 'logs/hpguebin_{}_{}_{}'.format(u,d,o)\n            with tf.summary.create_file_writer(logdir).as_default():\n                net = tf.keras.Sequential()\n                net.add(tf.keras.layers.Flatten())\n                net.add(tf.keras.layers.Dense(u,activation='relu'))\n                net.add(tf.keras.layers.Dropout(d))\n                net.add(tf.keras.layers.Dense(10,activation='softmax'))\n                net.compile(optimizer=o,loss=tf.losses.categorical_crossentropy,metrics=['accuracy','Recall'])\n                cb3 = hp.KerasCallback(logdir, {'유닛수':u, '드랍아웃비율':d, '옵티마이저':o})\n                net.fit(X,y,epochs=3,callbacks=cb3)\n                _rslt=net.evaluate(XX,yy)\n                _mymetric=_rslt[1]*0.8 + _rslt[2]*0.2  \n                tf.summary.scalar('애큐러시와리컬의가중평균(테스트셋)', _mymetric, step=1) \n\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5255 - accuracy: 0.8180 - recall: 0.7546\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.3993 - accuracy: 0.8588 - recall: 0.8294\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.3648 - accuracy: 0.8698 - recall: 0.8443\n313/313 [==============================] - 0s 830us/step - loss: 0.4063 - accuracy: 0.8545 - recall: 0.8286\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.7744 - accuracy: 0.7503 - recall: 0.5797\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5204 - accuracy: 0.8223 - recall: 0.7565\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4742 - accuracy: 0.8369 - recall: 0.7859\n313/313 [==============================] - 0s 828us/step - loss: 0.4899 - accuracy: 0.8304 - recall: 0.7831\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.7502 - accuracy: 0.7356 - recall: 0.6115\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5738 - accuracy: 0.7923 - recall: 0.7133\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5473 - accuracy: 0.8037 - recall: 0.7321\n313/313 [==============================] - 0s 865us/step - loss: 0.4319 - accuracy: 0.8448 - recall: 0.7919\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 1.0932 - accuracy: 0.6228 - recall: 0.3971\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.7616 - accuracy: 0.7388 - recall: 0.5956\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6828 - accuracy: 0.7684 - recall: 0.6478\n313/313 [==============================] - 0s 894us/step - loss: 0.5265 - accuracy: 0.8180 - recall: 0.7353\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4777 - accuracy: 0.8292 - recall: 0.7890\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.3603 - accuracy: 0.8682 - recall: 0.8427\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.3197 - accuracy: 0.8817 - recall: 0.8605\n313/313 [==============================] - 0s 846us/step - loss: 0.3803 - accuracy: 0.8628 - recall: 0.8428\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6685 - accuracy: 0.7883 - recall: 0.6444\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4815 - accuracy: 0.8372 - recall: 0.7781\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4408 - accuracy: 0.8498 - recall: 0.8021\n313/313 [==============================] - 0s 859us/step - loss: 0.4634 - accuracy: 0.8390 - recall: 0.7962\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5708 - accuracy: 0.7991 - recall: 0.7556\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4418 - accuracy: 0.8393 - recall: 0.8057\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4091 - accuracy: 0.8514 - recall: 0.8211\n313/313 [==============================] - 0s 850us/step - loss: 0.3937 - accuracy: 0.8587 - recall: 0.8238\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6930 - accuracy: 0.7752 - recall: 0.6338\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5048 - accuracy: 0.8274 - recall: 0.7651\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4608 - accuracy: 0.8417 - recall: 0.7910\n313/313 [==============================] - 0s 854us/step - loss: 0.4625 - accuracy: 0.8396 - recall: 0.7957\n\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0"
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-23-(12주차).html#숙제",
    "href": "post/Lecture/STBD/2022-05-23-(12주차).html#숙제",
    "title": "13. CNN-2",
    "section": "숙제",
    "text": "숙제\n- 아래의 네트워크에서 옵티마이저를 adam, sgd를 선택하여 각각 적합시켜보고 testset의 loss를 성능비교를 하라. epoch은 5정도로 설정하라.\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer=???,loss=tf.losses.categorical_crossentropy,metrics=['accuracy','Recall'])"
  },
  {
    "objectID": "post/Lecture/STBD/2022-05-30-(13주차).html",
    "href": "post/Lecture/STBD/2022-05-30-(13주차).html",
    "title": "13. model fitting",
    "section": "",
    "text": "imports\n\nimport numpy as np\nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt \n\n\n%load_ext tensorboard\n\n\n\n오버피팅\n\n오버피팅으로 착각하기 쉬운 상황\n3-(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i\\]\n여기에서 \\(t=(t_1,\\dots,t_{1000})=\\) np.linspace(0,5,1000) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.linspace(0,5,1000)\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.1)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2,3,1,0,0,0.5 이다)\n(풀이)\n- 다시 풀어보자\n\ny = y.reshape(1000,1)\nx1 = np.cos(t) \nx2 = np.cos(2*t)\nx3 = np.cos(3*t)\nx4 = np.cos(4*t)\nx5 = np.cos(5*t)\nX = tf.stack([x1,x2,x3,x4,x5],axis=1)\n\n2022-05-25 13:34:57.890242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(loss='mse',optimizer='adam')\nnet.fit(X,y,epochs=500,batch_size=100, validation_split=0.45, callbacks=tf.keras.callbacks.TensorBoard()) \n# 텐서보드를 이용한 시각화기능 추가 \n# validation_split 이용\n\nEpoch 1/500\n6/6 [==============================] - 0s 9ms/step - loss: 6.2200 - val_loss: 16.9707\nEpoch 2/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.1682 - val_loss: 16.9098\nEpoch 3/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.1168 - val_loss: 16.8487\nEpoch 4/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.0665 - val_loss: 16.7881\nEpoch 5/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.0170 - val_loss: 16.7266\nEpoch 6/500\n6/6 [==============================] - 0s 4ms/step - loss: 5.9663 - val_loss: 16.6675\nEpoch 7/500\n6/6 [==============================] - 0s 4ms/step - loss: 5.9185 - val_loss: 16.6086\nEpoch 8/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.8679 - val_loss: 16.5515\nEpoch 9/500\n6/6 [==============================] - 0s 4ms/step - loss: 5.8200 - val_loss: 16.4926\nEpoch 10/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.7724 - val_loss: 16.4347\nEpoch 11/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.7247 - val_loss: 16.3748\nEpoch 12/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.6759 - val_loss: 16.3188\nEpoch 13/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.6296 - val_loss: 16.2615\nEpoch 14/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.5824 - val_loss: 16.2049\nEpoch 15/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.5361 - val_loss: 16.1485\nEpoch 16/500\n6/6 [==============================] - 0s 4ms/step - loss: 5.4899 - val_loss: 16.0936\nEpoch 17/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.4436 - val_loss: 16.0364\nEpoch 18/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.3988 - val_loss: 15.9806\nEpoch 19/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.3545 - val_loss: 15.9251\nEpoch 20/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.3087 - val_loss: 15.8708\nEpoch 21/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.2653 - val_loss: 15.8172\nEpoch 22/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.2204 - val_loss: 15.7660\nEpoch 23/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.1774 - val_loss: 15.7140\nEpoch 24/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.1352 - val_loss: 15.6609\nEpoch 25/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.0921 - val_loss: 15.6077\nEpoch 26/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.0496 - val_loss: 15.5556\nEpoch 27/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.0074 - val_loss: 15.5049\nEpoch 28/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.9658 - val_loss: 15.4536\nEpoch 29/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.9243 - val_loss: 15.4012\nEpoch 30/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.8829 - val_loss: 15.3500\nEpoch 31/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.8414 - val_loss: 15.2996\nEpoch 32/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.8004 - val_loss: 15.2499\nEpoch 33/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.7603 - val_loss: 15.1995\nEpoch 34/500\n6/6 [==============================] - 0s 4ms/step - loss: 4.7195 - val_loss: 15.1498\nEpoch 35/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.6804 - val_loss: 15.1006\nEpoch 36/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.6396 - val_loss: 15.0534\nEpoch 37/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.6005 - val_loss: 15.0050\nEpoch 38/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.5618 - val_loss: 14.9572\nEpoch 39/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.5234 - val_loss: 14.9099\nEpoch 40/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.4840 - val_loss: 14.8627\nEpoch 41/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.4467 - val_loss: 14.8146\nEpoch 42/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.4087 - val_loss: 14.7684\nEpoch 43/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.3711 - val_loss: 14.7220\nEpoch 44/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.3334 - val_loss: 14.6753\nEpoch 45/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.2970 - val_loss: 14.6294\nEpoch 46/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.2603 - val_loss: 14.5826\nEpoch 47/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.2232 - val_loss: 14.5386\nEpoch 48/500\n6/6 [==============================] - 0s 4ms/step - loss: 4.1877 - val_loss: 14.4930\nEpoch 49/500\n6/6 [==============================] - 0s 4ms/step - loss: 4.1513 - val_loss: 14.4489\nEpoch 50/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.1169 - val_loss: 14.4047\nEpoch 51/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.0811 - val_loss: 14.3595\nEpoch 52/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.0459 - val_loss: 14.3128\nEpoch 53/500\n6/6 [==============================] - 0s 4ms/step - loss: 4.0111 - val_loss: 14.2681\nEpoch 54/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.9770 - val_loss: 14.2218\nEpoch 55/500\n6/6 [==============================] - 0s 4ms/step - loss: 3.9416 - val_loss: 14.1764\nEpoch 56/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.9088 - val_loss: 14.1324\nEpoch 57/500\n6/6 [==============================] - 0s 4ms/step - loss: 3.8748 - val_loss: 14.0885\nEpoch 58/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.8411 - val_loss: 14.0468\nEpoch 59/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.8085 - val_loss: 14.0052\nEpoch 60/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.7749 - val_loss: 13.9633\nEpoch 61/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.7425 - val_loss: 13.9234\nEpoch 62/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.7106 - val_loss: 13.8829\nEpoch 63/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.6800 - val_loss: 13.8406\nEpoch 64/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.6468 - val_loss: 13.8024\nEpoch 65/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.6158 - val_loss: 13.7614\nEpoch 66/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.5847 - val_loss: 13.7200\nEpoch 67/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.5536 - val_loss: 13.6788\nEpoch 68/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.5231 - val_loss: 13.6385\nEpoch 69/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.4927 - val_loss: 13.6000\nEpoch 70/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.4631 - val_loss: 13.5597\nEpoch 71/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.4322 - val_loss: 13.5206\nEpoch 72/500\n6/6 [==============================] - 0s 2ms/step - loss: 3.4026 - val_loss: 13.4812\nEpoch 73/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.3729 - val_loss: 13.4425\nEpoch 74/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.3431 - val_loss: 13.4058\nEpoch 75/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.3148 - val_loss: 13.3676\nEpoch 76/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.2857 - val_loss: 13.3286\nEpoch 77/500\n6/6 [==============================] - 0s 2ms/step - loss: 3.2565 - val_loss: 13.2909\nEpoch 78/500\n6/6 [==============================] - 0s 2ms/step - loss: 3.2284 - val_loss: 13.2534\nEpoch 79/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.1992 - val_loss: 13.2167\nEpoch 80/500\n6/6 [==============================] - 0s 4ms/step - loss: 3.1722 - val_loss: 13.1812\nEpoch 81/500\n6/6 [==============================] - 0s 4ms/step - loss: 3.1444 - val_loss: 13.1452\nEpoch 82/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.1169 - val_loss: 13.1083\nEpoch 83/500\n6/6 [==============================] - 0s 2ms/step - loss: 3.0901 - val_loss: 13.0695\nEpoch 84/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.0625 - val_loss: 13.0314\nEpoch 85/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.0359 - val_loss: 12.9948\nEpoch 86/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.0088 - val_loss: 12.9592\nEpoch 87/500\n6/6 [==============================] - 0s 4ms/step - loss: 2.9823 - val_loss: 12.9249\nEpoch 88/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.9566 - val_loss: 12.8900\nEpoch 89/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.9307 - val_loss: 12.8534\nEpoch 90/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.9050 - val_loss: 12.8185\nEpoch 91/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.8792 - val_loss: 12.7831\nEpoch 92/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.8542 - val_loss: 12.7482\nEpoch 93/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.8289 - val_loss: 12.7135\nEpoch 94/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.8041 - val_loss: 12.6790\nEpoch 95/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.7795 - val_loss: 12.6438\nEpoch 96/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.7546 - val_loss: 12.6110\nEpoch 97/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.7307 - val_loss: 12.5763\nEpoch 98/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.7060 - val_loss: 12.5430\nEpoch 99/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.6825 - val_loss: 12.5093\nEpoch 100/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.6584 - val_loss: 12.4760\nEpoch 101/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.6350 - val_loss: 12.4419\nEpoch 102/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.6117 - val_loss: 12.4088\nEpoch 103/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.5882 - val_loss: 12.3750\nEpoch 104/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.5657 - val_loss: 12.3433\nEpoch 105/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.5425 - val_loss: 12.3114\nEpoch 106/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.5202 - val_loss: 12.2804\nEpoch 107/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.4979 - val_loss: 12.2476\nEpoch 108/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.4755 - val_loss: 12.2145\nEpoch 109/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.4537 - val_loss: 12.1828\nEpoch 110/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.4314 - val_loss: 12.1527\nEpoch 111/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.4102 - val_loss: 12.1212\nEpoch 112/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3888 - val_loss: 12.0904\nEpoch 113/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3681 - val_loss: 12.0618\nEpoch 114/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3466 - val_loss: 12.0304\nEpoch 115/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3263 - val_loss: 12.0002\nEpoch 116/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3053 - val_loss: 11.9704\nEpoch 117/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.2849 - val_loss: 11.9400\nEpoch 118/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.2647 - val_loss: 11.9106\nEpoch 119/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.2438 - val_loss: 11.8820\nEpoch 120/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.2241 - val_loss: 11.8520\nEpoch 121/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.2039 - val_loss: 11.8224\nEpoch 122/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.1843 - val_loss: 11.7940\nEpoch 123/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.1648 - val_loss: 11.7655\nEpoch 124/500\n6/6 [==============================] - 0s 4ms/step - loss: 2.1454 - val_loss: 11.7361\nEpoch 125/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.1262 - val_loss: 11.7072\nEpoch 126/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.1071 - val_loss: 11.6805\nEpoch 127/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.0886 - val_loss: 11.6536\nEpoch 128/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.0695 - val_loss: 11.6254\nEpoch 129/500\n6/6 [==============================] - 0s 2ms/step - loss: 2.0515 - val_loss: 11.5969\nEpoch 130/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.0327 - val_loss: 11.5688\nEpoch 131/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.0150 - val_loss: 11.5409\nEpoch 132/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9966 - val_loss: 11.5148\nEpoch 133/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9790 - val_loss: 11.4873\nEpoch 134/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9609 - val_loss: 11.4611\nEpoch 135/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9438 - val_loss: 11.4336\nEpoch 136/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9268 - val_loss: 11.4073\nEpoch 137/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9093 - val_loss: 11.3819\nEpoch 138/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.8922 - val_loss: 11.3557\nEpoch 139/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.8756 - val_loss: 11.3285\nEpoch 140/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.8586 - val_loss: 11.3015\nEpoch 141/500\n6/6 [==============================] - 0s 2ms/step - loss: 1.8418 - val_loss: 11.2759\nEpoch 142/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.8256 - val_loss: 11.2502\nEpoch 143/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.8090 - val_loss: 11.2256\nEpoch 144/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7927 - val_loss: 11.1994\nEpoch 145/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7769 - val_loss: 11.1727\nEpoch 146/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7608 - val_loss: 11.1471\nEpoch 147/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7451 - val_loss: 11.1210\nEpoch 148/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7294 - val_loss: 11.0955\nEpoch 149/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.7142 - val_loss: 11.0686\nEpoch 150/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6985 - val_loss: 11.0437\nEpoch 151/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6833 - val_loss: 11.0186\nEpoch 152/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6682 - val_loss: 10.9943\nEpoch 153/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.6531 - val_loss: 10.9703\nEpoch 154/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6386 - val_loss: 10.9462\nEpoch 155/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6239 - val_loss: 10.9234\nEpoch 156/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6093 - val_loss: 10.9001\nEpoch 157/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5949 - val_loss: 10.8762\nEpoch 158/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5811 - val_loss: 10.8518\nEpoch 159/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5667 - val_loss: 10.8276\nEpoch 160/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5530 - val_loss: 10.8025\nEpoch 161/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5387 - val_loss: 10.7807\nEpoch 162/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5250 - val_loss: 10.7566\nEpoch 163/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5115 - val_loss: 10.7325\nEpoch 164/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4977 - val_loss: 10.7099\nEpoch 165/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4846 - val_loss: 10.6866\nEpoch 166/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4714 - val_loss: 10.6642\nEpoch 167/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4582 - val_loss: 10.6410\nEpoch 168/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4451 - val_loss: 10.6192\nEpoch 169/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4321 - val_loss: 10.5966\nEpoch 170/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4194 - val_loss: 10.5742\nEpoch 171/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4066 - val_loss: 10.5521\nEpoch 172/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3942 - val_loss: 10.5301\nEpoch 173/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3817 - val_loss: 10.5074\nEpoch 174/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3695 - val_loss: 10.4855\nEpoch 175/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3575 - val_loss: 10.4629\nEpoch 176/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.3451 - val_loss: 10.4417\nEpoch 177/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3332 - val_loss: 10.4193\nEpoch 178/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3214 - val_loss: 10.3978\nEpoch 179/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3094 - val_loss: 10.3760\nEpoch 180/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2979 - val_loss: 10.3535\nEpoch 181/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2867 - val_loss: 10.3313\nEpoch 182/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2751 - val_loss: 10.3107\nEpoch 183/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2640 - val_loss: 10.2911\nEpoch 184/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.2527 - val_loss: 10.2700\nEpoch 185/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2418 - val_loss: 10.2492\nEpoch 186/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.2310 - val_loss: 10.2279\nEpoch 187/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2201 - val_loss: 10.2073\nEpoch 188/500\n6/6 [==============================] - 0s 2ms/step - loss: 1.2098 - val_loss: 10.1864\nEpoch 189/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1989 - val_loss: 10.1660\nEpoch 190/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1886 - val_loss: 10.1447\nEpoch 191/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1780 - val_loss: 10.1253\nEpoch 192/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1680 - val_loss: 10.1046\nEpoch 193/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.1579 - val_loss: 10.0847\nEpoch 194/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1477 - val_loss: 10.0651\nEpoch 195/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1379 - val_loss: 10.0455\nEpoch 196/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1280 - val_loss: 10.0270\nEpoch 197/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.1183 - val_loss: 10.0083\nEpoch 198/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.1086 - val_loss: 9.9880\nEpoch 199/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0990 - val_loss: 9.9680\nEpoch 200/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0896 - val_loss: 9.9479\nEpoch 201/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0802 - val_loss: 9.9284\nEpoch 202/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0708 - val_loss: 9.9092\nEpoch 203/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0616 - val_loss: 9.8902\nEpoch 204/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0527 - val_loss: 9.8713\nEpoch 205/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0435 - val_loss: 9.8533\nEpoch 206/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0346 - val_loss: 9.8335\nEpoch 207/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0260 - val_loss: 9.8141\nEpoch 208/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0172 - val_loss: 9.7948\nEpoch 209/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0084 - val_loss: 9.7757\nEpoch 210/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0000 - val_loss: 9.7570\nEpoch 211/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9918 - val_loss: 9.7374\nEpoch 212/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9832 - val_loss: 9.7190\nEpoch 213/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9752 - val_loss: 9.6991\nEpoch 214/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9669 - val_loss: 9.6805\nEpoch 215/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9588 - val_loss: 9.6630\nEpoch 216/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9509 - val_loss: 9.6449\nEpoch 217/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9429 - val_loss: 9.6260\nEpoch 218/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9351 - val_loss: 9.6078\nEpoch 219/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9272 - val_loss: 9.5889\nEpoch 220/500\n6/6 [==============================] - 0s 2ms/step - loss: 0.9194 - val_loss: 9.5704\nEpoch 221/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9117 - val_loss: 9.5517\nEpoch 222/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9043 - val_loss: 9.5328\nEpoch 223/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8967 - val_loss: 9.5144\nEpoch 224/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8893 - val_loss: 9.4963\nEpoch 225/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8820 - val_loss: 9.4785\nEpoch 226/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8748 - val_loss: 9.4600\nEpoch 227/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8675 - val_loss: 9.4414\nEpoch 228/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8605 - val_loss: 9.4222\nEpoch 229/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.8535 - val_loss: 9.4037\nEpoch 230/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8464 - val_loss: 9.3854\nEpoch 231/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8396 - val_loss: 9.3677\nEpoch 232/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8328 - val_loss: 9.3485\nEpoch 233/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8262 - val_loss: 9.3306\nEpoch 234/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.8196 - val_loss: 9.3123\nEpoch 235/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8130 - val_loss: 9.2945\nEpoch 236/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8064 - val_loss: 9.2777\nEpoch 237/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8002 - val_loss: 9.2597\nEpoch 238/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7938 - val_loss: 9.2424\nEpoch 239/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.7875 - val_loss: 9.2246\nEpoch 240/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.7812 - val_loss: 9.2078\nEpoch 241/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7752 - val_loss: 9.1906\nEpoch 242/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7690 - val_loss: 9.1731\nEpoch 243/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.7630 - val_loss: 9.1547\nEpoch 244/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7570 - val_loss: 9.1369\nEpoch 245/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7511 - val_loss: 9.1204\nEpoch 246/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7454 - val_loss: 9.1022\nEpoch 247/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7395 - val_loss: 9.0852\nEpoch 248/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7338 - val_loss: 9.0677\nEpoch 249/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7282 - val_loss: 9.0500\nEpoch 250/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7227 - val_loss: 9.0341\nEpoch 251/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7172 - val_loss: 9.0179\nEpoch 252/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7118 - val_loss: 9.0014\nEpoch 253/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7064 - val_loss: 8.9845\nEpoch 254/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7011 - val_loss: 8.9669\nEpoch 255/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6958 - val_loss: 8.9504\nEpoch 256/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6907 - val_loss: 8.9342\nEpoch 257/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6855 - val_loss: 8.9176\nEpoch 258/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6804 - val_loss: 8.9012\nEpoch 259/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6755 - val_loss: 8.8848\nEpoch 260/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.6705 - val_loss: 8.8678\nEpoch 261/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6655 - val_loss: 8.8516\nEpoch 262/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6606 - val_loss: 8.8351\nEpoch 263/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6559 - val_loss: 8.8178\nEpoch 264/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6510 - val_loss: 8.8016\nEpoch 265/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6464 - val_loss: 8.7851\nEpoch 266/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.6419 - val_loss: 8.7681\nEpoch 267/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.6372 - val_loss: 8.7525\nEpoch 268/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6328 - val_loss: 8.7359\nEpoch 269/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6284 - val_loss: 8.7178\nEpoch 270/500\n6/6 [==============================] - 0s 2ms/step - loss: 0.6240 - val_loss: 8.7002\nEpoch 271/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6195 - val_loss: 8.6830\nEpoch 272/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6153 - val_loss: 8.6657\nEpoch 273/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6109 - val_loss: 8.6506\nEpoch 274/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6067 - val_loss: 8.6332\nEpoch 275/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6025 - val_loss: 8.6157\nEpoch 276/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5983 - val_loss: 8.5999\nEpoch 277/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5942 - val_loss: 8.5836\nEpoch 278/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5901 - val_loss: 8.5670\nEpoch 279/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5861 - val_loss: 8.5503\nEpoch 280/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5822 - val_loss: 8.5338\nEpoch 281/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5782 - val_loss: 8.5173\nEpoch 282/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5743 - val_loss: 8.5005\nEpoch 283/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5705 - val_loss: 8.4849\nEpoch 284/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5666 - val_loss: 8.4695\nEpoch 285/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5630 - val_loss: 8.4534\nEpoch 286/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5593 - val_loss: 8.4368\nEpoch 287/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5556 - val_loss: 8.4202\nEpoch 288/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5521 - val_loss: 8.4029\nEpoch 289/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5485 - val_loss: 8.3865\nEpoch 290/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.5449 - val_loss: 8.3706\nEpoch 291/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5416 - val_loss: 8.3542\nEpoch 292/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5380 - val_loss: 8.3383\nEpoch 293/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5346 - val_loss: 8.3220\nEpoch 294/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.5313 - val_loss: 8.3061\nEpoch 295/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5279 - val_loss: 8.2898\nEpoch 296/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5245 - val_loss: 8.2738\nEpoch 297/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5213 - val_loss: 8.2570\nEpoch 298/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5180 - val_loss: 8.2404\nEpoch 299/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5149 - val_loss: 8.2240\nEpoch 300/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5117 - val_loss: 8.2085\nEpoch 301/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5086 - val_loss: 8.1927\nEpoch 302/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5055 - val_loss: 8.1769\nEpoch 303/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5026 - val_loss: 8.1606\nEpoch 304/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4995 - val_loss: 8.1443\nEpoch 305/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4966 - val_loss: 8.1277\nEpoch 306/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4936 - val_loss: 8.1108\nEpoch 307/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4907 - val_loss: 8.0935\nEpoch 308/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4879 - val_loss: 8.0770\nEpoch 309/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4850 - val_loss: 8.0609\nEpoch 310/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4822 - val_loss: 8.0452\nEpoch 311/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4794 - val_loss: 8.0294\nEpoch 312/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4768 - val_loss: 8.0134\nEpoch 313/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4741 - val_loss: 7.9976\nEpoch 314/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4715 - val_loss: 7.9818\nEpoch 315/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4689 - val_loss: 7.9666\nEpoch 316/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4663 - val_loss: 7.9510\nEpoch 317/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4638 - val_loss: 7.9347\nEpoch 318/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4612 - val_loss: 7.9191\nEpoch 319/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4587 - val_loss: 7.9027\nEpoch 320/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4561 - val_loss: 7.8862\nEpoch 321/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4536 - val_loss: 7.8708\nEpoch 322/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4512 - val_loss: 7.8552\nEpoch 323/500\n6/6 [==============================] - 0s 2ms/step - loss: 0.4487 - val_loss: 7.8394\nEpoch 324/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4463 - val_loss: 7.8227\nEpoch 325/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4439 - val_loss: 7.8063\nEpoch 326/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4416 - val_loss: 7.7906\nEpoch 327/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4393 - val_loss: 7.7742\nEpoch 328/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4370 - val_loss: 7.7577\nEpoch 329/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4348 - val_loss: 7.7415\nEpoch 330/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4326 - val_loss: 7.7239\nEpoch 331/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4304 - val_loss: 7.7076\nEpoch 332/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4282 - val_loss: 7.6921\nEpoch 333/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4261 - val_loss: 7.6763\nEpoch 334/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4240 - val_loss: 7.6604\nEpoch 335/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4220 - val_loss: 7.6445\nEpoch 336/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4198 - val_loss: 7.6292\nEpoch 337/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4179 - val_loss: 7.6131\nEpoch 338/500\n6/6 [==============================] - 0s 2ms/step - loss: 0.4158 - val_loss: 7.5977\nEpoch 339/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4138 - val_loss: 7.5814\nEpoch 340/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4118 - val_loss: 7.5642\nEpoch 341/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4099 - val_loss: 7.5479\nEpoch 342/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4079 - val_loss: 7.5325\nEpoch 343/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4060 - val_loss: 7.5169\nEpoch 344/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4040 - val_loss: 7.5016\nEpoch 345/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.4022 - val_loss: 7.4851\nEpoch 346/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4003 - val_loss: 7.4683\nEpoch 347/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3985 - val_loss: 7.4519\nEpoch 348/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3966 - val_loss: 7.4350\nEpoch 349/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3948 - val_loss: 7.4181\nEpoch 350/500\n6/6 [==============================] - 0s 2ms/step - loss: 0.3930 - val_loss: 7.4023\nEpoch 351/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3912 - val_loss: 7.3859\nEpoch 352/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3895 - val_loss: 7.3701\nEpoch 353/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3877 - val_loss: 7.3544\nEpoch 354/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3860 - val_loss: 7.3373\nEpoch 355/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3842 - val_loss: 7.3210\nEpoch 356/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3825 - val_loss: 7.3045\nEpoch 357/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3808 - val_loss: 7.2885\nEpoch 358/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3791 - val_loss: 7.2724\nEpoch 359/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3775 - val_loss: 7.2565\nEpoch 360/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3758 - val_loss: 7.2409\nEpoch 361/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3742 - val_loss: 7.2244\nEpoch 362/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3727 - val_loss: 7.2077\nEpoch 363/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3710 - val_loss: 7.1918\nEpoch 364/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3695 - val_loss: 7.1761\nEpoch 365/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3679 - val_loss: 7.1599\nEpoch 366/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3664 - val_loss: 7.1431\nEpoch 367/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3649 - val_loss: 7.1263\nEpoch 368/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3634 - val_loss: 7.1100\nEpoch 369/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3619 - val_loss: 7.0939\nEpoch 370/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3604 - val_loss: 7.0774\nEpoch 371/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3589 - val_loss: 7.0615\nEpoch 372/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3575 - val_loss: 7.0453\nEpoch 373/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3561 - val_loss: 7.0284\nEpoch 374/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3547 - val_loss: 7.0119\nEpoch 375/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3533 - val_loss: 6.9952\nEpoch 376/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3519 - val_loss: 6.9800\nEpoch 377/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3505 - val_loss: 6.9635\nEpoch 378/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3491 - val_loss: 6.9466\nEpoch 379/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3477 - val_loss: 6.9295\nEpoch 380/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3464 - val_loss: 6.9124\nEpoch 381/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3452 - val_loss: 6.8957\nEpoch 382/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3438 - val_loss: 6.8792\nEpoch 383/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3425 - val_loss: 6.8631\nEpoch 384/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3412 - val_loss: 6.8465\nEpoch 385/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3399 - val_loss: 6.8299\nEpoch 386/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.3387 - val_loss: 6.8137\nEpoch 387/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3373 - val_loss: 6.7981\nEpoch 388/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.3361 - val_loss: 6.7825\nEpoch 389/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3348 - val_loss: 6.7660\nEpoch 390/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3336 - val_loss: 6.7500\nEpoch 391/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3323 - val_loss: 6.7337\nEpoch 392/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3311 - val_loss: 6.7167\nEpoch 393/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3299 - val_loss: 6.7001\nEpoch 394/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3287 - val_loss: 6.6835\nEpoch 395/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3275 - val_loss: 6.6664\nEpoch 396/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3263 - val_loss: 6.6488\nEpoch 397/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.3252 - val_loss: 6.6320\nEpoch 398/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3240 - val_loss: 6.6150\nEpoch 399/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3228 - val_loss: 6.5981\nEpoch 400/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3217 - val_loss: 6.5811\nEpoch 401/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3207 - val_loss: 6.5645\nEpoch 402/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3195 - val_loss: 6.5477\nEpoch 403/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3184 - val_loss: 6.5309\nEpoch 404/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.3173 - val_loss: 6.5144\nEpoch 405/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3161 - val_loss: 6.4976\nEpoch 406/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3150 - val_loss: 6.4804\nEpoch 407/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.3139 - val_loss: 6.4638\nEpoch 408/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3128 - val_loss: 6.4458\nEpoch 409/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3117 - val_loss: 6.4281\nEpoch 410/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3106 - val_loss: 6.4111\nEpoch 411/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3096 - val_loss: 6.3941\nEpoch 412/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3085 - val_loss: 6.3764\nEpoch 413/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3074 - val_loss: 6.3594\nEpoch 414/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3065 - val_loss: 6.3420\nEpoch 415/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3054 - val_loss: 6.3241\nEpoch 416/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3044 - val_loss: 6.3073\nEpoch 417/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3034 - val_loss: 6.2903\nEpoch 418/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3025 - val_loss: 6.2734\nEpoch 419/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3013 - val_loss: 6.2575\nEpoch 420/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3004 - val_loss: 6.2409\nEpoch 421/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2994 - val_loss: 6.2242\nEpoch 422/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2984 - val_loss: 6.2075\nEpoch 423/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2974 - val_loss: 6.1904\nEpoch 424/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2964 - val_loss: 6.1737\nEpoch 425/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2954 - val_loss: 6.1569\nEpoch 426/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2945 - val_loss: 6.1407\nEpoch 427/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2936 - val_loss: 6.1236\nEpoch 428/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2926 - val_loss: 6.1073\nEpoch 429/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2916 - val_loss: 6.0908\nEpoch 430/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2908 - val_loss: 6.0741\nEpoch 431/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2898 - val_loss: 6.0571\nEpoch 432/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2889 - val_loss: 6.0399\nEpoch 433/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2879 - val_loss: 6.0221\nEpoch 434/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2870 - val_loss: 6.0051\nEpoch 435/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2861 - val_loss: 5.9884\nEpoch 436/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2851 - val_loss: 5.9713\nEpoch 437/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2843 - val_loss: 5.9546\nEpoch 438/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.2833 - val_loss: 5.9381\nEpoch 439/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.2824 - val_loss: 5.9211\nEpoch 440/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2815 - val_loss: 5.9039\nEpoch 441/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2806 - val_loss: 5.8861\nEpoch 442/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2797 - val_loss: 5.8693\nEpoch 443/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2789 - val_loss: 5.8528\nEpoch 444/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2781 - val_loss: 5.8350\nEpoch 445/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2772 - val_loss: 5.8184\nEpoch 446/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2763 - val_loss: 5.8015\nEpoch 447/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2754 - val_loss: 5.7853\nEpoch 448/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2746 - val_loss: 5.7684\nEpoch 449/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2737 - val_loss: 5.7513\nEpoch 450/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2729 - val_loss: 5.7344\nEpoch 451/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2720 - val_loss: 5.7173\nEpoch 452/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2711 - val_loss: 5.7008\nEpoch 453/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2703 - val_loss: 5.6843\nEpoch 454/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2695 - val_loss: 5.6665\nEpoch 455/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2687 - val_loss: 5.6494\nEpoch 456/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2678 - val_loss: 5.6318\nEpoch 457/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2670 - val_loss: 5.6147\nEpoch 458/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2662 - val_loss: 5.5980\nEpoch 459/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2654 - val_loss: 5.5813\nEpoch 460/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2646 - val_loss: 5.5641\nEpoch 461/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2638 - val_loss: 5.5476\nEpoch 462/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.2630 - val_loss: 5.5310\nEpoch 463/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2622 - val_loss: 5.5145\nEpoch 464/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2613 - val_loss: 5.4979\nEpoch 465/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2605 - val_loss: 5.4806\nEpoch 466/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2597 - val_loss: 5.4643\nEpoch 467/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2589 - val_loss: 5.4476\nEpoch 468/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2581 - val_loss: 5.4310\nEpoch 469/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2574 - val_loss: 5.4142\nEpoch 470/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2566 - val_loss: 5.3978\nEpoch 471/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2558 - val_loss: 5.3805\nEpoch 472/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2550 - val_loss: 5.3637\nEpoch 473/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2542 - val_loss: 5.3466\nEpoch 474/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2535 - val_loss: 5.3306\nEpoch 475/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2527 - val_loss: 5.3135\nEpoch 476/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2519 - val_loss: 5.2972\nEpoch 477/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2511 - val_loss: 5.2801\nEpoch 478/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2504 - val_loss: 5.2632\nEpoch 479/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2495 - val_loss: 5.2459\nEpoch 480/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2487 - val_loss: 5.2288\nEpoch 481/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.2480 - val_loss: 5.2122\nEpoch 482/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2472 - val_loss: 5.1952\nEpoch 483/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2464 - val_loss: 5.1784\nEpoch 484/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2456 - val_loss: 5.1620\nEpoch 485/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2449 - val_loss: 5.1452\nEpoch 486/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2441 - val_loss: 5.1281\nEpoch 487/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2434 - val_loss: 5.1113\nEpoch 488/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2426 - val_loss: 5.0950\nEpoch 489/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2419 - val_loss: 5.0791\nEpoch 490/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2411 - val_loss: 5.0621\nEpoch 491/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2404 - val_loss: 5.0454\nEpoch 492/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2396 - val_loss: 5.0286\nEpoch 493/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2389 - val_loss: 5.0118\nEpoch 494/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2381 - val_loss: 4.9955\nEpoch 495/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2374 - val_loss: 4.9795\nEpoch 496/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.2367 - val_loss: 4.9630\nEpoch 497/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.2360 - val_loss: 4.9465\nEpoch 498/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2352 - val_loss: 4.9300\nEpoch 499/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.2344 - val_loss: 4.9141\nEpoch 500/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2337 - val_loss: 4.8977\n\n\n<keras.callbacks.History at 0x7f70988bbe50>\n\n\n- 결과시각화\n\nplt.plot(y,'.',alpha=0.1)\nplt.plot(net(X),'--')\n\n\n\n\n- 보여준 데이터에서는 잘 맞추는것 같지만 validation에서는 엉망이다. -> 오버피팅인가? -> 텐서보드로 확인\n\n# \n#%tensorboard --logdir logs --host 0.0.0.0 \n\n\n확인결과: 에폭마다 val_loss 가 줄어들고 있기는 하다 (늦게 줄어들뿐) -> 오버피팅이라기보다 val_set에 들어있는 자료를 예측하기에는 보여준 데이터가 불충분하다라고 해석하는것이 더 옳음 (모형자체의 문제는 아님)\n\n- 해결하는 방법? 그냥 더 학습시키면된다.\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(loss='mse',optimizer='adam')\nnet.fit(X,y,epochs=2000,batch_size=100, validation_split=0.45, callbacks=tf.keras.callbacks.TensorBoard()) \n# 텐서보드를 이용한 시각화기능 추가 \n# validation_split 이용\n\nEpoch 1/2000\n6/6 [==============================] - 0s 9ms/step - loss: 7.9563 - val_loss: 17.0027\nEpoch 2/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.9013 - val_loss: 16.9412\nEpoch 3/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.8463 - val_loss: 16.8810\nEpoch 4/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.7913 - val_loss: 16.8207\nEpoch 5/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.7380 - val_loss: 16.7617\nEpoch 6/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.6829 - val_loss: 16.7032\nEpoch 7/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.6308 - val_loss: 16.6446\nEpoch 8/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.5769 - val_loss: 16.5886\nEpoch 9/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.5245 - val_loss: 16.5321\nEpoch 10/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.4717 - val_loss: 16.4760\nEpoch 11/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.4201 - val_loss: 16.4201\nEpoch 12/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.3686 - val_loss: 16.3636\nEpoch 13/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.3162 - val_loss: 16.3079\nEpoch 14/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.2647 - val_loss: 16.2527\nEpoch 15/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.2148 - val_loss: 16.1978\nEpoch 16/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.1634 - val_loss: 16.1427\nEpoch 17/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.1142 - val_loss: 16.0877\nEpoch 18/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.0632 - val_loss: 16.0335\nEpoch 19/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.0148 - val_loss: 15.9786\nEpoch 20/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.9651 - val_loss: 15.9238\nEpoch 21/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.9164 - val_loss: 15.8699\nEpoch 22/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.8670 - val_loss: 15.8155\nEpoch 23/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.8191 - val_loss: 15.7617\nEpoch 24/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.7714 - val_loss: 15.7093\nEpoch 25/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.7231 - val_loss: 15.6568\nEpoch 26/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.6762 - val_loss: 15.6048\nEpoch 27/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.6286 - val_loss: 15.5528\nEpoch 28/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5816 - val_loss: 15.5012\nEpoch 29/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5349 - val_loss: 15.4495\nEpoch 30/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4885 - val_loss: 15.3982\nEpoch 31/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4417 - val_loss: 15.3463\nEpoch 32/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.3956 - val_loss: 15.2957\nEpoch 33/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3508 - val_loss: 15.2442\nEpoch 34/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3053 - val_loss: 15.1941\nEpoch 35/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2593 - val_loss: 15.1447\nEpoch 36/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2142 - val_loss: 15.0943\nEpoch 37/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1706 - val_loss: 15.0446\nEpoch 38/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1261 - val_loss: 14.9966\nEpoch 39/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.0825 - val_loss: 14.9488\nEpoch 40/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.0394 - val_loss: 14.9001\nEpoch 41/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9965 - val_loss: 14.8519\nEpoch 42/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.9530 - val_loss: 14.8035\nEpoch 43/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9106 - val_loss: 14.7552\nEpoch 44/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8678 - val_loss: 14.7074\nEpoch 45/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8262 - val_loss: 14.6600\nEpoch 46/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7841 - val_loss: 14.6128\nEpoch 47/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.7422 - val_loss: 14.5659\nEpoch 48/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7007 - val_loss: 14.5194\nEpoch 49/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6598 - val_loss: 14.4737\nEpoch 50/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6183 - val_loss: 14.4275\nEpoch 51/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5779 - val_loss: 14.3826\nEpoch 52/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5376 - val_loss: 14.3368\nEpoch 53/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.4973 - val_loss: 14.2902\nEpoch 54/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.4572 - val_loss: 14.2453\nEpoch 55/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.4176 - val_loss: 14.2002\nEpoch 56/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3785 - val_loss: 14.1551\nEpoch 57/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3387 - val_loss: 14.1109\nEpoch 58/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2996 - val_loss: 14.0679\nEpoch 59/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2613 - val_loss: 14.0241\nEpoch 60/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2221 - val_loss: 13.9811\nEpoch 61/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1839 - val_loss: 13.9382\nEpoch 62/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1467 - val_loss: 13.8949\nEpoch 63/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1086 - val_loss: 13.8506\nEpoch 64/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0710 - val_loss: 13.8065\nEpoch 65/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0336 - val_loss: 13.7637\nEpoch 66/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.9956 - val_loss: 13.7212\nEpoch 67/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.9589 - val_loss: 13.6790\nEpoch 68/2000\n6/6 [==============================] - 0s 2ms/step - loss: 4.9218 - val_loss: 13.6360\nEpoch 69/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8858 - val_loss: 13.5936\nEpoch 70/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8495 - val_loss: 13.5514\nEpoch 71/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8133 - val_loss: 13.5099\nEpoch 72/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7778 - val_loss: 13.4699\nEpoch 73/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7419 - val_loss: 13.4284\nEpoch 74/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7071 - val_loss: 13.3875\nEpoch 75/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6717 - val_loss: 13.3462\nEpoch 76/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6372 - val_loss: 13.3066\nEpoch 77/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6033 - val_loss: 13.2665\nEpoch 78/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5686 - val_loss: 13.2260\nEpoch 79/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5349 - val_loss: 13.1872\nEpoch 80/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5012 - val_loss: 13.1474\nEpoch 81/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4672 - val_loss: 13.1093\nEpoch 82/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4336 - val_loss: 13.0714\nEpoch 83/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4009 - val_loss: 13.0337\nEpoch 84/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.3675 - val_loss: 12.9957\nEpoch 85/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3348 - val_loss: 12.9574\nEpoch 86/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3025 - val_loss: 12.9182\nEpoch 87/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2700 - val_loss: 12.8800\nEpoch 88/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2382 - val_loss: 12.8425\nEpoch 89/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2063 - val_loss: 12.8047\nEpoch 90/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1745 - val_loss: 12.7663\nEpoch 91/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1428 - val_loss: 12.7295\nEpoch 92/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1119 - val_loss: 12.6919\nEpoch 93/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0804 - val_loss: 12.6557\nEpoch 94/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0493 - val_loss: 12.6196\nEpoch 95/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0188 - val_loss: 12.5834\nEpoch 96/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9878 - val_loss: 12.5464\nEpoch 97/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9580 - val_loss: 12.5099\nEpoch 98/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9274 - val_loss: 12.4748\nEpoch 99/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8977 - val_loss: 12.4391\nEpoch 100/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.8678 - val_loss: 12.4017\nEpoch 101/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8381 - val_loss: 12.3662\nEpoch 102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8084 - val_loss: 12.3317\nEpoch 103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7793 - val_loss: 12.2967\nEpoch 104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7504 - val_loss: 12.2616\nEpoch 105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7214 - val_loss: 12.2255\nEpoch 106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6928 - val_loss: 12.1913\nEpoch 107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6639 - val_loss: 12.1572\nEpoch 108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6359 - val_loss: 12.1229\nEpoch 109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6079 - val_loss: 12.0890\nEpoch 110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5798 - val_loss: 12.0553\nEpoch 111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5520 - val_loss: 12.0209\nEpoch 112/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5245 - val_loss: 11.9872\nEpoch 113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4968 - val_loss: 11.9544\nEpoch 114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4703 - val_loss: 11.9212\nEpoch 115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4430 - val_loss: 11.8894\nEpoch 116/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.4164 - val_loss: 11.8567\nEpoch 117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3899 - val_loss: 11.8250\nEpoch 118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3633 - val_loss: 11.7929\nEpoch 119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3368 - val_loss: 11.7604\nEpoch 120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3108 - val_loss: 11.7281\nEpoch 121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2852 - val_loss: 11.6949\nEpoch 122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2593 - val_loss: 11.6615\nEpoch 123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2339 - val_loss: 11.6295\nEpoch 124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2084 - val_loss: 11.5968\nEpoch 125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1835 - val_loss: 11.5650\nEpoch 126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1585 - val_loss: 11.5338\nEpoch 127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1335 - val_loss: 11.5018\nEpoch 128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1093 - val_loss: 11.4705\nEpoch 129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0847 - val_loss: 11.4392\nEpoch 130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0604 - val_loss: 11.4076\nEpoch 131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0360 - val_loss: 11.3776\nEpoch 132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0123 - val_loss: 11.3460\nEpoch 133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9884 - val_loss: 11.3158\nEpoch 134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9648 - val_loss: 11.2869\nEpoch 135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9414 - val_loss: 11.2564\nEpoch 136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9179 - val_loss: 11.2282\nEpoch 137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8950 - val_loss: 11.1989\nEpoch 138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8720 - val_loss: 11.1695\nEpoch 139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8492 - val_loss: 11.1400\nEpoch 140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8265 - val_loss: 11.1121\nEpoch 141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8047 - val_loss: 11.0833\nEpoch 142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7819 - val_loss: 11.0539\nEpoch 143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7598 - val_loss: 11.0244\nEpoch 144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7381 - val_loss: 10.9954\nEpoch 145/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.7157 - val_loss: 10.9659\nEpoch 146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6944 - val_loss: 10.9374\nEpoch 147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6728 - val_loss: 10.9100\nEpoch 148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6517 - val_loss: 10.8820\nEpoch 149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6303 - val_loss: 10.8545\nEpoch 150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6098 - val_loss: 10.8258\nEpoch 151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5886 - val_loss: 10.7982\nEpoch 152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5681 - val_loss: 10.7702\nEpoch 153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5477 - val_loss: 10.7426\nEpoch 154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5268 - val_loss: 10.7159\nEpoch 155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5070 - val_loss: 10.6879\nEpoch 156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4865 - val_loss: 10.6615\nEpoch 157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4665 - val_loss: 10.6347\nEpoch 158/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.4471 - val_loss: 10.6074\nEpoch 159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4269 - val_loss: 10.5801\nEpoch 160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4074 - val_loss: 10.5535\nEpoch 161/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.3880 - val_loss: 10.5256\nEpoch 162/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.3687 - val_loss: 10.4981\nEpoch 163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3497 - val_loss: 10.4711\nEpoch 164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3308 - val_loss: 10.4447\nEpoch 165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3117 - val_loss: 10.4198\nEpoch 166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2933 - val_loss: 10.3949\nEpoch 167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2749 - val_loss: 10.3696\nEpoch 168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2567 - val_loss: 10.3437\nEpoch 169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2385 - val_loss: 10.3177\nEpoch 170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2204 - val_loss: 10.2909\nEpoch 171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2022 - val_loss: 10.2653\nEpoch 172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1842 - val_loss: 10.2391\nEpoch 173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1669 - val_loss: 10.2139\nEpoch 174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1493 - val_loss: 10.1893\nEpoch 175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1320 - val_loss: 10.1643\nEpoch 176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1146 - val_loss: 10.1393\nEpoch 177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0978 - val_loss: 10.1148\nEpoch 178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0802 - val_loss: 10.0900\nEpoch 179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0637 - val_loss: 10.0662\nEpoch 180/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.0470 - val_loss: 10.0430\nEpoch 181/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.0304 - val_loss: 10.0192\nEpoch 182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0140 - val_loss: 9.9947\nEpoch 183/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.9974 - val_loss: 9.9698\nEpoch 184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9812 - val_loss: 9.9468\nEpoch 185/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.9653 - val_loss: 9.9227\nEpoch 186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9492 - val_loss: 9.8996\nEpoch 187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9333 - val_loss: 9.8767\nEpoch 188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9175 - val_loss: 9.8542\nEpoch 189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9018 - val_loss: 9.8322\nEpoch 190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8864 - val_loss: 9.8097\nEpoch 191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8710 - val_loss: 9.7886\nEpoch 192/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.8560 - val_loss: 9.7661\nEpoch 193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8408 - val_loss: 9.7440\nEpoch 194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8261 - val_loss: 9.7216\nEpoch 195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8109 - val_loss: 9.6998\nEpoch 196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7963 - val_loss: 9.6770\nEpoch 197/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.7817 - val_loss: 9.6547\nEpoch 198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7671 - val_loss: 9.6326\nEpoch 199/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7529 - val_loss: 9.6101\nEpoch 200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7385 - val_loss: 9.5889\nEpoch 201/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7247 - val_loss: 9.5673\nEpoch 202/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.7103 - val_loss: 9.5462\nEpoch 203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6965 - val_loss: 9.5249\nEpoch 204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6832 - val_loss: 9.5034\nEpoch 205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6688 - val_loss: 9.4818\nEpoch 206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6554 - val_loss: 9.4604\nEpoch 207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6420 - val_loss: 9.4388\nEpoch 208/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.6284 - val_loss: 9.4175\nEpoch 209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6153 - val_loss: 9.3953\nEpoch 210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6020 - val_loss: 9.3753\nEpoch 211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5890 - val_loss: 9.3550\nEpoch 212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5760 - val_loss: 9.3347\nEpoch 213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5630 - val_loss: 9.3146\nEpoch 214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5504 - val_loss: 9.2938\nEpoch 215/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.5378 - val_loss: 9.2736\nEpoch 216/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.5254 - val_loss: 9.2528\nEpoch 217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5129 - val_loss: 9.2324\nEpoch 218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5007 - val_loss: 9.2127\nEpoch 219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4883 - val_loss: 9.1932\nEpoch 220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4761 - val_loss: 9.1740\nEpoch 221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4642 - val_loss: 9.1550\nEpoch 222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4522 - val_loss: 9.1361\nEpoch 223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4407 - val_loss: 9.1162\nEpoch 224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4289 - val_loss: 9.0964\nEpoch 225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4172 - val_loss: 9.0773\nEpoch 226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4058 - val_loss: 9.0589\nEpoch 227/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.3944 - val_loss: 9.0388\nEpoch 228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3833 - val_loss: 9.0196\nEpoch 229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3721 - val_loss: 9.0016\nEpoch 230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3607 - val_loss: 8.9826\nEpoch 231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3498 - val_loss: 8.9648\nEpoch 232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3390 - val_loss: 8.9451\nEpoch 233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3282 - val_loss: 8.9263\nEpoch 234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3173 - val_loss: 8.9072\nEpoch 235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3065 - val_loss: 8.8890\nEpoch 236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2960 - val_loss: 8.8710\nEpoch 237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2857 - val_loss: 8.8527\nEpoch 238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2753 - val_loss: 8.8343\nEpoch 239/2000\n6/6 [==============================] - 0s 2ms/step - loss: 1.2650 - val_loss: 8.8167\nEpoch 240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2549 - val_loss: 8.7986\nEpoch 241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2446 - val_loss: 8.7807\nEpoch 242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2348 - val_loss: 8.7634\nEpoch 243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2248 - val_loss: 8.7452\nEpoch 244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2149 - val_loss: 8.7272\nEpoch 245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2051 - val_loss: 8.7098\nEpoch 246/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1953 - val_loss: 8.6919\nEpoch 247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1859 - val_loss: 8.6734\nEpoch 248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1761 - val_loss: 8.6563\nEpoch 249/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1668 - val_loss: 8.6384\nEpoch 250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1575 - val_loss: 8.6211\nEpoch 251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1482 - val_loss: 8.6034\nEpoch 252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1391 - val_loss: 8.5858\nEpoch 253/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1299 - val_loss: 8.5696\nEpoch 254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1210 - val_loss: 8.5526\nEpoch 255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1121 - val_loss: 8.5356\nEpoch 256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1033 - val_loss: 8.5179\nEpoch 257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0945 - val_loss: 8.5020\nEpoch 258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0857 - val_loss: 8.4856\nEpoch 259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0772 - val_loss: 8.4692\nEpoch 260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0686 - val_loss: 8.4531\nEpoch 261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0601 - val_loss: 8.4360\nEpoch 262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0518 - val_loss: 8.4194\nEpoch 263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0433 - val_loss: 8.4037\nEpoch 264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0351 - val_loss: 8.3865\nEpoch 265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0271 - val_loss: 8.3690\nEpoch 266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0190 - val_loss: 8.3524\nEpoch 267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0108 - val_loss: 8.3361\nEpoch 268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0029 - val_loss: 8.3203\nEpoch 269/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9951 - val_loss: 8.3041\nEpoch 270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9873 - val_loss: 8.2878\nEpoch 271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9796 - val_loss: 8.2707\nEpoch 272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9719 - val_loss: 8.2545\nEpoch 273/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9642 - val_loss: 8.2390\nEpoch 274/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9568 - val_loss: 8.2218\nEpoch 275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9494 - val_loss: 8.2058\nEpoch 276/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9420 - val_loss: 8.1896\nEpoch 277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9347 - val_loss: 8.1732\nEpoch 278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9275 - val_loss: 8.1561\nEpoch 279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9202 - val_loss: 8.1401\nEpoch 280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9131 - val_loss: 8.1239\nEpoch 281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9060 - val_loss: 8.1088\nEpoch 282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8990 - val_loss: 8.0931\nEpoch 283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8922 - val_loss: 8.0769\nEpoch 284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8853 - val_loss: 8.0603\nEpoch 285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8784 - val_loss: 8.0448\nEpoch 286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8718 - val_loss: 8.0294\nEpoch 287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8652 - val_loss: 8.0138\nEpoch 288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8584 - val_loss: 7.9985\nEpoch 289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8520 - val_loss: 7.9826\nEpoch 290/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8454 - val_loss: 7.9671\nEpoch 291/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8390 - val_loss: 7.9519\nEpoch 292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8326 - val_loss: 7.9362\nEpoch 293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8263 - val_loss: 7.9212\nEpoch 294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8201 - val_loss: 7.9061\nEpoch 295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8138 - val_loss: 7.8905\nEpoch 296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8079 - val_loss: 7.8750\nEpoch 297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8017 - val_loss: 7.8607\nEpoch 298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7958 - val_loss: 7.8459\nEpoch 299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7899 - val_loss: 7.8306\nEpoch 300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7840 - val_loss: 7.8159\nEpoch 301/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7783 - val_loss: 7.8007\nEpoch 302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7725 - val_loss: 7.7864\nEpoch 303/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.7669 - val_loss: 7.7719\nEpoch 304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7611 - val_loss: 7.7579\nEpoch 305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7556 - val_loss: 7.7428\nEpoch 306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7500 - val_loss: 7.7285\nEpoch 307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7445 - val_loss: 7.7141\nEpoch 308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7391 - val_loss: 7.6997\nEpoch 309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7337 - val_loss: 7.6853\nEpoch 310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7283 - val_loss: 7.6718\nEpoch 311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7231 - val_loss: 7.6580\nEpoch 312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7179 - val_loss: 7.6447\nEpoch 313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7128 - val_loss: 7.6302\nEpoch 314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7077 - val_loss: 7.6174\nEpoch 315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7026 - val_loss: 7.6038\nEpoch 316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6975 - val_loss: 7.5901\nEpoch 317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6925 - val_loss: 7.5768\nEpoch 318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6876 - val_loss: 7.5627\nEpoch 319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6827 - val_loss: 7.5483\nEpoch 320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6779 - val_loss: 7.5341\nEpoch 321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6731 - val_loss: 7.5198\nEpoch 322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6683 - val_loss: 7.5051\nEpoch 323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6636 - val_loss: 7.4905\nEpoch 324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6588 - val_loss: 7.4768\nEpoch 325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6543 - val_loss: 7.4629\nEpoch 326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6497 - val_loss: 7.4488\nEpoch 327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6452 - val_loss: 7.4349\nEpoch 328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6407 - val_loss: 7.4209\nEpoch 329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6361 - val_loss: 7.4075\nEpoch 330/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.6318 - val_loss: 7.3936\nEpoch 331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6273 - val_loss: 7.3804\nEpoch 332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6230 - val_loss: 7.3673\nEpoch 333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6187 - val_loss: 7.3541\nEpoch 334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6146 - val_loss: 7.3411\nEpoch 335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6104 - val_loss: 7.3282\nEpoch 336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6063 - val_loss: 7.3151\nEpoch 337/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.6021 - val_loss: 7.3019\nEpoch 338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5982 - val_loss: 7.2878\nEpoch 339/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.5941 - val_loss: 7.2744\nEpoch 340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5901 - val_loss: 7.2609\nEpoch 341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5862 - val_loss: 7.2473\nEpoch 342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5822 - val_loss: 7.2341\nEpoch 343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5785 - val_loss: 7.2205\nEpoch 344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5746 - val_loss: 7.2082\nEpoch 345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5709 - val_loss: 7.1954\nEpoch 346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5672 - val_loss: 7.1823\nEpoch 347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5635 - val_loss: 7.1690\nEpoch 348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5599 - val_loss: 7.1551\nEpoch 349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5561 - val_loss: 7.1426\nEpoch 350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5527 - val_loss: 7.1290\nEpoch 351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5491 - val_loss: 7.1156\nEpoch 352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5456 - val_loss: 7.1031\nEpoch 353/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5421 - val_loss: 7.0905\nEpoch 354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5387 - val_loss: 7.0773\nEpoch 355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5352 - val_loss: 7.0639\nEpoch 356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5319 - val_loss: 7.0513\nEpoch 357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5285 - val_loss: 7.0387\nEpoch 358/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5251 - val_loss: 7.0263\nEpoch 359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5218 - val_loss: 7.0130\nEpoch 360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5186 - val_loss: 7.0000\nEpoch 361/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5154 - val_loss: 6.9864\nEpoch 362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5121 - val_loss: 6.9732\nEpoch 363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5090 - val_loss: 6.9603\nEpoch 364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5057 - val_loss: 6.9482\nEpoch 365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5027 - val_loss: 6.9357\nEpoch 366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4997 - val_loss: 6.9228\nEpoch 367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4966 - val_loss: 6.9100\nEpoch 368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4936 - val_loss: 6.8972\nEpoch 369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4906 - val_loss: 6.8840\nEpoch 370/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4876 - val_loss: 6.8711\nEpoch 371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4847 - val_loss: 6.8580\nEpoch 372/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4818 - val_loss: 6.8453\nEpoch 373/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4790 - val_loss: 6.8319\nEpoch 374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4760 - val_loss: 6.8195\nEpoch 375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4732 - val_loss: 6.8068\nEpoch 376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4704 - val_loss: 6.7937\nEpoch 377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4676 - val_loss: 6.7809\nEpoch 378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4649 - val_loss: 6.7677\nEpoch 379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4622 - val_loss: 6.7545\nEpoch 380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4595 - val_loss: 6.7420\nEpoch 381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4569 - val_loss: 6.7292\nEpoch 382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4543 - val_loss: 6.7164\nEpoch 383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4517 - val_loss: 6.7039\nEpoch 384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4492 - val_loss: 6.6910\nEpoch 385/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4467 - val_loss: 6.6781\nEpoch 386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4441 - val_loss: 6.6659\nEpoch 387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4416 - val_loss: 6.6535\nEpoch 388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4392 - val_loss: 6.6404\nEpoch 389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4367 - val_loss: 6.6274\nEpoch 390/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4342 - val_loss: 6.6145\nEpoch 391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4318 - val_loss: 6.6011\nEpoch 392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4295 - val_loss: 6.5875\nEpoch 393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4270 - val_loss: 6.5747\nEpoch 394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4248 - val_loss: 6.5615\nEpoch 395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4224 - val_loss: 6.5482\nEpoch 396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4202 - val_loss: 6.5350\nEpoch 397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4178 - val_loss: 6.5221\nEpoch 398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4156 - val_loss: 6.5091\nEpoch 399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4134 - val_loss: 6.4958\nEpoch 400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4112 - val_loss: 6.4832\nEpoch 401/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4090 - val_loss: 6.4701\nEpoch 402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4068 - val_loss: 6.4580\nEpoch 403/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4047 - val_loss: 6.4458\nEpoch 404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4026 - val_loss: 6.4328\nEpoch 405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4005 - val_loss: 6.4201\nEpoch 406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3984 - val_loss: 6.4078\nEpoch 407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3964 - val_loss: 6.3951\nEpoch 408/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3943 - val_loss: 6.3820\nEpoch 409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3923 - val_loss: 6.3687\nEpoch 410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3904 - val_loss: 6.3556\nEpoch 411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3884 - val_loss: 6.3432\nEpoch 412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3864 - val_loss: 6.3316\nEpoch 413/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3845 - val_loss: 6.3191\nEpoch 414/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3826 - val_loss: 6.3064\nEpoch 415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3807 - val_loss: 6.2937\nEpoch 416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3788 - val_loss: 6.2801\nEpoch 417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3770 - val_loss: 6.2673\nEpoch 418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3751 - val_loss: 6.2552\nEpoch 419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3734 - val_loss: 6.2428\nEpoch 420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3716 - val_loss: 6.2297\nEpoch 421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3697 - val_loss: 6.2176\nEpoch 422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3680 - val_loss: 6.2058\nEpoch 423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3662 - val_loss: 6.1930\nEpoch 424/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3645 - val_loss: 6.1798\nEpoch 425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3628 - val_loss: 6.1656\nEpoch 426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3611 - val_loss: 6.1524\nEpoch 427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3593 - val_loss: 6.1409\nEpoch 428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3577 - val_loss: 6.1283\nEpoch 429/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3560 - val_loss: 6.1157\nEpoch 430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3543 - val_loss: 6.1029\nEpoch 431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3527 - val_loss: 6.0910\nEpoch 432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3511 - val_loss: 6.0784\nEpoch 433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3495 - val_loss: 6.0659\nEpoch 434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3479 - val_loss: 6.0532\nEpoch 435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3463 - val_loss: 6.0409\nEpoch 436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3449 - val_loss: 6.0271\nEpoch 437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3433 - val_loss: 6.0139\nEpoch 438/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3417 - val_loss: 6.0021\nEpoch 439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3403 - val_loss: 5.9891\nEpoch 440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3387 - val_loss: 5.9761\nEpoch 441/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3373 - val_loss: 5.9631\nEpoch 442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3358 - val_loss: 5.9510\nEpoch 443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3343 - val_loss: 5.9384\nEpoch 444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3329 - val_loss: 5.9251\nEpoch 445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3315 - val_loss: 5.9124\nEpoch 446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3300 - val_loss: 5.9006\nEpoch 447/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3286 - val_loss: 5.8884\nEpoch 448/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3273 - val_loss: 5.8753\nEpoch 449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3259 - val_loss: 5.8638\nEpoch 450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3246 - val_loss: 5.8502\nEpoch 451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3232 - val_loss: 5.8370\nEpoch 452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3219 - val_loss: 5.8248\nEpoch 453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3205 - val_loss: 5.8125\nEpoch 454/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.3192 - val_loss: 5.8001\nEpoch 455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3179 - val_loss: 5.7872\nEpoch 456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3166 - val_loss: 5.7746\nEpoch 457/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3153 - val_loss: 5.7613\nEpoch 458/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3141 - val_loss: 5.7485\nEpoch 459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3128 - val_loss: 5.7363\nEpoch 460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3115 - val_loss: 5.7242\nEpoch 461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3102 - val_loss: 5.7113\nEpoch 462/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3091 - val_loss: 5.6971\nEpoch 463/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3078 - val_loss: 5.6843\nEpoch 464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3066 - val_loss: 5.6719\nEpoch 465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3054 - val_loss: 5.6590\nEpoch 466/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3042 - val_loss: 5.6457\nEpoch 467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3030 - val_loss: 5.6332\nEpoch 468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3018 - val_loss: 5.6205\nEpoch 469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3007 - val_loss: 5.6081\nEpoch 470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2995 - val_loss: 5.5948\nEpoch 471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2984 - val_loss: 5.5822\nEpoch 472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2973 - val_loss: 5.5698\nEpoch 473/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2962 - val_loss: 5.5573\nEpoch 474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2950 - val_loss: 5.5454\nEpoch 475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2939 - val_loss: 5.5330\nEpoch 476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2929 - val_loss: 5.5206\nEpoch 477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2918 - val_loss: 5.5077\nEpoch 478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2907 - val_loss: 5.4955\nEpoch 479/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2897 - val_loss: 5.4826\nEpoch 480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2886 - val_loss: 5.4703\nEpoch 481/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2875 - val_loss: 5.4572\nEpoch 482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2865 - val_loss: 5.4437\nEpoch 483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2854 - val_loss: 5.4312\nEpoch 484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2844 - val_loss: 5.4180\nEpoch 485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2833 - val_loss: 5.4055\nEpoch 486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2823 - val_loss: 5.3928\nEpoch 487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2812 - val_loss: 5.3801\nEpoch 488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2803 - val_loss: 5.3680\nEpoch 489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2793 - val_loss: 5.3553\nEpoch 490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2784 - val_loss: 5.3432\nEpoch 491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2774 - val_loss: 5.3311\nEpoch 492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2764 - val_loss: 5.3187\nEpoch 493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2755 - val_loss: 5.3060\nEpoch 494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2745 - val_loss: 5.2937\nEpoch 495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2735 - val_loss: 5.2808\nEpoch 496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2726 - val_loss: 5.2676\nEpoch 497/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2717 - val_loss: 5.2543\nEpoch 498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2707 - val_loss: 5.2419\nEpoch 499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2697 - val_loss: 5.2294\nEpoch 500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2688 - val_loss: 5.2162\nEpoch 501/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2679 - val_loss: 5.2035\nEpoch 502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2670 - val_loss: 5.1906\nEpoch 503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2661 - val_loss: 5.1787\nEpoch 504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2653 - val_loss: 5.1656\nEpoch 505/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2643 - val_loss: 5.1536\nEpoch 506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2635 - val_loss: 5.1414\nEpoch 507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2626 - val_loss: 5.1286\nEpoch 508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2618 - val_loss: 5.1159\nEpoch 509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2609 - val_loss: 5.1031\nEpoch 510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2600 - val_loss: 5.0902\nEpoch 511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2592 - val_loss: 5.0771\nEpoch 512/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2583 - val_loss: 5.0634\nEpoch 513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2574 - val_loss: 5.0508\nEpoch 514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2566 - val_loss: 5.0378\nEpoch 515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2558 - val_loss: 5.0251\nEpoch 516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2549 - val_loss: 5.0124\nEpoch 517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2541 - val_loss: 5.0009\nEpoch 518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2533 - val_loss: 4.9884\nEpoch 519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2524 - val_loss: 4.9745\nEpoch 520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2516 - val_loss: 4.9611\nEpoch 521/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2508 - val_loss: 4.9477\nEpoch 522/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2500 - val_loss: 4.9343\nEpoch 523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2492 - val_loss: 4.9215\nEpoch 524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2483 - val_loss: 4.9094\nEpoch 525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2476 - val_loss: 4.8964\nEpoch 526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2468 - val_loss: 4.8839\nEpoch 527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2460 - val_loss: 4.8708\nEpoch 528/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2452 - val_loss: 4.8576\nEpoch 529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2444 - val_loss: 4.8448\nEpoch 530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2437 - val_loss: 4.8320\nEpoch 531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2429 - val_loss: 4.8194\nEpoch 532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2421 - val_loss: 4.8058\nEpoch 533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2413 - val_loss: 4.7929\nEpoch 534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2406 - val_loss: 4.7801\nEpoch 535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2398 - val_loss: 4.7671\nEpoch 536/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2390 - val_loss: 4.7535\nEpoch 537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2382 - val_loss: 4.7403\nEpoch 538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2375 - val_loss: 4.7273\nEpoch 539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2367 - val_loss: 4.7137\nEpoch 540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2360 - val_loss: 4.7006\nEpoch 541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2352 - val_loss: 4.6876\nEpoch 542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2345 - val_loss: 4.6743\nEpoch 543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2337 - val_loss: 4.6610\nEpoch 544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2330 - val_loss: 4.6474\nEpoch 545/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2323 - val_loss: 4.6331\nEpoch 546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2316 - val_loss: 4.6199\nEpoch 547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2308 - val_loss: 4.6071\nEpoch 548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2301 - val_loss: 4.5937\nEpoch 549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2294 - val_loss: 4.5801\nEpoch 550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2287 - val_loss: 4.5669\nEpoch 551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2280 - val_loss: 4.5541\nEpoch 552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2273 - val_loss: 4.5418\nEpoch 553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2266 - val_loss: 4.5291\nEpoch 554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2259 - val_loss: 4.5153\nEpoch 555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2252 - val_loss: 4.5024\nEpoch 556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2245 - val_loss: 4.4886\nEpoch 557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2238 - val_loss: 4.4760\nEpoch 558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2231 - val_loss: 4.4632\nEpoch 559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2225 - val_loss: 4.4498\nEpoch 560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2218 - val_loss: 4.4371\nEpoch 561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2211 - val_loss: 4.4243\nEpoch 562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2205 - val_loss: 4.4102\nEpoch 563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2198 - val_loss: 4.3965\nEpoch 564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2191 - val_loss: 4.3836\nEpoch 565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2184 - val_loss: 4.3712\nEpoch 566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2178 - val_loss: 4.3576\nEpoch 567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2171 - val_loss: 4.3441\nEpoch 568/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2164 - val_loss: 4.3317\nEpoch 569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2158 - val_loss: 4.3184\nEpoch 570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2151 - val_loss: 4.3051\nEpoch 571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2145 - val_loss: 4.2917\nEpoch 572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2138 - val_loss: 4.2786\nEpoch 573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2132 - val_loss: 4.2653\nEpoch 574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2125 - val_loss: 4.2523\nEpoch 575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2119 - val_loss: 4.2398\nEpoch 576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2112 - val_loss: 4.2273\nEpoch 577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2107 - val_loss: 4.2141\nEpoch 578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2100 - val_loss: 4.2012\nEpoch 579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2094 - val_loss: 4.1880\nEpoch 580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2087 - val_loss: 4.1755\nEpoch 581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2081 - val_loss: 4.1625\nEpoch 582/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2075 - val_loss: 4.1492\nEpoch 583/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2068 - val_loss: 4.1359\nEpoch 584/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2062 - val_loss: 4.1222\nEpoch 585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2055 - val_loss: 4.1091\nEpoch 586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2050 - val_loss: 4.0958\nEpoch 587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2043 - val_loss: 4.0840\nEpoch 588/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2037 - val_loss: 4.0713\nEpoch 589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2031 - val_loss: 4.0589\nEpoch 590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2025 - val_loss: 4.0462\nEpoch 591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2018 - val_loss: 4.0341\nEpoch 592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2013 - val_loss: 4.0212\nEpoch 593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2007 - val_loss: 4.0088\nEpoch 594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2001 - val_loss: 3.9966\nEpoch 595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1995 - val_loss: 3.9836\nEpoch 596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1989 - val_loss: 3.9706\nEpoch 597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1982 - val_loss: 3.9582\nEpoch 598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1977 - val_loss: 3.9448\nEpoch 599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1971 - val_loss: 3.9314\nEpoch 600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1964 - val_loss: 3.9187\nEpoch 601/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1959 - val_loss: 3.9060\nEpoch 602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1953 - val_loss: 3.8930\nEpoch 603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1947 - val_loss: 3.8804\nEpoch 604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1941 - val_loss: 3.8667\nEpoch 605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1935 - val_loss: 3.8544\nEpoch 606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1929 - val_loss: 3.8421\nEpoch 607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1924 - val_loss: 3.8289\nEpoch 608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1917 - val_loss: 3.8162\nEpoch 609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1912 - val_loss: 3.8026\nEpoch 610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1906 - val_loss: 3.7897\nEpoch 611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1900 - val_loss: 3.7766\nEpoch 612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1894 - val_loss: 3.7639\nEpoch 613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1888 - val_loss: 3.7507\nEpoch 614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1882 - val_loss: 3.7374\nEpoch 615/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1877 - val_loss: 3.7244\nEpoch 616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1871 - val_loss: 3.7116\nEpoch 617/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1865 - val_loss: 3.6990\nEpoch 618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1859 - val_loss: 3.6861\nEpoch 619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1854 - val_loss: 3.6728\nEpoch 620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1847 - val_loss: 3.6605\nEpoch 621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1842 - val_loss: 3.6471\nEpoch 622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1836 - val_loss: 3.6345\nEpoch 623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1830 - val_loss: 3.6220\nEpoch 624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1825 - val_loss: 3.6088\nEpoch 625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1819 - val_loss: 3.5953\nEpoch 626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1813 - val_loss: 3.5819\nEpoch 627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1807 - val_loss: 3.5693\nEpoch 628/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1802 - val_loss: 3.5565\nEpoch 629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1796 - val_loss: 3.5439\nEpoch 630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1791 - val_loss: 3.5319\nEpoch 631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1785 - val_loss: 3.5202\nEpoch 632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1780 - val_loss: 3.5072\nEpoch 633/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1774 - val_loss: 3.4947\nEpoch 634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1768 - val_loss: 3.4815\nEpoch 635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1763 - val_loss: 3.4685\nEpoch 636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1757 - val_loss: 3.4563\nEpoch 637/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1751 - val_loss: 3.4440\nEpoch 638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1746 - val_loss: 3.4307\nEpoch 639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1741 - val_loss: 3.4180\nEpoch 640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1735 - val_loss: 3.4056\nEpoch 641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1729 - val_loss: 3.3936\nEpoch 642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1724 - val_loss: 3.3813\nEpoch 643/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1719 - val_loss: 3.3682\nEpoch 644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1713 - val_loss: 3.3556\nEpoch 645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1708 - val_loss: 3.3429\nEpoch 646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1702 - val_loss: 3.3306\nEpoch 647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1697 - val_loss: 3.3180\nEpoch 648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1691 - val_loss: 3.3060\nEpoch 649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1686 - val_loss: 3.2933\nEpoch 650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1681 - val_loss: 3.2807\nEpoch 651/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1675 - val_loss: 3.2675\nEpoch 652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1670 - val_loss: 3.2545\nEpoch 653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1664 - val_loss: 3.2423\nEpoch 654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1658 - val_loss: 3.2296\nEpoch 655/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1653 - val_loss: 3.2170\nEpoch 656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1648 - val_loss: 3.2043\nEpoch 657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1642 - val_loss: 3.1921\nEpoch 658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1637 - val_loss: 3.1786\nEpoch 659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1631 - val_loss: 3.1659\nEpoch 660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1626 - val_loss: 3.1534\nEpoch 661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1621 - val_loss: 3.1400\nEpoch 662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1615 - val_loss: 3.1277\nEpoch 663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1610 - val_loss: 3.1148\nEpoch 664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1604 - val_loss: 3.1015\nEpoch 665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1599 - val_loss: 3.0887\nEpoch 666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 3.0764\nEpoch 667/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1588 - val_loss: 3.0640\nEpoch 668/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1583 - val_loss: 3.0516\nEpoch 669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1578 - val_loss: 3.0398\nEpoch 670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1572 - val_loss: 3.0274\nEpoch 671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1567 - val_loss: 3.0159\nEpoch 672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1562 - val_loss: 3.0039\nEpoch 673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1557 - val_loss: 2.9921\nEpoch 674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1551 - val_loss: 2.9799\nEpoch 675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1546 - val_loss: 2.9669\nEpoch 676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1540 - val_loss: 2.9550\nEpoch 677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1535 - val_loss: 2.9428\nEpoch 678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1530 - val_loss: 2.9305\nEpoch 679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1525 - val_loss: 2.9178\nEpoch 680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1519 - val_loss: 2.9059\nEpoch 681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1514 - val_loss: 2.8935\nEpoch 682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1509 - val_loss: 2.8816\nEpoch 683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1504 - val_loss: 2.8687\nEpoch 684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1499 - val_loss: 2.8565\nEpoch 685/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1494 - val_loss: 2.8445\nEpoch 686/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1489 - val_loss: 2.8318\nEpoch 687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1484 - val_loss: 2.8200\nEpoch 688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1479 - val_loss: 2.8081\nEpoch 689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1474 - val_loss: 2.7957\nEpoch 690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1468 - val_loss: 2.7837\nEpoch 691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1463 - val_loss: 2.7715\nEpoch 692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1458 - val_loss: 2.7596\nEpoch 693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1453 - val_loss: 2.7476\nEpoch 694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1448 - val_loss: 2.7354\nEpoch 695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1443 - val_loss: 2.7233\nEpoch 696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1438 - val_loss: 2.7113\nEpoch 697/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1433 - val_loss: 2.6996\nEpoch 698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1428 - val_loss: 2.6874\nEpoch 699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1423 - val_loss: 2.6760\nEpoch 700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1418 - val_loss: 2.6651\nEpoch 701/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1413 - val_loss: 2.6537\nEpoch 702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1408 - val_loss: 2.6435\nEpoch 703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1403 - val_loss: 2.6322\nEpoch 704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1398 - val_loss: 2.6204\nEpoch 705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1394 - val_loss: 2.6084\nEpoch 706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1389 - val_loss: 2.5964\nEpoch 707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1384 - val_loss: 2.5844\nEpoch 708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1378 - val_loss: 2.5730\nEpoch 709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1374 - val_loss: 2.5604\nEpoch 710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1369 - val_loss: 2.5488\nEpoch 711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1364 - val_loss: 2.5374\nEpoch 712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1359 - val_loss: 2.5266\nEpoch 713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1354 - val_loss: 2.5150\nEpoch 714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1350 - val_loss: 2.5034\nEpoch 715/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1345 - val_loss: 2.4924\nEpoch 716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1340 - val_loss: 2.4817\nEpoch 717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1335 - val_loss: 2.4699\nEpoch 718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1331 - val_loss: 2.4581\nEpoch 719/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1326 - val_loss: 2.4463\nEpoch 720/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1321 - val_loss: 2.4342\nEpoch 721/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1316 - val_loss: 2.4222\nEpoch 722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1311 - val_loss: 2.4105\nEpoch 723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1306 - val_loss: 2.3996\nEpoch 724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1302 - val_loss: 2.3888\nEpoch 725/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1297 - val_loss: 2.3774\nEpoch 726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1292 - val_loss: 2.3659\nEpoch 727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1287 - val_loss: 2.3542\nEpoch 728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1283 - val_loss: 2.3425\nEpoch 729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1278 - val_loss: 2.3306\nEpoch 730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1273 - val_loss: 2.3193\nEpoch 731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1269 - val_loss: 2.3079\nEpoch 732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1264 - val_loss: 2.2972\nEpoch 733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1260 - val_loss: 2.2863\nEpoch 734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1255 - val_loss: 2.2755\nEpoch 735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1250 - val_loss: 2.2650\nEpoch 736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1246 - val_loss: 2.2541\nEpoch 737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 2.2434\nEpoch 738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 2.2326\nEpoch 739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1232 - val_loss: 2.2211\nEpoch 740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1227 - val_loss: 2.2096\nEpoch 741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1223 - val_loss: 2.1982\nEpoch 742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1218 - val_loss: 2.1873\nEpoch 743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1213 - val_loss: 2.1772\nEpoch 744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1209 - val_loss: 2.1657\nEpoch 745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1204 - val_loss: 2.1546\nEpoch 746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1200 - val_loss: 2.1438\nEpoch 747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1195 - val_loss: 2.1334\nEpoch 748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1191 - val_loss: 2.1226\nEpoch 749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1186 - val_loss: 2.1122\nEpoch 750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1181 - val_loss: 2.1021\nEpoch 751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1177 - val_loss: 2.0914\nEpoch 752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1173 - val_loss: 2.0800\nEpoch 753/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1168 - val_loss: 2.0693\nEpoch 754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1164 - val_loss: 2.0587\nEpoch 755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1159 - val_loss: 2.0479\nEpoch 756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1154 - val_loss: 2.0373\nEpoch 757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1150 - val_loss: 2.0264\nEpoch 758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1146 - val_loss: 2.0153\nEpoch 759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1141 - val_loss: 2.0050\nEpoch 760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1137 - val_loss: 1.9949\nEpoch 761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1133 - val_loss: 1.9837\nEpoch 762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1128 - val_loss: 1.9731\nEpoch 763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1124 - val_loss: 1.9618\nEpoch 764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1119 - val_loss: 1.9511\nEpoch 765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1115 - val_loss: 1.9405\nEpoch 766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1111 - val_loss: 1.9295\nEpoch 767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1106 - val_loss: 1.9198\nEpoch 768/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1102 - val_loss: 1.9094\nEpoch 769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1098 - val_loss: 1.8989\nEpoch 770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1094 - val_loss: 1.8886\nEpoch 771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1089 - val_loss: 1.8785\nEpoch 772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1085 - val_loss: 1.8684\nEpoch 773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1081 - val_loss: 1.8580\nEpoch 774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1077 - val_loss: 1.8486\nEpoch 775/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1073 - val_loss: 1.8389\nEpoch 776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1069 - val_loss: 1.8292\nEpoch 777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 1.8192\nEpoch 778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1061 - val_loss: 1.8095\nEpoch 779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1057 - val_loss: 1.7997\nEpoch 780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1052 - val_loss: 1.7901\nEpoch 781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1049 - val_loss: 1.7800\nEpoch 782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1044 - val_loss: 1.7697\nEpoch 783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1040 - val_loss: 1.7598\nEpoch 784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1036 - val_loss: 1.7504\nEpoch 785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1032 - val_loss: 1.7409\nEpoch 786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 1.7314\nEpoch 787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1024 - val_loss: 1.7218\nEpoch 788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1020 - val_loss: 1.7124\nEpoch 789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1016 - val_loss: 1.7032\nEpoch 790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 1.6925\nEpoch 791/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1009 - val_loss: 1.6821\nEpoch 792/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1005 - val_loss: 1.6722\nEpoch 793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1001 - val_loss: 1.6626\nEpoch 794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0997 - val_loss: 1.6531\nEpoch 795/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0993 - val_loss: 1.6433\nEpoch 796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0989 - val_loss: 1.6334\nEpoch 797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0985 - val_loss: 1.6241\nEpoch 798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0981 - val_loss: 1.6152\nEpoch 799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0978 - val_loss: 1.6056\nEpoch 800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0974 - val_loss: 1.5965\nEpoch 801/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 1.5872\nEpoch 802/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 1.5778\nEpoch 803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0962 - val_loss: 1.5689\nEpoch 804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0959 - val_loss: 1.5599\nEpoch 805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0955 - val_loss: 1.5510\nEpoch 806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0951 - val_loss: 1.5423\nEpoch 807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0947 - val_loss: 1.5337\nEpoch 808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0944 - val_loss: 1.5249\nEpoch 809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0940 - val_loss: 1.5160\nEpoch 810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0936 - val_loss: 1.5071\nEpoch 811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0933 - val_loss: 1.4979\nEpoch 812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0929 - val_loss: 1.4890\nEpoch 813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0925 - val_loss: 1.4799\nEpoch 814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0921 - val_loss: 1.4709\nEpoch 815/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0918 - val_loss: 1.4622\nEpoch 816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0914 - val_loss: 1.4533\nEpoch 817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0910 - val_loss: 1.4441\nEpoch 818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0907 - val_loss: 1.4352\nEpoch 819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0903 - val_loss: 1.4268\nEpoch 820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0899 - val_loss: 1.4182\nEpoch 821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0896 - val_loss: 1.4094\nEpoch 822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0893 - val_loss: 1.4007\nEpoch 823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0889 - val_loss: 1.3927\nEpoch 824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0885 - val_loss: 1.3842\nEpoch 825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0882 - val_loss: 1.3756\nEpoch 826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0879 - val_loss: 1.3669\nEpoch 827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0875 - val_loss: 1.3588\nEpoch 828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0871 - val_loss: 1.3501\nEpoch 829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0868 - val_loss: 1.3413\nEpoch 830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0864 - val_loss: 1.3330\nEpoch 831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0861 - val_loss: 1.3249\nEpoch 832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0858 - val_loss: 1.3168\nEpoch 833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0854 - val_loss: 1.3080\nEpoch 834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0851 - val_loss: 1.2990\nEpoch 835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0847 - val_loss: 1.2905\nEpoch 836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0844 - val_loss: 1.2821\nEpoch 837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0840 - val_loss: 1.2737\nEpoch 838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0837 - val_loss: 1.2656\nEpoch 839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0834 - val_loss: 1.2577\nEpoch 840/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0831 - val_loss: 1.2491\nEpoch 841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0827 - val_loss: 1.2417\nEpoch 842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0824 - val_loss: 1.2343\nEpoch 843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0820 - val_loss: 1.2269\nEpoch 844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0817 - val_loss: 1.2187\nEpoch 845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0814 - val_loss: 1.2108\nEpoch 846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0811 - val_loss: 1.2027\nEpoch 847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0807 - val_loss: 1.1946\nEpoch 848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0804 - val_loss: 1.1866\nEpoch 849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0801 - val_loss: 1.1785\nEpoch 850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0797 - val_loss: 1.1710\nEpoch 851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0794 - val_loss: 1.1633\nEpoch 852/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0791 - val_loss: 1.1551\nEpoch 853/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0788 - val_loss: 1.1476\nEpoch 854/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0785 - val_loss: 1.1401\nEpoch 855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 1.1325\nEpoch 856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0779 - val_loss: 1.1250\nEpoch 857/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0776 - val_loss: 1.1173\nEpoch 858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0773 - val_loss: 1.1094\nEpoch 859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0769 - val_loss: 1.1020\nEpoch 860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0767 - val_loss: 1.0947\nEpoch 861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 1.0881\nEpoch 862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0760 - val_loss: 1.0809\nEpoch 863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0757 - val_loss: 1.0733\nEpoch 864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0754 - val_loss: 1.0658\nEpoch 865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 1.0587\nEpoch 866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 1.0514\nEpoch 867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0745 - val_loss: 1.0443\nEpoch 868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0742 - val_loss: 1.0366\nEpoch 869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0740 - val_loss: 1.0293\nEpoch 870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0736 - val_loss: 1.0222\nEpoch 871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 1.0156\nEpoch 872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0731 - val_loss: 1.0084\nEpoch 873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0728 - val_loss: 1.0014\nEpoch 874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0725 - val_loss: 0.9942\nEpoch 875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0722 - val_loss: 0.9875\nEpoch 876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.9811\nEpoch 877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0716 - val_loss: 0.9742\nEpoch 878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.9676\nEpoch 879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0711 - val_loss: 0.9613\nEpoch 880/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0708 - val_loss: 0.9546\nEpoch 881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0705 - val_loss: 0.9475\nEpoch 882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0702 - val_loss: 0.9403\nEpoch 883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.9331\nEpoch 884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.9261\nEpoch 885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0694 - val_loss: 0.9197\nEpoch 886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0691 - val_loss: 0.9135\nEpoch 887/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0689 - val_loss: 0.9066\nEpoch 888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0686 - val_loss: 0.8993\nEpoch 889/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.8924\nEpoch 890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8855\nEpoch 891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0678 - val_loss: 0.8796\nEpoch 892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0675 - val_loss: 0.8733\nEpoch 893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0673 - val_loss: 0.8668\nEpoch 894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0670 - val_loss: 0.8608\nEpoch 895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.8547\nEpoch 896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0665 - val_loss: 0.8485\nEpoch 897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0663 - val_loss: 0.8425\nEpoch 898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0660 - val_loss: 0.8362\nEpoch 899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.8303\nEpoch 900/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0655 - val_loss: 0.8244\nEpoch 901/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.8186\nEpoch 902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.8127\nEpoch 903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0648 - val_loss: 0.8066\nEpoch 904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0646 - val_loss: 0.8011\nEpoch 905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0643 - val_loss: 0.7951\nEpoch 906/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0641 - val_loss: 0.7896\nEpoch 907/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7837\nEpoch 908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.7782\nEpoch 909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0634 - val_loss: 0.7722\nEpoch 910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.7668\nEpoch 911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0629 - val_loss: 0.7612\nEpoch 912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.7556\nEpoch 913/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0625 - val_loss: 0.7500\nEpoch 914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0623 - val_loss: 0.7444\nEpoch 915/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0620 - val_loss: 0.7387\nEpoch 916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.7329\nEpoch 917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0616 - val_loss: 0.7272\nEpoch 918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0614 - val_loss: 0.7216\nEpoch 919/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0611 - val_loss: 0.7163\nEpoch 920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.7108\nEpoch 921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.7053\nEpoch 922/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.6999\nEpoch 923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.6943\nEpoch 924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.6890\nEpoch 925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0598 - val_loss: 0.6838\nEpoch 926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.6785\nEpoch 927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0594 - val_loss: 0.6733\nEpoch 928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.6678\nEpoch 929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6624\nEpoch 930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0587 - val_loss: 0.6573\nEpoch 931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0585 - val_loss: 0.6522\nEpoch 932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.6472\nEpoch 933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0581 - val_loss: 0.6422\nEpoch 934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0579 - val_loss: 0.6372\nEpoch 935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0577 - val_loss: 0.6322\nEpoch 936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0575 - val_loss: 0.6270\nEpoch 937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6220\nEpoch 938/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0571 - val_loss: 0.6170\nEpoch 939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.6121\nEpoch 940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0567 - val_loss: 0.6070\nEpoch 941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0565 - val_loss: 0.6022\nEpoch 942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0563 - val_loss: 0.5973\nEpoch 943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0561 - val_loss: 0.5923\nEpoch 944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0559 - val_loss: 0.5877\nEpoch 945/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5830\nEpoch 946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.5779\nEpoch 947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0553 - val_loss: 0.5730\nEpoch 948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0551 - val_loss: 0.5684\nEpoch 949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0549 - val_loss: 0.5634\nEpoch 950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0547 - val_loss: 0.5588\nEpoch 951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0545 - val_loss: 0.5542\nEpoch 952/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0544 - val_loss: 0.5495\nEpoch 953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5449\nEpoch 954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5401\nEpoch 955/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0538 - val_loss: 0.5357\nEpoch 956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0536 - val_loss: 0.5312\nEpoch 957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0534 - val_loss: 0.5266\nEpoch 958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0533 - val_loss: 0.5221\nEpoch 959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.5178\nEpoch 960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5137\nEpoch 961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.5098\nEpoch 962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5058\nEpoch 963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5017\nEpoch 964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.4976\nEpoch 965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.4935\nEpoch 966/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0519 - val_loss: 0.4896\nEpoch 967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4859\nEpoch 968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0516 - val_loss: 0.4814\nEpoch 969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4773\nEpoch 970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4733\nEpoch 971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0511 - val_loss: 0.4691\nEpoch 972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4649\nEpoch 973/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0508 - val_loss: 0.4611\nEpoch 974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.4570\nEpoch 975/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0505 - val_loss: 0.4527\nEpoch 976/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0503 - val_loss: 0.4487\nEpoch 977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4446\nEpoch 978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4406\nEpoch 979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.4368\nEpoch 980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0497 - val_loss: 0.4333\nEpoch 981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0495 - val_loss: 0.4294\nEpoch 982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0494 - val_loss: 0.4259\nEpoch 983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4224\nEpoch 984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.4187\nEpoch 985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0489 - val_loss: 0.4149\nEpoch 986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4111\nEpoch 987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4071\nEpoch 988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.4037\nEpoch 989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0483 - val_loss: 0.4002\nEpoch 990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.3968\nEpoch 991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.3931\nEpoch 992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3897\nEpoch 993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0478 - val_loss: 0.3861\nEpoch 994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3829\nEpoch 995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3797\nEpoch 996/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0474 - val_loss: 0.3764\nEpoch 997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3733\nEpoch 998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3703\nEpoch 999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3669\nEpoch 1000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0468 - val_loss: 0.3635\nEpoch 1001/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.3602\nEpoch 1002/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3569\nEpoch 1003/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3540\nEpoch 1004/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3506\nEpoch 1005/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0462 - val_loss: 0.3476\nEpoch 1006/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3444\nEpoch 1007/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3409\nEpoch 1008/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0458 - val_loss: 0.3378\nEpoch 1009/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3347\nEpoch 1010/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.3315\nEpoch 1011/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0454 - val_loss: 0.3286\nEpoch 1012/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3256\nEpoch 1013/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3225\nEpoch 1014/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3197\nEpoch 1015/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3168\nEpoch 1016/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.3138\nEpoch 1017/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.3111\nEpoch 1018/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3082\nEpoch 1019/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0445 - val_loss: 0.3054\nEpoch 1020/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.3027\nEpoch 1021/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.2998\nEpoch 1022/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2970\nEpoch 1023/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2940\nEpoch 1024/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0439 - val_loss: 0.2913\nEpoch 1025/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0438 - val_loss: 0.2886\nEpoch 1026/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.2858\nEpoch 1027/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 0.2831\nEpoch 1028/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2806\nEpoch 1029/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2782\nEpoch 1030/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0433 - val_loss: 0.2755\nEpoch 1031/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2731\nEpoch 1032/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2702\nEpoch 1033/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2675\nEpoch 1034/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2650\nEpoch 1035/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.2623\nEpoch 1036/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0427 - val_loss: 0.2599\nEpoch 1037/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2575\nEpoch 1038/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0425 - val_loss: 0.2553\nEpoch 1039/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0424 - val_loss: 0.2532\nEpoch 1040/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.2508\nEpoch 1041/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2486\nEpoch 1042/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2461\nEpoch 1043/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2439\nEpoch 1044/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2415\nEpoch 1045/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2393\nEpoch 1046/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2369\nEpoch 1047/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0416 - val_loss: 0.2347\nEpoch 1048/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2325\nEpoch 1049/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2303\nEpoch 1050/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2282\nEpoch 1051/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2259\nEpoch 1052/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2238\nEpoch 1053/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2215\nEpoch 1054/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2191\nEpoch 1055/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2170\nEpoch 1056/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2149\nEpoch 1057/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2129\nEpoch 1058/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2111\nEpoch 1059/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2093\nEpoch 1060/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2075\nEpoch 1061/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2056\nEpoch 1062/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2037\nEpoch 1063/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0403 - val_loss: 0.2018\nEpoch 1064/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1999\nEpoch 1065/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1978\nEpoch 1066/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1958\nEpoch 1067/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1939\nEpoch 1068/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1920\nEpoch 1069/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1904\nEpoch 1070/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1886\nEpoch 1071/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1868\nEpoch 1072/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0397 - val_loss: 0.1851\nEpoch 1073/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1834\nEpoch 1074/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1816\nEpoch 1075/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1799\nEpoch 1076/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1784\nEpoch 1077/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1765\nEpoch 1078/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0393 - val_loss: 0.1747\nEpoch 1079/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1731\nEpoch 1080/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1715\nEpoch 1081/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1699\nEpoch 1082/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1682\nEpoch 1083/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1667\nEpoch 1084/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0389 - val_loss: 0.1650\nEpoch 1085/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1636\nEpoch 1086/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1622\nEpoch 1087/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1606\nEpoch 1088/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1592\nEpoch 1089/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1578\nEpoch 1090/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1563\nEpoch 1091/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1547\nEpoch 1092/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1532\nEpoch 1093/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1517\nEpoch 1094/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1503\nEpoch 1095/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1489\nEpoch 1096/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1475\nEpoch 1097/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0381 - val_loss: 0.1460\nEpoch 1098/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1446\nEpoch 1099/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1434\nEpoch 1100/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1419\nEpoch 1101/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1407\nEpoch 1102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1393\nEpoch 1103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1381\nEpoch 1104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1367\nEpoch 1105/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 0.1355\nEpoch 1106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1343\nEpoch 1107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1333\nEpoch 1108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1321\nEpoch 1109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1308\nEpoch 1110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1295\nEpoch 1111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1283\nEpoch 1112/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1270\nEpoch 1113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1257\nEpoch 1114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1246\nEpoch 1115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1234\nEpoch 1116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1223\nEpoch 1117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1211\nEpoch 1118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1201\nEpoch 1119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1190\nEpoch 1120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1178\nEpoch 1121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1166\nEpoch 1122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1154\nEpoch 1123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1143\nEpoch 1124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1133\nEpoch 1125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1123\nEpoch 1126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1112\nEpoch 1127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1102\nEpoch 1128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1094\nEpoch 1129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1086\nEpoch 1130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1076\nEpoch 1131/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0366 - val_loss: 0.1066\nEpoch 1132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1057\nEpoch 1133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1049\nEpoch 1134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1040\nEpoch 1135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1030\nEpoch 1136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1021\nEpoch 1137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1012\nEpoch 1138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1003\nEpoch 1139/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0364 - val_loss: 0.0995\nEpoch 1140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0988\nEpoch 1141/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0363 - val_loss: 0.0980\nEpoch 1142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0972\nEpoch 1143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0965\nEpoch 1144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0956\nEpoch 1145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0947\nEpoch 1146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0939\nEpoch 1147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0930\nEpoch 1148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0923\nEpoch 1149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0916\nEpoch 1150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0910\nEpoch 1151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0903\nEpoch 1152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0895\nEpoch 1153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0888\nEpoch 1154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0880\nEpoch 1155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0874\nEpoch 1156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0867\nEpoch 1157/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.0858\nEpoch 1158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0851\nEpoch 1159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0844\nEpoch 1160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0837\nEpoch 1161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0830\nEpoch 1162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0824\nEpoch 1163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0818\nEpoch 1164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0812\nEpoch 1165/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0808\nEpoch 1166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0803\nEpoch 1167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0797\nEpoch 1168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0792\nEpoch 1169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0786\nEpoch 1170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0780\nEpoch 1171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0775\nEpoch 1172/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0768\nEpoch 1173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0762\nEpoch 1174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0756\nEpoch 1175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0750\nEpoch 1176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0745\nEpoch 1177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0740\nEpoch 1178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0734\nEpoch 1179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0729\nEpoch 1180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0724\nEpoch 1181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0719\nEpoch 1182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0714\nEpoch 1183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0709\nEpoch 1184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0704\nEpoch 1185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0698\nEpoch 1186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0694\nEpoch 1187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0689\nEpoch 1188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0684\nEpoch 1189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0679\nEpoch 1190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0674\nEpoch 1191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0669\nEpoch 1192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0666\nEpoch 1193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0661\nEpoch 1194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0657\nEpoch 1195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0652\nEpoch 1196/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 0.0648\nEpoch 1197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0644\nEpoch 1198/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 0.0640\nEpoch 1199/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0636\nEpoch 1200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0632\nEpoch 1201/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0627\nEpoch 1202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0623\nEpoch 1203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0621\nEpoch 1204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0617\nEpoch 1205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0614\nEpoch 1206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0611\nEpoch 1207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0607\nEpoch 1208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0604\nEpoch 1209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0601\nEpoch 1210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0597\nEpoch 1211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0594\nEpoch 1212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0590\nEpoch 1213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0587\nEpoch 1214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0583\nEpoch 1215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0581\nEpoch 1216/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 0.0578\nEpoch 1217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0575\nEpoch 1218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0571\nEpoch 1219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0568\nEpoch 1220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0565\nEpoch 1221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0562\nEpoch 1222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0559\nEpoch 1223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0556\nEpoch 1224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0553\nEpoch 1225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0551\nEpoch 1226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0548\nEpoch 1227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0545\nEpoch 1228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0543\nEpoch 1229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0540\nEpoch 1230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0537\nEpoch 1231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0535\nEpoch 1232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0532\nEpoch 1233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0530\nEpoch 1234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0527\nEpoch 1235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0525\nEpoch 1236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0523\nEpoch 1237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0521\nEpoch 1238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0518\nEpoch 1239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0516\nEpoch 1240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0514\nEpoch 1241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0513\nEpoch 1242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0511\nEpoch 1243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0509\nEpoch 1244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0507\nEpoch 1245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0505\nEpoch 1246/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0503\nEpoch 1247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0500\nEpoch 1248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0499\nEpoch 1249/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0497\nEpoch 1250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0495\nEpoch 1251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0493\nEpoch 1252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0491\nEpoch 1253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0489\nEpoch 1254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0488\nEpoch 1255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0486\nEpoch 1256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0484\nEpoch 1257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0482\nEpoch 1258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0480\nEpoch 1259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0479\nEpoch 1260/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0477\nEpoch 1261/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0475\nEpoch 1262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473\nEpoch 1263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473\nEpoch 1264/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0472\nEpoch 1265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0470\nEpoch 1266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0468\nEpoch 1267/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0467\nEpoch 1268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0465\nEpoch 1269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463\nEpoch 1270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462\nEpoch 1271/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0460\nEpoch 1272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458\nEpoch 1273/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0457\nEpoch 1274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456\nEpoch 1275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1276/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0453\nEpoch 1277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452\nEpoch 1278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0451\nEpoch 1279/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0450\nEpoch 1280/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0449\nEpoch 1281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448\nEpoch 1282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0447\nEpoch 1283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446\nEpoch 1284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0445\nEpoch 1285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444\nEpoch 1286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443\nEpoch 1287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442\nEpoch 1288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441\nEpoch 1289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440\nEpoch 1290/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439\nEpoch 1291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438\nEpoch 1292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0437\nEpoch 1293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0437\nEpoch 1294/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0436\nEpoch 1295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435\nEpoch 1296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433\nEpoch 1299/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.0432\nEpoch 1300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432\nEpoch 1301/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0429\nEpoch 1304/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1305/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424\nEpoch 1310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420\nEpoch 1316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420\nEpoch 1317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419\nEpoch 1318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419\nEpoch 1319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418\nEpoch 1320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0416\nEpoch 1323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413\nEpoch 1328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413\nEpoch 1329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1340/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1363/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1370/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1380/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1390/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1401/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1408/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1413/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1421/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1424/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1429/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1438/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1441/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1458/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1463/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1468/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1469/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1473/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1478/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1479/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1481/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1496/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1497/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1498/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1500/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1501/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1505/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1512/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1522/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1523/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1528/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1529/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1532/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1562/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1576/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1577/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1581/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1613/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1617/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1618/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1637/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1643/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1648/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1649/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1651/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1661/2000\n6/6 [==============================] - ETA: 0s - loss: 0.034 - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1665/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1666/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1675/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1676/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1685/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1697/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1701/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1705/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1720/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1721/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1722/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1725/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1734/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1737/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1742/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1763/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1781/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1801/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1806/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1815/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1824/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1833/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1840/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1852/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1854/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1856/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1859/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1874/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1875/2000\n6/6 [==============================] - 0s 5ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1876/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1880/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1882/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1883/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1887/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1889/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1901/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1907/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1913/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1919/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1921/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1922/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1929/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1938/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1945/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1952/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1961/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1970/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1973/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1975/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1983/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 2000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\n\n\n<keras.callbacks.History at 0x7f708bd97e50>\n\n\n\nplt.plot(y,'.',alpha=0.1)\nplt.plot(net(X),'--')\n\n\n\n\n\n# \n#%tensorboard --logdir logs --host 0.0.0.0 \n\n\n이런것은 오버핏이 아님!\n\n- 결론적으로 말해서 위와 같은 net는 설계하였을 경우 val을 빼는 것은 어리석음. (데이터만 버리는 꼴임)\n- 더 많은 데이터를 남겨주면 더 빨리 학습한다.\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(loss='mse',optimizer='adam')\nnet.fit(X,y,epochs=500,batch_size=100, validation_split=0.1, callbacks=tf.keras.callbacks.TensorBoard()) \n\nEpoch 1/500\n9/9 [==============================] - 0s 5ms/step - loss: 11.1529 - val_loss: 17.6322\nEpoch 2/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.0510 - val_loss: 17.4478\nEpoch 3/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.9482 - val_loss: 17.2670\nEpoch 4/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.8465 - val_loss: 17.0850\nEpoch 5/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.7443 - val_loss: 16.9074\nEpoch 6/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.6457 - val_loss: 16.7250\nEpoch 7/500\n9/9 [==============================] - 0s 1ms/step - loss: 10.5456 - val_loss: 16.5480\nEpoch 8/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.4474 - val_loss: 16.3721\nEpoch 9/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.3499 - val_loss: 16.1945\nEpoch 10/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.2516 - val_loss: 16.0212\nEpoch 11/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.1587 - val_loss: 15.8501\nEpoch 12/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.0600 - val_loss: 15.6844\nEpoch 13/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.9677 - val_loss: 15.5179\nEpoch 14/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.8747 - val_loss: 15.3479\nEpoch 15/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.7806 - val_loss: 15.1842\nEpoch 16/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.6911 - val_loss: 15.0164\nEpoch 17/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.5969 - val_loss: 14.8620\nEpoch 18/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.5088 - val_loss: 14.6981\nEpoch 19/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.4182 - val_loss: 14.5400\nEpoch 20/500\n9/9 [==============================] - 0s 1ms/step - loss: 9.3294 - val_loss: 14.3857\nEpoch 21/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.2421 - val_loss: 14.2279\nEpoch 22/500\n9/9 [==============================] - 0s 1ms/step - loss: 9.1551 - val_loss: 14.0740\nEpoch 23/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.0690 - val_loss: 13.9182\nEpoch 24/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.9823 - val_loss: 13.7646\nEpoch 25/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.8961 - val_loss: 13.6238\nEpoch 26/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.8128 - val_loss: 13.4795\nEpoch 27/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.7307 - val_loss: 13.3281\nEpoch 28/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.6474 - val_loss: 13.1848\nEpoch 29/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.5655 - val_loss: 13.0439\nEpoch 30/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.4836 - val_loss: 12.9032\nEpoch 31/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.4041 - val_loss: 12.7584\nEpoch 32/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.3237 - val_loss: 12.6194\nEpoch 33/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.2445 - val_loss: 12.4818\nEpoch 34/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.1652 - val_loss: 12.3472\nEpoch 35/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.0891 - val_loss: 12.2060\nEpoch 36/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.0112 - val_loss: 12.0756\nEpoch 37/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.9331 - val_loss: 11.9493\nEpoch 38/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.8597 - val_loss: 11.8165\nEpoch 39/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.7832 - val_loss: 11.6866\nEpoch 40/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.7085 - val_loss: 11.5609\nEpoch 41/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.6347 - val_loss: 11.4300\nEpoch 42/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.5625 - val_loss: 11.3039\nEpoch 43/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.4886 - val_loss: 11.1786\nEpoch 44/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.4170 - val_loss: 11.0613\nEpoch 45/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.3459 - val_loss: 10.9341\nEpoch 46/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.2742 - val_loss: 10.8175\nEpoch 47/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.2045 - val_loss: 10.6986\nEpoch 48/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.1351 - val_loss: 10.5782\nEpoch 49/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.0656 - val_loss: 10.4625\nEpoch 50/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.9977 - val_loss: 10.3436\nEpoch 51/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.9311 - val_loss: 10.2223\nEpoch 52/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.8624 - val_loss: 10.1109\nEpoch 53/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.7964 - val_loss: 9.9984\nEpoch 54/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.7296 - val_loss: 9.8888\nEpoch 55/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.6645 - val_loss: 9.7811\nEpoch 56/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.5996 - val_loss: 9.6738\nEpoch 57/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.5362 - val_loss: 9.5644\nEpoch 58/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.4717 - val_loss: 9.4555\nEpoch 59/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.4094 - val_loss: 9.3465\nEpoch 60/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.3461 - val_loss: 9.2445\nEpoch 61/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.2842 - val_loss: 9.1374\nEpoch 62/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.2229 - val_loss: 9.0369\nEpoch 63/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.1613 - val_loss: 8.9344\nEpoch 64/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.1003 - val_loss: 8.8330\nEpoch 65/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.0411 - val_loss: 8.7345\nEpoch 66/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.9818 - val_loss: 8.6328\nEpoch 67/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.9227 - val_loss: 8.5349\nEpoch 68/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.8640 - val_loss: 8.4380\nEpoch 69/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.8069 - val_loss: 8.3396\nEpoch 70/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.7490 - val_loss: 8.2460\nEpoch 71/500\n9/9 [==============================] - 0s 1ms/step - loss: 5.6922 - val_loss: 8.1507\nEpoch 72/500\n9/9 [==============================] - 0s 1ms/step - loss: 5.6366 - val_loss: 8.0578\nEpoch 73/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.5798 - val_loss: 7.9645\nEpoch 74/500\n9/9 [==============================] - 0s 1ms/step - loss: 5.5245 - val_loss: 7.8750\nEpoch 75/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.4699 - val_loss: 7.7816\nEpoch 76/500\n9/9 [==============================] - 0s 1ms/step - loss: 5.4155 - val_loss: 7.6949\nEpoch 77/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.3615 - val_loss: 7.6049\nEpoch 78/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.3088 - val_loss: 7.5146\nEpoch 79/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.2546 - val_loss: 7.4298\nEpoch 80/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.2024 - val_loss: 7.3431\nEpoch 81/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.1507 - val_loss: 7.2562\nEpoch 82/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.0991 - val_loss: 7.1726\nEpoch 83/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.0482 - val_loss: 7.0886\nEpoch 84/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.9975 - val_loss: 7.0058\nEpoch 85/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.9472 - val_loss: 6.9263\nEpoch 86/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.8969 - val_loss: 6.8459\nEpoch 87/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.8479 - val_loss: 6.7668\nEpoch 88/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.7994 - val_loss: 6.6891\nEpoch 89/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.7512 - val_loss: 6.6063\nEpoch 90/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.7022 - val_loss: 6.5310\nEpoch 91/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.6547 - val_loss: 6.4561\nEpoch 92/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.6079 - val_loss: 6.3786\nEpoch 93/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.5607 - val_loss: 6.3046\nEpoch 94/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.5144 - val_loss: 6.2286\nEpoch 95/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.4690 - val_loss: 6.1530\nEpoch 96/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.4237 - val_loss: 6.0801\nEpoch 97/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.3780 - val_loss: 6.0086\nEpoch 98/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.3333 - val_loss: 5.9393\nEpoch 99/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.2886 - val_loss: 5.8690\nEpoch 100/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.2452 - val_loss: 5.7963\nEpoch 101/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.2013 - val_loss: 5.7301\nEpoch 102/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.1583 - val_loss: 5.6612\nEpoch 103/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.1153 - val_loss: 5.5936\nEpoch 104/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.0736 - val_loss: 5.5238\nEpoch 105/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.0305 - val_loss: 5.4606\nEpoch 106/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.9894 - val_loss: 5.3935\nEpoch 107/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.9481 - val_loss: 5.3268\nEpoch 108/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.9076 - val_loss: 5.2610\nEpoch 109/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.8664 - val_loss: 5.1996\nEpoch 110/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.8257 - val_loss: 5.1385\nEpoch 111/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.7870 - val_loss: 5.0747\nEpoch 112/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.7474 - val_loss: 5.0111\nEpoch 113/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.7078 - val_loss: 4.9471\nEpoch 114/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.6688 - val_loss: 4.8894\nEpoch 115/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.6306 - val_loss: 4.8294\nEpoch 116/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.5922 - val_loss: 4.7704\nEpoch 117/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.5549 - val_loss: 4.7096\nEpoch 118/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.5174 - val_loss: 4.6514\nEpoch 119/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.4798 - val_loss: 4.5948\nEpoch 120/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.4434 - val_loss: 4.5386\nEpoch 121/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.4072 - val_loss: 4.4816\nEpoch 122/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.3708 - val_loss: 4.4268\nEpoch 123/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.3349 - val_loss: 4.3720\nEpoch 124/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.2994 - val_loss: 4.3174\nEpoch 125/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.2643 - val_loss: 4.2653\nEpoch 126/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.2300 - val_loss: 4.2094\nEpoch 127/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.1955 - val_loss: 4.1579\nEpoch 128/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.1605 - val_loss: 4.1050\nEpoch 129/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.1269 - val_loss: 4.0541\nEpoch 130/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.0935 - val_loss: 4.0013\nEpoch 131/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.0599 - val_loss: 3.9509\nEpoch 132/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.0270 - val_loss: 3.9000\nEpoch 133/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.9942 - val_loss: 3.8525\nEpoch 134/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.9619 - val_loss: 3.8035\nEpoch 135/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.9301 - val_loss: 3.7534\nEpoch 136/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.8981 - val_loss: 3.7043\nEpoch 137/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.8662 - val_loss: 3.6581\nEpoch 138/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.8350 - val_loss: 3.6088\nEpoch 139/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.8040 - val_loss: 3.5631\nEpoch 140/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.7735 - val_loss: 3.5183\nEpoch 141/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.7429 - val_loss: 3.4733\nEpoch 142/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.7129 - val_loss: 3.4262\nEpoch 143/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.6828 - val_loss: 3.3835\nEpoch 144/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.6533 - val_loss: 3.3392\nEpoch 145/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.6238 - val_loss: 3.2961\nEpoch 146/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.5950 - val_loss: 3.2534\nEpoch 147/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.5661 - val_loss: 3.2100\nEpoch 148/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.5375 - val_loss: 3.1676\nEpoch 149/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.5092 - val_loss: 3.1254\nEpoch 150/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.4813 - val_loss: 3.0836\nEpoch 151/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.4537 - val_loss: 3.0419\nEpoch 152/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.4258 - val_loss: 3.0024\nEpoch 153/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3987 - val_loss: 2.9623\nEpoch 154/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3716 - val_loss: 2.9227\nEpoch 155/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3446 - val_loss: 2.8838\nEpoch 156/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.3184 - val_loss: 2.8463\nEpoch 157/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2919 - val_loss: 2.8087\nEpoch 158/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2661 - val_loss: 2.7687\nEpoch 159/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2403 - val_loss: 2.7325\nEpoch 160/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.2147 - val_loss: 2.6946\nEpoch 161/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.1892 - val_loss: 2.6583\nEpoch 162/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.1645 - val_loss: 2.6212\nEpoch 163/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.1394 - val_loss: 2.5858\nEpoch 164/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.1146 - val_loss: 2.5519\nEpoch 165/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.0904 - val_loss: 2.5177\nEpoch 166/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.0662 - val_loss: 2.4819\nEpoch 167/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0422 - val_loss: 2.4472\nEpoch 168/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0186 - val_loss: 2.4146\nEpoch 169/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9950 - val_loss: 2.3812\nEpoch 170/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9717 - val_loss: 2.3481\nEpoch 171/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9486 - val_loss: 2.3145\nEpoch 172/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9259 - val_loss: 2.2815\nEpoch 173/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.9030 - val_loss: 2.2488\nEpoch 174/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.8807 - val_loss: 2.2179\nEpoch 175/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.8587 - val_loss: 2.1859\nEpoch 176/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.8365 - val_loss: 2.1555\nEpoch 177/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.8148 - val_loss: 2.1247\nEpoch 178/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7933 - val_loss: 2.0933\nEpoch 179/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7719 - val_loss: 2.0639\nEpoch 180/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7508 - val_loss: 2.0344\nEpoch 181/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7297 - val_loss: 2.0057\nEpoch 182/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7092 - val_loss: 1.9759\nEpoch 183/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.6885 - val_loss: 1.9478\nEpoch 184/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.6680 - val_loss: 1.9216\nEpoch 185/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.6480 - val_loss: 1.8931\nEpoch 186/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.6280 - val_loss: 1.8653\nEpoch 187/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.6082 - val_loss: 1.8372\nEpoch 188/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5887 - val_loss: 1.8108\nEpoch 189/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.5693 - val_loss: 1.7840\nEpoch 190/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5501 - val_loss: 1.7577\nEpoch 191/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.5310 - val_loss: 1.7306\nEpoch 192/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5123 - val_loss: 1.7052\nEpoch 193/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4938 - val_loss: 1.6806\nEpoch 194/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4752 - val_loss: 1.6546\nEpoch 195/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.4568 - val_loss: 1.6293\nEpoch 196/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4390 - val_loss: 1.6049\nEpoch 197/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4210 - val_loss: 1.5808\nEpoch 198/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4031 - val_loss: 1.5575\nEpoch 199/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3857 - val_loss: 1.5334\nEpoch 200/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3684 - val_loss: 1.5114\nEpoch 201/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3513 - val_loss: 1.4878\nEpoch 202/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3341 - val_loss: 1.4668\nEpoch 203/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3173 - val_loss: 1.4431\nEpoch 204/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3006 - val_loss: 1.4205\nEpoch 205/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2841 - val_loss: 1.3977\nEpoch 206/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2679 - val_loss: 1.3771\nEpoch 207/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2517 - val_loss: 1.3541\nEpoch 208/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.2358 - val_loss: 1.3334\nEpoch 209/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.2198 - val_loss: 1.3130\nEpoch 210/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.2042 - val_loss: 1.2929\nEpoch 211/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.1888 - val_loss: 1.2718\nEpoch 212/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1734 - val_loss: 1.2505\nEpoch 213/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1582 - val_loss: 1.2305\nEpoch 214/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1431 - val_loss: 1.2123\nEpoch 215/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1284 - val_loss: 1.1923\nEpoch 216/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1135 - val_loss: 1.1739\nEpoch 217/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0991 - val_loss: 1.1556\nEpoch 218/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0848 - val_loss: 1.1372\nEpoch 219/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0705 - val_loss: 1.1175\nEpoch 220/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0565 - val_loss: 1.0985\nEpoch 221/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0425 - val_loss: 1.0817\nEpoch 222/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.0286 - val_loss: 1.0638\nEpoch 223/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0150 - val_loss: 1.0465\nEpoch 224/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0016 - val_loss: 1.0295\nEpoch 225/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9882 - val_loss: 1.0121\nEpoch 226/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9751 - val_loss: 0.9955\nEpoch 227/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9620 - val_loss: 0.9794\nEpoch 228/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9490 - val_loss: 0.9625\nEpoch 229/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9363 - val_loss: 0.9461\nEpoch 230/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9237 - val_loss: 0.9302\nEpoch 231/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9113 - val_loss: 0.9141\nEpoch 232/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8988 - val_loss: 0.8987\nEpoch 233/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8868 - val_loss: 0.8831\nEpoch 234/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.8747 - val_loss: 0.8684\nEpoch 235/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8628 - val_loss: 0.8537\nEpoch 236/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8509 - val_loss: 0.8394\nEpoch 237/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.8393 - val_loss: 0.8247\nEpoch 238/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8278 - val_loss: 0.8101\nEpoch 239/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8164 - val_loss: 0.7968\nEpoch 240/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.8051 - val_loss: 0.7825\nEpoch 241/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7940 - val_loss: 0.7689\nEpoch 242/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7831 - val_loss: 0.7554\nEpoch 243/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7721 - val_loss: 0.7424\nEpoch 244/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7614 - val_loss: 0.7292\nEpoch 245/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7507 - val_loss: 0.7159\nEpoch 246/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7403 - val_loss: 0.7026\nEpoch 247/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7300 - val_loss: 0.6907\nEpoch 248/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7197 - val_loss: 0.6785\nEpoch 249/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7096 - val_loss: 0.6669\nEpoch 250/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6996 - val_loss: 0.6546\nEpoch 251/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6896 - val_loss: 0.6430\nEpoch 252/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6799 - val_loss: 0.6311\nEpoch 253/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6702 - val_loss: 0.6192\nEpoch 254/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6608 - val_loss: 0.6076\nEpoch 255/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6513 - val_loss: 0.5967\nEpoch 256/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6420 - val_loss: 0.5856\nEpoch 257/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6329 - val_loss: 0.5744\nEpoch 258/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6238 - val_loss: 0.5640\nEpoch 259/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6148 - val_loss: 0.5537\nEpoch 260/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6060 - val_loss: 0.5435\nEpoch 261/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5972 - val_loss: 0.5333\nEpoch 262/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5886 - val_loss: 0.5234\nEpoch 263/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5801 - val_loss: 0.5131\nEpoch 264/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5717 - val_loss: 0.5038\nEpoch 265/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5634 - val_loss: 0.4936\nEpoch 266/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5552 - val_loss: 0.4847\nEpoch 267/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5470 - val_loss: 0.4754\nEpoch 268/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5391 - val_loss: 0.4663\nEpoch 269/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5312 - val_loss: 0.4569\nEpoch 270/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 0.4483\nEpoch 271/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5157 - val_loss: 0.4397\nEpoch 272/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5081 - val_loss: 0.4307\nEpoch 273/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5007 - val_loss: 0.4224\nEpoch 274/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4933 - val_loss: 0.4148\nEpoch 275/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4860 - val_loss: 0.4066\nEpoch 276/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4788 - val_loss: 0.3987\nEpoch 277/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4717 - val_loss: 0.3905\nEpoch 278/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4647 - val_loss: 0.3831\nEpoch 279/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4577 - val_loss: 0.3754\nEpoch 280/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4509 - val_loss: 0.3683\nEpoch 281/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4442 - val_loss: 0.3608\nEpoch 282/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4376 - val_loss: 0.3536\nEpoch 283/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4310 - val_loss: 0.3468\nEpoch 284/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4245 - val_loss: 0.3395\nEpoch 285/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4182 - val_loss: 0.3326\nEpoch 286/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4120 - val_loss: 0.3255\nEpoch 287/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4057 - val_loss: 0.3190\nEpoch 288/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3996 - val_loss: 0.3124\nEpoch 289/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3936 - val_loss: 0.3065\nEpoch 290/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3876 - val_loss: 0.3002\nEpoch 291/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3818 - val_loss: 0.2941\nEpoch 292/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3760 - val_loss: 0.2880\nEpoch 293/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3703 - val_loss: 0.2820\nEpoch 294/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.2763\nEpoch 295/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3591 - val_loss: 0.2704\nEpoch 296/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3537 - val_loss: 0.2649\nEpoch 297/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3483 - val_loss: 0.2594\nEpoch 298/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3430 - val_loss: 0.2540\nEpoch 299/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3378 - val_loss: 0.2489\nEpoch 300/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3326 - val_loss: 0.2438\nEpoch 301/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3275 - val_loss: 0.2386\nEpoch 302/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3225 - val_loss: 0.2335\nEpoch 303/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3176 - val_loss: 0.2288\nEpoch 304/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3128 - val_loss: 0.2238\nEpoch 305/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3080 - val_loss: 0.2190\nEpoch 306/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3032 - val_loss: 0.2144\nEpoch 307/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2986 - val_loss: 0.2099\nEpoch 308/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2941 - val_loss: 0.2054\nEpoch 309/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2895 - val_loss: 0.2010\nEpoch 310/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2851 - val_loss: 0.1969\nEpoch 311/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2807 - val_loss: 0.1927\nEpoch 312/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2764 - val_loss: 0.1885\nEpoch 313/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2722 - val_loss: 0.1847\nEpoch 314/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2680 - val_loss: 0.1807\nEpoch 315/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2639 - val_loss: 0.1769\nEpoch 316/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2598 - val_loss: 0.1731\nEpoch 317/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2559 - val_loss: 0.1694\nEpoch 318/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2519 - val_loss: 0.1658\nEpoch 319/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2480 - val_loss: 0.1622\nEpoch 320/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2443 - val_loss: 0.1590\nEpoch 321/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2405 - val_loss: 0.1555\nEpoch 322/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2368 - val_loss: 0.1522\nEpoch 323/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2332 - val_loss: 0.1488\nEpoch 324/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2296 - val_loss: 0.1457\nEpoch 325/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2261 - val_loss: 0.1428\nEpoch 326/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2227 - val_loss: 0.1396\nEpoch 327/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2192 - val_loss: 0.1366\nEpoch 328/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2159 - val_loss: 0.1337\nEpoch 329/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2126 - val_loss: 0.1312\nEpoch 330/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2094 - val_loss: 0.1284\nEpoch 331/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2062 - val_loss: 0.1255\nEpoch 332/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2030 - val_loss: 0.1228\nEpoch 333/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2000 - val_loss: 0.1203\nEpoch 334/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1969 - val_loss: 0.1179\nEpoch 335/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1940 - val_loss: 0.1153\nEpoch 336/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1910 - val_loss: 0.1129\nEpoch 337/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1881 - val_loss: 0.1107\nEpoch 338/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1853 - val_loss: 0.1082\nEpoch 339/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1825 - val_loss: 0.1060\nEpoch 340/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1797 - val_loss: 0.1039\nEpoch 341/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1770 - val_loss: 0.1018\nEpoch 342/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1744 - val_loss: 0.0996\nEpoch 343/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1718 - val_loss: 0.0976\nEpoch 344/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1692 - val_loss: 0.0956\nEpoch 345/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1667 - val_loss: 0.0939\nEpoch 346/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1642 - val_loss: 0.0918\nEpoch 347/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1618 - val_loss: 0.0900\nEpoch 348/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1594 - val_loss: 0.0882\nEpoch 349/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1570 - val_loss: 0.0865\nEpoch 350/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1547 - val_loss: 0.0849\nEpoch 351/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1524 - val_loss: 0.0833\nEpoch 352/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1502 - val_loss: 0.0815\nEpoch 353/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1480 - val_loss: 0.0800\nEpoch 354/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1458 - val_loss: 0.0785\nEpoch 355/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1437 - val_loss: 0.0769\nEpoch 356/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1416 - val_loss: 0.0755\nEpoch 357/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1396 - val_loss: 0.0741\nEpoch 358/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1376 - val_loss: 0.0728\nEpoch 359/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1356 - val_loss: 0.0715\nEpoch 360/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1337 - val_loss: 0.0702\nEpoch 361/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1317 - val_loss: 0.0691\nEpoch 362/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1298 - val_loss: 0.0678\nEpoch 363/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1280 - val_loss: 0.0667\nEpoch 364/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1262 - val_loss: 0.0654\nEpoch 365/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.0643\nEpoch 366/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1227 - val_loss: 0.0632\nEpoch 367/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 0.0622\nEpoch 368/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1193 - val_loss: 0.0612\nEpoch 369/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1177 - val_loss: 0.0601\nEpoch 370/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1160 - val_loss: 0.0592\nEpoch 371/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1144 - val_loss: 0.0582\nEpoch 372/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1129 - val_loss: 0.0574\nEpoch 373/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1114 - val_loss: 0.0565\nEpoch 374/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1099 - val_loss: 0.0556\nEpoch 375/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1084 - val_loss: 0.0548\nEpoch 376/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1069 - val_loss: 0.0541\nEpoch 377/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1055 - val_loss: 0.0533\nEpoch 378/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1041 - val_loss: 0.0525\nEpoch 379/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1028 - val_loss: 0.0518\nEpoch 380/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1014 - val_loss: 0.0511\nEpoch 381/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1001 - val_loss: 0.0504\nEpoch 382/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0988 - val_loss: 0.0498\nEpoch 383/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0975 - val_loss: 0.0492\nEpoch 384/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0963 - val_loss: 0.0486\nEpoch 385/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0951 - val_loss: 0.0480\nEpoch 386/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0939 - val_loss: 0.0474\nEpoch 387/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0927 - val_loss: 0.0469\nEpoch 388/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0915 - val_loss: 0.0463\nEpoch 389/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0904 - val_loss: 0.0458\nEpoch 390/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0893 - val_loss: 0.0453\nEpoch 391/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0882 - val_loss: 0.0448\nEpoch 392/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0871 - val_loss: 0.0444\nEpoch 393/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0861 - val_loss: 0.0439\nEpoch 394/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0851 - val_loss: 0.0435\nEpoch 395/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0841 - val_loss: 0.0431\nEpoch 396/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.0427\nEpoch 397/500\n9/9 [==============================] - ETA: 0s - loss: 0.088 - 0s 2ms/step - loss: 0.0821 - val_loss: 0.0423\nEpoch 398/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0812 - val_loss: 0.0420\nEpoch 399/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0416\nEpoch 400/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0793 - val_loss: 0.0413\nEpoch 401/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0784 - val_loss: 0.0410\nEpoch 402/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0775 - val_loss: 0.0407\nEpoch 403/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0767 - val_loss: 0.0403\nEpoch 404/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0758 - val_loss: 0.0401\nEpoch 405/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0750 - val_loss: 0.0398\nEpoch 406/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0742 - val_loss: 0.0395\nEpoch 407/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0734 - val_loss: 0.0392\nEpoch 408/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0726 - val_loss: 0.0390\nEpoch 409/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0718 - val_loss: 0.0388\nEpoch 410/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0711 - val_loss: 0.0386\nEpoch 411/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0704 - val_loss: 0.0384\nEpoch 412/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0696 - val_loss: 0.0382\nEpoch 413/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0689 - val_loss: 0.0380\nEpoch 414/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0682 - val_loss: 0.0378\nEpoch 415/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0676 - val_loss: 0.0376\nEpoch 416/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0669 - val_loss: 0.0374\nEpoch 417/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0663 - val_loss: 0.0373\nEpoch 418/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0656 - val_loss: 0.0371\nEpoch 419/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0650 - val_loss: 0.0370\nEpoch 420/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0644 - val_loss: 0.0369\nEpoch 421/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0638 - val_loss: 0.0367\nEpoch 422/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0632 - val_loss: 0.0366\nEpoch 423/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0626 - val_loss: 0.0365\nEpoch 424/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0620 - val_loss: 0.0364\nEpoch 425/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0615 - val_loss: 0.0363\nEpoch 426/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0609 - val_loss: 0.0362\nEpoch 427/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0604 - val_loss: 0.0361\nEpoch 428/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0599 - val_loss: 0.0360\nEpoch 429/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0594 - val_loss: 0.0359\nEpoch 430/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0589 - val_loss: 0.0358\nEpoch 431/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0358\nEpoch 432/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0357\nEpoch 433/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0574 - val_loss: 0.0356\nEpoch 434/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0570 - val_loss: 0.0356\nEpoch 435/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0565 - val_loss: 0.0355\nEpoch 436/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0561 - val_loss: 0.0355\nEpoch 437/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0556 - val_loss: 0.0354\nEpoch 438/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0552 - val_loss: 0.0354\nEpoch 439/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0548 - val_loss: 0.0353\nEpoch 440/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0544 - val_loss: 0.0353\nEpoch 441/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0540 - val_loss: 0.0353\nEpoch 442/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0536 - val_loss: 0.0352\nEpoch 443/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0532 - val_loss: 0.0352\nEpoch 444/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0528 - val_loss: 0.0352\nEpoch 445/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0525 - val_loss: 0.0352\nEpoch 446/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0521 - val_loss: 0.0351\nEpoch 447/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0518 - val_loss: 0.0351\nEpoch 448/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0514 - val_loss: 0.0351\nEpoch 449/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0511 - val_loss: 0.0351\nEpoch 450/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0507 - val_loss: 0.0351\nEpoch 451/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0504 - val_loss: 0.0350\nEpoch 452/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0501 - val_loss: 0.0350\nEpoch 453/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0498 - val_loss: 0.0350\nEpoch 454/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0495 - val_loss: 0.0350\nEpoch 455/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0492 - val_loss: 0.0350\nEpoch 456/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0489 - val_loss: 0.0350\nEpoch 457/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0486 - val_loss: 0.0350\nEpoch 458/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0483 - val_loss: 0.0350\nEpoch 459/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0480 - val_loss: 0.0350\nEpoch 460/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0478 - val_loss: 0.0350\nEpoch 461/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0475 - val_loss: 0.0350\nEpoch 462/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0473 - val_loss: 0.0350\nEpoch 463/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0470 - val_loss: 0.0350\nEpoch 464/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0468 - val_loss: 0.0350\nEpoch 465/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0465 - val_loss: 0.0350\nEpoch 466/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0463 - val_loss: 0.0350\nEpoch 467/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0460 - val_loss: 0.0350\nEpoch 468/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0458 - val_loss: 0.0350\nEpoch 469/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0456 - val_loss: 0.0350\nEpoch 470/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0454 - val_loss: 0.0350\nEpoch 471/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0452 - val_loss: 0.0351\nEpoch 472/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0449 - val_loss: 0.0351\nEpoch 473/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.0351\nEpoch 474/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0445 - val_loss: 0.0351\nEpoch 475/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0443 - val_loss: 0.0351\nEpoch 476/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0442 - val_loss: 0.0351\nEpoch 477/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0440 - val_loss: 0.0351\nEpoch 478/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0438 - val_loss: 0.0351\nEpoch 479/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0436 - val_loss: 0.0351\nEpoch 480/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0434 - val_loss: 0.0351\nEpoch 481/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0432 - val_loss: 0.0351\nEpoch 482/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0431 - val_loss: 0.0351\nEpoch 483/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0429 - val_loss: 0.0351\nEpoch 484/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0428 - val_loss: 0.0352\nEpoch 485/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0426 - val_loss: 0.0352\nEpoch 486/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0424 - val_loss: 0.0352\nEpoch 487/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0423 - val_loss: 0.0352\nEpoch 488/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0421 - val_loss: 0.0352\nEpoch 489/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0420 - val_loss: 0.0352\nEpoch 490/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0418 - val_loss: 0.0352\nEpoch 491/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0417 - val_loss: 0.0352\nEpoch 492/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.0352\nEpoch 493/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0414 - val_loss: 0.0352\nEpoch 494/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0413 - val_loss: 0.0352\nEpoch 495/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0412 - val_loss: 0.0352\nEpoch 496/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0411 - val_loss: 0.0353\nEpoch 497/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 0.0353\nEpoch 498/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0408 - val_loss: 0.0353\nEpoch 499/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0407 - val_loss: 0.0353\nEpoch 500/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0406 - val_loss: 0.0353\n\n\n<keras.callbacks.History at 0x7f708be9a470>\n\n\n\nplt.plot(y,'.',alpha=0.1)\nplt.plot(net(X),'--')\n\n\n\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0 \n\n\n\n텐서보드: 적합결과 시각화\n- 시각화결과는 모두 텐서보드에서 보고 싶다! 적합결과를 보여주는 fig 오브젝트를 텐서보드에 끼워넣어서 출력하는 방법을 알아보자.\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(loss='mse',optimizer='adam')\nnet.fit(X,y,epochs=500,batch_size=100, validation_split=0.1, callbacks=tf.keras.callbacks.TensorBoard()) \n\nEpoch 1/500\n9/9 [==============================] - 0s 5ms/step - loss: 11.9832 - val_loss: 10.8663\nEpoch 2/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.8783 - val_loss: 10.7961\nEpoch 3/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.7750 - val_loss: 10.7292\nEpoch 4/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.6712 - val_loss: 10.6628\nEpoch 5/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.5688 - val_loss: 10.5916\nEpoch 6/500\n9/9 [==============================] - 0s 1ms/step - loss: 11.4671 - val_loss: 10.5235\nEpoch 7/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.3650 - val_loss: 10.4527\nEpoch 8/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.2672 - val_loss: 10.3818\nEpoch 9/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.1664 - val_loss: 10.3141\nEpoch 10/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.0684 - val_loss: 10.2464\nEpoch 11/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.9698 - val_loss: 10.1806\nEpoch 12/500\n9/9 [==============================] - 0s 1ms/step - loss: 10.8727 - val_loss: 10.1120\nEpoch 13/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.7777 - val_loss: 10.0403\nEpoch 14/500\n9/9 [==============================] - 0s 1ms/step - loss: 10.6817 - val_loss: 9.9739\nEpoch 15/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.5871 - val_loss: 9.9000\nEpoch 16/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.4931 - val_loss: 9.8320\nEpoch 17/500\n9/9 [==============================] - 0s 1ms/step - loss: 10.3994 - val_loss: 9.7659\nEpoch 18/500\n9/9 [==============================] - 0s 1ms/step - loss: 10.3062 - val_loss: 9.7024\nEpoch 19/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.2162 - val_loss: 9.6333\nEpoch 20/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.1243 - val_loss: 9.5670\nEpoch 21/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.0331 - val_loss: 9.5003\nEpoch 22/500\n9/9 [==============================] - 0s 1ms/step - loss: 9.9449 - val_loss: 9.4318\nEpoch 23/500\n9/9 [==============================] - 0s 1ms/step - loss: 9.8568 - val_loss: 9.3642\nEpoch 24/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.7675 - val_loss: 9.2992\nEpoch 25/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.6794 - val_loss: 9.2379\nEpoch 26/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.5940 - val_loss: 9.1682\nEpoch 27/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.5071 - val_loss: 9.1010\nEpoch 28/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.4226 - val_loss: 9.0350\nEpoch 29/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.3376 - val_loss: 8.9707\nEpoch 30/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.2533 - val_loss: 8.9077\nEpoch 31/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.1692 - val_loss: 8.8398\nEpoch 32/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.0872 - val_loss: 8.7771\nEpoch 33/500\n9/9 [==============================] - 0s 1ms/step - loss: 9.0056 - val_loss: 8.7131\nEpoch 34/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.9235 - val_loss: 8.6456\nEpoch 35/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.8424 - val_loss: 8.5801\nEpoch 36/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.7621 - val_loss: 8.5169\nEpoch 37/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.6824 - val_loss: 8.4507\nEpoch 38/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.6039 - val_loss: 8.3903\nEpoch 39/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.5253 - val_loss: 8.3273\nEpoch 40/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.4475 - val_loss: 8.2648\nEpoch 41/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.3703 - val_loss: 8.2014\nEpoch 42/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.2945 - val_loss: 8.1372\nEpoch 43/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.2177 - val_loss: 8.0751\nEpoch 44/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.1433 - val_loss: 8.0141\nEpoch 45/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.0678 - val_loss: 7.9512\nEpoch 46/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.9936 - val_loss: 7.8923\nEpoch 47/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.9200 - val_loss: 7.8267\nEpoch 48/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.8475 - val_loss: 7.7637\nEpoch 49/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.7742 - val_loss: 7.7034\nEpoch 50/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.7027 - val_loss: 7.6417\nEpoch 51/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.6314 - val_loss: 7.5816\nEpoch 52/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.5596 - val_loss: 7.5203\nEpoch 53/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.4893 - val_loss: 7.4622\nEpoch 54/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.4199 - val_loss: 7.4046\nEpoch 55/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.3517 - val_loss: 7.3413\nEpoch 56/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.2821 - val_loss: 7.2826\nEpoch 57/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.2138 - val_loss: 7.2205\nEpoch 58/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.1461 - val_loss: 7.1610\nEpoch 59/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.0787 - val_loss: 7.1049\nEpoch 60/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.0124 - val_loss: 7.0466\nEpoch 61/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.9463 - val_loss: 6.9873\nEpoch 62/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.8811 - val_loss: 6.9300\nEpoch 63/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.8151 - val_loss: 6.8720\nEpoch 64/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.7508 - val_loss: 6.8148\nEpoch 65/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.6870 - val_loss: 6.7572\nEpoch 66/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.6227 - val_loss: 6.7017\nEpoch 67/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.5596 - val_loss: 6.6439\nEpoch 68/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.4973 - val_loss: 6.5899\nEpoch 69/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.4353 - val_loss: 6.5328\nEpoch 70/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.3738 - val_loss: 6.4786\nEpoch 71/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.3121 - val_loss: 6.4191\nEpoch 72/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.2514 - val_loss: 6.3663\nEpoch 73/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.1910 - val_loss: 6.3117\nEpoch 74/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.1314 - val_loss: 6.2588\nEpoch 75/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.0724 - val_loss: 6.2038\nEpoch 76/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.0138 - val_loss: 6.1501\nEpoch 77/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.9543 - val_loss: 6.0937\nEpoch 78/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.8974 - val_loss: 6.0370\nEpoch 79/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.8396 - val_loss: 5.9847\nEpoch 80/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.7821 - val_loss: 5.9322\nEpoch 81/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.7258 - val_loss: 5.8811\nEpoch 82/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.6695 - val_loss: 5.8268\nEpoch 83/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.6137 - val_loss: 5.7750\nEpoch 84/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.5585 - val_loss: 5.7240\nEpoch 85/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.5036 - val_loss: 5.6720\nEpoch 86/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.4493 - val_loss: 5.6208\nEpoch 87/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.3949 - val_loss: 5.5691\nEpoch 88/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.3418 - val_loss: 5.5173\nEpoch 89/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.2885 - val_loss: 5.4681\nEpoch 90/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.2360 - val_loss: 5.4166\nEpoch 91/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.1837 - val_loss: 5.3662\nEpoch 92/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.1319 - val_loss: 5.3159\nEpoch 93/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.0797 - val_loss: 5.2649\nEpoch 94/500\n9/9 [==============================] - 0s 1ms/step - loss: 5.0286 - val_loss: 5.2175\nEpoch 95/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.9783 - val_loss: 5.1691\nEpoch 96/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.9281 - val_loss: 5.1196\nEpoch 97/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.8785 - val_loss: 5.0692\nEpoch 98/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.8286 - val_loss: 5.0219\nEpoch 99/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.7799 - val_loss: 4.9733\nEpoch 100/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.7310 - val_loss: 4.9264\nEpoch 101/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.6824 - val_loss: 4.8787\nEpoch 102/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.6351 - val_loss: 4.8324\nEpoch 103/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.5871 - val_loss: 4.7854\nEpoch 104/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.5403 - val_loss: 4.7400\nEpoch 105/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.4937 - val_loss: 4.6915\nEpoch 106/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.4473 - val_loss: 4.6465\nEpoch 107/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.4013 - val_loss: 4.5990\nEpoch 108/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.3553 - val_loss: 4.5538\nEpoch 109/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.3105 - val_loss: 4.5088\nEpoch 110/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.2656 - val_loss: 4.4612\nEpoch 111/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.2210 - val_loss: 4.4165\nEpoch 112/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.1765 - val_loss: 4.3754\nEpoch 113/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.1330 - val_loss: 4.3317\nEpoch 114/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.0899 - val_loss: 4.2881\nEpoch 115/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.0470 - val_loss: 4.2423\nEpoch 116/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.0040 - val_loss: 4.1998\nEpoch 117/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.9619 - val_loss: 4.1579\nEpoch 118/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.9198 - val_loss: 4.1154\nEpoch 119/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.8786 - val_loss: 4.0717\nEpoch 120/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.8369 - val_loss: 4.0288\nEpoch 121/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.7964 - val_loss: 3.9861\nEpoch 122/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.7558 - val_loss: 3.9436\nEpoch 123/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.7153 - val_loss: 3.9022\nEpoch 124/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.6752 - val_loss: 3.8638\nEpoch 125/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.6362 - val_loss: 3.8228\nEpoch 126/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.5969 - val_loss: 3.7823\nEpoch 127/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.5579 - val_loss: 3.7407\nEpoch 128/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.5196 - val_loss: 3.7022\nEpoch 129/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.4812 - val_loss: 3.6638\nEpoch 130/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.4437 - val_loss: 3.6228\nEpoch 131/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.4061 - val_loss: 3.5835\nEpoch 132/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.3685 - val_loss: 3.5470\nEpoch 133/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.3320 - val_loss: 3.5076\nEpoch 134/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.2952 - val_loss: 3.4692\nEpoch 135/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.2594 - val_loss: 3.4310\nEpoch 136/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.2230 - val_loss: 3.3932\nEpoch 137/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.1874 - val_loss: 3.3545\nEpoch 138/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.1525 - val_loss: 3.3177\nEpoch 139/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.1174 - val_loss: 3.2801\nEpoch 140/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.0823 - val_loss: 3.2447\nEpoch 141/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.0488 - val_loss: 3.2077\nEpoch 142/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.0140 - val_loss: 3.1742\nEpoch 143/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.9804 - val_loss: 3.1380\nEpoch 144/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.9471 - val_loss: 3.1027\nEpoch 145/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.9140 - val_loss: 3.0676\nEpoch 146/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.8808 - val_loss: 3.0307\nEpoch 147/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.8484 - val_loss: 2.9959\nEpoch 148/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.8164 - val_loss: 2.9628\nEpoch 149/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.7844 - val_loss: 2.9261\nEpoch 150/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.7524 - val_loss: 2.8912\nEpoch 151/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.7214 - val_loss: 2.8600\nEpoch 152/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.6900 - val_loss: 2.8276\nEpoch 153/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.6593 - val_loss: 2.7939\nEpoch 154/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.6286 - val_loss: 2.7605\nEpoch 155/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.5985 - val_loss: 2.7283\nEpoch 156/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.5685 - val_loss: 2.6961\nEpoch 157/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.5390 - val_loss: 2.6633\nEpoch 158/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.5093 - val_loss: 2.6319\nEpoch 159/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.4803 - val_loss: 2.6009\nEpoch 160/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.4516 - val_loss: 2.5689\nEpoch 161/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.4225 - val_loss: 2.5390\nEpoch 162/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3946 - val_loss: 2.5096\nEpoch 163/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3666 - val_loss: 2.4766\nEpoch 164/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3385 - val_loss: 2.4479\nEpoch 165/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.3107 - val_loss: 2.4180\nEpoch 166/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2840 - val_loss: 2.3889\nEpoch 167/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2567 - val_loss: 2.3590\nEpoch 168/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2301 - val_loss: 2.3314\nEpoch 169/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2035 - val_loss: 2.3024\nEpoch 170/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.1773 - val_loss: 2.2721\nEpoch 171/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.1512 - val_loss: 2.2444\nEpoch 172/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.1257 - val_loss: 2.2158\nEpoch 173/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.0998 - val_loss: 2.1893\nEpoch 174/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0749 - val_loss: 2.1608\nEpoch 175/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0499 - val_loss: 2.1329\nEpoch 176/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0252 - val_loss: 2.1053\nEpoch 177/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0007 - val_loss: 2.0789\nEpoch 178/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9762 - val_loss: 2.0535\nEpoch 179/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9523 - val_loss: 2.0267\nEpoch 180/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9286 - val_loss: 1.9995\nEpoch 181/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9051 - val_loss: 1.9733\nEpoch 182/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.8815 - val_loss: 1.9489\nEpoch 183/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.8588 - val_loss: 1.9230\nEpoch 184/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.8357 - val_loss: 1.8970\nEpoch 185/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.8133 - val_loss: 1.8730\nEpoch 186/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.7908 - val_loss: 1.8493\nEpoch 187/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.7688 - val_loss: 1.8231\nEpoch 188/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7467 - val_loss: 1.7991\nEpoch 189/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7249 - val_loss: 1.7762\nEpoch 190/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.7032 - val_loss: 1.7522\nEpoch 191/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.6821 - val_loss: 1.7295\nEpoch 192/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.6611 - val_loss: 1.7063\nEpoch 193/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.6404 - val_loss: 1.6835\nEpoch 194/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.6197 - val_loss: 1.6604\nEpoch 195/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5993 - val_loss: 1.6368\nEpoch 196/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5791 - val_loss: 1.6148\nEpoch 197/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.5591 - val_loss: 1.5931\nEpoch 198/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.5395 - val_loss: 1.5701\nEpoch 199/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5196 - val_loss: 1.5490\nEpoch 200/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.5003 - val_loss: 1.5291\nEpoch 201/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.4815 - val_loss: 1.5062\nEpoch 202/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4623 - val_loss: 1.4843\nEpoch 203/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4435 - val_loss: 1.4639\nEpoch 204/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4250 - val_loss: 1.4437\nEpoch 205/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4067 - val_loss: 1.4226\nEpoch 206/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3884 - val_loss: 1.4033\nEpoch 207/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3705 - val_loss: 1.3828\nEpoch 208/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3529 - val_loss: 1.3636\nEpoch 209/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3351 - val_loss: 1.3454\nEpoch 210/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3178 - val_loss: 1.3249\nEpoch 211/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3007 - val_loss: 1.3058\nEpoch 212/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2838 - val_loss: 1.2861\nEpoch 213/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2668 - val_loss: 1.2681\nEpoch 214/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2502 - val_loss: 1.2499\nEpoch 215/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.2336 - val_loss: 1.2314\nEpoch 216/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2174 - val_loss: 1.2133\nEpoch 217/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.2013 - val_loss: 1.1955\nEpoch 218/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1853 - val_loss: 1.1776\nEpoch 219/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.1697 - val_loss: 1.1601\nEpoch 220/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.1541 - val_loss: 1.1430\nEpoch 221/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1388 - val_loss: 1.1252\nEpoch 222/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.1236 - val_loss: 1.1084\nEpoch 223/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1084 - val_loss: 1.0921\nEpoch 224/500\n9/9 [==============================] - ETA: 0s - loss: 1.099 - 0s 1ms/step - loss: 1.0936 - val_loss: 1.0763\nEpoch 225/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0789 - val_loss: 1.0605\nEpoch 226/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.0643 - val_loss: 1.0438\nEpoch 227/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.0500 - val_loss: 1.0274\nEpoch 228/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.0359 - val_loss: 1.0106\nEpoch 229/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0217 - val_loss: 0.9951\nEpoch 230/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.0079 - val_loss: 0.9800\nEpoch 231/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9942 - val_loss: 0.9651\nEpoch 232/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9807 - val_loss: 0.9507\nEpoch 233/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9674 - val_loss: 0.9347\nEpoch 234/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9539 - val_loss: 0.9199\nEpoch 235/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9408 - val_loss: 0.9063\nEpoch 236/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9279 - val_loss: 0.8911\nEpoch 237/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9152 - val_loss: 0.8767\nEpoch 238/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9026 - val_loss: 0.8635\nEpoch 239/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8902 - val_loss: 0.8494\nEpoch 240/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8778 - val_loss: 0.8349\nEpoch 241/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8655 - val_loss: 0.8223\nEpoch 242/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8536 - val_loss: 0.8099\nEpoch 243/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8417 - val_loss: 0.7958\nEpoch 244/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.8298 - val_loss: 0.7832\nEpoch 245/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8182 - val_loss: 0.7705\nEpoch 246/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.8068 - val_loss: 0.7583\nEpoch 247/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7955 - val_loss: 0.7460\nEpoch 248/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7844 - val_loss: 0.7334\nEpoch 249/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7732 - val_loss: 0.7206\nEpoch 250/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7623 - val_loss: 0.7089\nEpoch 251/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7516 - val_loss: 0.6966\nEpoch 252/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7409 - val_loss: 0.6849\nEpoch 253/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7304 - val_loss: 0.6738\nEpoch 254/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7199 - val_loss: 0.6619\nEpoch 255/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7097 - val_loss: 0.6512\nEpoch 256/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6997 - val_loss: 0.6401\nEpoch 257/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6896 - val_loss: 0.6294\nEpoch 258/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6798 - val_loss: 0.6186\nEpoch 259/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6700 - val_loss: 0.6080\nEpoch 260/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.5974\nEpoch 261/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6508 - val_loss: 0.5873\nEpoch 262/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6415 - val_loss: 0.5765\nEpoch 263/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6322 - val_loss: 0.5665\nEpoch 264/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6231 - val_loss: 0.5565\nEpoch 265/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6140 - val_loss: 0.5466\nEpoch 266/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6052 - val_loss: 0.5370\nEpoch 267/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5964 - val_loss: 0.5284\nEpoch 268/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5876 - val_loss: 0.5181\nEpoch 269/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.5088\nEpoch 270/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5706 - val_loss: 0.4992\nEpoch 271/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5622 - val_loss: 0.4903\nEpoch 272/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5541 - val_loss: 0.4816\nEpoch 273/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5459 - val_loss: 0.4731\nEpoch 274/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5379 - val_loss: 0.4642\nEpoch 275/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5300 - val_loss: 0.4553\nEpoch 276/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5222 - val_loss: 0.4471\nEpoch 277/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5145 - val_loss: 0.4391\nEpoch 278/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5069 - val_loss: 0.4313\nEpoch 279/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4994 - val_loss: 0.4231\nEpoch 280/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4921 - val_loss: 0.4154\nEpoch 281/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4847 - val_loss: 0.4079\nEpoch 282/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4776 - val_loss: 0.4006\nEpoch 283/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4704 - val_loss: 0.3929\nEpoch 284/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4634 - val_loss: 0.3851\nEpoch 285/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4565 - val_loss: 0.3780\nEpoch 286/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4497 - val_loss: 0.3709\nEpoch 287/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4430 - val_loss: 0.3632\nEpoch 288/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4364 - val_loss: 0.3567\nEpoch 289/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4298 - val_loss: 0.3499\nEpoch 290/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4234 - val_loss: 0.3432\nEpoch 291/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4171 - val_loss: 0.3366\nEpoch 292/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4108 - val_loss: 0.3304\nEpoch 293/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4047 - val_loss: 0.3237\nEpoch 294/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3985 - val_loss: 0.3174\nEpoch 295/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3926 - val_loss: 0.3115\nEpoch 296/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3867 - val_loss: 0.3055\nEpoch 297/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3808 - val_loss: 0.2996\nEpoch 298/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3751 - val_loss: 0.2935\nEpoch 299/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3694 - val_loss: 0.2875\nEpoch 300/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3639 - val_loss: 0.2818\nEpoch 301/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3583 - val_loss: 0.2761\nEpoch 302/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3530 - val_loss: 0.2708\nEpoch 303/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3476 - val_loss: 0.2655\nEpoch 304/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3423 - val_loss: 0.2605\nEpoch 305/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3372 - val_loss: 0.2553\nEpoch 306/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3320 - val_loss: 0.2504\nEpoch 307/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3270 - val_loss: 0.2452\nEpoch 308/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3221 - val_loss: 0.2401\nEpoch 309/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3171 - val_loss: 0.2351\nEpoch 310/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3124 - val_loss: 0.2308\nEpoch 311/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3076 - val_loss: 0.2260\nEpoch 312/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3030 - val_loss: 0.2216\nEpoch 313/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2983 - val_loss: 0.2171\nEpoch 314/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2938 - val_loss: 0.2126\nEpoch 315/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2894 - val_loss: 0.2082\nEpoch 316/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2850 - val_loss: 0.2039\nEpoch 317/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2806 - val_loss: 0.1997\nEpoch 318/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2764 - val_loss: 0.1957\nEpoch 319/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2722 - val_loss: 0.1917\nEpoch 320/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2681 - val_loss: 0.1879\nEpoch 321/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2640 - val_loss: 0.1839\nEpoch 322/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2600 - val_loss: 0.1802\nEpoch 323/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2560 - val_loss: 0.1765\nEpoch 324/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2522 - val_loss: 0.1728\nEpoch 325/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2483 - val_loss: 0.1690\nEpoch 326/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2446 - val_loss: 0.1656\nEpoch 327/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2409 - val_loss: 0.1623\nEpoch 328/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2372 - val_loss: 0.1589\nEpoch 329/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2337 - val_loss: 0.1556\nEpoch 330/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2301 - val_loss: 0.1525\nEpoch 331/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2266 - val_loss: 0.1493\nEpoch 332/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2232 - val_loss: 0.1462\nEpoch 333/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2199 - val_loss: 0.1431\nEpoch 334/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2165 - val_loss: 0.1402\nEpoch 335/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2133 - val_loss: 0.1375\nEpoch 336/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2101 - val_loss: 0.1348\nEpoch 337/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2069 - val_loss: 0.1317\nEpoch 338/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2038 - val_loss: 0.1289\nEpoch 339/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2008 - val_loss: 0.1262\nEpoch 340/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1978 - val_loss: 0.1238\nEpoch 341/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1948 - val_loss: 0.1213\nEpoch 342/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1919 - val_loss: 0.1188\nEpoch 343/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1891 - val_loss: 0.1166\nEpoch 344/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1863 - val_loss: 0.1142\nEpoch 345/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1835 - val_loss: 0.1118\nEpoch 346/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1808 - val_loss: 0.1095\nEpoch 347/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1781 - val_loss: 0.1073\nEpoch 348/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1755 - val_loss: 0.1052\nEpoch 349/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1729 - val_loss: 0.1031\nEpoch 350/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1704 - val_loss: 0.1009\nEpoch 351/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1679 - val_loss: 0.0989\nEpoch 352/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1654 - val_loss: 0.0971\nEpoch 353/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1630 - val_loss: 0.0952\nEpoch 354/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1606 - val_loss: 0.0934\nEpoch 355/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1583 - val_loss: 0.0914\nEpoch 356/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1560 - val_loss: 0.0896\nEpoch 357/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1537 - val_loss: 0.0880\nEpoch 358/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1515 - val_loss: 0.0863\nEpoch 359/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1494 - val_loss: 0.0845\nEpoch 360/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1472 - val_loss: 0.0830\nEpoch 361/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1451 - val_loss: 0.0815\nEpoch 362/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1430 - val_loss: 0.0799\nEpoch 363/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1410 - val_loss: 0.0784\nEpoch 364/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1390 - val_loss: 0.0770\nEpoch 365/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1370 - val_loss: 0.0755\nEpoch 366/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1351 - val_loss: 0.0741\nEpoch 367/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1333 - val_loss: 0.0728\nEpoch 368/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1314 - val_loss: 0.0715\nEpoch 369/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1295 - val_loss: 0.0704\nEpoch 370/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1278 - val_loss: 0.0691\nEpoch 371/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1260 - val_loss: 0.0679\nEpoch 372/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1242 - val_loss: 0.0667\nEpoch 373/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1225 - val_loss: 0.0655\nEpoch 374/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1209 - val_loss: 0.0645\nEpoch 375/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1193 - val_loss: 0.0635\nEpoch 376/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1176 - val_loss: 0.0624\nEpoch 377/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1161 - val_loss: 0.0614\nEpoch 378/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1145 - val_loss: 0.0603\nEpoch 379/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1129 - val_loss: 0.0594\nEpoch 380/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1115 - val_loss: 0.0584\nEpoch 381/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1100 - val_loss: 0.0575\nEpoch 382/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1086 - val_loss: 0.0567\nEpoch 383/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1071 - val_loss: 0.0559\nEpoch 384/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1058 - val_loss: 0.0551\nEpoch 385/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1044 - val_loss: 0.0543\nEpoch 386/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1030 - val_loss: 0.0536\nEpoch 387/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1017 - val_loss: 0.0528\nEpoch 388/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1004 - val_loss: 0.0520\nEpoch 389/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0992 - val_loss: 0.0513\nEpoch 390/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0979 - val_loss: 0.0507\nEpoch 391/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0967 - val_loss: 0.0500\nEpoch 392/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0955 - val_loss: 0.0495\nEpoch 393/500\n9/9 [==============================] - 0s 4ms/step - loss: 0.0943 - val_loss: 0.0489\nEpoch 394/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0931 - val_loss: 0.0482\nEpoch 395/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0920 - val_loss: 0.0477\nEpoch 396/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0909 - val_loss: 0.0471\nEpoch 397/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0898 - val_loss: 0.0465\nEpoch 398/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0887 - val_loss: 0.0461\nEpoch 399/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0877 - val_loss: 0.0456\nEpoch 400/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0866 - val_loss: 0.0451\nEpoch 401/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0856 - val_loss: 0.0446\nEpoch 402/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0846 - val_loss: 0.0442\nEpoch 403/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0836 - val_loss: 0.0437\nEpoch 404/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0827 - val_loss: 0.0433\nEpoch 405/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0817 - val_loss: 0.0430\nEpoch 406/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0808 - val_loss: 0.0425\nEpoch 407/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0799 - val_loss: 0.0422\nEpoch 408/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0790 - val_loss: 0.0418\nEpoch 409/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0782 - val_loss: 0.0415\nEpoch 410/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0773 - val_loss: 0.0411\nEpoch 411/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0765 - val_loss: 0.0408\nEpoch 412/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0756 - val_loss: 0.0405\nEpoch 413/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0749 - val_loss: 0.0403\nEpoch 414/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0740 - val_loss: 0.0400\nEpoch 415/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0733 - val_loss: 0.0397\nEpoch 416/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0725 - val_loss: 0.0394\nEpoch 417/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0718 - val_loss: 0.0392\nEpoch 418/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0710 - val_loss: 0.0389\nEpoch 419/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0703 - val_loss: 0.0387\nEpoch 420/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0696 - val_loss: 0.0385\nEpoch 421/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0689 - val_loss: 0.0383\nEpoch 422/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0682 - val_loss: 0.0381\nEpoch 423/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0676 - val_loss: 0.0379\nEpoch 424/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0669 - val_loss: 0.0377\nEpoch 425/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0663 - val_loss: 0.0376\nEpoch 426/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0657 - val_loss: 0.0374\nEpoch 427/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0650 - val_loss: 0.0372\nEpoch 428/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0644 - val_loss: 0.0371\nEpoch 429/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0638 - val_loss: 0.0370\nEpoch 430/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0633 - val_loss: 0.0368\nEpoch 431/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0627 - val_loss: 0.0367\nEpoch 432/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0621 - val_loss: 0.0366\nEpoch 433/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0616 - val_loss: 0.0365\nEpoch 434/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0611 - val_loss: 0.0364\nEpoch 435/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0605 - val_loss: 0.0363\nEpoch 436/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0600 - val_loss: 0.0362\nEpoch 437/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0595 - val_loss: 0.0361\nEpoch 438/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0590 - val_loss: 0.0360\nEpoch 439/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0585 - val_loss: 0.0359\nEpoch 440/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0580 - val_loss: 0.0358\nEpoch 441/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0357\nEpoch 442/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0571 - val_loss: 0.0357\nEpoch 443/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0567 - val_loss: 0.0356\nEpoch 444/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0562 - val_loss: 0.0355\nEpoch 445/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0558 - val_loss: 0.0355\nEpoch 446/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0554 - val_loss: 0.0354\nEpoch 447/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0550 - val_loss: 0.0354\nEpoch 448/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0546 - val_loss: 0.0353\nEpoch 449/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0542 - val_loss: 0.0353\nEpoch 450/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0538 - val_loss: 0.0353\nEpoch 451/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0534 - val_loss: 0.0352\nEpoch 452/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0530 - val_loss: 0.0352\nEpoch 453/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0527 - val_loss: 0.0352\nEpoch 454/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0523 - val_loss: 0.0351\nEpoch 455/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0519 - val_loss: 0.0351\nEpoch 456/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0516 - val_loss: 0.0351\nEpoch 457/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0513 - val_loss: 0.0351\nEpoch 458/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0509 - val_loss: 0.0350\nEpoch 459/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0506 - val_loss: 0.0350\nEpoch 460/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0503 - val_loss: 0.0350\nEpoch 461/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0500 - val_loss: 0.0350\nEpoch 462/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0497 - val_loss: 0.0350\nEpoch 463/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0494 - val_loss: 0.0350\nEpoch 464/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0491 - val_loss: 0.0350\nEpoch 465/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0488 - val_loss: 0.0350\nEpoch 466/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0485 - val_loss: 0.0350\nEpoch 467/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0482 - val_loss: 0.0350\nEpoch 468/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0479 - val_loss: 0.0350\nEpoch 469/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.0350\nEpoch 470/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0474 - val_loss: 0.0350\nEpoch 471/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0472 - val_loss: 0.0350\nEpoch 472/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0469 - val_loss: 0.0350\nEpoch 473/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0467 - val_loss: 0.0350\nEpoch 474/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0464 - val_loss: 0.0350\nEpoch 475/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0462 - val_loss: 0.0350\nEpoch 476/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0460 - val_loss: 0.0350\nEpoch 477/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0457 - val_loss: 0.0350\nEpoch 478/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0455 - val_loss: 0.0350\nEpoch 479/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0453 - val_loss: 0.0350\nEpoch 480/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0451 - val_loss: 0.0350\nEpoch 481/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0449 - val_loss: 0.0350\nEpoch 482/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.0350\nEpoch 483/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0445 - val_loss: 0.0350\nEpoch 484/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0443 - val_loss: 0.0350\nEpoch 485/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0441 - val_loss: 0.0350\nEpoch 486/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0439 - val_loss: 0.0350\nEpoch 487/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0437 - val_loss: 0.0350\nEpoch 488/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0436 - val_loss: 0.0350\nEpoch 489/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0434 - val_loss: 0.0351\nEpoch 490/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0432 - val_loss: 0.0351\nEpoch 491/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0430 - val_loss: 0.0351\nEpoch 492/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0429 - val_loss: 0.0351\nEpoch 493/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0427 - val_loss: 0.0351\nEpoch 494/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0425 - val_loss: 0.0351\nEpoch 495/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0424 - val_loss: 0.0351\nEpoch 496/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0422 - val_loss: 0.0351\nEpoch 497/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0421 - val_loss: 0.0351\nEpoch 498/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0419 - val_loss: 0.0351\nEpoch 499/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0418 - val_loss: 0.0351\nEpoch 500/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0417 - val_loss: 0.0351\n\n\n<keras.callbacks.History at 0x7f70988babc0>\n\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0 \n\n- 끼워넣을 오브젝트 만들기\n\nfig, ax = plt.subplots() \nax.plot(y,'.',alpha=0.2)\nax.plot(net(X),'--')\n\n\n\n\n\nfig\n\n\n\n\n- 이제 fig 오브젝트를 끼워넣을 코드를 구성하자. (공식홈페이지 참고)\n\nhttps://www.tensorflow.org/tensorboard/image_summaries\n\n\n# 이코드는 한번만 실행\n#from datetime import datetime\nimport io\nlogdir = \"logs\" \n#logdir = \"logs\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ndef plot_to_image(fig): # 사용자가 지정한 그림오브젝트 fig를 넣으면 텐서보드에 끼워넣을수 있는 형태로 출력해주는 함수 \n    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n    # Save the plot to a PNG in memory.\n    buf = io.BytesIO()\n    fig.savefig(buf, format='png')\n    # Closing the figure prevents it from being displayed directly inside\n    # the notebook.\n    plt.close(fig)\n    buf.seek(0)\n    # Convert PNG buffer to TF image\n    image = tf.image.decode_png(buf.getvalue(), channels=4)\n    # Add the batch dimension\n    image = tf.expand_dims(image, 0)\n    return image\n\n\nwith tf.summary.create_file_writer(logdir).as_default():\n    tf.summary.image(\"적합결과시각화\", plot_to_image(fig), step=0)\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0\n\n\n\n\n학습과정분석\n\n텐서보드: 가중치 시각화\n- 에폭별로 가중치가 수렴하는 모양을 보고 싶다.\n3-(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i\\]\n여기에서 \\(t=(t_1,\\dots,t_{1000})=\\) np.linspace(0,5,1000) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.linspace(0,5,1000)\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.1)\n\n\n\n\n- 학습을 진행하면서 가중치가 어떻게 업데이트 되는지 시각화하자.\n\n# y = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\n\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\ncb1= tf.keras.callbacks.TensorBoard(update_freq='epoch',histogram_freq=100)\nnet.fit(X,y,epochs=2000, batch_size=100, validation_split=0.45,callbacks=cb1)\n\nEpoch 1/2000\n6/6 [==============================] - 0s 10ms/step - loss: 10.5761 - val_loss: 13.6319\nEpoch 2/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.5050 - val_loss: 13.5579\nEpoch 3/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.4352 - val_loss: 13.4839\nEpoch 4/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.3643 - val_loss: 13.4108\nEpoch 5/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.2954 - val_loss: 13.3387\nEpoch 6/2000\n6/6 [==============================] - 0s 4ms/step - loss: 10.2266 - val_loss: 13.2667\nEpoch 7/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.1574 - val_loss: 13.1948\nEpoch 8/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.0893 - val_loss: 13.1239\nEpoch 9/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.0220 - val_loss: 13.0539\nEpoch 10/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.9545 - val_loss: 12.9839\nEpoch 11/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.8885 - val_loss: 12.9143\nEpoch 12/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.8218 - val_loss: 12.8446\nEpoch 13/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.7546 - val_loss: 12.7763\nEpoch 14/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.6900 - val_loss: 12.7072\nEpoch 15/2000\n6/6 [==============================] - 0s 4ms/step - loss: 9.6251 - val_loss: 12.6390\nEpoch 16/2000\n6/6 [==============================] - 0s 4ms/step - loss: 9.5592 - val_loss: 12.5710\nEpoch 17/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.4956 - val_loss: 12.5030\nEpoch 18/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.4315 - val_loss: 12.4345\nEpoch 19/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.3670 - val_loss: 12.3678\nEpoch 20/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.3029 - val_loss: 12.3018\nEpoch 21/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.2407 - val_loss: 12.2358\nEpoch 22/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.1777 - val_loss: 12.1697\nEpoch 23/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.1158 - val_loss: 12.1035\nEpoch 24/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.0525 - val_loss: 12.0391\nEpoch 25/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.9924 - val_loss: 11.9733\nEpoch 26/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.9292 - val_loss: 11.9101\nEpoch 27/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.8694 - val_loss: 11.8461\nEpoch 28/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.8079 - val_loss: 11.7831\nEpoch 29/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.7476 - val_loss: 11.7193\nEpoch 30/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.6889 - val_loss: 11.6557\nEpoch 31/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.6272 - val_loss: 11.5937\nEpoch 32/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.5683 - val_loss: 11.5312\nEpoch 33/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.5101 - val_loss: 11.4687\nEpoch 34/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.4503 - val_loss: 11.4082\nEpoch 35/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.3924 - val_loss: 11.3462\nEpoch 36/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.3351 - val_loss: 11.2848\nEpoch 37/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.2768 - val_loss: 11.2241\nEpoch 38/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.2203 - val_loss: 11.1644\nEpoch 39/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.1620 - val_loss: 11.1063\nEpoch 40/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.1069 - val_loss: 11.0478\nEpoch 41/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.0510 - val_loss: 10.9894\nEpoch 42/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.9946 - val_loss: 10.9322\nEpoch 43/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.9396 - val_loss: 10.8745\nEpoch 44/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.8851 - val_loss: 10.8172\nEpoch 45/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.8309 - val_loss: 10.7601\nEpoch 46/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.7758 - val_loss: 10.7029\nEpoch 47/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.7227 - val_loss: 10.6449\nEpoch 48/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.6674 - val_loss: 10.5891\nEpoch 49/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.6149 - val_loss: 10.5327\nEpoch 50/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.5614 - val_loss: 10.4775\nEpoch 51/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.5091 - val_loss: 10.4221\nEpoch 52/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.4576 - val_loss: 10.3669\nEpoch 53/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.4050 - val_loss: 10.3126\nEpoch 54/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.3522 - val_loss: 10.2590\nEpoch 55/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.3023 - val_loss: 10.2050\nEpoch 56/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.2503 - val_loss: 10.1516\nEpoch 57/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.2003 - val_loss: 10.0971\nEpoch 58/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.1490 - val_loss: 10.0436\nEpoch 59/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.0988 - val_loss: 9.9912\nEpoch 60/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.0486 - val_loss: 9.9394\nEpoch 61/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.9988 - val_loss: 9.8870\nEpoch 62/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.9508 - val_loss: 9.8347\nEpoch 63/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.9000 - val_loss: 9.7832\nEpoch 64/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.8507 - val_loss: 9.7309\nEpoch 65/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.8023 - val_loss: 9.6794\nEpoch 66/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.7536 - val_loss: 9.6278\nEpoch 67/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.7058 - val_loss: 9.5765\nEpoch 68/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.6573 - val_loss: 9.5260\nEpoch 69/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.6098 - val_loss: 9.4752\nEpoch 70/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5625 - val_loss: 9.4245\nEpoch 71/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5147 - val_loss: 9.3755\nEpoch 72/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4692 - val_loss: 9.3261\nEpoch 73/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4218 - val_loss: 9.2776\nEpoch 74/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3757 - val_loss: 9.2286\nEpoch 75/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3297 - val_loss: 9.1803\nEpoch 76/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2845 - val_loss: 9.1314\nEpoch 77/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2383 - val_loss: 9.0835\nEpoch 78/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1934 - val_loss: 9.0348\nEpoch 79/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1484 - val_loss: 8.9875\nEpoch 80/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1032 - val_loss: 8.9405\nEpoch 81/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.0604 - val_loss: 8.8922\nEpoch 82/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.0153 - val_loss: 8.8456\nEpoch 83/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9708 - val_loss: 8.7999\nEpoch 84/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9286 - val_loss: 8.7539\nEpoch 85/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8848 - val_loss: 8.7093\nEpoch 86/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8427 - val_loss: 8.6642\nEpoch 87/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.8006 - val_loss: 8.6189\nEpoch 88/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7591 - val_loss: 8.5736\nEpoch 89/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7162 - val_loss: 8.5291\nEpoch 90/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6751 - val_loss: 8.4846\nEpoch 91/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6328 - val_loss: 8.4410\nEpoch 92/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5924 - val_loss: 8.3969\nEpoch 93/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.5510 - val_loss: 8.3528\nEpoch 94/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5104 - val_loss: 8.3092\nEpoch 95/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.4697 - val_loss: 8.2662\nEpoch 96/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.4291 - val_loss: 8.2228\nEpoch 97/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3892 - val_loss: 8.1801\nEpoch 98/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3491 - val_loss: 8.1378\nEpoch 99/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3104 - val_loss: 8.0953\nEpoch 100/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2704 - val_loss: 8.0537\nEpoch 101/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2316 - val_loss: 8.0121\nEpoch 102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1927 - val_loss: 7.9702\nEpoch 103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1547 - val_loss: 7.9291\nEpoch 104/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.1153 - val_loss: 7.8887\nEpoch 105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0776 - val_loss: 7.8474\nEpoch 106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0398 - val_loss: 7.8072\nEpoch 107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0018 - val_loss: 7.7670\nEpoch 108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.9650 - val_loss: 7.7268\nEpoch 109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.9272 - val_loss: 7.6866\nEpoch 110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8898 - val_loss: 7.6479\nEpoch 111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8535 - val_loss: 7.6090\nEpoch 112/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8171 - val_loss: 7.5696\nEpoch 113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7808 - val_loss: 7.5310\nEpoch 114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7459 - val_loss: 7.4919\nEpoch 115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7091 - val_loss: 7.4538\nEpoch 116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6743 - val_loss: 7.4156\nEpoch 117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6391 - val_loss: 7.3770\nEpoch 118/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.6037 - val_loss: 7.3395\nEpoch 119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5691 - val_loss: 7.3022\nEpoch 120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5346 - val_loss: 7.2650\nEpoch 121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5012 - val_loss: 7.2272\nEpoch 122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4662 - val_loss: 7.1898\nEpoch 123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4323 - val_loss: 7.1527\nEpoch 124/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.3984 - val_loss: 7.1157\nEpoch 125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3651 - val_loss: 7.0795\nEpoch 126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3322 - val_loss: 7.0431\nEpoch 127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2990 - val_loss: 7.0075\nEpoch 128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2665 - val_loss: 6.9722\nEpoch 129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2339 - val_loss: 6.9372\nEpoch 130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2018 - val_loss: 6.9018\nEpoch 131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1699 - val_loss: 6.8666\nEpoch 132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1382 - val_loss: 6.8318\nEpoch 133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1064 - val_loss: 6.7976\nEpoch 134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0750 - val_loss: 6.7637\nEpoch 135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0437 - val_loss: 6.7300\nEpoch 136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0137 - val_loss: 6.6962\nEpoch 137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9831 - val_loss: 6.6622\nEpoch 138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9520 - val_loss: 6.6287\nEpoch 139/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.9218 - val_loss: 6.5953\nEpoch 140/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.8921 - val_loss: 6.5620\nEpoch 141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8617 - val_loss: 6.5292\nEpoch 142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8319 - val_loss: 6.4965\nEpoch 143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8020 - val_loss: 6.4639\nEpoch 144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7732 - val_loss: 6.4315\nEpoch 145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7435 - val_loss: 6.3991\nEpoch 146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7143 - val_loss: 6.3669\nEpoch 147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6857 - val_loss: 6.3345\nEpoch 148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6562 - val_loss: 6.3026\nEpoch 149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6286 - val_loss: 6.2697\nEpoch 150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5990 - val_loss: 6.2380\nEpoch 151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5708 - val_loss: 6.2063\nEpoch 152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5426 - val_loss: 6.1751\nEpoch 153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5147 - val_loss: 6.1438\nEpoch 154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4869 - val_loss: 6.1125\nEpoch 155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4598 - val_loss: 6.0812\nEpoch 156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4324 - val_loss: 6.0509\nEpoch 157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4057 - val_loss: 6.0204\nEpoch 158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3787 - val_loss: 5.9906\nEpoch 159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3523 - val_loss: 5.9610\nEpoch 160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3257 - val_loss: 5.9312\nEpoch 161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2997 - val_loss: 5.9018\nEpoch 162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2740 - val_loss: 5.8723\nEpoch 163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2478 - val_loss: 5.8435\nEpoch 164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2222 - val_loss: 5.8143\nEpoch 165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1972 - val_loss: 5.7853\nEpoch 166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1719 - val_loss: 5.7565\nEpoch 167/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.1462 - val_loss: 5.7283\nEpoch 168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1213 - val_loss: 5.7000\nEpoch 169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0966 - val_loss: 5.6719\nEpoch 170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0715 - val_loss: 5.6443\nEpoch 171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0475 - val_loss: 5.6166\nEpoch 172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0236 - val_loss: 5.5888\nEpoch 173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9992 - val_loss: 5.5612\nEpoch 174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9749 - val_loss: 5.5341\nEpoch 175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9514 - val_loss: 5.5070\nEpoch 176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9280 - val_loss: 5.4798\nEpoch 177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9046 - val_loss: 5.4530\nEpoch 178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8809 - val_loss: 5.4262\nEpoch 179/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.8579 - val_loss: 5.3988\nEpoch 180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8347 - val_loss: 5.3721\nEpoch 181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8117 - val_loss: 5.3455\nEpoch 182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7892 - val_loss: 5.3190\nEpoch 183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7668 - val_loss: 5.2926\nEpoch 184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7439 - val_loss: 5.2664\nEpoch 185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7219 - val_loss: 5.2400\nEpoch 186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6998 - val_loss: 5.2143\nEpoch 187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6775 - val_loss: 5.1886\nEpoch 188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6560 - val_loss: 5.1632\nEpoch 189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6344 - val_loss: 5.1375\nEpoch 190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6128 - val_loss: 5.1125\nEpoch 191/2000\n6/6 [==============================] - 0s 2ms/step - loss: 2.5919 - val_loss: 5.0868\nEpoch 192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5703 - val_loss: 5.0617\nEpoch 193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5491 - val_loss: 5.0373\nEpoch 194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5285 - val_loss: 5.0125\nEpoch 195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5071 - val_loss: 4.9879\nEpoch 196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4868 - val_loss: 4.9628\nEpoch 197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4659 - val_loss: 4.9382\nEpoch 198/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.4455 - val_loss: 4.9135\nEpoch 199/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.4248 - val_loss: 4.8893\nEpoch 200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4049 - val_loss: 4.8653\nEpoch 201/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3847 - val_loss: 4.8420\nEpoch 202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3649 - val_loss: 4.8186\nEpoch 203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3451 - val_loss: 4.7950\nEpoch 204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3261 - val_loss: 4.7714\nEpoch 205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3060 - val_loss: 4.7487\nEpoch 206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2871 - val_loss: 4.7257\nEpoch 207/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.2681 - val_loss: 4.7027\nEpoch 208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2489 - val_loss: 4.6803\nEpoch 209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2303 - val_loss: 4.6579\nEpoch 210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2115 - val_loss: 4.6353\nEpoch 211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1931 - val_loss: 4.6133\nEpoch 212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1750 - val_loss: 4.5909\nEpoch 213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1568 - val_loss: 4.5686\nEpoch 214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1387 - val_loss: 4.5462\nEpoch 215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1208 - val_loss: 4.5243\nEpoch 216/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1031 - val_loss: 4.5023\nEpoch 217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0855 - val_loss: 4.4811\nEpoch 218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0677 - val_loss: 4.4596\nEpoch 219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0503 - val_loss: 4.4384\nEpoch 220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0331 - val_loss: 4.4170\nEpoch 221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0158 - val_loss: 4.3960\nEpoch 222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9988 - val_loss: 4.3747\nEpoch 223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9816 - val_loss: 4.3535\nEpoch 224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9646 - val_loss: 4.3322\nEpoch 225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9478 - val_loss: 4.3112\nEpoch 226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9309 - val_loss: 4.2906\nEpoch 227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9147 - val_loss: 4.2702\nEpoch 228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8978 - val_loss: 4.2495\nEpoch 229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8817 - val_loss: 4.2287\nEpoch 230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8654 - val_loss: 4.2084\nEpoch 231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8493 - val_loss: 4.1881\nEpoch 232/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.8336 - val_loss: 4.1677\nEpoch 233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8177 - val_loss: 4.1479\nEpoch 234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8023 - val_loss: 4.1278\nEpoch 235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7865 - val_loss: 4.1084\nEpoch 236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7710 - val_loss: 4.0889\nEpoch 237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7556 - val_loss: 4.0694\nEpoch 238/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.7405 - val_loss: 4.0502\nEpoch 239/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.7254 - val_loss: 4.0309\nEpoch 240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7103 - val_loss: 4.0120\nEpoch 241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6959 - val_loss: 3.9928\nEpoch 242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6808 - val_loss: 3.9739\nEpoch 243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6661 - val_loss: 3.9554\nEpoch 244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6518 - val_loss: 3.9367\nEpoch 245/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.6372 - val_loss: 3.9182\nEpoch 246/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.6232 - val_loss: 3.8994\nEpoch 247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6087 - val_loss: 3.8812\nEpoch 248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5946 - val_loss: 3.8630\nEpoch 249/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5808 - val_loss: 3.8450\nEpoch 250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5669 - val_loss: 3.8269\nEpoch 251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5532 - val_loss: 3.8090\nEpoch 252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5396 - val_loss: 3.7908\nEpoch 253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5259 - val_loss: 3.7723\nEpoch 254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5123 - val_loss: 3.7543\nEpoch 255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4990 - val_loss: 3.7366\nEpoch 256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4855 - val_loss: 3.7190\nEpoch 257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4724 - val_loss: 3.7013\nEpoch 258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4592 - val_loss: 3.6839\nEpoch 259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4462 - val_loss: 3.6666\nEpoch 260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4335 - val_loss: 3.6491\nEpoch 261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4207 - val_loss: 3.6321\nEpoch 262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4078 - val_loss: 3.6151\nEpoch 263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3955 - val_loss: 3.5979\nEpoch 264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3828 - val_loss: 3.5807\nEpoch 265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3705 - val_loss: 3.5637\nEpoch 266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3581 - val_loss: 3.5469\nEpoch 267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3462 - val_loss: 3.5302\nEpoch 268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3340 - val_loss: 3.5136\nEpoch 269/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.3218 - val_loss: 3.4973\nEpoch 270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3100 - val_loss: 3.4807\nEpoch 271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2982 - val_loss: 3.4645\nEpoch 272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2865 - val_loss: 3.4483\nEpoch 273/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.2750 - val_loss: 3.4321\nEpoch 274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2634 - val_loss: 3.4163\nEpoch 275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2521 - val_loss: 3.4002\nEpoch 276/2000\n6/6 [==============================] - 0s 2ms/step - loss: 1.2408 - val_loss: 3.3844\nEpoch 277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2293 - val_loss: 3.3688\nEpoch 278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2182 - val_loss: 3.3532\nEpoch 279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2075 - val_loss: 3.3375\nEpoch 280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1963 - val_loss: 3.3221\nEpoch 281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1854 - val_loss: 3.3067\nEpoch 282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1747 - val_loss: 3.2914\nEpoch 283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1640 - val_loss: 3.2761\nEpoch 284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1534 - val_loss: 3.2609\nEpoch 285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1429 - val_loss: 3.2457\nEpoch 286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1323 - val_loss: 3.2309\nEpoch 287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1218 - val_loss: 3.2161\nEpoch 288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1116 - val_loss: 3.2014\nEpoch 289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1014 - val_loss: 3.1869\nEpoch 290/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.0913 - val_loss: 3.1723\nEpoch 291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0815 - val_loss: 3.1577\nEpoch 292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0712 - val_loss: 3.1432\nEpoch 293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0614 - val_loss: 3.1287\nEpoch 294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0515 - val_loss: 3.1145\nEpoch 295/2000\n6/6 [==============================] - 0s 2ms/step - loss: 1.0418 - val_loss: 3.1001\nEpoch 296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0321 - val_loss: 3.0862\nEpoch 297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0224 - val_loss: 3.0722\nEpoch 298/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.0130 - val_loss: 3.0578\nEpoch 299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0034 - val_loss: 3.0437\nEpoch 300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9940 - val_loss: 3.0300\nEpoch 301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9848 - val_loss: 3.0160\nEpoch 302/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9756 - val_loss: 3.0022\nEpoch 303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9666 - val_loss: 2.9887\nEpoch 304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9575 - val_loss: 2.9754\nEpoch 305/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9486 - val_loss: 2.9620\nEpoch 306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9398 - val_loss: 2.9483\nEpoch 307/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9309 - val_loss: 2.9350\nEpoch 308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9222 - val_loss: 2.9220\nEpoch 309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9136 - val_loss: 2.9090\nEpoch 310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9052 - val_loss: 2.8960\nEpoch 311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8967 - val_loss: 2.8830\nEpoch 312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8884 - val_loss: 2.8703\nEpoch 313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8800 - val_loss: 2.8577\nEpoch 314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8718 - val_loss: 2.8449\nEpoch 315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8636 - val_loss: 2.8324\nEpoch 316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8555 - val_loss: 2.8196\nEpoch 317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8475 - val_loss: 2.8069\nEpoch 318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8396 - val_loss: 2.7941\nEpoch 319/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8316 - val_loss: 2.7815\nEpoch 320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8237 - val_loss: 2.7691\nEpoch 321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8159 - val_loss: 2.7563\nEpoch 322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8081 - val_loss: 2.7440\nEpoch 323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8005 - val_loss: 2.7317\nEpoch 324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7930 - val_loss: 2.7191\nEpoch 325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7853 - val_loss: 2.7069\nEpoch 326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7778 - val_loss: 2.6946\nEpoch 327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7704 - val_loss: 2.6826\nEpoch 328/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.7630 - val_loss: 2.6707\nEpoch 329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7557 - val_loss: 2.6586\nEpoch 330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7484 - val_loss: 2.6466\nEpoch 331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7412 - val_loss: 2.6347\nEpoch 332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7342 - val_loss: 2.6225\nEpoch 333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7270 - val_loss: 2.6108\nEpoch 334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7201 - val_loss: 2.5992\nEpoch 335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7131 - val_loss: 2.5877\nEpoch 336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7063 - val_loss: 2.5761\nEpoch 337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6995 - val_loss: 2.5646\nEpoch 338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 2.5533\nEpoch 339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6861 - val_loss: 2.5420\nEpoch 340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6795 - val_loss: 2.5309\nEpoch 341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6730 - val_loss: 2.5197\nEpoch 342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6665 - val_loss: 2.5087\nEpoch 343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6600 - val_loss: 2.4978\nEpoch 344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6537 - val_loss: 2.4868\nEpoch 345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6473 - val_loss: 2.4760\nEpoch 346/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.6411 - val_loss: 2.4649\nEpoch 347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6348 - val_loss: 2.4543\nEpoch 348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6285 - val_loss: 2.4438\nEpoch 349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6225 - val_loss: 2.4331\nEpoch 350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6164 - val_loss: 2.4223\nEpoch 351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6103 - val_loss: 2.4117\nEpoch 352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6044 - val_loss: 2.4009\nEpoch 353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5984 - val_loss: 2.3903\nEpoch 354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5925 - val_loss: 2.3800\nEpoch 355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5867 - val_loss: 2.3697\nEpoch 356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5812 - val_loss: 2.3593\nEpoch 357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5754 - val_loss: 2.3490\nEpoch 358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5698 - val_loss: 2.3388\nEpoch 359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5642 - val_loss: 2.3287\nEpoch 360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5587 - val_loss: 2.3189\nEpoch 361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5533 - val_loss: 2.3090\nEpoch 362/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5478 - val_loss: 2.2990\nEpoch 363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5424 - val_loss: 2.2892\nEpoch 364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5372 - val_loss: 2.2795\nEpoch 365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5320 - val_loss: 2.2696\nEpoch 366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5268 - val_loss: 2.2600\nEpoch 367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5215 - val_loss: 2.2505\nEpoch 368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5165 - val_loss: 2.2408\nEpoch 369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5113 - val_loss: 2.2315\nEpoch 370/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5064 - val_loss: 2.2219\nEpoch 371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5013 - val_loss: 2.2122\nEpoch 372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4964 - val_loss: 2.2026\nEpoch 373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4914 - val_loss: 2.1931\nEpoch 374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4866 - val_loss: 2.1835\nEpoch 375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4818 - val_loss: 2.1744\nEpoch 376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4772 - val_loss: 2.1650\nEpoch 377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4723 - val_loss: 2.1559\nEpoch 378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4677 - val_loss: 2.1469\nEpoch 379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4631 - val_loss: 2.1378\nEpoch 380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4585 - val_loss: 2.1288\nEpoch 381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4540 - val_loss: 2.1199\nEpoch 382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4496 - val_loss: 2.1110\nEpoch 383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4452 - val_loss: 2.1021\nEpoch 384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4408 - val_loss: 2.0934\nEpoch 385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4365 - val_loss: 2.0848\nEpoch 386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4322 - val_loss: 2.0760\nEpoch 387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4278 - val_loss: 2.0677\nEpoch 388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4237 - val_loss: 2.0591\nEpoch 389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4196 - val_loss: 2.0505\nEpoch 390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4154 - val_loss: 2.0419\nEpoch 391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4113 - val_loss: 2.0335\nEpoch 392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4072 - val_loss: 2.0253\nEpoch 393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4031 - val_loss: 2.0169\nEpoch 394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3992 - val_loss: 2.0086\nEpoch 395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3952 - val_loss: 2.0003\nEpoch 396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3913 - val_loss: 1.9921\nEpoch 397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3875 - val_loss: 1.9839\nEpoch 398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3837 - val_loss: 1.9756\nEpoch 399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3798 - val_loss: 1.9674\nEpoch 400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3760 - val_loss: 1.9594\nEpoch 401/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3723 - val_loss: 1.9515\nEpoch 402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3687 - val_loss: 1.9434\nEpoch 403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3650 - val_loss: 1.9356\nEpoch 404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3613 - val_loss: 1.9277\nEpoch 405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3578 - val_loss: 1.9199\nEpoch 406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3543 - val_loss: 1.9120\nEpoch 407/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3508 - val_loss: 1.9041\nEpoch 408/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3474 - val_loss: 1.8962\nEpoch 409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3438 - val_loss: 1.8886\nEpoch 410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3405 - val_loss: 1.8808\nEpoch 411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3370 - val_loss: 1.8730\nEpoch 412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3337 - val_loss: 1.8654\nEpoch 413/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3304 - val_loss: 1.8578\nEpoch 414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3271 - val_loss: 1.8502\nEpoch 415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3238 - val_loss: 1.8428\nEpoch 416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3207 - val_loss: 1.8351\nEpoch 417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3175 - val_loss: 1.8276\nEpoch 418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3144 - val_loss: 1.8201\nEpoch 419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3113 - val_loss: 1.8128\nEpoch 420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3082 - val_loss: 1.8056\nEpoch 421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3052 - val_loss: 1.7985\nEpoch 422/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3022 - val_loss: 1.7914\nEpoch 423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2992 - val_loss: 1.7842\nEpoch 424/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2963 - val_loss: 1.7773\nEpoch 425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2935 - val_loss: 1.7702\nEpoch 426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2906 - val_loss: 1.7632\nEpoch 427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2877 - val_loss: 1.7563\nEpoch 428/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2850 - val_loss: 1.7494\nEpoch 429/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2822 - val_loss: 1.7424\nEpoch 430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2794 - val_loss: 1.7355\nEpoch 431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2767 - val_loss: 1.7285\nEpoch 432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2740 - val_loss: 1.7217\nEpoch 433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2713 - val_loss: 1.7148\nEpoch 434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2687 - val_loss: 1.7078\nEpoch 435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2660 - val_loss: 1.7011\nEpoch 436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2634 - val_loss: 1.6943\nEpoch 437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2609 - val_loss: 1.6876\nEpoch 438/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2583 - val_loss: 1.6811\nEpoch 439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2559 - val_loss: 1.6744\nEpoch 440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2534 - val_loss: 1.6676\nEpoch 441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2509 - val_loss: 1.6609\nEpoch 442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2485 - val_loss: 1.6541\nEpoch 443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2461 - val_loss: 1.6475\nEpoch 444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2437 - val_loss: 1.6411\nEpoch 445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2414 - val_loss: 1.6346\nEpoch 446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2391 - val_loss: 1.6283\nEpoch 447/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2367 - val_loss: 1.6220\nEpoch 448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2345 - val_loss: 1.6157\nEpoch 449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2322 - val_loss: 1.6094\nEpoch 450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2300 - val_loss: 1.6031\nEpoch 451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2278 - val_loss: 1.5968\nEpoch 452/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2256 - val_loss: 1.5905\nEpoch 453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2234 - val_loss: 1.5844\nEpoch 454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2213 - val_loss: 1.5781\nEpoch 455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2192 - val_loss: 1.5718\nEpoch 456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2171 - val_loss: 1.5656\nEpoch 457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2151 - val_loss: 1.5595\nEpoch 458/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2130 - val_loss: 1.5536\nEpoch 459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2111 - val_loss: 1.5476\nEpoch 460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2091 - val_loss: 1.5415\nEpoch 461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2071 - val_loss: 1.5357\nEpoch 462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2052 - val_loss: 1.5299\nEpoch 463/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2033 - val_loss: 1.5241\nEpoch 464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2014 - val_loss: 1.5181\nEpoch 465/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1995 - val_loss: 1.5122\nEpoch 466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1976 - val_loss: 1.5063\nEpoch 467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1958 - val_loss: 1.5006\nEpoch 468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1940 - val_loss: 1.4947\nEpoch 469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1922 - val_loss: 1.4890\nEpoch 470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1904 - val_loss: 1.4832\nEpoch 471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1887 - val_loss: 1.4777\nEpoch 472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1869 - val_loss: 1.4722\nEpoch 473/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1852 - val_loss: 1.4668\nEpoch 474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1835 - val_loss: 1.4614\nEpoch 475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1819 - val_loss: 1.4560\nEpoch 476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1803 - val_loss: 1.4504\nEpoch 477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1787 - val_loss: 1.4450\nEpoch 478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1771 - val_loss: 1.4397\nEpoch 479/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1755 - val_loss: 1.4342\nEpoch 480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1739 - val_loss: 1.4290\nEpoch 481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1724 - val_loss: 1.4235\nEpoch 482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1709 - val_loss: 1.4181\nEpoch 483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1693 - val_loss: 1.4129\nEpoch 484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1679 - val_loss: 1.4075\nEpoch 485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1664 - val_loss: 1.4023\nEpoch 486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1649 - val_loss: 1.3974\nEpoch 487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1635 - val_loss: 1.3922\nEpoch 488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1621 - val_loss: 1.3870\nEpoch 489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1606 - val_loss: 1.3818\nEpoch 490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 1.3766\nEpoch 491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1579 - val_loss: 1.3714\nEpoch 492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1565 - val_loss: 1.3665\nEpoch 493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1552 - val_loss: 1.3615\nEpoch 494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1539 - val_loss: 1.3565\nEpoch 495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1526 - val_loss: 1.3517\nEpoch 496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1513 - val_loss: 1.3466\nEpoch 497/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1500 - val_loss: 1.3417\nEpoch 498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1488 - val_loss: 1.3366\nEpoch 499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1475 - val_loss: 1.3318\nEpoch 500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1463 - val_loss: 1.3271\nEpoch 501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1451 - val_loss: 1.3222\nEpoch 502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1439 - val_loss: 1.3173\nEpoch 503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1427 - val_loss: 1.3124\nEpoch 504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1416 - val_loss: 1.3075\nEpoch 505/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1404 - val_loss: 1.3027\nEpoch 506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1392 - val_loss: 1.2979\nEpoch 507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1381 - val_loss: 1.2933\nEpoch 508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1370 - val_loss: 1.2886\nEpoch 509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1360 - val_loss: 1.2839\nEpoch 510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1349 - val_loss: 1.2792\nEpoch 511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1338 - val_loss: 1.2747\nEpoch 512/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1327 - val_loss: 1.2702\nEpoch 513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1317 - val_loss: 1.2654\nEpoch 514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1307 - val_loss: 1.2608\nEpoch 515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1297 - val_loss: 1.2564\nEpoch 516/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1287 - val_loss: 1.2521\nEpoch 517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1277 - val_loss: 1.2476\nEpoch 518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1267 - val_loss: 1.2432\nEpoch 519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1258 - val_loss: 1.2386\nEpoch 520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 1.2342\nEpoch 521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1238 - val_loss: 1.2300\nEpoch 522/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1229 - val_loss: 1.2255\nEpoch 523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1220 - val_loss: 1.2212\nEpoch 524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1211 - val_loss: 1.2168\nEpoch 525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1202 - val_loss: 1.2126\nEpoch 526/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1193 - val_loss: 1.2082\nEpoch 527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1185 - val_loss: 1.2039\nEpoch 528/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1176 - val_loss: 1.1997\nEpoch 529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1167 - val_loss: 1.1954\nEpoch 530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1159 - val_loss: 1.1911\nEpoch 531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1151 - val_loss: 1.1870\nEpoch 532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1142 - val_loss: 1.1831\nEpoch 533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1134 - val_loss: 1.1791\nEpoch 534/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1127 - val_loss: 1.1750\nEpoch 535/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1119 - val_loss: 1.1710\nEpoch 536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1111 - val_loss: 1.1669\nEpoch 537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1103 - val_loss: 1.1629\nEpoch 538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1096 - val_loss: 1.1590\nEpoch 539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1089 - val_loss: 1.1551\nEpoch 540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1081 - val_loss: 1.1512\nEpoch 541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1074 - val_loss: 1.1471\nEpoch 542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1067 - val_loss: 1.1431\nEpoch 543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1060 - val_loss: 1.1388\nEpoch 544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1053 - val_loss: 1.1347\nEpoch 545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1046 - val_loss: 1.1307\nEpoch 546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1039 - val_loss: 1.1267\nEpoch 547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1032 - val_loss: 1.1226\nEpoch 548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1026 - val_loss: 1.1187\nEpoch 549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1020 - val_loss: 1.1147\nEpoch 550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 1.1107\nEpoch 551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1006 - val_loss: 1.1068\nEpoch 552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1000 - val_loss: 1.1028\nEpoch 553/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0994 - val_loss: 1.0990\nEpoch 554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0988 - val_loss: 1.0952\nEpoch 555/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0982 - val_loss: 1.0914\nEpoch 556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0976 - val_loss: 1.0878\nEpoch 557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 1.0838\nEpoch 558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 1.0799\nEpoch 559/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0958 - val_loss: 1.0761\nEpoch 560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0953 - val_loss: 1.0723\nEpoch 561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0947 - val_loss: 1.0685\nEpoch 562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0942 - val_loss: 1.0649\nEpoch 563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0936 - val_loss: 1.0613\nEpoch 564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0931 - val_loss: 1.0577\nEpoch 565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0926 - val_loss: 1.0539\nEpoch 566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0921 - val_loss: 1.0501\nEpoch 567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0915 - val_loss: 1.0466\nEpoch 568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0910 - val_loss: 1.0428\nEpoch 569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0905 - val_loss: 1.0394\nEpoch 570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 1.0356\nEpoch 571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0895 - val_loss: 1.0320\nEpoch 572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0891 - val_loss: 1.0284\nEpoch 573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0886 - val_loss: 1.0249\nEpoch 574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0881 - val_loss: 1.0212\nEpoch 575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0877 - val_loss: 1.0176\nEpoch 576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0872 - val_loss: 1.0141\nEpoch 577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0867 - val_loss: 1.0107\nEpoch 578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0863 - val_loss: 1.0072\nEpoch 579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0859 - val_loss: 1.0037\nEpoch 580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0854 - val_loss: 1.0002\nEpoch 581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0850 - val_loss: 0.9969\nEpoch 582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0846 - val_loss: 0.9933\nEpoch 583/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0842 - val_loss: 0.9898\nEpoch 584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0838 - val_loss: 0.9865\nEpoch 585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0834 - val_loss: 0.9831\nEpoch 586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0830 - val_loss: 0.9796\nEpoch 587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0826 - val_loss: 0.9761\nEpoch 588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0822 - val_loss: 0.9726\nEpoch 589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0818 - val_loss: 0.9689\nEpoch 590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0814 - val_loss: 0.9655\nEpoch 591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0811 - val_loss: 0.9621\nEpoch 592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0807 - val_loss: 0.9589\nEpoch 593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0803 - val_loss: 0.9555\nEpoch 594/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0800 - val_loss: 0.9522\nEpoch 595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0796 - val_loss: 0.9488\nEpoch 596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0792 - val_loss: 0.9454\nEpoch 597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0789 - val_loss: 0.9419\nEpoch 598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0786 - val_loss: 0.9388\nEpoch 599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 0.9355\nEpoch 600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0779 - val_loss: 0.9324\nEpoch 601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0776 - val_loss: 0.9292\nEpoch 602/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0773 - val_loss: 0.9261\nEpoch 603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0770 - val_loss: 0.9230\nEpoch 604/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0767 - val_loss: 0.9196\nEpoch 605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 0.9165\nEpoch 606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0760 - val_loss: 0.9135\nEpoch 607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0758 - val_loss: 0.9102\nEpoch 608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0755 - val_loss: 0.9070\nEpoch 609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0752 - val_loss: 0.9039\nEpoch 610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0749 - val_loss: 0.9008\nEpoch 611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0746 - val_loss: 0.8976\nEpoch 612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0743 - val_loss: 0.8945\nEpoch 613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0740 - val_loss: 0.8914\nEpoch 614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0737 - val_loss: 0.8883\nEpoch 615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0735 - val_loss: 0.8852\nEpoch 616/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0732 - val_loss: 0.8820\nEpoch 617/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0729 - val_loss: 0.8789\nEpoch 618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0727 - val_loss: 0.8759\nEpoch 619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0724 - val_loss: 0.8729\nEpoch 620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0722 - val_loss: 0.8700\nEpoch 621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.8669\nEpoch 622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0717 - val_loss: 0.8641\nEpoch 623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.8610\nEpoch 624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0712 - val_loss: 0.8578\nEpoch 625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0709 - val_loss: 0.8547\nEpoch 626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0707 - val_loss: 0.8518\nEpoch 627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0705 - val_loss: 0.8489\nEpoch 628/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0702 - val_loss: 0.8459\nEpoch 629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.8430\nEpoch 630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0698 - val_loss: 0.8400\nEpoch 631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0696 - val_loss: 0.8370\nEpoch 632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0693 - val_loss: 0.8339\nEpoch 633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0691 - val_loss: 0.8309\nEpoch 634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0689 - val_loss: 0.8279\nEpoch 635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0687 - val_loss: 0.8249\nEpoch 636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0685 - val_loss: 0.8221\nEpoch 637/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.8193\nEpoch 638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8163\nEpoch 639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0679 - val_loss: 0.8137\nEpoch 640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0677 - val_loss: 0.8107\nEpoch 641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0675 - val_loss: 0.8078\nEpoch 642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0673 - val_loss: 0.8048\nEpoch 643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0671 - val_loss: 0.8015\nEpoch 644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0669 - val_loss: 0.7986\nEpoch 645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0667 - val_loss: 0.7956\nEpoch 646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0665 - val_loss: 0.7927\nEpoch 647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0663 - val_loss: 0.7898\nEpoch 648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0662 - val_loss: 0.7870\nEpoch 649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0660 - val_loss: 0.7843\nEpoch 650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.7816\nEpoch 651/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.7787\nEpoch 652/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0654 - val_loss: 0.7759\nEpoch 653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.7731\nEpoch 654/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0651 - val_loss: 0.7703\nEpoch 655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0649 - val_loss: 0.7676\nEpoch 656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0647 - val_loss: 0.7649\nEpoch 657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0646 - val_loss: 0.7621\nEpoch 658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0644 - val_loss: 0.7593\nEpoch 659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0642 - val_loss: 0.7565\nEpoch 660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.7537\nEpoch 661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7511\nEpoch 662/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0638 - val_loss: 0.7482\nEpoch 663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.7454\nEpoch 664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0635 - val_loss: 0.7426\nEpoch 665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0633 - val_loss: 0.7400\nEpoch 666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0631 - val_loss: 0.7373\nEpoch 667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0630 - val_loss: 0.7345\nEpoch 668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0628 - val_loss: 0.7320\nEpoch 669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.7292\nEpoch 670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0625 - val_loss: 0.7266\nEpoch 671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0624 - val_loss: 0.7240\nEpoch 672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0623 - val_loss: 0.7214\nEpoch 673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0621 - val_loss: 0.7188\nEpoch 674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0620 - val_loss: 0.7162\nEpoch 675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.7136\nEpoch 676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0617 - val_loss: 0.7108\nEpoch 677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0615 - val_loss: 0.7081\nEpoch 678/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0614 - val_loss: 0.7053\nEpoch 679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0613 - val_loss: 0.7027\nEpoch 680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0611 - val_loss: 0.7002\nEpoch 681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.6976\nEpoch 682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.6952\nEpoch 683/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0607 - val_loss: 0.6927\nEpoch 684/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0606 - val_loss: 0.6903\nEpoch 685/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0605 - val_loss: 0.6877\nEpoch 686/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0603 - val_loss: 0.6851\nEpoch 687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 0.6827\nEpoch 688/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0601 - val_loss: 0.6801\nEpoch 689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.6775\nEpoch 690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0598 - val_loss: 0.6750\nEpoch 691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.6725\nEpoch 692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.6700\nEpoch 693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0595 - val_loss: 0.6674\nEpoch 694/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0593 - val_loss: 0.6648\nEpoch 695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.6624\nEpoch 696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.6600\nEpoch 697/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6576\nEpoch 698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0589 - val_loss: 0.6551\nEpoch 699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.6525\nEpoch 700/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.6500\nEpoch 701/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.6474\nEpoch 702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 0.6449\nEpoch 703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.6424\nEpoch 704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0582 - val_loss: 0.6398\nEpoch 705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0581 - val_loss: 0.6376\nEpoch 706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0580 - val_loss: 0.6352\nEpoch 707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.6328\nEpoch 708/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.6303\nEpoch 709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0576 - val_loss: 0.6278\nEpoch 710/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0575 - val_loss: 0.6253\nEpoch 711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0574 - val_loss: 0.6231\nEpoch 712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6207\nEpoch 713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0572 - val_loss: 0.6183\nEpoch 714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0571 - val_loss: 0.6157\nEpoch 715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0570 - val_loss: 0.6132\nEpoch 716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.6107\nEpoch 717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0568 - val_loss: 0.6083\nEpoch 718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0566 - val_loss: 0.6059\nEpoch 719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0566 - val_loss: 0.6035\nEpoch 720/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0564 - val_loss: 0.6012\nEpoch 721/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0563 - val_loss: 0.5989\nEpoch 722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0562 - val_loss: 0.5964\nEpoch 723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0561 - val_loss: 0.5941\nEpoch 724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0560 - val_loss: 0.5915\nEpoch 725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0559 - val_loss: 0.5893\nEpoch 726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0558 - val_loss: 0.5869\nEpoch 727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5844\nEpoch 728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0556 - val_loss: 0.5821\nEpoch 729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.5799\nEpoch 730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0554 - val_loss: 0.5777\nEpoch 731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0553 - val_loss: 0.5753\nEpoch 732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0552 - val_loss: 0.5728\nEpoch 733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0551 - val_loss: 0.5701\nEpoch 734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.5676\nEpoch 735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0549 - val_loss: 0.5653\nEpoch 736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0548 - val_loss: 0.5630\nEpoch 737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0547 - val_loss: 0.5606\nEpoch 738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0546 - val_loss: 0.5584\nEpoch 739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0545 - val_loss: 0.5560\nEpoch 740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0544 - val_loss: 0.5537\nEpoch 741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0543 - val_loss: 0.5514\nEpoch 742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5491\nEpoch 743/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0541 - val_loss: 0.5468\nEpoch 744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5446\nEpoch 745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5421\nEpoch 746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0539 - val_loss: 0.5399\nEpoch 747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0538 - val_loss: 0.5374\nEpoch 748/2000\n6/6 [==============================] - 0s 6ms/step - loss: 0.0537 - val_loss: 0.5350\nEpoch 749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0536 - val_loss: 0.5327\nEpoch 750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.5305\nEpoch 751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0534 - val_loss: 0.5284\nEpoch 752/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0533 - val_loss: 0.5262\nEpoch 753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0532 - val_loss: 0.5242\nEpoch 754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.5220\nEpoch 755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0530 - val_loss: 0.5198\nEpoch 756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5177\nEpoch 757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5156\nEpoch 758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0528 - val_loss: 0.5135\nEpoch 759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.5112\nEpoch 760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5089\nEpoch 761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0525 - val_loss: 0.5067\nEpoch 762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5046\nEpoch 763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0523 - val_loss: 0.5024\nEpoch 764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.5002\nEpoch 765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.4980\nEpoch 766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0520 - val_loss: 0.4958\nEpoch 767/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0520 - val_loss: 0.4936\nEpoch 768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.4915\nEpoch 769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0518 - val_loss: 0.4895\nEpoch 770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4875\nEpoch 771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0516 - val_loss: 0.4855\nEpoch 772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.4833\nEpoch 773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4813\nEpoch 774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4791\nEpoch 775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0513 - val_loss: 0.4772\nEpoch 776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4752\nEpoch 777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0511 - val_loss: 0.4731\nEpoch 778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.4709\nEpoch 779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4689\nEpoch 780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4670\nEpoch 781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0508 - val_loss: 0.4652\nEpoch 782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0507 - val_loss: 0.4631\nEpoch 783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.4610\nEpoch 784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0505 - val_loss: 0.4586\nEpoch 785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4565\nEpoch 786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4545\nEpoch 787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.4524\nEpoch 788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0502 - val_loss: 0.4504\nEpoch 789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4484\nEpoch 790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4464\nEpoch 791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4442\nEpoch 792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.4421\nEpoch 793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.4401\nEpoch 794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0497 - val_loss: 0.4380\nEpoch 795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0496 - val_loss: 0.4359\nEpoch 796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0495 - val_loss: 0.4339\nEpoch 797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0495 - val_loss: 0.4318\nEpoch 798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0494 - val_loss: 0.4299\nEpoch 799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.4280\nEpoch 800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4262\nEpoch 801/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.4242\nEpoch 802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.4221\nEpoch 803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0490 - val_loss: 0.4202\nEpoch 804/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0489 - val_loss: 0.4183\nEpoch 805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4164\nEpoch 806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4144\nEpoch 807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0487 - val_loss: 0.4125\nEpoch 808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4106\nEpoch 809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.4085\nEpoch 810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4066\nEpoch 811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4047\nEpoch 812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0483 - val_loss: 0.4027\nEpoch 813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.4008\nEpoch 814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.3989\nEpoch 815/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.3970\nEpoch 816/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0480 - val_loss: 0.3949\nEpoch 817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3930\nEpoch 818/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0478 - val_loss: 0.3912\nEpoch 819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0478 - val_loss: 0.3894\nEpoch 820/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0477 - val_loss: 0.3874\nEpoch 821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3857\nEpoch 822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3841\nEpoch 823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3824\nEpoch 824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.3807\nEpoch 825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0473 - val_loss: 0.3787\nEpoch 826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3770\nEpoch 827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3750\nEpoch 828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3731\nEpoch 829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3715\nEpoch 830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3696\nEpoch 831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3677\nEpoch 832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0468 - val_loss: 0.3660\nEpoch 833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.3641\nEpoch 834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3624\nEpoch 835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3604\nEpoch 836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.3585\nEpoch 837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3564\nEpoch 838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3546\nEpoch 839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3528\nEpoch 840/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0462 - val_loss: 0.3510\nEpoch 841/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0461 - val_loss: 0.3492\nEpoch 842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0461 - val_loss: 0.3473\nEpoch 843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3457\nEpoch 844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3439\nEpoch 845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3422\nEpoch 846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0458 - val_loss: 0.3405\nEpoch 847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3388\nEpoch 848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3372\nEpoch 849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.3356\nEpoch 850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0455 - val_loss: 0.3339\nEpoch 851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0455 - val_loss: 0.3322\nEpoch 852/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0454 - val_loss: 0.3303\nEpoch 853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3286\nEpoch 854/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3270\nEpoch 855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3253\nEpoch 856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3237\nEpoch 857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3220\nEpoch 858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.3204\nEpoch 859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3187\nEpoch 860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3169\nEpoch 861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.3150\nEpoch 862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.3133\nEpoch 863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.3115\nEpoch 864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3098\nEpoch 865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3080\nEpoch 866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3063\nEpoch 867/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0444 - val_loss: 0.3048\nEpoch 868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.3032\nEpoch 869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.3017\nEpoch 870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.3002\nEpoch 871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2985\nEpoch 872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2970\nEpoch 873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2955\nEpoch 874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2940\nEpoch 875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.2925\nEpoch 876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2908\nEpoch 877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2892\nEpoch 878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.2877\nEpoch 879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 0.2861\nEpoch 880/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0436 - val_loss: 0.2846\nEpoch 881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2829\nEpoch 882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2812\nEpoch 883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2797\nEpoch 884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.2782\nEpoch 885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.2769\nEpoch 886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2753\nEpoch 887/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0432 - val_loss: 0.2739\nEpoch 888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2724\nEpoch 889/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2710\nEpoch 890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2694\nEpoch 891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2679\nEpoch 892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2663\nEpoch 893/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0428 - val_loss: 0.2647\nEpoch 894/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2633\nEpoch 895/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2617\nEpoch 896/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0426 - val_loss: 0.2602\nEpoch 897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2587\nEpoch 898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 0.2571\nEpoch 899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2558\nEpoch 900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2544\nEpoch 901/2000\n6/6 [==============================] - 0s 5ms/step - loss: 0.0423 - val_loss: 0.2529\nEpoch 902/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0423 - val_loss: 0.2514\nEpoch 903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2499\nEpoch 904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2484\nEpoch 905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2470\nEpoch 906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2456\nEpoch 907/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0420 - val_loss: 0.2441\nEpoch 908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2425\nEpoch 909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2410\nEpoch 910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2395\nEpoch 911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2381\nEpoch 912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2366\nEpoch 913/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2355\nEpoch 914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2341\nEpoch 915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2328\nEpoch 916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2314\nEpoch 917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2301\nEpoch 918/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0414 - val_loss: 0.2288\nEpoch 919/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0413 - val_loss: 0.2274\nEpoch 920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2262\nEpoch 921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2251\nEpoch 922/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0412 - val_loss: 0.2237\nEpoch 923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2223\nEpoch 924/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0411 - val_loss: 0.2210\nEpoch 925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2197\nEpoch 926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2184\nEpoch 927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2171\nEpoch 928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2157\nEpoch 929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2144\nEpoch 930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2131\nEpoch 931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2119\nEpoch 932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2106\nEpoch 933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2094\nEpoch 934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2080\nEpoch 935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2067\nEpoch 936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2056\nEpoch 937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2044\nEpoch 938/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0404 - val_loss: 0.2032\nEpoch 939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2021\nEpoch 940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2008\nEpoch 941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1997\nEpoch 942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1985\nEpoch 943/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0401 - val_loss: 0.1976\nEpoch 944/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0401 - val_loss: 0.1964\nEpoch 945/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0400 - val_loss: 0.1952\nEpoch 946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1941\nEpoch 947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1928\nEpoch 948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1916\nEpoch 949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1903\nEpoch 950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1892\nEpoch 951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1882\nEpoch 952/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0397 - val_loss: 0.1870\nEpoch 953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1859\nEpoch 954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1847\nEpoch 955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1835\nEpoch 956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1824\nEpoch 957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1814\nEpoch 958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1803\nEpoch 959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1794\nEpoch 960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1783\nEpoch 961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1771\nEpoch 962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1760\nEpoch 963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1749\nEpoch 964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1738\nEpoch 965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1726\nEpoch 966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1716\nEpoch 967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1705\nEpoch 968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1694\nEpoch 969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1685\nEpoch 970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1674\nEpoch 971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1662\nEpoch 972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1650\nEpoch 973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1639\nEpoch 974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1628\nEpoch 975/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1618\nEpoch 976/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0387 - val_loss: 0.1609\nEpoch 977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1600\nEpoch 978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1590\nEpoch 979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1579\nEpoch 980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1569\nEpoch 981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1560\nEpoch 982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1552\nEpoch 983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1543\nEpoch 984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1533\nEpoch 985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1523\nEpoch 986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1514\nEpoch 987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1503\nEpoch 988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1493\nEpoch 989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1484\nEpoch 990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1475\nEpoch 991/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0381 - val_loss: 0.1465\nEpoch 992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1457\nEpoch 993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1449\nEpoch 994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1439\nEpoch 995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1430\nEpoch 996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1420\nEpoch 997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1411\nEpoch 998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1401\nEpoch 999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1392\nEpoch 1000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1382\nEpoch 1001/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0378 - val_loss: 0.1374\nEpoch 1002/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1364\nEpoch 1003/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1357\nEpoch 1004/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1349\nEpoch 1005/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1341\nEpoch 1006/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1333\nEpoch 1007/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1325\nEpoch 1008/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1318\nEpoch 1009/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1311\nEpoch 1010/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1301\nEpoch 1011/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1293\nEpoch 1012/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1285\nEpoch 1013/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1277\nEpoch 1014/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1269\nEpoch 1015/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1262\nEpoch 1016/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1254\nEpoch 1017/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0373 - val_loss: 0.1246\nEpoch 1018/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1239\nEpoch 1019/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1231\nEpoch 1020/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1223\nEpoch 1021/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1216\nEpoch 1022/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1209\nEpoch 1023/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1202\nEpoch 1024/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1194\nEpoch 1025/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0371 - val_loss: 0.1187\nEpoch 1026/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1179\nEpoch 1027/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0370 - val_loss: 0.1170\nEpoch 1028/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1162\nEpoch 1029/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1155\nEpoch 1030/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1148\nEpoch 1031/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0369 - val_loss: 0.1141\nEpoch 1032/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0369 - val_loss: 0.1135\nEpoch 1033/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0368 - val_loss: 0.1128\nEpoch 1034/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1120\nEpoch 1035/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1114\nEpoch 1036/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1107\nEpoch 1037/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1102\nEpoch 1038/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1096\nEpoch 1039/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1090\nEpoch 1040/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1084\nEpoch 1041/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0366 - val_loss: 0.1077\nEpoch 1042/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1071\nEpoch 1043/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1065\nEpoch 1044/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1059\nEpoch 1045/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1052\nEpoch 1046/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0365 - val_loss: 0.1045\nEpoch 1047/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1039\nEpoch 1048/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1031\nEpoch 1049/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1025\nEpoch 1050/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1018\nEpoch 1051/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1013\nEpoch 1052/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1006\nEpoch 1053/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0999\nEpoch 1054/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0992\nEpoch 1055/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0985\nEpoch 1056/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0980\nEpoch 1057/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0362 - val_loss: 0.0974\nEpoch 1058/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0968\nEpoch 1059/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0962\nEpoch 1060/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0957\nEpoch 1061/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0950\nEpoch 1062/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0945\nEpoch 1063/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0939\nEpoch 1064/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0933\nEpoch 1065/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0926\nEpoch 1066/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0921\nEpoch 1067/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0916\nEpoch 1068/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0911\nEpoch 1069/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0905\nEpoch 1070/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0900\nEpoch 1071/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0895\nEpoch 1072/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0890\nEpoch 1073/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0885\nEpoch 1074/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0880\nEpoch 1075/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0876\nEpoch 1076/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0871\nEpoch 1077/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0865\nEpoch 1078/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0860\nEpoch 1079/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0857\nEpoch 1080/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.0852\nEpoch 1081/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0846\nEpoch 1082/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0842\nEpoch 1083/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0836\nEpoch 1084/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0830\nEpoch 1085/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0825\nEpoch 1086/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0821\nEpoch 1087/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0816\nEpoch 1088/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0811\nEpoch 1089/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0806\nEpoch 1090/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0801\nEpoch 1091/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0796\nEpoch 1092/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0792\nEpoch 1093/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0787\nEpoch 1094/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0782\nEpoch 1095/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0778\nEpoch 1096/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0774\nEpoch 1097/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0770\nEpoch 1098/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0766\nEpoch 1099/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0762\nEpoch 1100/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0759\nEpoch 1101/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0755\nEpoch 1102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0751\nEpoch 1103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0747\nEpoch 1104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0743\nEpoch 1105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0739\nEpoch 1106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0735\nEpoch 1107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0731\nEpoch 1108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0727\nEpoch 1109/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0724\nEpoch 1110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0721\nEpoch 1111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0717\nEpoch 1112/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0714\nEpoch 1113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0710\nEpoch 1114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0706\nEpoch 1115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0702\nEpoch 1116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0698\nEpoch 1117/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0352 - val_loss: 0.0694\nEpoch 1118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0690\nEpoch 1119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0686\nEpoch 1120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0683\nEpoch 1121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0679\nEpoch 1122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0675\nEpoch 1123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0673\nEpoch 1124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0669\nEpoch 1125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0664\nEpoch 1126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0660\nEpoch 1127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0656\nEpoch 1128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0652\nEpoch 1129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0649\nEpoch 1130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0646\nEpoch 1131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0643\nEpoch 1132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0640\nEpoch 1133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0637\nEpoch 1134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0634\nEpoch 1135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0631\nEpoch 1136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0628\nEpoch 1137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0626\nEpoch 1138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0623\nEpoch 1139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0620\nEpoch 1140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0617\nEpoch 1141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0615\nEpoch 1142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0613\nEpoch 1143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0610\nEpoch 1144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0607\nEpoch 1145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0603\nEpoch 1146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0601\nEpoch 1147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0598\nEpoch 1148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0596\nEpoch 1149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0594\nEpoch 1150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0591\nEpoch 1151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0588\nEpoch 1152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0585\nEpoch 1153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0582\nEpoch 1154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0581\nEpoch 1155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0579\nEpoch 1156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0576\nEpoch 1157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0574\nEpoch 1158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0572\nEpoch 1159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0570\nEpoch 1160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0568\nEpoch 1161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0566\nEpoch 1162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0564\nEpoch 1163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0563\nEpoch 1164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0560\nEpoch 1165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0558\nEpoch 1166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0556\nEpoch 1167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0553\nEpoch 1168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0551\nEpoch 1169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0549\nEpoch 1170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0548\nEpoch 1171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0546\nEpoch 1172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0544\nEpoch 1173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0542\nEpoch 1174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0540\nEpoch 1175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0538\nEpoch 1176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0536\nEpoch 1177/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0534\nEpoch 1178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0533\nEpoch 1179/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0531\nEpoch 1180/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0530\nEpoch 1181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0529\nEpoch 1182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0527\nEpoch 1183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0525\nEpoch 1184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0524\nEpoch 1185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0522\nEpoch 1186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0520\nEpoch 1187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0519\nEpoch 1188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0517\nEpoch 1189/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0516\nEpoch 1190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0514\nEpoch 1191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0512\nEpoch 1192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0510\nEpoch 1193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0508\nEpoch 1194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0508\nEpoch 1195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0506\nEpoch 1196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0504\nEpoch 1197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0502\nEpoch 1198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0501\nEpoch 1199/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0499\nEpoch 1200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0497\nEpoch 1201/2000\n6/6 [==============================] - 0s 5ms/step - loss: 0.0345 - val_loss: 0.0496\nEpoch 1202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0494\nEpoch 1203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0492\nEpoch 1204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0491\nEpoch 1205/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0490\nEpoch 1206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0489\nEpoch 1207/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0487\nEpoch 1208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0486\nEpoch 1209/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0483\nEpoch 1210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0482\nEpoch 1211/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0481\nEpoch 1212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0479\nEpoch 1213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0478\nEpoch 1214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0477\nEpoch 1215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0476\nEpoch 1216/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0475\nEpoch 1217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473\nEpoch 1218/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0472\nEpoch 1219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0471\nEpoch 1220/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0470\nEpoch 1221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0469\nEpoch 1222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0468\nEpoch 1223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0467\nEpoch 1224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0466\nEpoch 1225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0465\nEpoch 1226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0464\nEpoch 1227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463\nEpoch 1228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462\nEpoch 1229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0461\nEpoch 1230/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0460\nEpoch 1231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0459\nEpoch 1232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458\nEpoch 1233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456\nEpoch 1234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0455\nEpoch 1235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0453\nEpoch 1238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452\nEpoch 1239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0451\nEpoch 1240/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0450\nEpoch 1241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0449\nEpoch 1242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448\nEpoch 1243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0447\nEpoch 1244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446\nEpoch 1245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446\nEpoch 1246/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0444\nEpoch 1247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444\nEpoch 1248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443\nEpoch 1249/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443\nEpoch 1250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442\nEpoch 1251/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0441\nEpoch 1252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440\nEpoch 1253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439\nEpoch 1254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438\nEpoch 1255/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0437\nEpoch 1256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0436\nEpoch 1257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435\nEpoch 1258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433\nEpoch 1260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432\nEpoch 1261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0429\nEpoch 1266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1273/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424\nEpoch 1275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424\nEpoch 1276/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0420\nEpoch 1282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419\nEpoch 1283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419\nEpoch 1284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419\nEpoch 1285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419\nEpoch 1286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418\nEpoch 1287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418\nEpoch 1288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0417\nEpoch 1290/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416\nEpoch 1291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416\nEpoch 1292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1293/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413\nEpoch 1297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1332/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1336/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1338/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1339/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1355/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1360/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1363/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1370/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1380/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1400/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1401/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1405/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1408/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1409/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1413/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1424/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1429/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1438/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1458/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1459/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1461/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1463/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1473/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1474/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1479/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1484/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1485/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1486/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1497/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1500/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1505/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1507/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1512/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1513/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1514/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1521/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1522/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1527/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1528/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1532/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1538/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1551/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1578/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1593/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1594/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1605/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1611/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1612/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1617/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1626/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1630/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1631/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1637/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1641/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1645/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1647/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1651/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1659/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1663/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1684/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1685/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1687/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1694/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1697/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1701/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1711/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1715/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1720/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1721/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1726/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1735/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1739/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1758/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1770/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1771/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1773/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1776/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1783/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1786/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1799/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1800/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1801/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1815/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1838/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1840/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1842/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1845/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1850/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1852/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1854/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1857/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1880/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1887/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1889/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1901/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1907/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1913/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1919/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1922/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1933/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1938/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1945/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1952/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1953/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1975/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1980/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1989/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1995/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1998/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 2000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\n\n\n<keras.callbacks.History at 0x7f708335a740>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_9/kernel:0' shape=(5, 1) dtype=float32, numpy=\n array([[ 2.9966218e+00],\n        [ 1.0097879e+00],\n        [-1.4235497e-02],\n        [ 3.8510602e-04],\n        [ 4.8625717e-01]], dtype=float32)>,\n <tf.Variable 'dense_9/bias:0' shape=(1,) dtype=float32, numpy=array([-2.0080342], dtype=float32)>]\n\n\n\nnet.summary()\n\nModel: \"sequential_9\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_9 (Dense)             (None, 1)                 6         \n                                                                 \n=================================================================\nTotal params: 6\nTrainable params: 6\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0\n\n\n\n텐서보드: 사용자지정그림 에폭별로 시각화 (1)\n- 100에폭마다 적합결과를 시각화 하고 싶다 + 가중치와 같이!! - https://www.tensorflow.org/guide/keras/custom_callback\n- 일단 “100에폭마다 가중치적합과정 시각화 + 최종적합곡선 시각화” 까지 구현\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\ncb1= tf.keras.callbacks.TensorBoard(update_freq='epoch',histogram_freq=100)\nnet.fit(X,y,epochs=2000, batch_size=100, validation_split=0.45,callbacks=cb1)\n\nEpoch 1/2000\n6/6 [==============================] - 0s 11ms/step - loss: 6.6990 - val_loss: 14.1016\nEpoch 2/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.6442 - val_loss: 14.0394\nEpoch 3/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5913 - val_loss: 13.9775\nEpoch 4/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5361 - val_loss: 13.9172\nEpoch 5/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4823 - val_loss: 13.8573\nEpoch 6/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4314 - val_loss: 13.7958\nEpoch 7/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3769 - val_loss: 13.7367\nEpoch 8/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3262 - val_loss: 13.6770\nEpoch 9/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2734 - val_loss: 13.6171\nEpoch 10/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2212 - val_loss: 13.5591\nEpoch 11/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1701 - val_loss: 13.5023\nEpoch 12/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1201 - val_loss: 13.4448\nEpoch 13/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.0705 - val_loss: 13.3874\nEpoch 14/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.0197 - val_loss: 13.3311\nEpoch 15/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9705 - val_loss: 13.2742\nEpoch 16/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9195 - val_loss: 13.2184\nEpoch 17/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8713 - val_loss: 13.1627\nEpoch 18/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8223 - val_loss: 13.1072\nEpoch 19/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7729 - val_loss: 13.0523\nEpoch 20/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7259 - val_loss: 12.9976\nEpoch 21/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6771 - val_loss: 12.9449\nEpoch 22/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6299 - val_loss: 12.8918\nEpoch 23/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5827 - val_loss: 12.8385\nEpoch 24/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5371 - val_loss: 12.7853\nEpoch 25/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.4892 - val_loss: 12.7333\nEpoch 26/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.4440 - val_loss: 12.6799\nEpoch 27/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.3974 - val_loss: 12.6281\nEpoch 28/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3519 - val_loss: 12.5757\nEpoch 29/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3069 - val_loss: 12.5253\nEpoch 30/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2628 - val_loss: 12.4748\nEpoch 31/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2181 - val_loss: 12.4244\nEpoch 32/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1746 - val_loss: 12.3737\nEpoch 33/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1311 - val_loss: 12.3234\nEpoch 34/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0871 - val_loss: 12.2756\nEpoch 35/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0448 - val_loss: 12.2277\nEpoch 36/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0017 - val_loss: 12.1806\nEpoch 37/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.9595 - val_loss: 12.1327\nEpoch 38/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.9173 - val_loss: 12.0851\nEpoch 39/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8764 - val_loss: 12.0371\nEpoch 40/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8342 - val_loss: 11.9906\nEpoch 41/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7936 - val_loss: 11.9438\nEpoch 42/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7533 - val_loss: 11.8970\nEpoch 43/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.7121 - val_loss: 11.8512\nEpoch 44/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6723 - val_loss: 11.8053\nEpoch 45/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6318 - val_loss: 11.7611\nEpoch 46/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5923 - val_loss: 11.7176\nEpoch 47/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5532 - val_loss: 11.6732\nEpoch 48/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5130 - val_loss: 11.6284\nEpoch 49/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4748 - val_loss: 11.5833\nEpoch 50/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4350 - val_loss: 11.5400\nEpoch 51/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3965 - val_loss: 11.4966\nEpoch 52/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3589 - val_loss: 11.4541\nEpoch 53/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3203 - val_loss: 11.4124\nEpoch 54/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2825 - val_loss: 11.3700\nEpoch 55/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2451 - val_loss: 11.3271\nEpoch 56/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2078 - val_loss: 11.2843\nEpoch 57/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1703 - val_loss: 11.2416\nEpoch 58/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1330 - val_loss: 11.1995\nEpoch 59/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.0959 - val_loss: 11.1581\nEpoch 60/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0600 - val_loss: 11.1171\nEpoch 61/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0237 - val_loss: 11.0773\nEpoch 62/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9879 - val_loss: 11.0384\nEpoch 63/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9532 - val_loss: 10.9997\nEpoch 64/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9196 - val_loss: 10.9601\nEpoch 65/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.8845 - val_loss: 10.9219\nEpoch 66/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8504 - val_loss: 10.8832\nEpoch 67/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8163 - val_loss: 10.8449\nEpoch 68/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.7821 - val_loss: 10.8072\nEpoch 69/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.7485 - val_loss: 10.7692\nEpoch 70/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7154 - val_loss: 10.7316\nEpoch 71/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6823 - val_loss: 10.6955\nEpoch 72/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6498 - val_loss: 10.6591\nEpoch 73/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6170 - val_loss: 10.6233\nEpoch 74/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5846 - val_loss: 10.5880\nEpoch 75/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5533 - val_loss: 10.5517\nEpoch 76/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5209 - val_loss: 10.5146\nEpoch 77/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4886 - val_loss: 10.4796\nEpoch 78/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4579 - val_loss: 10.4437\nEpoch 79/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4264 - val_loss: 10.4078\nEpoch 80/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3956 - val_loss: 10.3729\nEpoch 81/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3637 - val_loss: 10.3384\nEpoch 82/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3335 - val_loss: 10.3034\nEpoch 83/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.3032 - val_loss: 10.2686\nEpoch 84/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2735 - val_loss: 10.2348\nEpoch 85/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2434 - val_loss: 10.2016\nEpoch 86/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2150 - val_loss: 10.1684\nEpoch 87/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1850 - val_loss: 10.1359\nEpoch 88/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1563 - val_loss: 10.1026\nEpoch 89/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.1270 - val_loss: 10.0714\nEpoch 90/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0990 - val_loss: 10.0392\nEpoch 91/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.0712 - val_loss: 10.0072\nEpoch 92/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0426 - val_loss: 9.9763\nEpoch 93/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0153 - val_loss: 9.9454\nEpoch 94/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9878 - val_loss: 9.9147\nEpoch 95/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9603 - val_loss: 9.8845\nEpoch 96/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9336 - val_loss: 9.8549\nEpoch 97/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9068 - val_loss: 9.8259\nEpoch 98/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8804 - val_loss: 9.7963\nEpoch 99/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8538 - val_loss: 9.7673\nEpoch 100/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.8276 - val_loss: 9.7382\nEpoch 101/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.8017 - val_loss: 9.7100\nEpoch 102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7755 - val_loss: 9.6811\nEpoch 103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7499 - val_loss: 9.6516\nEpoch 104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7244 - val_loss: 9.6223\nEpoch 105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6994 - val_loss: 9.5934\nEpoch 106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6744 - val_loss: 9.5652\nEpoch 107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6495 - val_loss: 9.5379\nEpoch 108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6246 - val_loss: 9.5108\nEpoch 109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6008 - val_loss: 9.4831\nEpoch 110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5761 - val_loss: 9.4555\nEpoch 111/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.5520 - val_loss: 9.4283\nEpoch 112/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.5287 - val_loss: 9.4010\nEpoch 113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5052 - val_loss: 9.3744\nEpoch 114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4813 - val_loss: 9.3473\nEpoch 115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4583 - val_loss: 9.3206\nEpoch 116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4353 - val_loss: 9.2952\nEpoch 117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4123 - val_loss: 9.2694\nEpoch 118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3897 - val_loss: 9.2431\nEpoch 119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3671 - val_loss: 9.2181\nEpoch 120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3451 - val_loss: 9.1919\nEpoch 121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3226 - val_loss: 9.1674\nEpoch 122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3013 - val_loss: 9.1414\nEpoch 123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2790 - val_loss: 9.1171\nEpoch 124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2575 - val_loss: 9.0930\nEpoch 125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2361 - val_loss: 9.0694\nEpoch 126/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.2154 - val_loss: 9.0448\nEpoch 127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1942 - val_loss: 9.0199\nEpoch 128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1733 - val_loss: 8.9960\nEpoch 129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1526 - val_loss: 8.9720\nEpoch 130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1320 - val_loss: 8.9488\nEpoch 131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1119 - val_loss: 8.9255\nEpoch 132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0918 - val_loss: 8.9022\nEpoch 133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0718 - val_loss: 8.8786\nEpoch 134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0517 - val_loss: 8.8560\nEpoch 135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0321 - val_loss: 8.8331\nEpoch 136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0127 - val_loss: 8.8099\nEpoch 137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9936 - val_loss: 8.7871\nEpoch 138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9746 - val_loss: 8.7652\nEpoch 139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9557 - val_loss: 8.7429\nEpoch 140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9368 - val_loss: 8.7217\nEpoch 141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9192 - val_loss: 8.7001\nEpoch 142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9003 - val_loss: 8.6779\nEpoch 143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8826 - val_loss: 8.6553\nEpoch 144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8645 - val_loss: 8.6343\nEpoch 145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8470 - val_loss: 8.6119\nEpoch 146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8295 - val_loss: 8.5907\nEpoch 147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8117 - val_loss: 8.5700\nEpoch 148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7949 - val_loss: 8.5491\nEpoch 149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7779 - val_loss: 8.5288\nEpoch 150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7607 - val_loss: 8.5087\nEpoch 151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7440 - val_loss: 8.4884\nEpoch 152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7277 - val_loss: 8.4686\nEpoch 153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7107 - val_loss: 8.4488\nEpoch 154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6944 - val_loss: 8.4297\nEpoch 155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6783 - val_loss: 8.4099\nEpoch 156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6626 - val_loss: 8.3904\nEpoch 157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6466 - val_loss: 8.3712\nEpoch 158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6305 - val_loss: 8.3510\nEpoch 159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6148 - val_loss: 8.3309\nEpoch 160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5992 - val_loss: 8.3106\nEpoch 161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5839 - val_loss: 8.2924\nEpoch 162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5687 - val_loss: 8.2737\nEpoch 163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5537 - val_loss: 8.2558\nEpoch 164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5393 - val_loss: 8.2368\nEpoch 165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5241 - val_loss: 8.2189\nEpoch 166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5098 - val_loss: 8.2008\nEpoch 167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4951 - val_loss: 8.1831\nEpoch 168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4808 - val_loss: 8.1653\nEpoch 169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4669 - val_loss: 8.1461\nEpoch 170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4523 - val_loss: 8.1289\nEpoch 171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4385 - val_loss: 8.1115\nEpoch 172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4247 - val_loss: 8.0941\nEpoch 173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4108 - val_loss: 8.0772\nEpoch 174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3972 - val_loss: 8.0592\nEpoch 175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3837 - val_loss: 8.0412\nEpoch 176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3700 - val_loss: 8.0239\nEpoch 177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3571 - val_loss: 8.0066\nEpoch 178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3440 - val_loss: 7.9895\nEpoch 179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3308 - val_loss: 7.9722\nEpoch 180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3181 - val_loss: 7.9550\nEpoch 181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3055 - val_loss: 7.9387\nEpoch 182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2926 - val_loss: 7.9229\nEpoch 183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2805 - val_loss: 7.9067\nEpoch 184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2681 - val_loss: 7.8900\nEpoch 185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2556 - val_loss: 7.8737\nEpoch 186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2437 - val_loss: 7.8576\nEpoch 187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2318 - val_loss: 7.8403\nEpoch 188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2196 - val_loss: 7.8245\nEpoch 189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2079 - val_loss: 7.8084\nEpoch 190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1965 - val_loss: 7.7924\nEpoch 191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1850 - val_loss: 7.7777\nEpoch 192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1735 - val_loss: 7.7618\nEpoch 193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1625 - val_loss: 7.7459\nEpoch 194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1512 - val_loss: 7.7310\nEpoch 195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1403 - val_loss: 7.7166\nEpoch 196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1294 - val_loss: 7.7019\nEpoch 197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1186 - val_loss: 7.6871\nEpoch 198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1078 - val_loss: 7.6719\nEpoch 199/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0973 - val_loss: 7.6572\nEpoch 200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0866 - val_loss: 7.6426\nEpoch 201/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.0762 - val_loss: 7.6278\nEpoch 202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0657 - val_loss: 7.6135\nEpoch 203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0558 - val_loss: 7.5977\nEpoch 204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0456 - val_loss: 7.5829\nEpoch 205/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.0357 - val_loss: 7.5681\nEpoch 206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0256 - val_loss: 7.5545\nEpoch 207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0160 - val_loss: 7.5397\nEpoch 208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0064 - val_loss: 7.5247\nEpoch 209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9968 - val_loss: 7.5098\nEpoch 210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9873 - val_loss: 7.4949\nEpoch 211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9781 - val_loss: 7.4804\nEpoch 212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9685 - val_loss: 7.4660\nEpoch 213/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9593 - val_loss: 7.4515\nEpoch 214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9503 - val_loss: 7.4373\nEpoch 215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9412 - val_loss: 7.4229\nEpoch 216/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9323 - val_loss: 7.4090\nEpoch 217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9237 - val_loss: 7.3950\nEpoch 218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9149 - val_loss: 7.3820\nEpoch 219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9065 - val_loss: 7.3686\nEpoch 220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8979 - val_loss: 7.3558\nEpoch 221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8896 - val_loss: 7.3422\nEpoch 222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8814 - val_loss: 7.3290\nEpoch 223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8731 - val_loss: 7.3155\nEpoch 224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8654 - val_loss: 7.3014\nEpoch 225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8572 - val_loss: 7.2881\nEpoch 226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8492 - val_loss: 7.2753\nEpoch 227/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8416 - val_loss: 7.2624\nEpoch 228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8338 - val_loss: 7.2492\nEpoch 229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8262 - val_loss: 7.2358\nEpoch 230/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8185 - val_loss: 7.2236\nEpoch 231/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8110 - val_loss: 7.2112\nEpoch 232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8036 - val_loss: 7.1979\nEpoch 233/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.7962 - val_loss: 7.1849\nEpoch 234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7890 - val_loss: 7.1720\nEpoch 235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7817 - val_loss: 7.1592\nEpoch 236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7746 - val_loss: 7.1468\nEpoch 237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7674 - val_loss: 7.1337\nEpoch 238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7605 - val_loss: 7.1196\nEpoch 239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7536 - val_loss: 7.1064\nEpoch 240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7467 - val_loss: 7.0940\nEpoch 241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7399 - val_loss: 7.0820\nEpoch 242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7333 - val_loss: 7.0690\nEpoch 243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7267 - val_loss: 7.0568\nEpoch 244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7202 - val_loss: 7.0443\nEpoch 245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7139 - val_loss: 7.0311\nEpoch 246/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7075 - val_loss: 7.0185\nEpoch 247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7012 - val_loss: 7.0054\nEpoch 248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6948 - val_loss: 6.9938\nEpoch 249/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6887 - val_loss: 6.9809\nEpoch 250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6827 - val_loss: 6.9690\nEpoch 251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6768 - val_loss: 6.9565\nEpoch 252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6709 - val_loss: 6.9443\nEpoch 253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6650 - val_loss: 6.9322\nEpoch 254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6590 - val_loss: 6.9206\nEpoch 255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6533 - val_loss: 6.9084\nEpoch 256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6477 - val_loss: 6.8959\nEpoch 257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6422 - val_loss: 6.8833\nEpoch 258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6366 - val_loss: 6.8716\nEpoch 259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6312 - val_loss: 6.8591\nEpoch 260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6258 - val_loss: 6.8477\nEpoch 261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6206 - val_loss: 6.8359\nEpoch 262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6153 - val_loss: 6.8237\nEpoch 263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6101 - val_loss: 6.8123\nEpoch 264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6050 - val_loss: 6.8006\nEpoch 265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6000 - val_loss: 6.7892\nEpoch 266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5949 - val_loss: 6.7776\nEpoch 267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5900 - val_loss: 6.7652\nEpoch 268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5849 - val_loss: 6.7529\nEpoch 269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5801 - val_loss: 6.7406\nEpoch 270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5753 - val_loss: 6.7281\nEpoch 271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5705 - val_loss: 6.7161\nEpoch 272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5658 - val_loss: 6.7042\nEpoch 273/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5612 - val_loss: 6.6921\nEpoch 274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5567 - val_loss: 6.6803\nEpoch 275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5521 - val_loss: 6.6690\nEpoch 276/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5478 - val_loss: 6.6569\nEpoch 277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5433 - val_loss: 6.6454\nEpoch 278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5391 - val_loss: 6.6337\nEpoch 279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5346 - val_loss: 6.6222\nEpoch 280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5306 - val_loss: 6.6105\nEpoch 281/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5264 - val_loss: 6.5989\nEpoch 282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5223 - val_loss: 6.5878\nEpoch 283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5183 - val_loss: 6.5764\nEpoch 284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5143 - val_loss: 6.5650\nEpoch 285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5103 - val_loss: 6.5537\nEpoch 286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5065 - val_loss: 6.5421\nEpoch 287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5025 - val_loss: 6.5305\nEpoch 288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4988 - val_loss: 6.5192\nEpoch 289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4951 - val_loss: 6.5072\nEpoch 290/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4914 - val_loss: 6.4958\nEpoch 291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4876 - val_loss: 6.4849\nEpoch 292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4840 - val_loss: 6.4732\nEpoch 293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4805 - val_loss: 6.4623\nEpoch 294/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4768 - val_loss: 6.4509\nEpoch 295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4733 - val_loss: 6.4391\nEpoch 296/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4698 - val_loss: 6.4276\nEpoch 297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4664 - val_loss: 6.4161\nEpoch 298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4630 - val_loss: 6.4040\nEpoch 299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4595 - val_loss: 6.3927\nEpoch 300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4562 - val_loss: 6.3814\nEpoch 301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4529 - val_loss: 6.3698\nEpoch 302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4497 - val_loss: 6.3581\nEpoch 303/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4463 - val_loss: 6.3466\nEpoch 304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4432 - val_loss: 6.3346\nEpoch 305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4402 - val_loss: 6.3230\nEpoch 306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4370 - val_loss: 6.3121\nEpoch 307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4340 - val_loss: 6.3007\nEpoch 308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4310 - val_loss: 6.2892\nEpoch 309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4280 - val_loss: 6.2780\nEpoch 310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4251 - val_loss: 6.2666\nEpoch 311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4222 - val_loss: 6.2554\nEpoch 312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4194 - val_loss: 6.2441\nEpoch 313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4165 - val_loss: 6.2326\nEpoch 314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4137 - val_loss: 6.2205\nEpoch 315/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4110 - val_loss: 6.2087\nEpoch 316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4082 - val_loss: 6.1973\nEpoch 317/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4056 - val_loss: 6.1859\nEpoch 318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4030 - val_loss: 6.1745\nEpoch 319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4004 - val_loss: 6.1624\nEpoch 320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3977 - val_loss: 6.1507\nEpoch 321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3952 - val_loss: 6.1391\nEpoch 322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3927 - val_loss: 6.1269\nEpoch 323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3902 - val_loss: 6.1151\nEpoch 324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3878 - val_loss: 6.1037\nEpoch 325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3854 - val_loss: 6.0923\nEpoch 326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3829 - val_loss: 6.0804\nEpoch 327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3805 - val_loss: 6.0692\nEpoch 328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3782 - val_loss: 6.0571\nEpoch 329/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3759 - val_loss: 6.0447\nEpoch 330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3735 - val_loss: 6.0329\nEpoch 331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3713 - val_loss: 6.0203\nEpoch 332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3690 - val_loss: 6.0089\nEpoch 333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3668 - val_loss: 5.9969\nEpoch 334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3645 - val_loss: 5.9852\nEpoch 335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3625 - val_loss: 5.9734\nEpoch 336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3603 - val_loss: 5.9614\nEpoch 337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3581 - val_loss: 5.9501\nEpoch 338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3561 - val_loss: 5.9386\nEpoch 339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3541 - val_loss: 5.9270\nEpoch 340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3521 - val_loss: 5.9156\nEpoch 341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3501 - val_loss: 5.9042\nEpoch 342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3481 - val_loss: 5.8924\nEpoch 343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3463 - val_loss: 5.8808\nEpoch 344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3442 - val_loss: 5.8688\nEpoch 345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3424 - val_loss: 5.8569\nEpoch 346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3405 - val_loss: 5.8452\nEpoch 347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3386 - val_loss: 5.8337\nEpoch 348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3369 - val_loss: 5.8213\nEpoch 349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3350 - val_loss: 5.8097\nEpoch 350/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3332 - val_loss: 5.7979\nEpoch 351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3315 - val_loss: 5.7865\nEpoch 352/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3297 - val_loss: 5.7747\nEpoch 353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3280 - val_loss: 5.7631\nEpoch 354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3263 - val_loss: 5.7515\nEpoch 355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3246 - val_loss: 5.7400\nEpoch 356/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3230 - val_loss: 5.7283\nEpoch 357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3213 - val_loss: 5.7165\nEpoch 358/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3197 - val_loss: 5.7047\nEpoch 359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3181 - val_loss: 5.6931\nEpoch 360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3165 - val_loss: 5.6812\nEpoch 361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3150 - val_loss: 5.6690\nEpoch 362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3134 - val_loss: 5.6579\nEpoch 363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3118 - val_loss: 5.6461\nEpoch 364/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3104 - val_loss: 5.6340\nEpoch 365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3089 - val_loss: 5.6223\nEpoch 366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3074 - val_loss: 5.6100\nEpoch 367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3060 - val_loss: 5.5979\nEpoch 368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3045 - val_loss: 5.5858\nEpoch 369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3030 - val_loss: 5.5742\nEpoch 370/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3016 - val_loss: 5.5622\nEpoch 371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3002 - val_loss: 5.5502\nEpoch 372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2989 - val_loss: 5.5378\nEpoch 373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2975 - val_loss: 5.5254\nEpoch 374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2961 - val_loss: 5.5131\nEpoch 375/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2948 - val_loss: 5.5012\nEpoch 376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2936 - val_loss: 5.4891\nEpoch 377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2923 - val_loss: 5.4766\nEpoch 378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2910 - val_loss: 5.4642\nEpoch 379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2897 - val_loss: 5.4516\nEpoch 380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2885 - val_loss: 5.4399\nEpoch 381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2872 - val_loss: 5.4283\nEpoch 382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2860 - val_loss: 5.4164\nEpoch 383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2848 - val_loss: 5.4039\nEpoch 384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2836 - val_loss: 5.3920\nEpoch 385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2824 - val_loss: 5.3798\nEpoch 386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2812 - val_loss: 5.3679\nEpoch 387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2801 - val_loss: 5.3552\nEpoch 388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2790 - val_loss: 5.3430\nEpoch 389/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2778 - val_loss: 5.3307\nEpoch 390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2767 - val_loss: 5.3186\nEpoch 391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2756 - val_loss: 5.3061\nEpoch 392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2745 - val_loss: 5.2937\nEpoch 393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2734 - val_loss: 5.2816\nEpoch 394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2724 - val_loss: 5.2698\nEpoch 395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2713 - val_loss: 5.2575\nEpoch 396/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2702 - val_loss: 5.2452\nEpoch 397/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.2692 - val_loss: 5.2331\nEpoch 398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2681 - val_loss: 5.2213\nEpoch 399/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2671 - val_loss: 5.2088\nEpoch 400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2661 - val_loss: 5.1963\nEpoch 401/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2651 - val_loss: 5.1842\nEpoch 402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2641 - val_loss: 5.1719\nEpoch 403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2631 - val_loss: 5.1593\nEpoch 404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2620 - val_loss: 5.1472\nEpoch 405/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2611 - val_loss: 5.1346\nEpoch 406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2601 - val_loss: 5.1224\nEpoch 407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2592 - val_loss: 5.1101\nEpoch 408/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2583 - val_loss: 5.0975\nEpoch 409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2573 - val_loss: 5.0852\nEpoch 410/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2564 - val_loss: 5.0724\nEpoch 411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2555 - val_loss: 5.0598\nEpoch 412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2546 - val_loss: 5.0475\nEpoch 413/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2536 - val_loss: 5.0351\nEpoch 414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2528 - val_loss: 5.0226\nEpoch 415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2519 - val_loss: 5.0102\nEpoch 416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2510 - val_loss: 4.9976\nEpoch 417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2501 - val_loss: 4.9857\nEpoch 418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2492 - val_loss: 4.9732\nEpoch 419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2483 - val_loss: 4.9606\nEpoch 420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2475 - val_loss: 4.9481\nEpoch 421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2466 - val_loss: 4.9357\nEpoch 422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2458 - val_loss: 4.9231\nEpoch 423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2450 - val_loss: 4.9108\nEpoch 424/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2442 - val_loss: 4.8982\nEpoch 425/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.2433 - val_loss: 4.8859\nEpoch 426/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.2425 - val_loss: 4.8733\nEpoch 427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2417 - val_loss: 4.8607\nEpoch 428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2409 - val_loss: 4.8482\nEpoch 429/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2402 - val_loss: 4.8354\nEpoch 430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2393 - val_loss: 4.8233\nEpoch 431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2386 - val_loss: 4.8107\nEpoch 432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2378 - val_loss: 4.7986\nEpoch 433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2371 - val_loss: 4.7858\nEpoch 434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2363 - val_loss: 4.7728\nEpoch 435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2355 - val_loss: 4.7603\nEpoch 436/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2347 - val_loss: 4.7470\nEpoch 437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2340 - val_loss: 4.7336\nEpoch 438/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2331 - val_loss: 4.7212\nEpoch 439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2324 - val_loss: 4.7081\nEpoch 440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2317 - val_loss: 4.6952\nEpoch 441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2309 - val_loss: 4.6824\nEpoch 442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2301 - val_loss: 4.6697\nEpoch 443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2294 - val_loss: 4.6565\nEpoch 444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2286 - val_loss: 4.6438\nEpoch 445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2280 - val_loss: 4.6312\nEpoch 446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2272 - val_loss: 4.6187\nEpoch 447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2265 - val_loss: 4.6062\nEpoch 448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2258 - val_loss: 4.5933\nEpoch 449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2251 - val_loss: 4.5809\nEpoch 450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2244 - val_loss: 4.5682\nEpoch 451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2237 - val_loss: 4.5557\nEpoch 452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2230 - val_loss: 4.5432\nEpoch 453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2223 - val_loss: 4.5304\nEpoch 454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2216 - val_loss: 4.5172\nEpoch 455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2209 - val_loss: 4.5045\nEpoch 456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2203 - val_loss: 4.4913\nEpoch 457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2196 - val_loss: 4.4780\nEpoch 458/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2189 - val_loss: 4.4653\nEpoch 459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2182 - val_loss: 4.4529\nEpoch 460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2176 - val_loss: 4.4401\nEpoch 461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2169 - val_loss: 4.4275\nEpoch 462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2163 - val_loss: 4.4144\nEpoch 463/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2156 - val_loss: 4.4018\nEpoch 464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2150 - val_loss: 4.3893\nEpoch 465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2143 - val_loss: 4.3762\nEpoch 466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2137 - val_loss: 4.3634\nEpoch 467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2131 - val_loss: 4.3504\nEpoch 468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2125 - val_loss: 4.3378\nEpoch 469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2118 - val_loss: 4.3251\nEpoch 470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2112 - val_loss: 4.3126\nEpoch 471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2106 - val_loss: 4.2997\nEpoch 472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2100 - val_loss: 4.2867\nEpoch 473/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2093 - val_loss: 4.2739\nEpoch 474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2086 - val_loss: 4.2609\nEpoch 475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2080 - val_loss: 4.2479\nEpoch 476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2074 - val_loss: 4.2348\nEpoch 477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2068 - val_loss: 4.2221\nEpoch 478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2062 - val_loss: 4.2093\nEpoch 479/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2056 - val_loss: 4.1964\nEpoch 480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2050 - val_loss: 4.1831\nEpoch 481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2044 - val_loss: 4.1703\nEpoch 482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2038 - val_loss: 4.1576\nEpoch 483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2031 - val_loss: 4.1450\nEpoch 484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2025 - val_loss: 4.1318\nEpoch 485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2019 - val_loss: 4.1189\nEpoch 486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2013 - val_loss: 4.1055\nEpoch 487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2007 - val_loss: 4.0927\nEpoch 488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2001 - val_loss: 4.0799\nEpoch 489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1995 - val_loss: 4.0670\nEpoch 490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1989 - val_loss: 4.0543\nEpoch 491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1983 - val_loss: 4.0412\nEpoch 492/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1977 - val_loss: 4.0288\nEpoch 493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1971 - val_loss: 4.0160\nEpoch 494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1965 - val_loss: 4.0029\nEpoch 495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1960 - val_loss: 3.9893\nEpoch 496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1954 - val_loss: 3.9760\nEpoch 497/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1948 - val_loss: 3.9628\nEpoch 498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1942 - val_loss: 3.9498\nEpoch 499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1936 - val_loss: 3.9373\nEpoch 500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1931 - val_loss: 3.9246\nEpoch 501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1925 - val_loss: 3.9113\nEpoch 502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1919 - val_loss: 3.8987\nEpoch 503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1914 - val_loss: 3.8862\nEpoch 504/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1907 - val_loss: 3.8739\nEpoch 505/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1901 - val_loss: 3.8608\nEpoch 506/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1896 - val_loss: 3.8478\nEpoch 507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1889 - val_loss: 3.8350\nEpoch 508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1884 - val_loss: 3.8218\nEpoch 509/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1878 - val_loss: 3.8091\nEpoch 510/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1872 - val_loss: 3.7960\nEpoch 511/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1867 - val_loss: 3.7827\nEpoch 512/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1861 - val_loss: 3.7695\nEpoch 513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1855 - val_loss: 3.7564\nEpoch 514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1849 - val_loss: 3.7437\nEpoch 515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1843 - val_loss: 3.7308\nEpoch 516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1837 - val_loss: 3.7178\nEpoch 517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1832 - val_loss: 3.7048\nEpoch 518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1826 - val_loss: 3.6917\nEpoch 519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1821 - val_loss: 3.6781\nEpoch 520/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1815 - val_loss: 3.6648\nEpoch 521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1809 - val_loss: 3.6515\nEpoch 522/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1804 - val_loss: 3.6382\nEpoch 523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1799 - val_loss: 3.6252\nEpoch 524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1793 - val_loss: 3.6128\nEpoch 525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1787 - val_loss: 3.5998\nEpoch 526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1781 - val_loss: 3.5868\nEpoch 527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1776 - val_loss: 3.5735\nEpoch 528/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1770 - val_loss: 3.5601\nEpoch 529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1765 - val_loss: 3.5467\nEpoch 530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1759 - val_loss: 3.5337\nEpoch 531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1753 - val_loss: 3.5207\nEpoch 532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1748 - val_loss: 3.5074\nEpoch 533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1743 - val_loss: 3.4943\nEpoch 534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1737 - val_loss: 3.4814\nEpoch 535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1731 - val_loss: 3.4686\nEpoch 536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1726 - val_loss: 3.4557\nEpoch 537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1721 - val_loss: 3.4433\nEpoch 538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1715 - val_loss: 3.4307\nEpoch 539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1710 - val_loss: 3.4174\nEpoch 540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1704 - val_loss: 3.4044\nEpoch 541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1699 - val_loss: 3.3917\nEpoch 542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1693 - val_loss: 3.3786\nEpoch 543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1688 - val_loss: 3.3658\nEpoch 544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1683 - val_loss: 3.3522\nEpoch 545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1677 - val_loss: 3.3393\nEpoch 546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1672 - val_loss: 3.3264\nEpoch 547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1667 - val_loss: 3.3137\nEpoch 548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1661 - val_loss: 3.3012\nEpoch 549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1656 - val_loss: 3.2885\nEpoch 550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1651 - val_loss: 3.2761\nEpoch 551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1646 - val_loss: 3.2636\nEpoch 552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1641 - val_loss: 3.2506\nEpoch 553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1635 - val_loss: 3.2386\nEpoch 554/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1630 - val_loss: 3.2257\nEpoch 555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1625 - val_loss: 3.2130\nEpoch 556/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1619 - val_loss: 3.2007\nEpoch 557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1614 - val_loss: 3.1881\nEpoch 558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1609 - val_loss: 3.1755\nEpoch 559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1604 - val_loss: 3.1625\nEpoch 560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1598 - val_loss: 3.1501\nEpoch 561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 3.1372\nEpoch 562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1588 - val_loss: 3.1248\nEpoch 563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1582 - val_loss: 3.1119\nEpoch 564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1577 - val_loss: 3.0991\nEpoch 565/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1572 - val_loss: 3.0867\nEpoch 566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1567 - val_loss: 3.0744\nEpoch 567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1562 - val_loss: 3.0622\nEpoch 568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1557 - val_loss: 3.0497\nEpoch 569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1552 - val_loss: 3.0377\nEpoch 570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1547 - val_loss: 3.0254\nEpoch 571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1541 - val_loss: 3.0136\nEpoch 572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1536 - val_loss: 3.0013\nEpoch 573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1531 - val_loss: 2.9889\nEpoch 574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1526 - val_loss: 2.9764\nEpoch 575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1521 - val_loss: 2.9640\nEpoch 576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1516 - val_loss: 2.9517\nEpoch 577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1511 - val_loss: 2.9392\nEpoch 578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1505 - val_loss: 2.9271\nEpoch 579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1500 - val_loss: 2.9147\nEpoch 580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1495 - val_loss: 2.9022\nEpoch 581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1490 - val_loss: 2.8904\nEpoch 582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1485 - val_loss: 2.8783\nEpoch 583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1480 - val_loss: 2.8657\nEpoch 584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1475 - val_loss: 2.8539\nEpoch 585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1470 - val_loss: 2.8414\nEpoch 586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1465 - val_loss: 2.8292\nEpoch 587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1460 - val_loss: 2.8170\nEpoch 588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1455 - val_loss: 2.8047\nEpoch 589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1451 - val_loss: 2.7925\nEpoch 590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1445 - val_loss: 2.7802\nEpoch 591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1440 - val_loss: 2.7680\nEpoch 592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1435 - val_loss: 2.7559\nEpoch 593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1430 - val_loss: 2.7437\nEpoch 594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1426 - val_loss: 2.7318\nEpoch 595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1420 - val_loss: 2.7202\nEpoch 596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1415 - val_loss: 2.7084\nEpoch 597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1411 - val_loss: 2.6963\nEpoch 598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1405 - val_loss: 2.6843\nEpoch 599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1400 - val_loss: 2.6728\nEpoch 600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1396 - val_loss: 2.6609\nEpoch 601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1391 - val_loss: 2.6491\nEpoch 602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1386 - val_loss: 2.6370\nEpoch 603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1381 - val_loss: 2.6249\nEpoch 604/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1376 - val_loss: 2.6129\nEpoch 605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1371 - val_loss: 2.6011\nEpoch 606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1366 - val_loss: 2.5889\nEpoch 607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1361 - val_loss: 2.5768\nEpoch 608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1356 - val_loss: 2.5650\nEpoch 609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1351 - val_loss: 2.5529\nEpoch 610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1347 - val_loss: 2.5414\nEpoch 611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1342 - val_loss: 2.5297\nEpoch 612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1337 - val_loss: 2.5178\nEpoch 613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1332 - val_loss: 2.5065\nEpoch 614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1327 - val_loss: 2.4948\nEpoch 615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1323 - val_loss: 2.4828\nEpoch 616/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1318 - val_loss: 2.4708\nEpoch 617/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1313 - val_loss: 2.4591\nEpoch 618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1308 - val_loss: 2.4475\nEpoch 619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1304 - val_loss: 2.4358\nEpoch 620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1299 - val_loss: 2.4243\nEpoch 621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1294 - val_loss: 2.4127\nEpoch 622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1290 - val_loss: 2.4011\nEpoch 623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1285 - val_loss: 2.3899\nEpoch 624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1280 - val_loss: 2.3783\nEpoch 625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1275 - val_loss: 2.3666\nEpoch 626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1271 - val_loss: 2.3553\nEpoch 627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1266 - val_loss: 2.3438\nEpoch 628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1262 - val_loss: 2.3323\nEpoch 629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1258 - val_loss: 2.3208\nEpoch 630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 2.3097\nEpoch 631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 2.2984\nEpoch 632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 2.2874\nEpoch 633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1239 - val_loss: 2.2767\nEpoch 634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 2.2654\nEpoch 635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1230 - val_loss: 2.2545\nEpoch 636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1225 - val_loss: 2.2435\nEpoch 637/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1221 - val_loss: 2.2321\nEpoch 638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1217 - val_loss: 2.2217\nEpoch 639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1213 - val_loss: 2.2106\nEpoch 640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1208 - val_loss: 2.2000\nEpoch 641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1204 - val_loss: 2.1890\nEpoch 642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1199 - val_loss: 2.1783\nEpoch 643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1194 - val_loss: 2.1677\nEpoch 644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1190 - val_loss: 2.1568\nEpoch 645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1186 - val_loss: 2.1462\nEpoch 646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1182 - val_loss: 2.1353\nEpoch 647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1177 - val_loss: 2.1245\nEpoch 648/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1173 - val_loss: 2.1141\nEpoch 649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1168 - val_loss: 2.1036\nEpoch 650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1164 - val_loss: 2.0928\nEpoch 651/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1160 - val_loss: 2.0823\nEpoch 652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1155 - val_loss: 2.0714\nEpoch 653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1151 - val_loss: 2.0609\nEpoch 654/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1147 - val_loss: 2.0502\nEpoch 655/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1143 - val_loss: 2.0392\nEpoch 656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1138 - val_loss: 2.0286\nEpoch 657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1134 - val_loss: 2.0182\nEpoch 658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1130 - val_loss: 2.0077\nEpoch 659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1125 - val_loss: 1.9972\nEpoch 660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1121 - val_loss: 1.9869\nEpoch 661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1117 - val_loss: 1.9767\nEpoch 662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1113 - val_loss: 1.9659\nEpoch 663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1109 - val_loss: 1.9554\nEpoch 664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1104 - val_loss: 1.9449\nEpoch 665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1100 - val_loss: 1.9350\nEpoch 666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1096 - val_loss: 1.9250\nEpoch 667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1091 - val_loss: 1.9149\nEpoch 668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1087 - val_loss: 1.9043\nEpoch 669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 1.8937\nEpoch 670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1079 - val_loss: 1.8833\nEpoch 671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1075 - val_loss: 1.8730\nEpoch 672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1071 - val_loss: 1.8626\nEpoch 673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1066 - val_loss: 1.8525\nEpoch 674/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1062 - val_loss: 1.8425\nEpoch 675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1058 - val_loss: 1.8325\nEpoch 676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1054 - val_loss: 1.8222\nEpoch 677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1050 - val_loss: 1.8121\nEpoch 678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1045 - val_loss: 1.8023\nEpoch 679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1042 - val_loss: 1.7921\nEpoch 680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1038 - val_loss: 1.7819\nEpoch 681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1033 - val_loss: 1.7721\nEpoch 682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1029 - val_loss: 1.7620\nEpoch 683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1025 - val_loss: 1.7520\nEpoch 684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1021 - val_loss: 1.7421\nEpoch 685/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1018 - val_loss: 1.7322\nEpoch 686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 1.7224\nEpoch 687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1010 - val_loss: 1.7127\nEpoch 688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1006 - val_loss: 1.7031\nEpoch 689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1002 - val_loss: 1.6937\nEpoch 690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0998 - val_loss: 1.6841\nEpoch 691/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0994 - val_loss: 1.6741\nEpoch 692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0990 - val_loss: 1.6640\nEpoch 693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0986 - val_loss: 1.6543\nEpoch 694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0982 - val_loss: 1.6454\nEpoch 695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0979 - val_loss: 1.6361\nEpoch 696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0975 - val_loss: 1.6266\nEpoch 697/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0971 - val_loss: 1.6170\nEpoch 698/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 1.6073\nEpoch 699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0963 - val_loss: 1.5979\nEpoch 700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0960 - val_loss: 1.5886\nEpoch 701/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0956 - val_loss: 1.5796\nEpoch 702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0952 - val_loss: 1.5706\nEpoch 703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0949 - val_loss: 1.5613\nEpoch 704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0945 - val_loss: 1.5523\nEpoch 705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0941 - val_loss: 1.5429\nEpoch 706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0938 - val_loss: 1.5336\nEpoch 707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0934 - val_loss: 1.5247\nEpoch 708/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0930 - val_loss: 1.5156\nEpoch 709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0926 - val_loss: 1.5068\nEpoch 710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0923 - val_loss: 1.4980\nEpoch 711/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0919 - val_loss: 1.4888\nEpoch 712/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0915 - val_loss: 1.4796\nEpoch 713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0912 - val_loss: 1.4702\nEpoch 714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0908 - val_loss: 1.4612\nEpoch 715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0904 - val_loss: 1.4523\nEpoch 716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 1.4433\nEpoch 717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0897 - val_loss: 1.4345\nEpoch 718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0894 - val_loss: 1.4258\nEpoch 719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0890 - val_loss: 1.4173\nEpoch 720/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0886 - val_loss: 1.4085\nEpoch 721/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0883 - val_loss: 1.3997\nEpoch 722/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0879 - val_loss: 1.3913\nEpoch 723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0876 - val_loss: 1.3827\nEpoch 724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0873 - val_loss: 1.3742\nEpoch 725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0869 - val_loss: 1.3656\nEpoch 726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0866 - val_loss: 1.3571\nEpoch 727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0862 - val_loss: 1.3485\nEpoch 728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0859 - val_loss: 1.3402\nEpoch 729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0856 - val_loss: 1.3317\nEpoch 730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0852 - val_loss: 1.3236\nEpoch 731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0849 - val_loss: 1.3152\nEpoch 732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0845 - val_loss: 1.3069\nEpoch 733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0842 - val_loss: 1.2986\nEpoch 734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0839 - val_loss: 1.2901\nEpoch 735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0835 - val_loss: 1.2821\nEpoch 736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0832 - val_loss: 1.2735\nEpoch 737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0828 - val_loss: 1.2655\nEpoch 738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0825 - val_loss: 1.2574\nEpoch 739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0822 - val_loss: 1.2491\nEpoch 740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0819 - val_loss: 1.2410\nEpoch 741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0815 - val_loss: 1.2326\nEpoch 742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0812 - val_loss: 1.2245\nEpoch 743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0809 - val_loss: 1.2162\nEpoch 744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0806 - val_loss: 1.2082\nEpoch 745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0802 - val_loss: 1.2005\nEpoch 746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0799 - val_loss: 1.1927\nEpoch 747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0796 - val_loss: 1.1848\nEpoch 748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0793 - val_loss: 1.1771\nEpoch 749/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0790 - val_loss: 1.1692\nEpoch 750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0787 - val_loss: 1.1613\nEpoch 751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0784 - val_loss: 1.1534\nEpoch 752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0780 - val_loss: 1.1456\nEpoch 753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0778 - val_loss: 1.1380\nEpoch 754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0774 - val_loss: 1.1301\nEpoch 755/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0771 - val_loss: 1.1222\nEpoch 756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0768 - val_loss: 1.1146\nEpoch 757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0765 - val_loss: 1.1073\nEpoch 758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0762 - val_loss: 1.0998\nEpoch 759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0759 - val_loss: 1.0928\nEpoch 760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0756 - val_loss: 1.0854\nEpoch 761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0754 - val_loss: 1.0781\nEpoch 762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 1.0707\nEpoch 763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 1.0630\nEpoch 764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0745 - val_loss: 1.0560\nEpoch 765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0742 - val_loss: 1.0486\nEpoch 766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0739 - val_loss: 1.0412\nEpoch 767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0736 - val_loss: 1.0341\nEpoch 768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 1.0270\nEpoch 769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0730 - val_loss: 1.0200\nEpoch 770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0727 - val_loss: 1.0131\nEpoch 771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0725 - val_loss: 1.0066\nEpoch 772/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0722 - val_loss: 0.9998\nEpoch 773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.9929\nEpoch 774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0716 - val_loss: 0.9858\nEpoch 775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0713 - val_loss: 0.9790\nEpoch 776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0711 - val_loss: 0.9719\nEpoch 777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0708 - val_loss: 0.9653\nEpoch 778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0705 - val_loss: 0.9583\nEpoch 779/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0702 - val_loss: 0.9515\nEpoch 780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.9449\nEpoch 781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.9380\nEpoch 782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0694 - val_loss: 0.9312\nEpoch 783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0692 - val_loss: 0.9246\nEpoch 784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0689 - val_loss: 0.9182\nEpoch 785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0686 - val_loss: 0.9118\nEpoch 786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0684 - val_loss: 0.9054\nEpoch 787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8986\nEpoch 788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0679 - val_loss: 0.8915\nEpoch 789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0676 - val_loss: 0.8849\nEpoch 790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0673 - val_loss: 0.8786\nEpoch 791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0671 - val_loss: 0.8724\nEpoch 792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.8663\nEpoch 793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0666 - val_loss: 0.8602\nEpoch 794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0663 - val_loss: 0.8539\nEpoch 795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0661 - val_loss: 0.8475\nEpoch 796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.8416\nEpoch 797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.8354\nEpoch 798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.8295\nEpoch 799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.8233\nEpoch 800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0648 - val_loss: 0.8172\nEpoch 801/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0646 - val_loss: 0.8115\nEpoch 802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0644 - val_loss: 0.8055\nEpoch 803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.7994\nEpoch 804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7937\nEpoch 805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.7877\nEpoch 806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0634 - val_loss: 0.7819\nEpoch 807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.7761\nEpoch 808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0629 - val_loss: 0.7702\nEpoch 809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.7640\nEpoch 810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0625 - val_loss: 0.7584\nEpoch 811/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0622 - val_loss: 0.7529\nEpoch 812/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0620 - val_loss: 0.7469\nEpoch 813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.7411\nEpoch 814/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0615 - val_loss: 0.7350\nEpoch 815/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0613 - val_loss: 0.7292\nEpoch 816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.7238\nEpoch 817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.7180\nEpoch 818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0606 - val_loss: 0.7123\nEpoch 819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0604 - val_loss: 0.7069\nEpoch 820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0601 - val_loss: 0.7016\nEpoch 821/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0599 - val_loss: 0.6959\nEpoch 822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.6902\nEpoch 823/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0595 - val_loss: 0.6845\nEpoch 824/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.6790\nEpoch 825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6735\nEpoch 826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.6681\nEpoch 827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0586 - val_loss: 0.6627\nEpoch 828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 0.6575\nEpoch 829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0582 - val_loss: 0.6524\nEpoch 830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0580 - val_loss: 0.6472\nEpoch 831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.6421\nEpoch 832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0576 - val_loss: 0.6370\nEpoch 833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6318\nEpoch 834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0572 - val_loss: 0.6265\nEpoch 835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.6211\nEpoch 836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0567 - val_loss: 0.6162\nEpoch 837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0565 - val_loss: 0.6111\nEpoch 838/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0563 - val_loss: 0.6061\nEpoch 839/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0561 - val_loss: 0.6013\nEpoch 840/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0559 - val_loss: 0.5965\nEpoch 841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5917\nEpoch 842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0556 - val_loss: 0.5869\nEpoch 843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0554 - val_loss: 0.5819\nEpoch 844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0552 - val_loss: 0.5771\nEpoch 845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.5722\nEpoch 846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0548 - val_loss: 0.5672\nEpoch 847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0546 - val_loss: 0.5627\nEpoch 848/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0544 - val_loss: 0.5580\nEpoch 849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5534\nEpoch 850/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0540 - val_loss: 0.5487\nEpoch 851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0539 - val_loss: 0.5441\nEpoch 852/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0537 - val_loss: 0.5395\nEpoch 853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.5350\nEpoch 854/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0533 - val_loss: 0.5304\nEpoch 855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.5259\nEpoch 856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5214\nEpoch 857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0528 - val_loss: 0.5170\nEpoch 858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5125\nEpoch 859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5082\nEpoch 860/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0522 - val_loss: 0.5039\nEpoch 861/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0521 - val_loss: 0.4995\nEpoch 862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.4952\nEpoch 863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4909\nEpoch 864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.4865\nEpoch 865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4823\nEpoch 866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4780\nEpoch 867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.4738\nEpoch 868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4696\nEpoch 869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0507 - val_loss: 0.4655\nEpoch 870/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0505 - val_loss: 0.4615\nEpoch 871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4575\nEpoch 872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0502 - val_loss: 0.4533\nEpoch 873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4495\nEpoch 874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.4459\nEpoch 875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.4420\nEpoch 876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0496 - val_loss: 0.4379\nEpoch 877/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0494 - val_loss: 0.4340\nEpoch 878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.4303\nEpoch 879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4264\nEpoch 880/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0490 - val_loss: 0.4226\nEpoch 881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0489 - val_loss: 0.4190\nEpoch 882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0487 - val_loss: 0.4154\nEpoch 883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4118\nEpoch 884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4082\nEpoch 885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0483 - val_loss: 0.4049\nEpoch 886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.4012\nEpoch 887/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.3976\nEpoch 888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3939\nEpoch 889/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0477 - val_loss: 0.3903\nEpoch 890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3869\nEpoch 891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.3831\nEpoch 892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0473 - val_loss: 0.3796\nEpoch 893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3762\nEpoch 894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3729\nEpoch 895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3695\nEpoch 896/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0467 - val_loss: 0.3662\nEpoch 897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3629\nEpoch 898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.3596\nEpoch 899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3563\nEpoch 900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0462 - val_loss: 0.3530\nEpoch 901/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0461 - val_loss: 0.3499\nEpoch 902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3468\nEpoch 903/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0458 - val_loss: 0.3435\nEpoch 904/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0457 - val_loss: 0.3403\nEpoch 905/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0456 - val_loss: 0.3371\nEpoch 906/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0455 - val_loss: 0.3340\nEpoch 907/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3310\nEpoch 908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3280\nEpoch 909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3251\nEpoch 910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.3219\nEpoch 911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3189\nEpoch 912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.3159\nEpoch 913/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3130\nEpoch 914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3100\nEpoch 915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.3072\nEpoch 916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.3044\nEpoch 917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.3015\nEpoch 918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2987\nEpoch 919/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0440 - val_loss: 0.2960\nEpoch 920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.2933\nEpoch 921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2905\nEpoch 922/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0437 - val_loss: 0.2878\nEpoch 923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2853\nEpoch 924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2827\nEpoch 925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.2801\nEpoch 926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2777\nEpoch 927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2752\nEpoch 928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2727\nEpoch 929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2703\nEpoch 930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.2679\nEpoch 931/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2655\nEpoch 932/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2633\nEpoch 933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2607\nEpoch 934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 0.2584\nEpoch 935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2559\nEpoch 936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.2534\nEpoch 937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2510\nEpoch 938/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2486\nEpoch 939/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0420 - val_loss: 0.2461\nEpoch 940/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0419 - val_loss: 0.2437\nEpoch 941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2413\nEpoch 942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2390\nEpoch 943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2367\nEpoch 944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2345\nEpoch 945/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2322\nEpoch 946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2300\nEpoch 947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2276\nEpoch 948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2254\nEpoch 949/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0411 - val_loss: 0.2232\nEpoch 950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2211\nEpoch 951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2190\nEpoch 952/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2170\nEpoch 953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2151\nEpoch 954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2131\nEpoch 955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2111\nEpoch 956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2091\nEpoch 957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2071\nEpoch 958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2051\nEpoch 959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2030\nEpoch 960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.2010\nEpoch 961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1989\nEpoch 962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1972\nEpoch 963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1953\nEpoch 964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1932\nEpoch 965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1913\nEpoch 966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1894\nEpoch 967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1875\nEpoch 968/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0396 - val_loss: 0.1856\nEpoch 969/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0396 - val_loss: 0.1839\nEpoch 970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1821\nEpoch 971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1802\nEpoch 972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1784\nEpoch 973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1767\nEpoch 974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1751\nEpoch 975/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1734\nEpoch 976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1719\nEpoch 977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1703\nEpoch 978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1687\nEpoch 979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1670\nEpoch 980/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0388 - val_loss: 0.1656\nEpoch 981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1639\nEpoch 982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1624\nEpoch 983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1608\nEpoch 984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1592\nEpoch 985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1577\nEpoch 986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1563\nEpoch 987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1548\nEpoch 988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1533\nEpoch 989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1518\nEpoch 990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1504\nEpoch 991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1488\nEpoch 992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1475\nEpoch 993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1462\nEpoch 994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1448\nEpoch 995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1434\nEpoch 996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1421\nEpoch 997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1407\nEpoch 998/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 0.1393\nEpoch 999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1380\nEpoch 1000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1367\nEpoch 1001/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1355\nEpoch 1002/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1343\nEpoch 1003/2000\n6/6 [==============================] - ETA: 0s - loss: 0.032 - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1331\nEpoch 1004/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1320\nEpoch 1005/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1308\nEpoch 1006/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1296\nEpoch 1007/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1284\nEpoch 1008/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1272\nEpoch 1009/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1261\nEpoch 1010/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1250\nEpoch 1011/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1239\nEpoch 1012/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1229\nEpoch 1013/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1218\nEpoch 1014/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1207\nEpoch 1015/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1196\nEpoch 1016/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1185\nEpoch 1017/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1174\nEpoch 1018/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1163\nEpoch 1019/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1153\nEpoch 1020/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1143\nEpoch 1021/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1133\nEpoch 1022/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1123\nEpoch 1023/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1114\nEpoch 1024/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0367 - val_loss: 0.1104\nEpoch 1025/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1093\nEpoch 1026/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1082\nEpoch 1027/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1073\nEpoch 1028/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1063\nEpoch 1029/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1054\nEpoch 1030/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1044\nEpoch 1031/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0365 - val_loss: 0.1036\nEpoch 1032/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1027\nEpoch 1033/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1019\nEpoch 1034/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1009\nEpoch 1035/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.1001\nEpoch 1036/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0992\nEpoch 1037/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0983\nEpoch 1038/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0975\nEpoch 1039/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0965\nEpoch 1040/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0956\nEpoch 1041/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0948\nEpoch 1042/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0940\nEpoch 1043/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0932\nEpoch 1044/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0925\nEpoch 1045/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0917\nEpoch 1046/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0910\nEpoch 1047/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0360 - val_loss: 0.0902\nEpoch 1048/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0894\nEpoch 1049/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0886\nEpoch 1050/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0878\nEpoch 1051/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0870\nEpoch 1052/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0863\nEpoch 1053/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0857\nEpoch 1054/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0851\nEpoch 1055/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0844\nEpoch 1056/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0838\nEpoch 1057/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0831\nEpoch 1058/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0825\nEpoch 1059/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0819\nEpoch 1060/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0813\nEpoch 1061/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0807\nEpoch 1062/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0801\nEpoch 1063/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0795\nEpoch 1064/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0789\nEpoch 1065/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0783\nEpoch 1066/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0777\nEpoch 1067/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0771\nEpoch 1068/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0765\nEpoch 1069/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0759\nEpoch 1070/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0753\nEpoch 1071/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0748\nEpoch 1072/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0742\nEpoch 1073/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0736\nEpoch 1074/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0730\nEpoch 1075/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0724\nEpoch 1076/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0719\nEpoch 1077/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0714\nEpoch 1078/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0710\nEpoch 1079/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0705\nEpoch 1080/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0700\nEpoch 1081/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0695\nEpoch 1082/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0690\nEpoch 1083/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.0685\nEpoch 1084/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.0680\nEpoch 1085/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0675\nEpoch 1086/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0671\nEpoch 1087/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0667\nEpoch 1088/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0662\nEpoch 1089/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0659\nEpoch 1090/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 0.0655\nEpoch 1091/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0651\nEpoch 1092/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0646\nEpoch 1093/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0642\nEpoch 1094/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0637\nEpoch 1095/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0633\nEpoch 1096/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0629\nEpoch 1097/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0625\nEpoch 1098/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0622\nEpoch 1099/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0618\nEpoch 1100/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0614\nEpoch 1101/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0611\nEpoch 1102/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 0.0607\nEpoch 1103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0604\nEpoch 1104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0601\nEpoch 1105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0598\nEpoch 1106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0595\nEpoch 1107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0591\nEpoch 1108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0587\nEpoch 1109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0585\nEpoch 1110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0582\nEpoch 1111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0579\nEpoch 1112/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0575\nEpoch 1113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0572\nEpoch 1114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0570\nEpoch 1115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0567\nEpoch 1116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0563\nEpoch 1117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0560\nEpoch 1118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0557\nEpoch 1119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0555\nEpoch 1120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0551\nEpoch 1121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0548\nEpoch 1122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0545\nEpoch 1123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0542\nEpoch 1124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0539\nEpoch 1125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0538\nEpoch 1126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0536\nEpoch 1127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0533\nEpoch 1128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0531\nEpoch 1129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0528\nEpoch 1130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0526\nEpoch 1131/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0524\nEpoch 1132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0522\nEpoch 1133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0520\nEpoch 1134/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0518\nEpoch 1135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0515\nEpoch 1136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0513\nEpoch 1137/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0511\nEpoch 1138/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0509\nEpoch 1139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0507\nEpoch 1140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0505\nEpoch 1141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0503\nEpoch 1142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0502\nEpoch 1143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0500\nEpoch 1144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0498\nEpoch 1145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0496\nEpoch 1146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0494\nEpoch 1147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0492\nEpoch 1148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0490\nEpoch 1149/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0488\nEpoch 1150/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0486\nEpoch 1151/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0484\nEpoch 1152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0482\nEpoch 1153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0481\nEpoch 1154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0480\nEpoch 1155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0478\nEpoch 1156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0476\nEpoch 1157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0474\nEpoch 1158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0472\nEpoch 1159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0471\nEpoch 1160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0470\nEpoch 1161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0468\nEpoch 1162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0467\nEpoch 1163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0465\nEpoch 1164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463\nEpoch 1165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462\nEpoch 1166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0461\nEpoch 1167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0460\nEpoch 1168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0459\nEpoch 1169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458\nEpoch 1170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456\nEpoch 1171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0455\nEpoch 1172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1173/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0453\nEpoch 1174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452\nEpoch 1175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0451\nEpoch 1176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0450\nEpoch 1177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0449\nEpoch 1178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448\nEpoch 1179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0447\nEpoch 1180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446\nEpoch 1181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444\nEpoch 1182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443\nEpoch 1183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442\nEpoch 1184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441\nEpoch 1185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440\nEpoch 1186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439\nEpoch 1187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438\nEpoch 1188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0436\nEpoch 1189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435\nEpoch 1190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435\nEpoch 1191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433\nEpoch 1193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432\nEpoch 1194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1197/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0429\nEpoch 1198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1199/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1201/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424\nEpoch 1205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420\nEpoch 1211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419\nEpoch 1212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419\nEpoch 1213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419\nEpoch 1214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0418\nEpoch 1215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418\nEpoch 1216/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416\nEpoch 1219/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413\nEpoch 1224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1234/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1246/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1249/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1250/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1257/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1259/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1273/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1276/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1277/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1286/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1289/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1290/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1292/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1312/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1314/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1323/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1324/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1325/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1347/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1365/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1370/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1371/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1379/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1396/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1401/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1408/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1409/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1411/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1413/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1419/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1424/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1429/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1438/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1445/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1446/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1451/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1458/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1463/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1469/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1473/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1475/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1479/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1488/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1497/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1505/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1506/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1509/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1512/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1517/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1522/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1524/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1528/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1538/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1542/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1552/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1567/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1580/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1587/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1591/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1593/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1598/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1610/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1617/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1634/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1637/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1651/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1653/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1659/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1665/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1667/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1682/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1685/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1697/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1701/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1712/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1713/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1720/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1721/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1739/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1748/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1750/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1751/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1782/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1788/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1790/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1801/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1812/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1815/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1836/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1840/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1842/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1847/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1851/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1852/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1854/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1878/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1879/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1880/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1882/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1887/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1889/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1891/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1901/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1905/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1907/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1911/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1913/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1919/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1922/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1923/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1926/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1938/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1945/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1950/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1952/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1953/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1958/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1966/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1967/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1975/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1988/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 2000/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\n\n\n<keras.callbacks.History at 0x7f708353a650>\n\n\n\nfig, ax = plt.subplots() \nax.plot(y,'.',alpha=0.2)\nax.plot(net(X),'--')\nwith tf.summary.create_file_writer(logdir).as_default():\n    tf.summary.image(\"적합결과시각화\", plot_to_image(fig), step=0)\n\n\n# \n#%tensorboard --logdir logs --host 0.0.0.0 \n\n- 아래의 코드를 100에폭마다 실행하고 싶다.\nfig, ax = plt.subplots() \nax.plot(y,'.',alpha=0.2)\nax.plot(net(X),'--')\nwith tf.summary.create_file_writer(logdir).as_default():\n    tf.summary.image(\"적합결과시각화\", plot_to_image(fig), step=0)\n- 일단 net.fit직전까지의 코드를 구현\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\n\n- 사용자정의 콜백클래스를 만듬\n\nclass PlotYhat(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self,epoch,logs): # 입력은 무조건 self, epoch, logs를 써야합니다 --> 이 함수안에 에폭이 끝날때마다 할 동작을 정의한다. \n        if epoch % 100 ==0: \n            fig, ax = plt.subplots() \n            ax.plot(y,'.',alpha=0.2)\n            ax.plot(net(X),'--')\n            with tf.summary.create_file_writer(logdir).as_default():\n                tf.summary.image(\"적합결과시각화\"+str(epoch), plot_to_image(fig), step=0)\n\n- 내가 만든 클래스에서 cb2를 생성\n\n#collapse_output\n!rm -rf logs\ncb1= tf.keras.callbacks.TensorBoard(update_freq='epoch',histogram_freq=100)\ncb2= PlotYhat() \nnet.fit(X,y,epochs=2000, batch_size=100, validation_split=0.45,callbacks=[cb1,cb2])\n\nEpoch 1/2000\n1/6 [====>.........................] - ETA: 0s - loss: 2.9239WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0007s vs `on_train_batch_end` time: 0.0019s). Check your callbacks.\n6/6 [==============================] - 0s 6ms/step - loss: 2.8708 - val_loss: 9.1608\nEpoch 2/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8444 - val_loss: 9.1250\nEpoch 3/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8186 - val_loss: 9.0901\nEpoch 4/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7930 - val_loss: 9.0545\nEpoch 5/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7673 - val_loss: 9.0188\nEpoch 6/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7417 - val_loss: 8.9828\nEpoch 7/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7169 - val_loss: 8.9476\nEpoch 8/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6913 - val_loss: 8.9134\nEpoch 9/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.6667 - val_loss: 8.8786\nEpoch 10/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6420 - val_loss: 8.8450\nEpoch 11/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6175 - val_loss: 8.8107\nEpoch 12/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5932 - val_loss: 8.7766\nEpoch 13/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5690 - val_loss: 8.7433\nEpoch 14/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5453 - val_loss: 8.7103\nEpoch 15/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.5214 - val_loss: 8.6774\nEpoch 16/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.4980 - val_loss: 8.6453\nEpoch 17/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4749 - val_loss: 8.6120\nEpoch 18/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4520 - val_loss: 8.5786\nEpoch 19/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4293 - val_loss: 8.5454\nEpoch 20/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4065 - val_loss: 8.5130\nEpoch 21/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3840 - val_loss: 8.4813\nEpoch 22/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3618 - val_loss: 8.4501\nEpoch 23/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3398 - val_loss: 8.4176\nEpoch 24/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3182 - val_loss: 8.3857\nEpoch 25/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2960 - val_loss: 8.3532\nEpoch 26/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2748 - val_loss: 8.3213\nEpoch 27/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2534 - val_loss: 8.2902\nEpoch 28/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2321 - val_loss: 8.2592\nEpoch 29/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2113 - val_loss: 8.2272\nEpoch 30/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1904 - val_loss: 8.1962\nEpoch 31/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1696 - val_loss: 8.1648\nEpoch 32/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1493 - val_loss: 8.1346\nEpoch 33/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1288 - val_loss: 8.1042\nEpoch 34/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1088 - val_loss: 8.0739\nEpoch 35/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0887 - val_loss: 8.0442\nEpoch 36/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0688 - val_loss: 8.0146\nEpoch 37/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0491 - val_loss: 7.9854\nEpoch 38/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0297 - val_loss: 7.9553\nEpoch 39/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0102 - val_loss: 7.9261\nEpoch 40/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9913 - val_loss: 7.8968\nEpoch 41/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9720 - val_loss: 7.8684\nEpoch 42/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9533 - val_loss: 7.8398\nEpoch 43/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9346 - val_loss: 7.8111\nEpoch 44/2000\n6/6 [==============================] - 0s 2ms/step - loss: 1.9162 - val_loss: 7.7827\nEpoch 45/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8976 - val_loss: 7.7536\nEpoch 46/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8796 - val_loss: 7.7262\nEpoch 47/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8613 - val_loss: 7.6986\nEpoch 48/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8437 - val_loss: 7.6714\nEpoch 49/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.8257 - val_loss: 7.6441\nEpoch 50/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.8083 - val_loss: 7.6171\nEpoch 51/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.7911 - val_loss: 7.5898\nEpoch 52/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7739 - val_loss: 7.5620\nEpoch 53/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7568 - val_loss: 7.5342\nEpoch 54/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7399 - val_loss: 7.5075\nEpoch 55/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7231 - val_loss: 7.4809\nEpoch 56/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7066 - val_loss: 7.4545\nEpoch 57/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6898 - val_loss: 7.4279\nEpoch 58/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6734 - val_loss: 7.4019\nEpoch 59/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6573 - val_loss: 7.3760\nEpoch 60/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6410 - val_loss: 7.3502\nEpoch 61/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6251 - val_loss: 7.3247\nEpoch 62/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6095 - val_loss: 7.2996\nEpoch 63/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5937 - val_loss: 7.2744\nEpoch 64/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.5782 - val_loss: 7.2490\nEpoch 65/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5626 - val_loss: 7.2237\nEpoch 66/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5476 - val_loss: 7.1979\nEpoch 67/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5324 - val_loss: 7.1726\nEpoch 68/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.5175 - val_loss: 7.1478\nEpoch 69/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5027 - val_loss: 7.1222\nEpoch 70/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4878 - val_loss: 7.0974\nEpoch 71/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4732 - val_loss: 7.0722\nEpoch 72/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4588 - val_loss: 7.0463\nEpoch 73/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4445 - val_loss: 7.0214\nEpoch 74/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4301 - val_loss: 6.9963\nEpoch 75/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4162 - val_loss: 6.9712\nEpoch 76/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.4024 - val_loss: 6.9467\nEpoch 77/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3885 - val_loss: 6.9229\nEpoch 78/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3749 - val_loss: 6.8992\nEpoch 79/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3612 - val_loss: 6.8755\nEpoch 80/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3479 - val_loss: 6.8522\nEpoch 81/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3345 - val_loss: 6.8282\nEpoch 82/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3213 - val_loss: 6.8048\nEpoch 83/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3083 - val_loss: 6.7817\nEpoch 84/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2955 - val_loss: 6.7583\nEpoch 85/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2826 - val_loss: 6.7351\nEpoch 86/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2700 - val_loss: 6.7123\nEpoch 87/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2574 - val_loss: 6.6893\nEpoch 88/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2449 - val_loss: 6.6665\nEpoch 89/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2327 - val_loss: 6.6433\nEpoch 90/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2205 - val_loss: 6.6207\nEpoch 91/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2085 - val_loss: 6.5983\nEpoch 92/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1964 - val_loss: 6.5757\nEpoch 93/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1844 - val_loss: 6.5534\nEpoch 94/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1727 - val_loss: 6.5304\nEpoch 95/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1610 - val_loss: 6.5080\nEpoch 96/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1494 - val_loss: 6.4857\nEpoch 97/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1381 - val_loss: 6.4647\nEpoch 98/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1267 - val_loss: 6.4424\nEpoch 99/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1155 - val_loss: 6.4206\nEpoch 100/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1044 - val_loss: 6.3984\nEpoch 101/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.0934 - val_loss: 6.3769\nEpoch 102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0824 - val_loss: 6.3556\nEpoch 103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0716 - val_loss: 6.3349\nEpoch 104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0609 - val_loss: 6.3136\nEpoch 105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0506 - val_loss: 6.2930\nEpoch 106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0399 - val_loss: 6.2721\nEpoch 107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0296 - val_loss: 6.2521\nEpoch 108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0194 - val_loss: 6.2312\nEpoch 109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0093 - val_loss: 6.2111\nEpoch 110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9992 - val_loss: 6.1916\nEpoch 111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9893 - val_loss: 6.1708\nEpoch 112/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9792 - val_loss: 6.1501\nEpoch 113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9696 - val_loss: 6.1293\nEpoch 114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9600 - val_loss: 6.1098\nEpoch 115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9503 - val_loss: 6.0898\nEpoch 116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9407 - val_loss: 6.0703\nEpoch 117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9313 - val_loss: 6.0498\nEpoch 118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9219 - val_loss: 6.0300\nEpoch 119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9129 - val_loss: 6.0099\nEpoch 120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9036 - val_loss: 5.9900\nEpoch 121/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8947 - val_loss: 5.9708\nEpoch 122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8857 - val_loss: 5.9519\nEpoch 123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8769 - val_loss: 5.9320\nEpoch 124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8682 - val_loss: 5.9135\nEpoch 125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8595 - val_loss: 5.8947\nEpoch 126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8508 - val_loss: 5.8760\nEpoch 127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8425 - val_loss: 5.8576\nEpoch 128/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8340 - val_loss: 5.8392\nEpoch 129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8256 - val_loss: 5.8206\nEpoch 130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8175 - val_loss: 5.8018\nEpoch 131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8094 - val_loss: 5.7832\nEpoch 132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8012 - val_loss: 5.7644\nEpoch 133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7932 - val_loss: 5.7460\nEpoch 134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7853 - val_loss: 5.7276\nEpoch 135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7774 - val_loss: 5.7094\nEpoch 136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7698 - val_loss: 5.6909\nEpoch 137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7620 - val_loss: 5.6725\nEpoch 138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7544 - val_loss: 5.6538\nEpoch 139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7470 - val_loss: 5.6356\nEpoch 140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7397 - val_loss: 5.6178\nEpoch 141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7322 - val_loss: 5.6006\nEpoch 142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7251 - val_loss: 5.5828\nEpoch 143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7178 - val_loss: 5.5649\nEpoch 144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7107 - val_loss: 5.5471\nEpoch 145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7036 - val_loss: 5.5300\nEpoch 146/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.6967 - val_loss: 5.5130\nEpoch 147/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.6900 - val_loss: 5.4949\nEpoch 148/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.6831 - val_loss: 5.4773\nEpoch 149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6765 - val_loss: 5.4599\nEpoch 150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6698 - val_loss: 5.4428\nEpoch 151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6633 - val_loss: 5.4258\nEpoch 152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6568 - val_loss: 5.4088\nEpoch 153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6504 - val_loss: 5.3916\nEpoch 154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6440 - val_loss: 5.3746\nEpoch 155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6376 - val_loss: 5.3580\nEpoch 156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6313 - val_loss: 5.3409\nEpoch 157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6252 - val_loss: 5.3243\nEpoch 158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6191 - val_loss: 5.3078\nEpoch 159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6132 - val_loss: 5.2917\nEpoch 160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6071 - val_loss: 5.2757\nEpoch 161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6012 - val_loss: 5.2593\nEpoch 162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5954 - val_loss: 5.2427\nEpoch 163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5896 - val_loss: 5.2265\nEpoch 164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5839 - val_loss: 5.2100\nEpoch 165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5783 - val_loss: 5.1937\nEpoch 166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5726 - val_loss: 5.1774\nEpoch 167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5672 - val_loss: 5.1611\nEpoch 168/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5616 - val_loss: 5.1447\nEpoch 169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5562 - val_loss: 5.1285\nEpoch 170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5508 - val_loss: 5.1128\nEpoch 171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5455 - val_loss: 5.0969\nEpoch 172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5403 - val_loss: 5.0809\nEpoch 173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5351 - val_loss: 5.0651\nEpoch 174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5299 - val_loss: 5.0495\nEpoch 175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5248 - val_loss: 5.0335\nEpoch 176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5199 - val_loss: 5.0180\nEpoch 177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5150 - val_loss: 5.0023\nEpoch 178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5100 - val_loss: 4.9867\nEpoch 179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5052 - val_loss: 4.9713\nEpoch 180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5005 - val_loss: 4.9561\nEpoch 181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4957 - val_loss: 4.9413\nEpoch 182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4911 - val_loss: 4.9263\nEpoch 183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4864 - val_loss: 4.9110\nEpoch 184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4818 - val_loss: 4.8960\nEpoch 185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4773 - val_loss: 4.8807\nEpoch 186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4727 - val_loss: 4.8652\nEpoch 187/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4684 - val_loss: 4.8500\nEpoch 188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4640 - val_loss: 4.8352\nEpoch 189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4597 - val_loss: 4.8199\nEpoch 190/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4554 - val_loss: 4.8049\nEpoch 191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4511 - val_loss: 4.7902\nEpoch 192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4470 - val_loss: 4.7757\nEpoch 193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4430 - val_loss: 4.7610\nEpoch 194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4388 - val_loss: 4.7460\nEpoch 195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4348 - val_loss: 4.7315\nEpoch 196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4308 - val_loss: 4.7174\nEpoch 197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4270 - val_loss: 4.7021\nEpoch 198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4231 - val_loss: 4.6869\nEpoch 199/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4192 - val_loss: 4.6721\nEpoch 200/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4154 - val_loss: 4.6581\nEpoch 201/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4117 - val_loss: 4.6438\nEpoch 202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4080 - val_loss: 4.6298\nEpoch 203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4042 - val_loss: 4.6157\nEpoch 204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4007 - val_loss: 4.6012\nEpoch 205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3971 - val_loss: 4.5873\nEpoch 206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3935 - val_loss: 4.5730\nEpoch 207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3900 - val_loss: 4.5583\nEpoch 208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3865 - val_loss: 4.5440\nEpoch 209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3830 - val_loss: 4.5298\nEpoch 210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3796 - val_loss: 4.5153\nEpoch 211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3763 - val_loss: 4.5010\nEpoch 212/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3730 - val_loss: 4.4872\nEpoch 213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3697 - val_loss: 4.4732\nEpoch 214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3665 - val_loss: 4.4590\nEpoch 215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3633 - val_loss: 4.4456\nEpoch 216/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3602 - val_loss: 4.4320\nEpoch 217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3571 - val_loss: 4.4182\nEpoch 218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3540 - val_loss: 4.4044\nEpoch 219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3510 - val_loss: 4.3908\nEpoch 220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3480 - val_loss: 4.3772\nEpoch 221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3451 - val_loss: 4.3643\nEpoch 222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3422 - val_loss: 4.3514\nEpoch 223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3392 - val_loss: 4.3379\nEpoch 224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3364 - val_loss: 4.3246\nEpoch 225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3336 - val_loss: 4.3112\nEpoch 226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3309 - val_loss: 4.2979\nEpoch 227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3281 - val_loss: 4.2849\nEpoch 228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3255 - val_loss: 4.2719\nEpoch 229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3228 - val_loss: 4.2590\nEpoch 230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3202 - val_loss: 4.2463\nEpoch 231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3176 - val_loss: 4.2340\nEpoch 232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3150 - val_loss: 4.2210\nEpoch 233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3125 - val_loss: 4.2083\nEpoch 234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3100 - val_loss: 4.1954\nEpoch 235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3075 - val_loss: 4.1827\nEpoch 236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3051 - val_loss: 4.1695\nEpoch 237/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3026 - val_loss: 4.1566\nEpoch 238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3002 - val_loss: 4.1435\nEpoch 239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2978 - val_loss: 4.1301\nEpoch 240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2955 - val_loss: 4.1168\nEpoch 241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2931 - val_loss: 4.1040\nEpoch 242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2908 - val_loss: 4.0912\nEpoch 243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2886 - val_loss: 4.0783\nEpoch 244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2864 - val_loss: 4.0650\nEpoch 245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2842 - val_loss: 4.0519\nEpoch 246/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2820 - val_loss: 4.0394\nEpoch 247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2799 - val_loss: 4.0260\nEpoch 248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2778 - val_loss: 4.0132\nEpoch 249/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2756 - val_loss: 4.0004\nEpoch 250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2736 - val_loss: 3.9875\nEpoch 251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2716 - val_loss: 3.9748\nEpoch 252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2695 - val_loss: 3.9623\nEpoch 253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2675 - val_loss: 3.9500\nEpoch 254/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2656 - val_loss: 3.9379\nEpoch 255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2637 - val_loss: 3.9257\nEpoch 256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2618 - val_loss: 3.9131\nEpoch 257/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2598 - val_loss: 3.9008\nEpoch 258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2581 - val_loss: 3.8884\nEpoch 259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2562 - val_loss: 3.8763\nEpoch 260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2544 - val_loss: 3.8639\nEpoch 261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2526 - val_loss: 3.8518\nEpoch 262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2508 - val_loss: 3.8400\nEpoch 263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2491 - val_loss: 3.8280\nEpoch 264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2474 - val_loss: 3.8163\nEpoch 265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2457 - val_loss: 3.8043\nEpoch 266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2440 - val_loss: 3.7922\nEpoch 267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2424 - val_loss: 3.7796\nEpoch 268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2407 - val_loss: 3.7679\nEpoch 269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2391 - val_loss: 3.7561\nEpoch 270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2375 - val_loss: 3.7440\nEpoch 271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2358 - val_loss: 3.7320\nEpoch 272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2342 - val_loss: 3.7200\nEpoch 273/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2327 - val_loss: 3.7085\nEpoch 274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2311 - val_loss: 3.6968\nEpoch 275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2296 - val_loss: 3.6854\nEpoch 276/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2282 - val_loss: 3.6738\nEpoch 277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2267 - val_loss: 3.6621\nEpoch 278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2252 - val_loss: 3.6506\nEpoch 279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2238 - val_loss: 3.6389\nEpoch 280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2225 - val_loss: 3.6272\nEpoch 281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2210 - val_loss: 3.6159\nEpoch 282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2197 - val_loss: 3.6041\nEpoch 283/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2184 - val_loss: 3.5925\nEpoch 284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2170 - val_loss: 3.5810\nEpoch 285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2157 - val_loss: 3.5693\nEpoch 286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2144 - val_loss: 3.5573\nEpoch 287/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2132 - val_loss: 3.5455\nEpoch 288/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2118 - val_loss: 3.5341\nEpoch 289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2106 - val_loss: 3.5220\nEpoch 290/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2093 - val_loss: 3.5100\nEpoch 291/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2080 - val_loss: 3.4983\nEpoch 292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2068 - val_loss: 3.4868\nEpoch 293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2056 - val_loss: 3.4754\nEpoch 294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2044 - val_loss: 3.4642\nEpoch 295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2033 - val_loss: 3.4525\nEpoch 296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2021 - val_loss: 3.4414\nEpoch 297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2010 - val_loss: 3.4307\nEpoch 298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1998 - val_loss: 3.4204\nEpoch 299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1987 - val_loss: 3.4090\nEpoch 300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1976 - val_loss: 3.3979\nEpoch 301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1966 - val_loss: 3.3865\nEpoch 302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1955 - val_loss: 3.3759\nEpoch 303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1944 - val_loss: 3.3648\nEpoch 304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1934 - val_loss: 3.3537\nEpoch 305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1923 - val_loss: 3.3431\nEpoch 306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1913 - val_loss: 3.3316\nEpoch 307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1903 - val_loss: 3.3203\nEpoch 308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1893 - val_loss: 3.3087\nEpoch 309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1882 - val_loss: 3.2972\nEpoch 310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1873 - val_loss: 3.2862\nEpoch 311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1864 - val_loss: 3.2753\nEpoch 312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1854 - val_loss: 3.2644\nEpoch 313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1844 - val_loss: 3.2537\nEpoch 314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1836 - val_loss: 3.2427\nEpoch 315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1826 - val_loss: 3.2318\nEpoch 316/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1817 - val_loss: 3.2210\nEpoch 317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1808 - val_loss: 3.2101\nEpoch 318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1799 - val_loss: 3.1994\nEpoch 319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1790 - val_loss: 3.1886\nEpoch 320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1781 - val_loss: 3.1771\nEpoch 321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1772 - val_loss: 3.1665\nEpoch 322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1764 - val_loss: 3.1559\nEpoch 323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1756 - val_loss: 3.1453\nEpoch 324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1748 - val_loss: 3.1339\nEpoch 325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1739 - val_loss: 3.1228\nEpoch 326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1731 - val_loss: 3.1123\nEpoch 327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1723 - val_loss: 3.1017\nEpoch 328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1716 - val_loss: 3.0913\nEpoch 329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1708 - val_loss: 3.0808\nEpoch 330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1700 - val_loss: 3.0705\nEpoch 331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1692 - val_loss: 3.0602\nEpoch 332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1685 - val_loss: 3.0498\nEpoch 333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1677 - val_loss: 3.0388\nEpoch 334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1670 - val_loss: 3.0285\nEpoch 335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1662 - val_loss: 3.0177\nEpoch 336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1655 - val_loss: 3.0074\nEpoch 337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1648 - val_loss: 2.9975\nEpoch 338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1641 - val_loss: 2.9873\nEpoch 339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1634 - val_loss: 2.9772\nEpoch 340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1627 - val_loss: 2.9667\nEpoch 341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1620 - val_loss: 2.9566\nEpoch 342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1613 - val_loss: 2.9468\nEpoch 343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1607 - val_loss: 2.9365\nEpoch 344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1599 - val_loss: 2.9265\nEpoch 345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 2.9164\nEpoch 346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1586 - val_loss: 2.9060\nEpoch 347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1580 - val_loss: 2.8955\nEpoch 348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1573 - val_loss: 2.8854\nEpoch 349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1567 - val_loss: 2.8753\nEpoch 350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1560 - val_loss: 2.8653\nEpoch 351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1554 - val_loss: 2.8548\nEpoch 352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1548 - val_loss: 2.8449\nEpoch 353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1542 - val_loss: 2.8345\nEpoch 354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1536 - val_loss: 2.8244\nEpoch 355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1530 - val_loss: 2.8139\nEpoch 356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1524 - val_loss: 2.8030\nEpoch 357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1517 - val_loss: 2.7928\nEpoch 358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1512 - val_loss: 2.7827\nEpoch 359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1505 - val_loss: 2.7725\nEpoch 360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1500 - val_loss: 2.7618\nEpoch 361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1494 - val_loss: 2.7521\nEpoch 362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1488 - val_loss: 2.7423\nEpoch 363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1483 - val_loss: 2.7323\nEpoch 364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1477 - val_loss: 2.7222\nEpoch 365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1471 - val_loss: 2.7124\nEpoch 366/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1466 - val_loss: 2.7025\nEpoch 367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1460 - val_loss: 2.6925\nEpoch 368/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1455 - val_loss: 2.6823\nEpoch 369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1449 - val_loss: 2.6724\nEpoch 370/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1444 - val_loss: 2.6625\nEpoch 371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1438 - val_loss: 2.6529\nEpoch 372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1433 - val_loss: 2.6425\nEpoch 373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1428 - val_loss: 2.6325\nEpoch 374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1422 - val_loss: 2.6229\nEpoch 375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1417 - val_loss: 2.6133\nEpoch 376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1412 - val_loss: 2.6035\nEpoch 377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1407 - val_loss: 2.5939\nEpoch 378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1402 - val_loss: 2.5842\nEpoch 379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1397 - val_loss: 2.5744\nEpoch 380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1392 - val_loss: 2.5642\nEpoch 381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1387 - val_loss: 2.5543\nEpoch 382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1382 - val_loss: 2.5446\nEpoch 383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1378 - val_loss: 2.5352\nEpoch 384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1372 - val_loss: 2.5256\nEpoch 385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1368 - val_loss: 2.5152\nEpoch 386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1363 - val_loss: 2.5051\nEpoch 387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1358 - val_loss: 2.4956\nEpoch 388/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1353 - val_loss: 2.4856\nEpoch 389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1348 - val_loss: 2.4758\nEpoch 390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1344 - val_loss: 2.4662\nEpoch 391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1339 - val_loss: 2.4572\nEpoch 392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1335 - val_loss: 2.4477\nEpoch 393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1330 - val_loss: 2.4390\nEpoch 394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1325 - val_loss: 2.4299\nEpoch 395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1321 - val_loss: 2.4209\nEpoch 396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1316 - val_loss: 2.4115\nEpoch 397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1312 - val_loss: 2.4028\nEpoch 398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1307 - val_loss: 2.3940\nEpoch 399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1303 - val_loss: 2.3844\nEpoch 400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1299 - val_loss: 2.3751\nEpoch 401/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1295 - val_loss: 2.3659\nEpoch 402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1290 - val_loss: 2.3568\nEpoch 403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1286 - val_loss: 2.3472\nEpoch 404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1282 - val_loss: 2.3378\nEpoch 405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1277 - val_loss: 2.3288\nEpoch 406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1273 - val_loss: 2.3193\nEpoch 407/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1269 - val_loss: 2.3105\nEpoch 408/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1265 - val_loss: 2.3011\nEpoch 409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1261 - val_loss: 2.2917\nEpoch 410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1256 - val_loss: 2.2824\nEpoch 411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1252 - val_loss: 2.2731\nEpoch 412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 2.2642\nEpoch 413/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 2.2553\nEpoch 414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1240 - val_loss: 2.2461\nEpoch 415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 2.2373\nEpoch 416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1232 - val_loss: 2.2284\nEpoch 417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1228 - val_loss: 2.2194\nEpoch 418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1224 - val_loss: 2.2105\nEpoch 419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1220 - val_loss: 2.2017\nEpoch 420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1216 - val_loss: 2.1927\nEpoch 421/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1212 - val_loss: 2.1841\nEpoch 422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1208 - val_loss: 2.1749\nEpoch 423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1205 - val_loss: 2.1657\nEpoch 424/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1200 - val_loss: 2.1567\nEpoch 425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1196 - val_loss: 2.1476\nEpoch 426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1192 - val_loss: 2.1388\nEpoch 427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1189 - val_loss: 2.1294\nEpoch 428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1185 - val_loss: 2.1209\nEpoch 429/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1181 - val_loss: 2.1120\nEpoch 430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1177 - val_loss: 2.1027\nEpoch 431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1173 - val_loss: 2.0937\nEpoch 432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1170 - val_loss: 2.0851\nEpoch 433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1166 - val_loss: 2.0769\nEpoch 434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1162 - val_loss: 2.0680\nEpoch 435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1158 - val_loss: 2.0597\nEpoch 436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1154 - val_loss: 2.0511\nEpoch 437/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1151 - val_loss: 2.0421\nEpoch 438/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1147 - val_loss: 2.0333\nEpoch 439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1143 - val_loss: 2.0251\nEpoch 440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1140 - val_loss: 2.0163\nEpoch 441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1136 - val_loss: 2.0074\nEpoch 442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1133 - val_loss: 1.9985\nEpoch 443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1129 - val_loss: 1.9900\nEpoch 444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1125 - val_loss: 1.9811\nEpoch 445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1121 - val_loss: 1.9727\nEpoch 446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1118 - val_loss: 1.9641\nEpoch 447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1114 - val_loss: 1.9554\nEpoch 448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1111 - val_loss: 1.9468\nEpoch 449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1107 - val_loss: 1.9385\nEpoch 450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1104 - val_loss: 1.9299\nEpoch 451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1100 - val_loss: 1.9216\nEpoch 452/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1097 - val_loss: 1.9130\nEpoch 453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1093 - val_loss: 1.9047\nEpoch 454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1090 - val_loss: 1.8963\nEpoch 455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1086 - val_loss: 1.8880\nEpoch 456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 1.8797\nEpoch 457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1079 - val_loss: 1.8712\nEpoch 458/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1076 - val_loss: 1.8626\nEpoch 459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1072 - val_loss: 1.8541\nEpoch 460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1068 - val_loss: 1.8456\nEpoch 461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 1.8371\nEpoch 462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1062 - val_loss: 1.8291\nEpoch 463/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1058 - val_loss: 1.8207\nEpoch 464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1055 - val_loss: 1.8123\nEpoch 465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1051 - val_loss: 1.8044\nEpoch 466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1048 - val_loss: 1.7963\nEpoch 467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1044 - val_loss: 1.7885\nEpoch 468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1041 - val_loss: 1.7803\nEpoch 469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1038 - val_loss: 1.7722\nEpoch 470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1034 - val_loss: 1.7646\nEpoch 471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1031 - val_loss: 1.7565\nEpoch 472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 1.7484\nEpoch 473/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1025 - val_loss: 1.7402\nEpoch 474/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1021 - val_loss: 1.7330\nEpoch 475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1018 - val_loss: 1.7250\nEpoch 476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1015 - val_loss: 1.7168\nEpoch 477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1012 - val_loss: 1.7089\nEpoch 478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1008 - val_loss: 1.7008\nEpoch 479/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1005 - val_loss: 1.6927\nEpoch 480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1002 - val_loss: 1.6846\nEpoch 481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0999 - val_loss: 1.6768\nEpoch 482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0996 - val_loss: 1.6686\nEpoch 483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0992 - val_loss: 1.6610\nEpoch 484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0989 - val_loss: 1.6530\nEpoch 485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0986 - val_loss: 1.6448\nEpoch 486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0983 - val_loss: 1.6373\nEpoch 487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0980 - val_loss: 1.6292\nEpoch 488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0976 - val_loss: 1.6219\nEpoch 489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0973 - val_loss: 1.6139\nEpoch 490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 1.6062\nEpoch 491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0967 - val_loss: 1.5984\nEpoch 492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 1.5904\nEpoch 493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0961 - val_loss: 1.5827\nEpoch 494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0957 - val_loss: 1.5750\nEpoch 495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0954 - val_loss: 1.5672\nEpoch 496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0951 - val_loss: 1.5593\nEpoch 497/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0948 - val_loss: 1.5515\nEpoch 498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0945 - val_loss: 1.5438\nEpoch 499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0942 - val_loss: 1.5357\nEpoch 500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0938 - val_loss: 1.5281\nEpoch 501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0935 - val_loss: 1.5207\nEpoch 502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0932 - val_loss: 1.5133\nEpoch 503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0929 - val_loss: 1.5063\nEpoch 504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0926 - val_loss: 1.4992\nEpoch 505/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0923 - val_loss: 1.4920\nEpoch 506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0920 - val_loss: 1.4839\nEpoch 507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0917 - val_loss: 1.4763\nEpoch 508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0914 - val_loss: 1.4686\nEpoch 509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0911 - val_loss: 1.4608\nEpoch 510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0908 - val_loss: 1.4534\nEpoch 511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0905 - val_loss: 1.4456\nEpoch 512/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0902 - val_loss: 1.4380\nEpoch 513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0899 - val_loss: 1.4307\nEpoch 514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0896 - val_loss: 1.4231\nEpoch 515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0893 - val_loss: 1.4158\nEpoch 516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0890 - val_loss: 1.4086\nEpoch 517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0887 - val_loss: 1.4011\nEpoch 518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0884 - val_loss: 1.3936\nEpoch 519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0881 - val_loss: 1.3862\nEpoch 520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0878 - val_loss: 1.3786\nEpoch 521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0875 - val_loss: 1.3714\nEpoch 522/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0872 - val_loss: 1.3644\nEpoch 523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0869 - val_loss: 1.3574\nEpoch 524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0866 - val_loss: 1.3505\nEpoch 525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0864 - val_loss: 1.3437\nEpoch 526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0861 - val_loss: 1.3364\nEpoch 527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0858 - val_loss: 1.3296\nEpoch 528/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0855 - val_loss: 1.3227\nEpoch 529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0852 - val_loss: 1.3158\nEpoch 530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0849 - val_loss: 1.3094\nEpoch 531/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0847 - val_loss: 1.3025\nEpoch 532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0844 - val_loss: 1.2952\nEpoch 533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0841 - val_loss: 1.2883\nEpoch 534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0838 - val_loss: 1.2809\nEpoch 535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0835 - val_loss: 1.2734\nEpoch 536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0832 - val_loss: 1.2665\nEpoch 537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0830 - val_loss: 1.2597\nEpoch 538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0827 - val_loss: 1.2527\nEpoch 539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0824 - val_loss: 1.2453\nEpoch 540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0821 - val_loss: 1.2380\nEpoch 541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0818 - val_loss: 1.2310\nEpoch 542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0815 - val_loss: 1.2237\nEpoch 543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0812 - val_loss: 1.2171\nEpoch 544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0810 - val_loss: 1.2103\nEpoch 545/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0807 - val_loss: 1.2032\nEpoch 546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0804 - val_loss: 1.1967\nEpoch 547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0801 - val_loss: 1.1904\nEpoch 548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0798 - val_loss: 1.1842\nEpoch 549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0796 - val_loss: 1.1775\nEpoch 550/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0793 - val_loss: 1.1708\nEpoch 551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0790 - val_loss: 1.1644\nEpoch 552/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0788 - val_loss: 1.1575\nEpoch 553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0785 - val_loss: 1.1506\nEpoch 554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 1.1441\nEpoch 555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0779 - val_loss: 1.1371\nEpoch 556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0777 - val_loss: 1.1302\nEpoch 557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0774 - val_loss: 1.1237\nEpoch 558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0771 - val_loss: 1.1172\nEpoch 559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0769 - val_loss: 1.1104\nEpoch 560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0766 - val_loss: 1.1040\nEpoch 561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 1.0975\nEpoch 562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0761 - val_loss: 1.0910\nEpoch 563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0758 - val_loss: 1.0850\nEpoch 564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0756 - val_loss: 1.0784\nEpoch 565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0753 - val_loss: 1.0720\nEpoch 566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 1.0654\nEpoch 567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 1.0591\nEpoch 568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0746 - val_loss: 1.0525\nEpoch 569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0743 - val_loss: 1.0462\nEpoch 570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0741 - val_loss: 1.0400\nEpoch 571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0738 - val_loss: 1.0336\nEpoch 572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0736 - val_loss: 1.0274\nEpoch 573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 1.0215\nEpoch 574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0731 - val_loss: 1.0155\nEpoch 575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0728 - val_loss: 1.0095\nEpoch 576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0726 - val_loss: 1.0033\nEpoch 577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0723 - val_loss: 0.9969\nEpoch 578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0721 - val_loss: 0.9911\nEpoch 579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.9855\nEpoch 580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0716 - val_loss: 0.9800\nEpoch 581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.9744\nEpoch 582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0711 - val_loss: 0.9686\nEpoch 583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0709 - val_loss: 0.9626\nEpoch 584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0707 - val_loss: 0.9567\nEpoch 585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0704 - val_loss: 0.9504\nEpoch 586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0702 - val_loss: 0.9447\nEpoch 587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0699 - val_loss: 0.9390\nEpoch 588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.9330\nEpoch 589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0695 - val_loss: 0.9273\nEpoch 590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0692 - val_loss: 0.9217\nEpoch 591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0690 - val_loss: 0.9159\nEpoch 592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0688 - val_loss: 0.9097\nEpoch 593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0685 - val_loss: 0.9039\nEpoch 594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.8983\nEpoch 595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8930\nEpoch 596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0678 - val_loss: 0.8872\nEpoch 597/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0676 - val_loss: 0.8821\nEpoch 598/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0674 - val_loss: 0.8764\nEpoch 599/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0672 - val_loss: 0.8709\nEpoch 600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0669 - val_loss: 0.8655\nEpoch 601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0667 - val_loss: 0.8599\nEpoch 602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0665 - val_loss: 0.8545\nEpoch 603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0662 - val_loss: 0.8489\nEpoch 604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0660 - val_loss: 0.8432\nEpoch 605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.8379\nEpoch 606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.8323\nEpoch 607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0654 - val_loss: 0.8270\nEpoch 608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.8218\nEpoch 609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0649 - val_loss: 0.8166\nEpoch 610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0647 - val_loss: 0.8109\nEpoch 611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0645 - val_loss: 0.8056\nEpoch 612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0643 - val_loss: 0.8000\nEpoch 613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.7947\nEpoch 614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7896\nEpoch 615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0637 - val_loss: 0.7842\nEpoch 616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0635 - val_loss: 0.7790\nEpoch 617/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.7744\nEpoch 618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0630 - val_loss: 0.7692\nEpoch 619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0628 - val_loss: 0.7644\nEpoch 620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0626 - val_loss: 0.7597\nEpoch 621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0624 - val_loss: 0.7546\nEpoch 622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0622 - val_loss: 0.7497\nEpoch 623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0620 - val_loss: 0.7450\nEpoch 624/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0618 - val_loss: 0.7403\nEpoch 625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0616 - val_loss: 0.7351\nEpoch 626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0614 - val_loss: 0.7301\nEpoch 627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0612 - val_loss: 0.7254\nEpoch 628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.7203\nEpoch 629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0608 - val_loss: 0.7157\nEpoch 630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.7109\nEpoch 631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.7062\nEpoch 632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.7016\nEpoch 633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0601 - val_loss: 0.6969\nEpoch 634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0599 - val_loss: 0.6921\nEpoch 635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.6874\nEpoch 636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0595 - val_loss: 0.6825\nEpoch 637/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0593 - val_loss: 0.6778\nEpoch 638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.6729\nEpoch 639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6679\nEpoch 640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.6632\nEpoch 641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0586 - val_loss: 0.6585\nEpoch 642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 0.6537\nEpoch 643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0582 - val_loss: 0.6489\nEpoch 644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0580 - val_loss: 0.6440\nEpoch 645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.6392\nEpoch 646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0576 - val_loss: 0.6343\nEpoch 647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0574 - val_loss: 0.6300\nEpoch 648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6258\nEpoch 649/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0571 - val_loss: 0.6210\nEpoch 650/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0569 - val_loss: 0.6163\nEpoch 651/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0567 - val_loss: 0.6116\nEpoch 652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0565 - val_loss: 0.6071\nEpoch 653/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0564 - val_loss: 0.6025\nEpoch 654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0562 - val_loss: 0.5979\nEpoch 655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0560 - val_loss: 0.5934\nEpoch 656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0558 - val_loss: 0.5893\nEpoch 657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5852\nEpoch 658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.5812\nEpoch 659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0553 - val_loss: 0.5772\nEpoch 660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0552 - val_loss: 0.5731\nEpoch 661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.5690\nEpoch 662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0548 - val_loss: 0.5651\nEpoch 663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0547 - val_loss: 0.5608\nEpoch 664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0545 - val_loss: 0.5567\nEpoch 665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0543 - val_loss: 0.5525\nEpoch 666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5482\nEpoch 667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5440\nEpoch 668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0538 - val_loss: 0.5399\nEpoch 669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0537 - val_loss: 0.5359\nEpoch 670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.5321\nEpoch 671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0533 - val_loss: 0.5282\nEpoch 672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0532 - val_loss: 0.5240\nEpoch 673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0530 - val_loss: 0.5203\nEpoch 674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5162\nEpoch 675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.5126\nEpoch 676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5090\nEpoch 677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5054\nEpoch 678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0523 - val_loss: 0.5018\nEpoch 679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.4978\nEpoch 680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.4943\nEpoch 681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0518 - val_loss: 0.4907\nEpoch 682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4869\nEpoch 683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.4836\nEpoch 684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0513 - val_loss: 0.4795\nEpoch 685/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4756\nEpoch 686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.4716\nEpoch 687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4680\nEpoch 688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0507 - val_loss: 0.4640\nEpoch 689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.4605\nEpoch 690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4566\nEpoch 691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.4532\nEpoch 692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4497\nEpoch 693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4460\nEpoch 694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.4424\nEpoch 695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0497 - val_loss: 0.4388\nEpoch 696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0496 - val_loss: 0.4354\nEpoch 697/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0494 - val_loss: 0.4322\nEpoch 698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.4287\nEpoch 699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4255\nEpoch 700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0490 - val_loss: 0.4220\nEpoch 701/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0489 - val_loss: 0.4185\nEpoch 702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4151\nEpoch 703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4117\nEpoch 704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.4084\nEpoch 705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4050\nEpoch 706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.4019\nEpoch 707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.3990\nEpoch 708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.3958\nEpoch 709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3925\nEpoch 710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0477 - val_loss: 0.3891\nEpoch 711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3859\nEpoch 712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3827\nEpoch 713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.3799\nEpoch 714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3766\nEpoch 715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3737\nEpoch 716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3704\nEpoch 717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3674\nEpoch 718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.3644\nEpoch 719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3615\nEpoch 720/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.3587\nEpoch 721/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3559\nEpoch 722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3527\nEpoch 723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0462 - val_loss: 0.3499\nEpoch 724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3468\nEpoch 725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3439\nEpoch 726/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0458 - val_loss: 0.3409\nEpoch 727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3379\nEpoch 728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.3352\nEpoch 729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0455 - val_loss: 0.3323\nEpoch 730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0454 - val_loss: 0.3293\nEpoch 731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3264\nEpoch 732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3236\nEpoch 733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.3209\nEpoch 734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3183\nEpoch 735/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0448 - val_loss: 0.3156\nEpoch 736/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0447 - val_loss: 0.3129\nEpoch 737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3105\nEpoch 738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3079\nEpoch 739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.3053\nEpoch 740/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0443 - val_loss: 0.3026\nEpoch 741/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0442 - val_loss: 0.3001\nEpoch 742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2975\nEpoch 743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2950\nEpoch 744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.2923\nEpoch 745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2897\nEpoch 746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.2873\nEpoch 747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 0.2851\nEpoch 748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2827\nEpoch 749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2804\nEpoch 750/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0433 - val_loss: 0.2780\nEpoch 751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2757\nEpoch 752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2732\nEpoch 753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2710\nEpoch 754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2687\nEpoch 755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.2662\nEpoch 756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0427 - val_loss: 0.2639\nEpoch 757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2617\nEpoch 758/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0425 - val_loss: 0.2595\nEpoch 759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 0.2574\nEpoch 760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2552\nEpoch 761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.2530\nEpoch 762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2508\nEpoch 763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2487\nEpoch 764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2465\nEpoch 765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2444\nEpoch 766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2422\nEpoch 767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2400\nEpoch 768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2380\nEpoch 769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2356\nEpoch 770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2336\nEpoch 771/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0415 - val_loss: 0.2313\nEpoch 772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2293\nEpoch 773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2273\nEpoch 774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2255\nEpoch 775/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0411 - val_loss: 0.2236\nEpoch 776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2217\nEpoch 777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2200\nEpoch 778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2178\nEpoch 779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2158\nEpoch 780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2139\nEpoch 781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2120\nEpoch 782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2101\nEpoch 783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2082\nEpoch 784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2063\nEpoch 785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2045\nEpoch 786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2026\nEpoch 787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2007\nEpoch 788/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0402 - val_loss: 0.1989\nEpoch 789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1971\nEpoch 790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1955\nEpoch 791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1938\nEpoch 792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1920\nEpoch 793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1903\nEpoch 794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1883\nEpoch 795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1866\nEpoch 796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1848\nEpoch 797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1831\nEpoch 798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1816\nEpoch 799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1799\nEpoch 800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1782\nEpoch 801/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0393 - val_loss: 0.1765\nEpoch 802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1750\nEpoch 803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1736\nEpoch 804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1721\nEpoch 805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1708\nEpoch 806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1693\nEpoch 807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1678\nEpoch 808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1663\nEpoch 809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1648\nEpoch 810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1632\nEpoch 811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1617\nEpoch 812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1603\nEpoch 813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1589\nEpoch 814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1575\nEpoch 815/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.1562\nEpoch 816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1547\nEpoch 817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1534\nEpoch 818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1521\nEpoch 819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1507\nEpoch 820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1493\nEpoch 821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1479\nEpoch 822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1466\nEpoch 823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1453\nEpoch 824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1441\nEpoch 825/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0380 - val_loss: 0.1429\nEpoch 826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1416\nEpoch 827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1404\nEpoch 828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1392\nEpoch 829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1378\nEpoch 830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1365\nEpoch 831/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 0.1353\nEpoch 832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1342\nEpoch 833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1330\nEpoch 834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1319\nEpoch 835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1309\nEpoch 836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1297\nEpoch 837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1287\nEpoch 838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1276\nEpoch 839/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.1265\nEpoch 840/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1253\nEpoch 841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1241\nEpoch 842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1229\nEpoch 843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1218\nEpoch 844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1209\nEpoch 845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1198\nEpoch 846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1189\nEpoch 847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1178\nEpoch 848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1168\nEpoch 849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1159\nEpoch 850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1149\nEpoch 851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1139\nEpoch 852/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1129\nEpoch 853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1120\nEpoch 854/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1111\nEpoch 855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1102\nEpoch 856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1093\nEpoch 857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1083\nEpoch 858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1074\nEpoch 859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1066\nEpoch 860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1057\nEpoch 861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1047\nEpoch 862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1038\nEpoch 863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1029\nEpoch 864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1019\nEpoch 865/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0364 - val_loss: 0.1009\nEpoch 866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.1000\nEpoch 867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0992\nEpoch 868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0984\nEpoch 869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0976\nEpoch 870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0968\nEpoch 871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0962\nEpoch 872/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0362 - val_loss: 0.0954\nEpoch 873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0946\nEpoch 874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0939\nEpoch 875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0931\nEpoch 876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0924\nEpoch 877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0916\nEpoch 878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0908\nEpoch 879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0902\nEpoch 880/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0895\nEpoch 881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0889\nEpoch 882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0881\nEpoch 883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0875\nEpoch 884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0868\nEpoch 885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0861\nEpoch 886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0853\nEpoch 887/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0846\nEpoch 888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0839\nEpoch 889/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0833\nEpoch 890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0827\nEpoch 891/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0357 - val_loss: 0.0821\nEpoch 892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0815\nEpoch 893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0810\nEpoch 894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0804\nEpoch 895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0799\nEpoch 896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0794\nEpoch 897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0789\nEpoch 898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0783\nEpoch 899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0778\nEpoch 900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0773\nEpoch 901/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0768\nEpoch 902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0762\nEpoch 903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0756\nEpoch 904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0751\nEpoch 905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0746\nEpoch 906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0740\nEpoch 907/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0735\nEpoch 908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0731\nEpoch 909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0726\nEpoch 910/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0723\nEpoch 911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0719\nEpoch 912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0714\nEpoch 913/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0352 - val_loss: 0.0709\nEpoch 914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0703\nEpoch 915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0699\nEpoch 916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0694\nEpoch 917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0690\nEpoch 918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0684\nEpoch 919/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0680\nEpoch 920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0675\nEpoch 921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0671\nEpoch 922/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0666\nEpoch 923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0663\nEpoch 924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0659\nEpoch 925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0654\nEpoch 926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0649\nEpoch 927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0644\nEpoch 928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0639\nEpoch 929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0634\nEpoch 930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0631\nEpoch 931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0628\nEpoch 932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0624\nEpoch 933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0622\nEpoch 934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0618\nEpoch 935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0614\nEpoch 936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0611\nEpoch 937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0608\nEpoch 938/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 0.0606\nEpoch 939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0603\nEpoch 940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0599\nEpoch 941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0597\nEpoch 942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0594\nEpoch 943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0591\nEpoch 944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0588\nEpoch 945/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.0584\nEpoch 946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0580\nEpoch 947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0577\nEpoch 948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0575\nEpoch 949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0571\nEpoch 950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0569\nEpoch 951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0565\nEpoch 952/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0563\nEpoch 953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0560\nEpoch 954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0557\nEpoch 955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0554\nEpoch 956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0552\nEpoch 957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0549\nEpoch 958/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0347 - val_loss: 0.0547\nEpoch 959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0544\nEpoch 960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0542\nEpoch 961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0540\nEpoch 962/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0537\nEpoch 963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0534\nEpoch 964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0532\nEpoch 965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0529\nEpoch 966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0527\nEpoch 967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0524\nEpoch 968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0522\nEpoch 969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0520\nEpoch 970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0519\nEpoch 971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0517\nEpoch 972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0515\nEpoch 973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0513\nEpoch 974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0511\nEpoch 975/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0509\nEpoch 976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0507\nEpoch 977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0505\nEpoch 978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0503\nEpoch 979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0501\nEpoch 980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0499\nEpoch 981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0497\nEpoch 982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0495\nEpoch 983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0494\nEpoch 984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0492\nEpoch 985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0490\nEpoch 986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0488\nEpoch 987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0486\nEpoch 988/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.0484\nEpoch 989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0482\nEpoch 990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0481\nEpoch 991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0479\nEpoch 992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0478\nEpoch 993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0476\nEpoch 994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0475\nEpoch 995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473\nEpoch 996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0472\nEpoch 997/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0470\nEpoch 998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0469\nEpoch 999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0467\nEpoch 1000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0466\nEpoch 1001/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0464\nEpoch 1002/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463\nEpoch 1003/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462\nEpoch 1004/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0461\nEpoch 1005/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0459\nEpoch 1006/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458\nEpoch 1007/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458\nEpoch 1008/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0457\nEpoch 1009/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0457\nEpoch 1010/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456\nEpoch 1011/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0455\nEpoch 1012/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1013/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1014/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0452\nEpoch 1015/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452\nEpoch 1016/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0450\nEpoch 1017/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448\nEpoch 1018/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446\nEpoch 1019/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0445\nEpoch 1020/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444\nEpoch 1021/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443\nEpoch 1022/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442\nEpoch 1023/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441\nEpoch 1024/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441\nEpoch 1025/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440\nEpoch 1026/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439\nEpoch 1027/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438\nEpoch 1028/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0437\nEpoch 1029/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0436\nEpoch 1030/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1031/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1032/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433\nEpoch 1033/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432\nEpoch 1034/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1035/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1036/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1037/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0429\nEpoch 1038/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1039/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1040/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1041/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1042/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1043/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424\nEpoch 1044/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1045/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1046/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1047/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1048/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1049/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420\nEpoch 1050/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419\nEpoch 1051/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0418\nEpoch 1052/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0418\nEpoch 1053/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0417\nEpoch 1054/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1055/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1056/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0416\nEpoch 1057/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416\nEpoch 1058/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416\nEpoch 1059/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1060/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1061/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1062/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1063/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413\nEpoch 1064/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1065/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1066/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1067/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1068/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1069/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1070/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1071/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1072/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1073/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1074/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1075/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1076/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1077/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1078/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1079/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1080/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1081/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1082/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1083/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1084/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1085/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1086/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1087/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1088/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1089/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1090/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1091/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1092/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1093/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1094/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1095/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1096/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1097/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1098/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1099/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1100/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1101/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1112/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1188/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1199/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1201/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1216/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1233/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1246/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1249/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1264/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1268/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1272/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1273/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1276/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1290/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1297/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1307/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1308/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1337/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1370/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1384/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1401/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1408/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1413/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1418/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1424/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1427/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1429/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1438/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1458/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1463/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1473/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1479/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1490/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1497/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1503/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1505/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1511/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1512/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1516/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1522/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1527/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1528/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1534/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1535/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1543/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1548/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1577/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1579/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1592/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1609/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1610/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1611/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1617/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1634/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1637/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1638/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1642/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1651/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1668/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1669/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1675/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1679/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1683/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1685/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1689/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1697/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1698/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1701/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1703/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1704/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1705/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1706/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1713/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1720/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1721/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1731/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1745/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1759/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1763/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1782/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1792/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1796/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1799/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1801/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1808/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1813/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1815/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1816/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1818/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1819/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1823/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1833/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1840/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1843/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1847/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1852/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1854/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1866/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1872/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1876/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1878/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1879/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1880/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1885/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1887/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1889/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1892/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1901/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1907/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1908/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1909/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1913/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1919/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1922/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1927/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1933/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1938/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1945/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1946/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1952/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1962/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1971/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1974/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1975/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1992/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 2000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\n\n\n<keras.callbacks.History at 0x7f67c81a26b0>\n\n\n\n# \n#%tensorboard --logdir logs --host 0.0.0.0 \n\n\n\n텐서보드: 사용자지정그림 에폭별로 시각화 (2)\n- 중간층의 출력결과를 시각화하고 싶다.\n4. Piecewise-linear regression (15점)\n아래의 모형을 고려하자.\nmodel: \\(y_i=\\begin{cases} x_i +0.3\\epsilon_i & x\\leq 0 \\\\ 3.5x_i +0.3\\epsilon_i & x>0 \\end{cases}\\)\n아래는 위의 모형에서 생성한 샘플이다.\n\n## data \nnp.random.seed(43052)\nN=100\nx= np.linspace(-1,1,N).reshape(N,1)\ny= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x<0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1)\n\n(풀이)\n\ntf.random.set_seed(43055) \nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2))\nnet.add(tf.keras.layers.Activation(tf.nn.relu))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer='sgd',loss='mse')\nnet.fit(x,y,epochs=1,batch_size=100)\n\n1/1 [==============================] - 0s 79ms/step - loss: 2.1414\n\n\n<keras.callbacks.History at 0x7f68849e9420>\n\n\n\nl1,a1,l2 =net.layers\n\n\nfig, (ax1,ax2,ax3) = plt.subplots(1,3) \nfig.set_figwidth(9)\nax1.plot(x,y,'.',alpha=0.2); ax1.plot(x,l1(x),'--'); \nax2.plot(x,y,'.',alpha=0.2); ax2.plot(x,a1(l1(x)),'--'); \nax3.plot(x,y,'.',alpha=0.2); ax3.plot(x,l2(a1(l1(x))),'--r');\n\n\n\n\n\n이런 그림이 100에폭마다 그려졌으면 좋겠다.\n\n- 새로운 클래스를 만들자.\n\nclass PlotMidlayer(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self,epoch,logs): # 입력은 무조건 self, epoch, logs를 써야합니다 --> 이 함수안에 에폭이 끝날때마다 할 동작을 정의한다. \n        if epoch % 100 ==0: \n            fig, (ax1,ax2,ax3) = plt.subplots(1,3) \n            fig.set_figwidth(9)\n            ax1.plot(x,y,'.',alpha=0.2); ax1.plot(x,l1(x),'--'); \n            ax2.plot(x,y,'.',alpha=0.2); ax2.plot(x,a1(l1(x)),'--'); \n            ax3.plot(x,y,'.',alpha=0.2); ax3.plot(x,l2(a1(l1(x))),'--r');            \n            with tf.summary.create_file_writer(logdir).as_default():\n                tf.summary.image(\"적합결과시각화\"+str(epoch), plot_to_image(fig), step=0)\n\n\n!rm -rf logs\ncb1= tf.keras.callbacks.TensorBoard(update_freq='epoch',histogram_freq=100)\ncb2= PlotMidlayer() \nnet.fit(x,y,epochs=1000, batch_size=100,verbose=0 ,callbacks=[cb1,cb2])\n\n<keras.callbacks.History at 0x7f6ef421c280>\n\n\n\n# \n#%tensorboard --logdir logs --host 0.0.0.0"
  },
  {
    "objectID": "post/Lecture/STBD/2022-06-13-기말고사 문제풀이.html#imports",
    "href": "post/Lecture/STBD/2022-06-13-기말고사 문제풀이.html#imports",
    "title": "14. Final term",
    "section": "imports",
    "text": "imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\n%load_ext tensorboard\n\n\n%tensorflow_version 2.x\nimport tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n  raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))\n\nFound GPU at: /device:GPU:0\n\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')"
  },
  {
    "objectID": "post/Lecture/STBD/2022-06-13-기말고사 문제풀이.html#fashion_mnist-dnn-30점",
    "href": "post/Lecture/STBD/2022-06-13-기말고사 문제풀이.html#fashion_mnist-dnn-30점",
    "title": "14. Final term",
    "section": "1. Fashion_mnist, DNN (30점)",
    "text": "1. Fashion_mnist, DNN (30점)\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \"x784\" -> \"node1\"\n    \n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node20\"\n    \"x2\" -> \"node20\"\n    \"..\" -> \"node20\"\n    \"x784\" -> \"node20\"\n\n\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"node1 \"\n    \"node2\" -> \"node1 \"\n    \"...\" -> \"node1 \"\n    \"node20\" -> \"node1 \"\n    \n    \"node1\" -> \"node2 \"\n    \"node2\" -> \"node2 \"\n    \"...\" -> \"node2 \"\n    \"node20\" -> \"node2 \"\n    \n    \"node1\" -> \"... \"\n    \"node2\" -> \"... \"\n    \"...\" -> \"... \"\n    \"node20\" -> \"... \"\n\n    \"node1\" -> \"node30 \"\n    \"node2\" -> \"node30 \"\n    \"...\" -> \"node30 \"\n    \"node20\" -> \"node30 \"\n\n\n    label = \"Layer 2: relu\"\n}\nsubgraph cluster_4{\n    style=filled;\n    color=lightgrey;\n\n    \"node1 \" -> \"y10\"\n    \"node2 \" -> \"y10\"\n    \"... \" -> \"y10\"\n    \"node30 \" -> \"y10\"\n    \n    \"node1 \" -> \"y1\"\n    \"node2 \" -> \"y1\"\n    \"... \" -> \"y1\"\n    \"node30 \" -> \"y1\"\n    \n    \"node1 \" -> \".\"\n    \"node2 \" -> \".\"\n    \"... \" -> \".\"\n    \"node30 \" -> \".\"\n    \n    label = \"Layer 3: softmax\"\n}\n''')\n\n\n\n\n(1) tf.keras.datasets.fashion_mnist.load_data()을 이용하여 fashion_mnist 자료를 불러온 뒤 아래의 네트워크를 이용하여 적합하라. (6점)\n\n평가지표로 accuracy를 이용할 것\nepoch은 10으로 설정할 것\noptimizer는 adam을 이용할 것\n\n\nSolution\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX= tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nX.shape,XX.shape,y.shape, yy.shape\n\n(TensorShape([60000, 28, 28, 1]),\n TensorShape([10000, 28, 28, 1]),\n (60000, 10),\n (10000, 10))\n\n\n\nwith tf.device(\"/device:GPU:0\") :  \n    tf.random.set_seed(202150256)\n\n    net1 = tf.keras.Sequential()\n    net1.add(tf.keras.layers.Flatten())\n    net1.add(tf.keras.layers.Dense(20,activation=\"relu\"))\n    net1.add(tf.keras.layers.Dense(30,activation=\"relu\"))\n    net1.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n    net1.compile(optimizer=\"adam\",loss=tf.losses.categorical_crossentropy,metrics=[\"accuracy\"]) \n    net1.fit(X,y,epochs=10,verbose=1)\n\nEpoch 1/10\n1875/1875 [==============================] - 7s 2ms/step - loss: 2.1564 - accuracy: 0.3564\nEpoch 2/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.3186 - accuracy: 0.4421\nEpoch 3/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 1.2579 - accuracy: 0.4616\nEpoch 4/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.0641 - accuracy: 0.5364\nEpoch 5/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.9958 - accuracy: 0.5610\nEpoch 6/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.9831 - accuracy: 0.5718\nEpoch 7/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.9711 - accuracy: 0.5751\nEpoch 8/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.9348 - accuracy: 0.5914\nEpoch 9/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.9211 - accuracy: 0.5998\nEpoch 10/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.9046 - accuracy: 0.6149\n\n\n(2) (1)에서 적합된 네트워크를 이용하여 test data의 accuracy를 구하라. (6점)\n\n\nSolution\n\nresult1 = net1.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 2ms/step - loss: 0.9063 - accuracy: 0.6474\n\n\n(3) train set에서 20%의 자료를 validation 으로 분리하여 50에폭동안 학습하라. 텐서보드를 이용하여 train accuracy와 validation accuracy를 시각화 하고 결과를 해석하라. 오버피팅이라고 볼 수 있는가? (6점)\n\n\nSolution\n\ntf.random.set_seed(202150256)\n!rm -rf logs\nwith tf.device(\"/device:GPU:0\") :\n    net2 = tf.keras.Sequential()\n    net2.add(tf.keras.layers.Flatten())\n    net2.add(tf.keras.layers.Dense(20,activation=\"relu\"))\n    net2.add(tf.keras.layers.Dense(30,activation=\"relu\"))\n    net2.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n    net2.compile(optimizer=\"adam\",loss=tf.losses.categorical_crossentropy,metrics=[\"accuracy\"]) \n    cb1 = tf.keras.callbacks.TensorBoard()\n    net2.fit(X,y,epochs=50,validation_split=0.2,callbacks=cb1,verbose=0)\n\n\n%tensorboard --logdir logs\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n- 오버피팅이 아니다. accuracy와 loss를 살펴본 결과 에포크가 증가할 수록 train data와 validation data에 대한 accuracy, loss 비슷하며 심지어 validation data 더 뛰어난 성능을 보이는 구간도 존재한다.\n\n!kill 483\n\n(4) (3)에서 적합된 네트워크를 이용하여 test data의 accuracy를 구하라. (2)의 결과와 비교하라. (6점)\n\n\nSolution\n\nresult2 = net2.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.7737\n\n\n\nx = [\"DNN\",\"DNN with val,epoch=50\"]\nacc = [result1[1],result2[1]]\n\nplt.bar(x[0],acc[0],width=0.6)\nplt.bar(x[1],acc[1],width=0.6)\nplt.ylim([0.4,0.8])\nplt.title(\"accuracy\")\n\nText(0.5, 1.0, 'accuracy')\n\n\n\n\n\n\nepoch 수와 validation data를 추가하여 학습한 모델이 test데이터에 대한 정확도가 더 높았다.\n\n(5) 조기종료기능을 이용하여 (3)의 네트워크를 다시 학습하라. 학습결과를 텐서보드를 이용하여 시각화 하라. (6점) - patience=3 으로 설정할 것\n\n\nSolution\n\ntf.random.set_seed(202150256)\n!rm -rf logs\nwith tf.device(\"/device:GPU:0\") :\n    net3 = tf.keras.Sequential()\n    net3.add(tf.keras.layers.Flatten())\n    net3.add(tf.keras.layers.Dense(20,activation=\"relu\"))\n    net3.add(tf.keras.layers.Dense(30,activation=\"relu\"))\n    net3.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n    net3.compile(optimizer=\"adam\",loss=tf.losses.categorical_crossentropy,metrics=[\"accuracy\"]) \n    cb1 = tf.keras.callbacks.TensorBoard()\n    cb2 = tf.keras.callbacks.EarlyStopping(patience=3)\n    net3.fit(X,y,epochs=50,validation_split=0.2,callbacks=[cb1,cb2],verbose=0)\n\n\n%tensorboard --logdir logs\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n!kill 617"
  },
  {
    "objectID": "post/Lecture/STBD/2022-06-13-기말고사 문제풀이.html#fashion_mnist-cnn-30점",
    "href": "post/Lecture/STBD/2022-06-13-기말고사 문제풀이.html#fashion_mnist-cnn-30점",
    "title": "14. Final term",
    "section": "2. Fashion_mnist, CNN (30점)",
    "text": "2. Fashion_mnist, CNN (30점)\n(1) tf.keras.datasets.fashion_mnist.load_data()을 이용하여 fashion_mnist 자료를 불러온 뒤 아래의 네트워크를 이용하여 적합하라. (10점)\n\n이때 n1=6, n2=16, n3=120 으로 설정한다, 드랍아웃비율은 20%로 설정한다.\nnet.summary()를 출력하여 설계결과를 확인하라.\n\n\n\nSolution\n\n!rm -rf logs\nwith tf.device(\"/device:GPU:0\") :\n    net4 = tf.keras.Sequential()\n    net4.add(tf.keras.layers.Conv2D(6,(5,5)))\n    net4.add(tf.keras.layers.MaxPool2D())\n    net4.add(tf.keras.layers.Conv2D(16,(5,5)))\n    net4.add(tf.keras.layers.MaxPool2D())\n    net4.add(tf.keras.layers.Flatten())\n    net4.add(tf.keras.layers.Dense(120,activation=\"relu\"))\n    net4.add(tf.keras.layers.Dropout(0.2))\n    net4.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n    net4.compile(optimizer=\"adam\",loss=tf.losses.categorical_crossentropy,metrics=\"accuracy\")\n    net4.fit(X,y,epochs=1,verbose=0) \n\n\nnet4.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (32, 24, 24, 6)           156       \n                                                                 \n max_pooling2d (MaxPooling2D  (32, 12, 12, 6)          0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (32, 8, 8, 16)            2416      \n                                                                 \n max_pooling2d_1 (MaxPooling  (32, 4, 4, 16)           0         \n 2D)                                                             \n                                                                 \n flatten_3 (Flatten)         (32, 256)                 0         \n                                                                 \n dense_9 (Dense)             (32, 120)                 30840     \n                                                                 \n dropout (Dropout)           (32, 120)                 0         \n                                                                 \n dense_10 (Dense)            (32, 10)                  1210      \n                                                                 \n=================================================================\nTotal params: 34,622\nTrainable params: 34,622\nNon-trainable params: 0\n_________________________________________________________________\n\n\n(2) n1=(6,64,128), n2=(16,256)에 대하여 test set의 loss가 최소화되는 조합을 찾아라. 결과를 텐서보드로 시각화하는 코드를 작성하라. (20점) - epoc은 3회로 한정한다. - validation_split은 0.2로 설정한다.\n\n\nSolution\n\nfrom tensorboard.plugins.hparams import api as hp\n\n\n!rm -rf logs\nN1= [6,64,128]\nN2= [16,256]\nfor n1 in N1 :\n    for n2 in N2 : \n      logdir =  \"logs/gc_{}_{}\".format(n1,n2) \n      with tf.summary.create_file_writer(logdir).as_default() :\n           with tf.device(\"/device:GPU:0\") :\n              net5 = tf.keras.Sequential()\n              net5.add(tf.keras.layers.Conv2D(n1,(5,5)))\n              net5.add(tf.keras.layers.MaxPool2D())\n              net5.add(tf.keras.layers.Conv2D(n2,(5,5)))\n              net5.add(tf.keras.layers.MaxPool2D())\n              net5.add(tf.keras.layers.Flatten())\n              net5.add(tf.keras.layers.Dense(120,activation=\"relu\"))\n              net5.add(tf.keras.layers.Dropout(0.2))\n              net5.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n              net5.compile(optimizer=\"adam\",loss=tf.losses.categorical_crossentropy,metrics=\"accuracy\")\n              cb3 = hp.KerasCallback(logdir,{\"n1\":n1,\"n2\":n2})\n              net5.fit(X,y,epochs=3, validation_split=0.2,callbacks=cb3,verbose=0,batch_size=200)\n              _result = net5.evaluate(XX,yy)\n              tf.summary.scalar('loss', _result[0], step=1)\n\n313/313 [==============================] - 1s 3ms/step - loss: 0.5943 - accuracy: 0.7761\n313/313 [==============================] - 1s 3ms/step - loss: 0.3916 - accuracy: 0.8671\n313/313 [==============================] - 1s 3ms/step - loss: 0.4765 - accuracy: 0.8311\n313/313 [==============================] - 1s 3ms/step - loss: 0.4310 - accuracy: 0.8517\n313/313 [==============================] - 1s 3ms/step - loss: 0.5023 - accuracy: 0.8182\n313/313 [==============================] - 1s 3ms/step - loss: 0.7565 - accuracy: 0.7443\n\n\n\n%tensorboard --logdir logs\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\\(n_1 = 6, n_2 = 256\\)인 경우가 가장 test loss가 작았다."
  },
  {
    "objectID": "post/Lecture/STBD/2022-06-13-기말고사 문제풀이.html#cifar10-30점",
    "href": "post/Lecture/STBD/2022-06-13-기말고사 문제풀이.html#cifar10-30점",
    "title": "14. Final term",
    "section": "3. CIFAR10 (30점)",
    "text": "3. CIFAR10 (30점)\ntf.keras.datasets.cifar10.load_data()을 이용하여 CIFAR10을 불러온 뒤 적당한 네트워크를 사용하여 적합하라.\n\n결과를 텐서보드로 시각화할 필요는 없다.\n자유롭게 모형을 설계하여 적합하라.\ntest set의 accuracy가 70%이상인 경우만 정답으로 인정한다.\n\n\nSolution\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170500096/170498071 [==============================] - 11s 0us/step\n170508288/170498071 [==============================] - 11s 0us/step\n\n\n\nX = tf.constant(x_train.reshape(-1,32,32,3),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,32,32,3),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n\ntf.random.set_seed(202150256)\n!rm -rf logs\nwith tf.device(\"/device:GPU:0\") :\n    net6 = tf.keras.Sequential()\n    net6.add(tf.keras.layers.Conv2D(128, (2, 2), activation='relu'))\n    net6.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    net6.add(tf.keras.layers.Conv2D(128, (2, 2), activation='relu'))\n    net6.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    net6.add(tf.keras.layers.Conv2D(256, (2, 2), activation='relu'))\n    net6.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    net6.add(tf.keras.layers.Conv2D(256, (2, 2), activation='relu'))\n    net6.add(tf.keras.layers.Flatten())\n    net6.add(tf.keras.layers.Dense(512, activation='relu'))\n    net6.add(tf.keras.layers.Dropout(0.5))\n    net6.add(tf.keras.layers.Dense(10, activation='softmax'))\n    net6.compile(optimizer=\"adam\",loss=tf.losses.categorical_crossentropy,metrics=\"accuracy\")\n    net6.fit(X,y,epochs=10,verbose=0,batch_size=200)\n\n\nresult6 = net6.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 4ms/step - loss: 0.8870 - accuracy: 0.7130"
  },
  {
    "objectID": "post/Lecture/STBD/2022-06-13-기말고사 문제풀이.html#다음을-읽고-물음에-답하라.-10점",
    "href": "post/Lecture/STBD/2022-06-13-기말고사 문제풀이.html#다음을-읽고-물음에-답하라.-10점",
    "title": "14. Final term",
    "section": "4. 다음을 읽고 물음에 답하라. (10점)",
    "text": "4. 다음을 읽고 물음에 답하라. (10점)\n(1) (1,128,128,3)의 shape을 가진 텐서가 tf.keras.layers.Conv2D(5,(2,2))으로 만들어진 커널을 통과할시 나오는 shape은?\n\nSolution\n(1,127,127,5)\n(2) (1,24,24,16)의 shape을 가진 텐서가 tf.keras.layers.Flatten()을 통과할때 나오는 텐서의 shape은?\n\n\nSolution\n(1,9216)\n(3)-(5)\n아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i\\]\n여기에서 \\(t=(t_1,\\dots,t_{1000})=\\) np.linspace(0,5,1000) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.linspace(0,5,1000)\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.1)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2, 3, 1, 0, 0, 0.5 이다)\n(3) 모형에 대한 설명 중 옳은 것을 모두 골라라.\n(하영) 이 모형의 경우 MSEloss를 최소화하는 \\(\\hat{\\beta}_0,\\dots,\\hat{\\beta}_5\\)를 구하는것은 최대우도함수를 최대화하는 \\(\\hat{\\beta}_0,\\dots,\\hat{\\beta}_5\\)를 구하는 것과 같다.\n(재인) 하영의 말이 옳은 이유는 오차항이 정규분포를 따른다는 가정이 있기 때문이다.\n(서연) 이 모형에서 적절한 학습률이 선택되더라도 경사하강법을 이용하면 MSEloss를 최소화하는 \\(\\hat{\\beta}_0,\\dots,\\hat{\\beta}_5\\)를 종종 구할 수 없는 문제가 생긴다. 왜냐하면 손실함수가 convex하지 않아서 local minimum에 빠질 위험이 있기 때문이다.\n(규빈) 만약에 경사하강법 대신 확률적 경사하강법을 쓴다면 local minimum을 언제나 탈출 할 수 있다. 따라서 서연이 언급한 문제점은 생기지 않는다.\n\n\nSolution\n하영,재인,서연\n(4) 다음은 아래 모형을 학습한 결과이다. 옳게 해석한 것을 모두 고르시오.\n\ny = y.reshape(1000,1)\nx1 = np.cos(t) \nx2 = np.cos(2*t)\nx3 = np.cos(3*t)\nx4 = np.cos(4*t)\nx5 = np.cos(5*t)\nX = tf.stack([x1,x2,x3,x4,x5],axis=1)\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(loss='mse',optimizer='adam')\nnet.fit(X,y,epochs=500,batch_size=100, validation_split=0.45,verbose=0) \n\n\nplt.plot(y,'.',alpha=0.1)\nplt.plot(net(X),'--')\n\n(재인) 처음 550개의 데이터만 학습하고 이후의 450개의 데이터는 학습하지 않고 validation으로 이용하였다.\n(서연) validation에서의 적합결과가 좋지 않다.\n(규빈) validation의 적합결과가 좋지 않기 때문에 오버피팅을 의심할 수 있다. 따라서 만약에 네트워크에 드랍아웃층을 추가한다면 오버피팅을 방지하는 효과가 있어 validation의 loss가 줄어들 것이다.\n(하영) 이 모형의 경우 더 많은 epoch으로 학습한다면 train loss와 validation loss를 둘 다 줄일 수 있다.\n\n\nSolution\n재인. 서연, 하영\n(5) 다음을 잘 읽고 참 거짓을 판별하라. - Convolution은 선형변환이다. - CNN을 이용하면 언제나 손실함수를 MSEloss로 선택해야 한다. - CNN은 adam optimizer를 통해서만 최적화할 수 있다. - 이미지자료는 CNN을 이용하여서만 분석할 수 있으며 DNN으로는 분석불가능하다. - CNN은 칼라이미지일 경우에만 적용가능하다.\n\n\nSolution\nO, X, X, X, X"
  },
  {
    "objectID": "post/Lecture/STDV/2022-04-22-5wk.html",
    "href": "post/Lecture/STDV/2022-04-22-5wk.html",
    "title": "04. qqplot, 찰스미나도의 도표",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\nnp.random.seed(202150256)\nx = np.random.normal(loc = 2, scale = 1.5, size =1000)\ny = stats.t.rvs(df=10,size =1000)/np.sqrt(10/8)*1.5 + 2\n\n\nnp.std(y),np.mean(y)\n\n(1.4739981187361109, 1.9504733080884165)\n\n\n- 우리가 관측한 \\(x_1\\dots x_{1000}\\) 이 \\(N(2,1.5^2)\\) 에서 나온 샘플인지 궁금하다.\n- 아이디어\n\n관측한 값을 순서대로 나열하여 \\(x_{(1)}\\dots x_{(1000)}\\)을 만든다.\n파이썬이나 R로 \\(N (2, 1.5^2)\\) 에서 1000개의 정규분포를 생성. 그리고 순서대로 나열하여 \\(\\tilde x_{(1)}\\dots \\tilde x_{(1000)}\\) 을 만는다.\n\\(x_{(1)} \\approx \\tilde x_{(1)} \\dots x_{(1000)} \\approx \\tilde x_{(1000)}\\) 이면 우리가 관측한 \\(x\\)는 정규분포일 것이다.\n\n\n값을 정렬\n\n\nx.sort()\n\n\n편리한 계산을 위해 표준화 과정을 거침\n\n\nxx = (x-np.mean(x))/np.std(x,ddof=1)\n\n\n가장 작은 관측값 2개를 추출\n\n\nxx[:2]\n\narray([-3.60370086, -2.76081303])\n\n\n\n아래의 값을 확인해 보니 얼추 비슷하게 나오는 것 같다.\n\n\nstats.norm.ppf(0.001), stats.norm.ppf(0.002)\n\n(-3.090232306167813, -2.878161739095483)\n\n\n\n이제 이론적인 값을 생성\n\n\nm = [i / 1000 for i in range(1000)]\n\n\nq = [stats.norm.ppf(m[i]) for i in range(len(m))]\n\n\n\\(xx \\approx q\\) 임을 확인하기 위해서 \\((q,q)\\) 그래프와 \\((q, xx)\\) 그래프를 그려서 겹쳐보자.\n\n\nplt.plot(q,xx,\"o\")\nplt.plot(q,q,\"-\")\n\n\n\n\n- 해석 : 점들이 주황색선 근처에 모여있을 수록 정규분포에 가깝다고 볼 수 있다.\n- 아래와 같은 방법으로도 그릴 수 있음.\n\n_ = stats.probplot(x,plot=plt)\n\n\n\n\n- 우리가 그린 그림과 비교\n\nfig,axes = plt.subplots(1,2,figsize=(10,3))\nax1, ax2 = axes\n\nax1.plot(q,xx,\"o\")\nax1.plot(q,q,\"-\")\n_ = stats.probplot(xx,plot=ax2)\n\n\n\n\n- 그림이 살짝 다른 이유는 우리가 한 것 처럼 \\(m = (\\frac {1}{1000}\\dots \\frac {1000}{1000})\\) 이 아닌 약간 보정한 값을 계산하기 때문임.\n- 이제 우리가 처음에 생성한 정규분포와 t분포의 qqplot을 그려 비교해보자.\n\nfig, axes =plt.subplots(1,2, figsize = (10,5))\nax1,ax2=axes\n_ = stats.probplot(x,plot=ax1)\n_ = stats.probplot(y,plot=ax2)\nax1.set_title(\"normal dist\")\nax2.set_title(\"t dist\")\n\nText(0.5, 1.0, 't dist')\n\n\n\n\n\n\nt분포 : 푸른점들이 대체로 붉은선 놓여 있는듯 하지만 양끝단에서는 그렇지 않다.(중앙부근은 정규분포와 비슷하지만, 꼬리부분은 정규분포와 확실히 다르다.)\n\n완쪽 꼬리 : 이론적으로 나와야 할 값보다 더 작은 값이 실제로 관측됨\n오른쪽 꼬리 : 이론적으로 나와야할 값보다 더 큰 값이 실제로 관측됨\n따라서 \\(y\\)의 분포는 정규분포보다 더 두꺼운 꼬리를 가진다고 해석할 수 있다.\n\n\n\n\n\nimport seaborn as sns\n\n\nfig, axes = plt.subplots(2,3,figsize=(10,5))\n\n(ax1, ax2, ax3), (ax4, ax5, ax6) = axes\n\nsns.boxplot(x,ax=ax1,orient=\"h\")\nsns.histplot(x,ax=ax2)\n_ = stats.probplot(x,plot=ax3)\nax1.set_title(\"distribution of x\")\nax2.set_title(\"distribution of x\")\n\nsns.boxplot(y,ax=ax4,orient=\"h\")\nsns.histplot(y,ax=ax5)\n_ = stats.probplot(y,plot=ax6)\nax4.set_title(\"distribution of y\")\nax5.set_title(\"distribution of y\")\n\nfig.tight_layout()"
  },
  {
    "objectID": "post/Lecture/STDV/2022-04-22-5wk.html#방법-1",
    "href": "post/Lecture/STDV/2022-04-22-5wk.html#방법-1",
    "title": "04. qqplot, 찰스미나도의 도표",
    "section": "방법 1",
    "text": "방법 1\n\nq = []\n\nfor i in range(len(m)) :\n    q = q + [stats.norm.ppf(m[i])]\n\n\nq[:5]\n\n[-inf,\n -3.090232306167813,\n -2.878161739095483,\n -2.7477813854449926,\n -2.6520698079021954]"
  },
  {
    "objectID": "post/Lecture/STDV/2022-04-22-5wk.html#방법-2",
    "href": "post/Lecture/STDV/2022-04-22-5wk.html#방법-2",
    "title": "04. qqplot, 찰스미나도의 도표",
    "section": "방법 2",
    "text": "방법 2\n\nq = [stats.norm.ppf(m[i]) for i in range(len(m))]\n\n\nq[:5]\n\n[-inf,\n -3.090232306167813,\n -2.878161739095483,\n -2.7477813854449926,\n -2.6520698079021954]"
  },
  {
    "objectID": "post/Lecture/STDV/2022-04-22-5wk.html#방법-3",
    "href": "post/Lecture/STDV/2022-04-22-5wk.html#방법-3",
    "title": "04. qqplot, 찰스미나도의 도표",
    "section": "방법 3",
    "text": "방법 3\n\nq = list(map(stats.norm.ppf,m))\nq[:5]\n\n[-inf,\n -3.090232306167813,\n -2.878161739095483,\n -2.7477813854449926,\n -2.6520698079021954]"
  },
  {
    "objectID": "post/Lecture/STDV/2022-04-22-5wk.html#방법-4",
    "href": "post/Lecture/STDV/2022-04-22-5wk.html#방법-4",
    "title": "04. qqplot, 찰스미나도의 도표",
    "section": "방법 4",
    "text": "방법 4\n\nlist(stats.norm.ppf(m))[:5]\n\n[-inf,\n -3.090232306167813,\n -2.878161739095483,\n -2.7477813854449926,\n -2.6520698079021954]"
  },
  {
    "objectID": "post/Lecture/STDV/2022-04-22-5wk.html#예제-1",
    "href": "post/Lecture/STDV/2022-04-22-5wk.html#예제-1",
    "title": "04. qqplot, 찰스미나도의 도표",
    "section": "예제 1",
    "text": "예제 1\n\nf = lambda x,y,z : x + y + z\n\n\nf(2,3,4)\n\n9"
  },
  {
    "objectID": "post/Lecture/STDV/2022-04-22-5wk.html#예제-2",
    "href": "post/Lecture/STDV/2022-04-22-5wk.html#예제-2",
    "title": "04. qqplot, 찰스미나도의 도표",
    "section": "예제 2",
    "text": "예제 2\n\nx = lambda a=\"fee\", b=\"file\", c=\"foe\" : a + b + c\n\n\nx\n\n<function __main__.<lambda>(a='fee', b='file', c='foe')>\n\n\n\nx(\"wee\")\n\n'weefilefoe'"
  },
  {
    "objectID": "post/Lecture/STDV/2022-04-22-5wk.html#예제-3",
    "href": "post/Lecture/STDV/2022-04-22-5wk.html#예제-3",
    "title": "04. qqplot, 찰스미나도의 도표",
    "section": "예제 3",
    "text": "예제 3\n\nl = [lambda x : x**2, lambda x: x**3, lambda x : x**4]\n\n\nfor f in l :\n    print(f(2))\n\n4\n8\n16"
  },
  {
    "objectID": "post/Lecture/STDV/2022-04-22-5wk.html#예제-4",
    "href": "post/Lecture/STDV/2022-04-22-5wk.html#예제-4",
    "title": "04. qqplot, 찰스미나도의 도표",
    "section": "예제 4",
    "text": "예제 4\n\ndct = {\"f1\": lambda x : x+1, \"f2\" : lambda x : x+22, \"f3\": lambda x : x + 33}\n\n\ndct[\"f1\"](1), dct[\"f2\"](1), dct[\"f3\"](1)\n\n(2, 23, 34)"
  },
  {
    "objectID": "post/Lecture/STDV/2022-04-22-5wk.html#예제-5",
    "href": "post/Lecture/STDV/2022-04-22-5wk.html#예제-5",
    "title": "04. qqplot, 찰스미나도의 도표",
    "section": "예제 5",
    "text": "예제 5\n\nlower = lambda x,y : x if x<y else y\n\n\nlower(\"a\",\"b\")\n\n'a'"
  },
  {
    "objectID": "post/Lecture/STDV/2022-04-22-5wk.html#예제-6",
    "href": "post/Lecture/STDV/2022-04-22-5wk.html#예제-6",
    "title": "04. qqplot, 찰스미나도의 도표",
    "section": "예제 6",
    "text": "예제 6\n\ndef action(x) :\n    return (lambda y : x + y)\n\n\nact = action (99)\nact2 = action(98)\n\n\nact(2), act2(2)\n\n(101, 100)"
  },
  {
    "objectID": "post/Lecture/STDV/2022-04-22-5wk.html#예제-7",
    "href": "post/Lecture/STDV/2022-04-22-5wk.html#예제-7",
    "title": "04. qqplot, 찰스미나도의 도표",
    "section": "예제 7",
    "text": "예제 7\n\naction = lambda x : (lambda y : x+y)\n\n\nact = action (99)\nact2 = action(98)\n\nact(2), act2(2)\n\n(101, 100)"
  },
  {
    "objectID": "post/Lecture/STDV/2022-04-22-5wk.html#map",
    "href": "post/Lecture/STDV/2022-04-22-5wk.html#map",
    "title": "04. qqplot, 찰스미나도의 도표",
    "section": "map",
    "text": "map\n\n예제 1\n\ndef inc(x) : return x+1\n\n\nlist(map(inc, [1,2,3,4]))\n\n[2, 3, 4, 5]\n\n\n- 예제 1의 변형 (with lambda)\n\nlist(map(lambda x: x+1,[2,3,4,5]))\n\n[3, 4, 5, 6]\n\n\n\n\n에제 2\n- map과 list comprehension\n\n함수선언 : x라는 문자열에 “X”가 포함되어있는지 확인하는 함수\n\n\nf = lambda x : \"X\" in x\n\n(map)\n\nlist(map(f,[\"X1\",\"X2\",\"X3\"]))\n\n[True, True, True]\n\n\n(list comprehension)\n\n[f(x) for x in [\"X1\",\"X2\",\"X3\"]]\n\n[True, True, True]\n\n\n\n\n예제 3\n-두 개의 입력을 받는함수의 경우 map과 list comprehension 비교\n\npow(2,5)\n\n32\n\n\n(map)\n\nlist(map(pow,[2,3],[0,1]))\n\n[1, 3]\n\n\n(list comprehension)\n\n[pow(x,y) for x,y in zip([2,3],[0,1])]\n\n[1, 3]\n\n\n\n\n예제 4\nmap은 하나의 함수에 다향한 입력인 경우 사용가능하다.\n\nl = lambda x: x+1,lambda x : x+2, lambda x: x+3\n\n\nlist(map(l,[100,200,300]))\n\nTypeError: ignored\n\n\nlist comporehension은 다양한 함수, 다양한 입력이 가능함\n\n[l[i](x) for i,x in zip([0,1,2],[100,200,300])]\n\n[101, 202, 303]"
  },
  {
    "objectID": "post/Lecture/STDV/2022-04-22-5wk.html#summary",
    "href": "post/Lecture/STDV/2022-04-22-5wk.html#summary",
    "title": "04. qqplot, 찰스미나도의 도표",
    "section": "Summary",
    "text": "Summary\n- 종합 : 리스트컴프리헨션과 비교하면 map은 반복인덱스를 쓰지 않는 장점이 있는 반면 다양한 함수를 한꺼번에 적용하지 못하는 제약이 있음"
  },
  {
    "objectID": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html",
    "href": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html",
    "title": "01. intro",
    "section": "",
    "text": "- 해당 자료는 전북대학교 통계학과 고급데이터시각화 수업을 바탕으로 작성하였습니다.\n- 자료출처"
  },
  {
    "objectID": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html#예제-1",
    "href": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html#예제-1",
    "title": "01. intro",
    "section": "예제 1",
    "text": "예제 1\n- 평균은 좋은 추정값인가?\n- 전북고등학교에서 통계학을 수업하는 A선생님과 B선생님의 있다. A선생님에게서 수업을 들을 학생들의 평균은 79.1이고 B선생님에게서 수업을 들은 학생들의 평균은 78.3이다.\n\ny1=[75,75,76,76,77,77,79,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들\ny2=[76,76,77,77,78,78,80,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들 \n\n\nnp.mean(y1), np.mean(y2)\n\n(79.1, 78.3)\n\n\n- 의사결정 : A선생님에게 배운 학생들의 실력이 평균적으로 더 좋을 것이다.\n\n평균은 A반이 더 높다\n그런데 98점을 받은 학생이 A반에 포함되어서 A반의 천체평균이 높게 나온것!\n98점을 받은 학생을 제외하고 전체적으로는 B반 학생들이 시험을 더 잘봄"
  },
  {
    "objectID": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html#matplotlib",
    "href": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html#matplotlib",
    "title": "01. intro",
    "section": "matplotlib",
    "text": "matplotlib\n\nplt.boxplot([y1,y2])\nplt.legend([\"A\",\"B\"])\n\n<matplotlib.legend.Legend at 0x1f0c61c5fd0>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html#plotly",
    "href": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html#plotly",
    "title": "01. intro",
    "section": "plotly",
    "text": "plotly\n\nimport plotly.express as px\nimport pandas as pd\n\n\ny1=[75,75,76,76,77,77,79,79,79,98] # A선생님에게 통계학을 배운 학생의 점수들\ny2=[76,76,77,77,78,78,80,80,80,81] # B선생님에게 통계학을 배운 학생의 점수들 \n\n\ndf= pd.DataFrame({'score':y1+y2,'class':['A']*len(y1) + ['B']*len(y2)})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      score\n      class\n    \n  \n  \n    \n      0\n      75\n      A\n    \n    \n      1\n      75\n      A\n    \n    \n      2\n      76\n      A\n    \n    \n      3\n      76\n      A\n    \n    \n      4\n      77\n      A\n    \n  \n\n\n\n\n\npx.box(df,x=\"class\", y= \"score\")"
  },
  {
    "objectID": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html#matplotlib-1",
    "href": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html#matplotlib-1",
    "title": "01. intro",
    "section": "matplotlib",
    "text": "matplotlib\n\nplt.hist([y1,y2],bins=50);\nplt.legend([\"A\",\"B\"])\n\n<matplotlib.legend.Legend at 0x1f0cc1a9790>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html#seaborn",
    "href": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html#seaborn",
    "title": "01. intro",
    "section": "seaborn",
    "text": "seaborn\n\nimport seaborn as sns \n\n\ndf=pd.DataFrame({'score':np.concatenate([y1,y2]), 'class':['A']*len(y1)+['B']*len(y2)})\ndf.head()\n\n\n\n\n\n  \n    \n      \n      score\n      class\n    \n  \n  \n    \n      0\n      0.944267\n      A\n    \n    \n      1\n      -0.393919\n      A\n    \n    \n      2\n      -0.854069\n      A\n    \n    \n      3\n      -0.732953\n      A\n    \n    \n      4\n      1.329115\n      A\n    \n  \n\n\n\n\n\nsns.histplot(df,x=\"score\",hue=\"class\")\n\n<AxesSubplot:xlabel='score', ylabel='Count'>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html#plotnine",
    "href": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html#plotnine",
    "title": "01. intro",
    "section": "plotnine",
    "text": "plotnine\n\nfrom plotnine import *\n\n\nggplot(df) + geom_histogram(aes(x=\"score\",fill=\"class\"),position=\"identity\",alpha=0.5) \n\nC:\\Users\\rkdcj\\anaconda3\\lib\\site-packages\\plotnine\\stats\\stat_bin.py:95: PlotnineWarning: 'stat_bin()' using 'bins = 79'. Pick better value with 'binwidth'.\n\n\n\n\n\n<ggplot: (133361723299)>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html#plotly-1",
    "href": "post/Lecture/STDV/2023-02-24-boxplot, histogram.html#plotly-1",
    "title": "01. intro",
    "section": "plotly",
    "text": "plotly\n\nimport plotly.figure_factory as ff\n\n\nhist_data = [y1,y2]\n\ngroup_labels = [\"A\", \"B\"]\n\nff.create_distplot(hist_data, group_labels,bin_size=0.2,show_rug=False)"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-08-02wk.html",
    "href": "post/Lecture/STDV/2023-04-08-02wk.html",
    "title": "02. Histogram equalization",
    "section": "",
    "text": "- 흑백이미지\n\n차원 : 세로픽셀수 x 가로픽셀수\n값 : 0 ~ 255 (값이 클수록 흰색)\n\n- 칼라이미지\n\n차원 : 세로픽셀수 x 가로픽셀수 x 3\n값 : 0 ~ 255 (값이 클수록 진한 빨강, 진판 파랑, 진한 녹색) \\(\\to\\) 빛의 3원색을 이용하여 모든 색깔을 표현할 수 있다.\n\n\n#!pip install opencv-python\n\n\n\n\n#!pip install wget\n\n\nimport cv2 as cv\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport wget\n\n\n\n\n\nurl1 =  \"https://upload.wikimedia.org/wikipedia/commons/0/08/Unequalized_Hawkes_Bay_NZ.jpg\"\nwget.download(url1)\n\n100% [............................................................................] 110895 / 110895\n\n\n'Unequalized_Hawkes_Bay_NZ.jpg'\n\n\n\nimg = cv.imread(\"Unequalized_Hawkes_Bay_NZ.jpg\",0)\n\n\nplt.imshow(img,cmap=\"gray\",vmin=0,vmax=255)\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x1a502a0c4c0>\n\n\n\n\n\n\n이미지 자료는 사실 0 ~ 255 사이에 어떠한 숫자들이 포함된 매트릭스일 뿐이다.\n\n\nimg\n\narray([[127, 145, 149, ..., 168, 167, 166],\n       [165, 152, 143, ..., 168, 169, 168],\n       [171, 145, 140, ..., 156, 154, 151],\n       ...,\n       [147, 132, 134, ..., 146, 145, 144],\n       [146, 130, 132, ..., 146, 145, 144],\n       [145, 128, 129, ..., 146, 145, 144]], dtype=uint8)\n\n\n\n이미지라고 믿었던 img는 그냥 numpy matrix 였다.\n위의 매트릭스에 있는 숫자들을 색깔로 표현하여 값이 클수록 하얗게, 값이 작을수록 검게 그린다.\n극단적으로 0은 검은색, 255는 흰색이다.\n\n- 이미지가 넘파이 매트릭스일 뿐이라는 것을 판다스를 활용하여 시각화한다면 더 잘 이해할 수 있다.\n\nplt.imshow(img[200:300,400:500],cmap=\"gray\", vmin = 0, vmax = 255)\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x1a502b424c0>\n\n\n\n\n\n\ndf = pd.DataFrame(img)\n\n\ndf.iloc[200:300, 400:500].style.set_properties(**{\"font-size\" : \"10pt\"}).background_gradient(\"gray\",vmin=0, vmax=255)\n\n\n\n\n  \n    \n       \n      400\n      401\n      402\n      403\n      404\n      405\n      406\n      407\n      408\n      409\n      410\n      411\n      412\n      413\n      414\n      415\n      416\n      417\n      418\n      419\n      420\n      421\n      422\n      423\n      424\n      425\n      426\n      427\n      428\n      429\n      430\n      431\n      432\n      433\n      434\n      435\n      436\n      437\n      438\n      439\n      440\n      441\n      442\n      443\n      444\n      445\n      446\n      447\n      448\n      449\n      450\n      451\n      452\n      453\n      454\n      455\n      456\n      457\n      458\n      459\n      460\n      461\n      462\n      463\n      464\n      465\n      466\n      467\n      468\n      469\n      470\n      471\n      472\n      473\n      474\n      475\n      476\n      477\n      478\n      479\n      480\n      481\n      482\n      483\n      484\n      485\n      486\n      487\n      488\n      489\n      490\n      491\n      492\n      493\n      494\n      495\n      496\n      497\n      498\n      499\n    \n  \n  \n    \n      200\n      155\n      155\n      151\n      149\n      152\n      152\n      151\n      152\n      152\n      152\n      152\n      153\n      154\n      155\n      157\n      158\n      164\n      157\n      150\n      148\n      146\n      143\n      139\n      138\n      138\n      140\n      142\n      140\n      138\n      137\n      137\n      137\n      131\n      142\n      147\n      143\n      143\n      150\n      153\n      151\n      149\n      148\n      146\n      143\n      141\n      141\n      142\n      143\n      143\n      143\n      140\n      138\n      137\n      138\n      136\n      133\n      133\n      134\n      133\n      131\n      128\n      127\n      129\n      131\n      128\n      129\n      132\n      129\n      130\n      129\n      125\n      131\n      128\n      128\n      128\n      130\n      132\n      133\n      132\n      131\n      137\n      136\n      135\n      138\n      140\n      133\n      127\n      129\n      132\n      130\n      130\n      132\n      135\n      135\n      134\n      134\n      137\n      138\n      137\n      134\n    \n    \n      201\n      139\n      148\n      152\n      149\n      145\n      142\n      147\n      156\n      150\n      152\n      154\n      154\n      153\n      153\n      155\n      157\n      158\n      159\n      155\n      149\n      144\n      142\n      139\n      136\n      138\n      139\n      140\n      139\n      137\n      136\n      136\n      137\n      141\n      140\n      139\n      141\n      147\n      152\n      152\n      150\n      151\n      150\n      149\n      147\n      145\n      143\n      143\n      143\n      141\n      142\n      142\n      141\n      141\n      141\n      138\n      134\n      133\n      132\n      132\n      131\n      130\n      131\n      133\n      135\n      128\n      128\n      132\n      130\n      132\n      131\n      126\n      132\n      129\n      129\n      128\n      129\n      130\n      132\n      132\n      133\n      137\n      135\n      132\n      133\n      134\n      129\n      126\n      130\n      129\n      129\n      131\n      133\n      134\n      133\n      134\n      136\n      136\n      137\n      137\n      135\n    \n    \n      202\n      138\n      145\n      149\n      149\n      149\n      144\n      138\n      139\n      144\n      148\n      153\n      155\n      153\n      152\n      153\n      155\n      153\n      158\n      157\n      147\n      140\n      140\n      139\n      136\n      137\n      135\n      135\n      137\n      137\n      135\n      134\n      136\n      142\n      138\n      139\n      145\n      147\n      144\n      145\n      151\n      150\n      150\n      151\n      150\n      149\n      147\n      145\n      143\n      140\n      140\n      140\n      140\n      140\n      141\n      139\n      135\n      133\n      131\n      129\n      128\n      129\n      130\n      132\n      133\n      128\n      128\n      131\n      131\n      134\n      133\n      128\n      133\n      132\n      132\n      131\n      130\n      129\n      131\n      133\n      135\n      133\n      132\n      129\n      129\n      132\n      131\n      133\n      140\n      135\n      136\n      137\n      137\n      134\n      131\n      131\n      134\n      135\n      136\n      136\n      136\n    \n    \n      203\n      152\n      151\n      148\n      150\n      156\n      152\n      141\n      134\n      137\n      141\n      147\n      151\n      152\n      152\n      153\n      154\n      152\n      155\n      153\n      144\n      137\n      138\n      139\n      139\n      134\n      130\n      130\n      134\n      135\n      133\n      133\n      136\n      133\n      132\n      137\n      146\n      147\n      141\n      141\n      147\n      146\n      147\n      148\n      149\n      150\n      149\n      147\n      145\n      143\n      142\n      140\n      137\n      137\n      139\n      139\n      137\n      136\n      133\n      129\n      127\n      127\n      128\n      128\n      128\n      129\n      127\n      130\n      129\n      133\n      134\n      130\n      136\n      136\n      136\n      134\n      132\n      130\n      131\n      133\n      135\n      132\n      133\n      132\n      133\n      137\n      137\n      140\n      147\n      144\n      143\n      142\n      140\n      137\n      133\n      134\n      136\n      136\n      136\n      135\n      135\n    \n    \n      204\n      155\n      156\n      153\n      151\n      153\n      153\n      152\n      155\n      141\n      141\n      142\n      144\n      147\n      149\n      151\n      151\n      153\n      152\n      149\n      145\n      140\n      137\n      138\n      140\n      133\n      130\n      129\n      131\n      132\n      131\n      132\n      134\n      130\n      127\n      130\n      139\n      147\n      147\n      144\n      141\n      144\n      144\n      144\n      146\n      147\n      147\n      147\n      146\n      147\n      146\n      143\n      140\n      139\n      140\n      139\n      137\n      138\n      134\n      130\n      127\n      128\n      129\n      129\n      128\n      129\n      127\n      129\n      127\n      131\n      134\n      131\n      139\n      139\n      138\n      136\n      133\n      131\n      130\n      132\n      134\n      137\n      140\n      141\n      141\n      143\n      140\n      139\n      143\n      145\n      141\n      138\n      139\n      140\n      141\n      142\n      144\n      138\n      136\n      134\n      133\n    \n    \n      205\n      145\n      151\n      152\n      153\n      154\n      152\n      152\n      156\n      151\n      147\n      141\n      139\n      141\n      144\n      147\n      148\n      151\n      149\n      149\n      148\n      145\n      138\n      135\n      136\n      134\n      133\n      131\n      130\n      130\n      130\n      131\n      132\n      134\n      133\n      134\n      136\n      139\n      142\n      143\n      143\n      145\n      144\n      142\n      143\n      144\n      145\n      144\n      144\n      146\n      146\n      145\n      143\n      141\n      140\n      137\n      133\n      134\n      131\n      127\n      126\n      128\n      129\n      130\n      129\n      129\n      128\n      130\n      127\n      130\n      133\n      132\n      141\n      140\n      138\n      136\n      133\n      130\n      130\n      132\n      133\n      139\n      143\n      143\n      143\n      145\n      141\n      137\n      139\n      144\n      138\n      134\n      136\n      141\n      144\n      145\n      145\n      141\n      137\n      134\n      134\n    \n    \n      206\n      143\n      147\n      149\n      151\n      155\n      152\n      146\n      145\n      154\n      151\n      146\n      143\n      142\n      143\n      144\n      145\n      144\n      146\n      147\n      146\n      142\n      138\n      136\n      134\n      133\n      135\n      134\n      130\n      129\n      132\n      134\n      134\n      135\n      136\n      137\n      136\n      133\n      135\n      142\n      150\n      147\n      145\n      143\n      143\n      144\n      145\n      144\n      143\n      145\n      145\n      144\n      142\n      141\n      140\n      136\n      132\n      130\n      128\n      126\n      126\n      127\n      128\n      129\n      129\n      129\n      129\n      131\n      129\n      130\n      133\n      133\n      143\n      143\n      139\n      135\n      132\n      131\n      132\n      134\n      136\n      138\n      141\n      140\n      139\n      142\n      141\n      139\n      140\n      143\n      137\n      133\n      136\n      141\n      143\n      143\n      142\n      143\n      139\n      136\n      136\n    \n    \n      207\n      148\n      151\n      150\n      149\n      151\n      150\n      149\n      151\n      150\n      150\n      151\n      150\n      148\n      145\n      144\n      144\n      137\n      144\n      145\n      139\n      135\n      137\n      138\n      135\n      131\n      136\n      136\n      130\n      129\n      135\n      138\n      137\n      134\n      129\n      128\n      132\n      137\n      140\n      146\n      153\n      147\n      145\n      144\n      144\n      145\n      146\n      145\n      143\n      147\n      147\n      144\n      142\n      141\n      141\n      139\n      136\n      132\n      131\n      129\n      128\n      128\n      128\n      128\n      129\n      128\n      130\n      133\n      131\n      131\n      133\n      133\n      144\n      145\n      141\n      135\n      132\n      132\n      135\n      138\n      140\n      141\n      141\n      137\n      135\n      139\n      140\n      139\n      140\n      139\n      135\n      133\n      136\n      141\n      143\n      143\n      143\n      144\n      140\n      137\n      138\n    \n    \n      208\n      150\n      148\n      151\n      145\n      143\n      147\n      148\n      153\n      158\n      153\n      150\n      152\n      149\n      143\n      141\n      144\n      145\n      145\n      144\n      142\n      140\n      137\n      133\n      131\n      130\n      133\n      131\n      130\n      137\n      143\n      140\n      135\n      141\n      139\n      130\n      134\n      147\n      142\n      135\n      144\n      147\n      143\n      146\n      148\n      144\n      146\n      149\n      144\n      148\n      144\n      140\n      141\n      143\n      144\n      141\n      137\n      132\n      128\n      126\n      127\n      128\n      127\n      125\n      125\n      128\n      127\n      129\n      135\n      138\n      138\n      138\n      139\n      138\n      133\n      129\n      130\n      133\n      135\n      138\n      140\n      140\n      136\n      135\n      137\n      138\n      136\n      136\n      139\n      139\n      137\n      137\n      138\n      139\n      140\n      141\n      143\n      141\n      144\n      143\n      143\n    \n    \n      209\n      153\n      147\n      147\n      143\n      144\n      147\n      144\n      146\n      150\n      149\n      150\n      152\n      153\n      151\n      146\n      143\n      142\n      142\n      141\n      139\n      137\n      135\n      134\n      134\n      133\n      131\n      130\n      129\n      130\n      132\n      134\n      133\n      136\n      139\n      137\n      130\n      132\n      146\n      151\n      142\n      145\n      146\n      153\n      156\n      154\n      156\n      156\n      149\n      145\n      148\n      149\n      148\n      143\n      140\n      139\n      140\n      143\n      137\n      130\n      125\n      122\n      122\n      126\n      129\n      128\n      128\n      129\n      132\n      137\n      139\n      139\n      137\n      133\n      130\n      129\n      133\n      138\n      139\n      137\n      135\n      140\n      136\n      134\n      136\n      136\n      133\n      132\n      135\n      137\n      136\n      136\n      137\n      136\n      136\n      137\n      140\n      137\n      140\n      139\n      139\n    \n    \n      210\n      154\n      147\n      147\n      145\n      145\n      145\n      141\n      145\n      143\n      145\n      147\n      149\n      153\n      155\n      151\n      144\n      144\n      143\n      141\n      139\n      137\n      136\n      137\n      137\n      136\n      130\n      132\n      135\n      129\n      129\n      133\n      133\n      137\n      128\n      138\n      144\n      135\n      141\n      149\n      139\n      141\n      142\n      146\n      146\n      145\n      152\n      155\n      150\n      145\n      145\n      144\n      144\n      144\n      143\n      141\n      140\n      142\n      139\n      135\n      132\n      127\n      124\n      125\n      127\n      126\n      126\n      127\n      128\n      133\n      137\n      136\n      133\n      132\n      130\n      131\n      137\n      143\n      142\n      138\n      134\n      138\n      135\n      134\n      136\n      136\n      134\n      134\n      135\n      135\n      135\n      136\n      137\n      135\n      134\n      136\n      140\n      137\n      140\n      139\n      138\n    \n    \n      211\n      153\n      149\n      152\n      151\n      148\n      146\n      143\n      150\n      143\n      143\n      143\n      145\n      148\n      150\n      150\n      148\n      149\n      146\n      142\n      140\n      139\n      139\n      138\n      137\n      137\n      131\n      137\n      143\n      137\n      139\n      141\n      134\n      145\n      129\n      145\n      167\n      157\n      144\n      144\n      140\n      139\n      142\n      143\n      140\n      141\n      149\n      153\n      151\n      148\n      145\n      141\n      141\n      142\n      143\n      142\n      141\n      139\n      138\n      139\n      139\n      135\n      129\n      124\n      122\n      125\n      124\n      124\n      127\n      130\n      132\n      133\n      134\n      133\n      131\n      132\n      137\n      140\n      140\n      137\n      135\n      138\n      135\n      133\n      134\n      134\n      133\n      134\n      135\n      133\n      134\n      136\n      136\n      135\n      134\n      136\n      140\n      139\n      141\n      139\n      139\n    \n    \n      212\n      156\n      151\n      153\n      151\n      150\n      149\n      145\n      149\n      147\n      144\n      144\n      146\n      146\n      143\n      144\n      149\n      149\n      144\n      139\n      138\n      139\n      139\n      136\n      132\n      136\n      132\n      138\n      142\n      141\n      147\n      147\n      132\n      137\n      136\n      145\n      155\n      154\n      147\n      141\n      135\n      133\n      140\n      142\n      141\n      144\n      148\n      148\n      147\n      149\n      151\n      152\n      149\n      143\n      140\n      141\n      144\n      146\n      142\n      140\n      139\n      137\n      133\n      129\n      127\n      127\n      123\n      124\n      128\n      130\n      127\n      130\n      135\n      130\n      131\n      133\n      136\n      136\n      134\n      134\n      134\n      138\n      135\n      132\n      131\n      131\n      131\n      131\n      133\n      131\n      132\n      134\n      135\n      133\n      131\n      134\n      137\n      135\n      139\n      137\n      136\n    \n    \n      213\n      160\n      152\n      151\n      149\n      151\n      153\n      145\n      142\n      148\n      145\n      146\n      150\n      148\n      141\n      141\n      147\n      146\n      142\n      137\n      137\n      139\n      139\n      135\n      131\n      134\n      132\n      134\n      134\n      136\n      144\n      143\n      131\n      128\n      132\n      130\n      129\n      134\n      138\n      137\n      137\n      132\n      138\n      138\n      137\n      141\n      143\n      144\n      147\n      148\n      150\n      151\n      152\n      150\n      148\n      145\n      144\n      145\n      142\n      140\n      142\n      143\n      141\n      139\n      137\n      129\n      125\n      123\n      126\n      127\n      125\n      127\n      131\n      129\n      131\n      135\n      138\n      137\n      134\n      133\n      134\n      135\n      133\n      132\n      132\n      133\n      135\n      136\n      138\n      135\n      135\n      136\n      136\n      134\n      132\n      132\n      135\n      133\n      137\n      136\n      134\n    \n    \n      214\n      160\n      157\n      157\n      150\n      150\n      153\n      146\n      143\n      146\n      146\n      147\n      149\n      147\n      144\n      143\n      144\n      144\n      142\n      138\n      137\n      138\n      137\n      136\n      134\n      132\n      133\n      133\n      132\n      132\n      135\n      136\n      133\n      131\n      130\n      129\n      131\n      133\n      135\n      139\n      146\n      140\n      142\n      138\n      137\n      141\n      141\n      142\n      152\n      147\n      144\n      143\n      147\n      153\n      155\n      151\n      147\n      142\n      142\n      143\n      146\n      148\n      146\n      143\n      142\n      134\n      132\n      127\n      124\n      126\n      129\n      128\n      124\n      130\n      131\n      134\n      137\n      137\n      134\n      133\n      134\n      135\n      134\n      134\n      135\n      137\n      140\n      141\n      142\n      138\n      137\n      138\n      138\n      136\n      134\n      133\n      135\n      134\n      140\n      139\n      135\n    \n    \n      215\n      157\n      162\n      167\n      156\n      148\n      151\n      149\n      151\n      145\n      145\n      145\n      143\n      144\n      146\n      146\n      143\n      143\n      142\n      139\n      137\n      135\n      134\n      134\n      135\n      130\n      134\n      136\n      136\n      134\n      130\n      131\n      138\n      128\n      130\n      138\n      141\n      136\n      138\n      143\n      140\n      137\n      140\n      138\n      140\n      143\n      137\n      134\n      143\n      147\n      147\n      146\n      147\n      148\n      150\n      153\n      155\n      149\n      147\n      146\n      146\n      143\n      141\n      141\n      143\n      141\n      141\n      133\n      124\n      128\n      137\n      134\n      122\n      129\n      128\n      129\n      131\n      132\n      132\n      132\n      133\n      138\n      138\n      138\n      138\n      138\n      139\n      138\n      137\n      137\n      136\n      136\n      137\n      136\n      134\n      133\n      134\n      136\n      142\n      141\n      136\n    \n    \n      216\n      156\n      146\n      149\n      163\n      166\n      153\n      146\n      152\n      145\n      144\n      143\n      144\n      145\n      145\n      143\n      141\n      139\n      141\n      136\n      133\n      138\n      136\n      130\n      131\n      131\n      132\n      136\n      137\n      134\n      130\n      130\n      132\n      132\n      135\n      126\n      136\n      141\n      137\n      146\n      141\n      135\n      137\n      138\n      141\n      143\n      138\n      137\n      143\n      146\n      146\n      146\n      148\n      149\n      145\n      143\n      148\n      147\n      147\n      146\n      144\n      142\n      141\n      140\n      140\n      142\n      143\n      143\n      144\n      144\n      143\n      140\n      138\n      123\n      126\n      137\n      137\n      141\n      131\n      133\n      132\n      138\n      141\n      137\n      135\n      138\n      135\n      132\n      137\n      137\n      135\n      135\n      137\n      136\n      132\n      131\n      134\n      134\n      137\n      139\n      137\n    \n    \n      217\n      155\n      156\n      158\n      158\n      159\n      158\n      154\n      148\n      150\n      146\n      142\n      143\n      145\n      145\n      142\n      138\n      136\n      135\n      131\n      132\n      137\n      138\n      135\n      133\n      136\n      131\n      129\n      130\n      132\n      131\n      128\n      127\n      143\n      144\n      130\n      132\n      134\n      132\n      144\n      141\n      134\n      138\n      138\n      136\n      137\n      136\n      135\n      138\n      132\n      144\n      151\n      146\n      144\n      150\n      152\n      147\n      142\n      143\n      143\n      143\n      143\n      142\n      142\n      142\n      144\n      144\n      144\n      145\n      146\n      146\n      146\n      145\n      136\n      133\n      132\n      135\n      136\n      136\n      133\n      133\n      133\n      134\n      133\n      135\n      139\n      138\n      135\n      135\n      134\n      133\n      134\n      136\n      134\n      129\n      129\n      132\n      133\n      134\n      135\n      135\n    \n    \n      218\n      153\n      161\n      162\n      156\n      157\n      164\n      164\n      156\n      152\n      150\n      148\n      145\n      142\n      141\n      141\n      141\n      141\n      135\n      132\n      133\n      135\n      136\n      135\n      130\n      135\n      130\n      127\n      128\n      130\n      129\n      127\n      124\n      139\n      145\n      136\n      135\n      134\n      132\n      139\n      132\n      132\n      139\n      137\n      132\n      134\n      136\n      137\n      137\n      135\n      141\n      154\n      158\n      153\n      152\n      151\n      144\n      142\n      142\n      143\n      142\n      142\n      140\n      139\n      138\n      144\n      143\n      142\n      142\n      142\n      143\n      144\n      145\n      144\n      138\n      127\n      134\n      132\n      138\n      129\n      127\n      131\n      127\n      129\n      135\n      138\n      137\n      135\n      130\n      130\n      131\n      132\n      134\n      131\n      127\n      127\n      131\n      134\n      134\n      134\n      135\n    \n    \n      219\n      153\n      155\n      157\n      158\n      159\n      161\n      164\n      165\n      157\n      155\n      151\n      147\n      144\n      142\n      142\n      142\n      148\n      139\n      137\n      136\n      132\n      131\n      132\n      126\n      129\n      130\n      131\n      131\n      130\n      128\n      126\n      126\n      127\n      137\n      136\n      136\n      135\n      134\n      136\n      127\n      131\n      136\n      136\n      134\n      137\n      139\n      138\n      140\n      133\n      133\n      146\n      155\n      150\n      150\n      152\n      145\n      143\n      142\n      141\n      140\n      139\n      138\n      136\n      135\n      142\n      141\n      140\n      138\n      137\n      137\n      137\n      138\n      152\n      147\n      139\n      143\n      142\n      144\n      135\n      130\n      131\n      125\n      128\n      135\n      134\n      134\n      134\n      129\n      131\n      132\n      134\n      134\n      131\n      128\n      130\n      135\n      133\n      133\n      134\n      136\n    \n    \n      220\n      151\n      151\n      153\n      157\n      157\n      156\n      158\n      162\n      164\n      159\n      153\n      150\n      150\n      148\n      144\n      140\n      145\n      137\n      137\n      137\n      130\n      129\n      131\n      129\n      127\n      129\n      131\n      131\n      130\n      129\n      128\n      128\n      125\n      134\n      133\n      130\n      129\n      132\n      136\n      130\n      130\n      133\n      134\n      137\n      140\n      136\n      134\n      137\n      137\n      146\n      156\n      151\n      140\n      147\n      154\n      144\n      141\n      139\n      138\n      137\n      137\n      137\n      137\n      136\n      138\n      138\n      139\n      138\n      137\n      136\n      136\n      137\n      147\n      146\n      149\n      145\n      148\n      142\n      141\n      136\n      136\n      130\n      133\n      138\n      133\n      131\n      134\n      133\n      133\n      135\n      136\n      136\n      133\n      132\n      135\n      140\n      131\n      132\n      133\n      134\n    \n    \n      221\n      147\n      151\n      154\n      154\n      155\n      158\n      160\n      159\n      161\n      164\n      164\n      160\n      152\n      146\n      144\n      145\n      142\n      137\n      137\n      136\n      130\n      127\n      129\n      131\n      131\n      129\n      127\n      126\n      128\n      130\n      130\n      127\n      126\n      132\n      133\n      129\n      128\n      132\n      134\n      130\n      131\n      131\n      131\n      135\n      137\n      131\n      127\n      131\n      136\n      148\n      162\n      161\n      146\n      141\n      144\n      142\n      142\n      140\n      136\n      135\n      135\n      135\n      134\n      134\n      131\n      134\n      137\n      138\n      138\n      138\n      139\n      139\n      139\n      140\n      147\n      140\n      143\n      137\n      142\n      141\n      147\n      143\n      144\n      144\n      136\n      129\n      128\n      131\n      131\n      133\n      135\n      135\n      134\n      134\n      137\n      140\n      134\n      134\n      134\n      132\n    \n    \n      222\n      146\n      151\n      153\n      152\n      154\n      159\n      160\n      158\n      156\n      162\n      168\n      166\n      157\n      150\n      148\n      150\n      145\n      142\n      138\n      136\n      133\n      127\n      125\n      129\n      131\n      131\n      129\n      126\n      126\n      128\n      128\n      127\n      125\n      129\n      134\n      132\n      132\n      134\n      130\n      127\n      130\n      132\n      131\n      131\n      133\n      132\n      129\n      131\n      130\n      133\n      144\n      155\n      152\n      140\n      138\n      145\n      143\n      140\n      137\n      135\n      134\n      134\n      132\n      130\n      129\n      132\n      137\n      139\n      139\n      140\n      140\n      141\n      141\n      144\n      147\n      143\n      142\n      142\n      145\n      147\n      149\n      149\n      149\n      148\n      142\n      130\n      124\n      129\n      129\n      132\n      135\n      137\n      138\n      138\n      139\n      140\n      138\n      136\n      133\n      131\n    \n    \n      223\n      151\n      149\n      149\n      151\n      152\n      151\n      152\n      153\n      159\n      157\n      157\n      162\n      167\n      165\n      155\n      145\n      147\n      146\n      139\n      136\n      137\n      130\n      124\n      130\n      128\n      134\n      136\n      132\n      126\n      124\n      126\n      127\n      125\n      126\n      131\n      129\n      131\n      134\n      131\n      130\n      129\n      133\n      131\n      128\n      132\n      136\n      136\n      136\n      154\n      150\n      143\n      141\n      145\n      146\n      142\n      140\n      138\n      136\n      135\n      134\n      135\n      136\n      134\n      133\n      133\n      136\n      140\n      142\n      141\n      141\n      142\n      142\n      141\n      146\n      143\n      145\n      138\n      144\n      141\n      143\n      140\n      144\n      145\n      148\n      147\n      135\n      127\n      133\n      131\n      134\n      139\n      142\n      144\n      145\n      144\n      142\n      139\n      135\n      130\n      129\n    \n    \n      224\n      152\n      151\n      150\n      150\n      150\n      151\n      152\n      152\n      151\n      155\n      160\n      161\n      161\n      161\n      164\n      167\n      146\n      149\n      145\n      136\n      131\n      132\n      131\n      126\n      128\n      126\n      126\n      128\n      130\n      130\n      127\n      124\n      125\n      126\n      127\n      128\n      130\n      133\n      133\n      131\n      128\n      129\n      130\n      133\n      135\n      132\n      131\n      137\n      144\n      143\n      151\n      151\n      146\n      139\n      133\n      137\n      137\n      135\n      136\n      136\n      132\n      131\n      134\n      133\n      133\n      138\n      140\n      139\n      138\n      134\n      133\n      139\n      141\n      142\n      143\n      144\n      142\n      140\n      139\n      141\n      139\n      146\n      146\n      140\n      139\n      138\n      132\n      127\n      129\n      137\n      145\n      145\n      144\n      145\n      147\n      149\n      141\n      127\n      124\n      130\n    \n    \n      225\n      151\n      149\n      147\n      146\n      146\n      147\n      149\n      149\n      151\n      153\n      155\n      156\n      157\n      159\n      161\n      164\n      162\n      151\n      144\n      143\n      140\n      132\n      128\n      129\n      132\n      129\n      127\n      127\n      128\n      129\n      128\n      127\n      126\n      128\n      129\n      130\n      131\n      132\n      130\n      127\n      131\n      131\n      130\n      132\n      134\n      130\n      129\n      134\n      130\n      134\n      146\n      147\n      143\n      138\n      132\n      136\n      133\n      132\n      134\n      135\n      130\n      130\n      132\n      130\n      133\n      136\n      136\n      136\n      138\n      136\n      136\n      142\n      144\n      143\n      143\n      143\n      141\n      139\n      138\n      139\n      142\n      143\n      141\n      141\n      142\n      136\n      128\n      127\n      137\n      145\n      148\n      143\n      141\n      143\n      144\n      140\n      141\n      132\n      128\n      128\n    \n    \n      226\n      148\n      147\n      146\n      146\n      148\n      150\n      151\n      151\n      152\n      151\n      150\n      151\n      153\n      156\n      159\n      160\n      164\n      162\n      155\n      147\n      142\n      140\n      135\n      130\n      125\n      125\n      126\n      127\n      129\n      129\n      128\n      127\n      131\n      132\n      131\n      131\n      132\n      132\n      130\n      127\n      132\n      132\n      129\n      129\n      131\n      128\n      127\n      132\n      131\n      138\n      149\n      146\n      140\n      137\n      132\n      133\n      131\n      131\n      134\n      135\n      131\n      131\n      132\n      130\n      131\n      134\n      133\n      133\n      135\n      135\n      139\n      147\n      147\n      144\n      142\n      142\n      141\n      138\n      137\n      138\n      144\n      141\n      138\n      140\n      139\n      130\n      127\n      135\n      139\n      146\n      146\n      139\n      138\n      144\n      143\n      136\n      141\n      137\n      132\n      126\n    \n    \n      227\n      148\n      149\n      149\n      150\n      151\n      151\n      150\n      149\n      150\n      150\n      150\n      151\n      153\n      155\n      156\n      157\n      157\n      165\n      166\n      156\n      148\n      146\n      143\n      137\n      126\n      126\n      126\n      126\n      126\n      126\n      127\n      128\n      133\n      133\n      131\n      129\n      130\n      131\n      131\n      129\n      129\n      130\n      128\n      127\n      128\n      126\n      128\n      134\n      144\n      148\n      153\n      145\n      139\n      140\n      136\n      135\n      133\n      132\n      135\n      136\n      132\n      132\n      134\n      132\n      129\n      132\n      133\n      132\n      133\n      133\n      137\n      147\n      147\n      144\n      140\n      140\n      139\n      138\n      136\n      136\n      143\n      141\n      139\n      137\n      132\n      125\n      131\n      147\n      142\n      144\n      142\n      138\n      140\n      145\n      145\n      140\n      141\n      139\n      135\n      130\n    \n    \n      228\n      153\n      152\n      150\n      149\n      148\n      146\n      145\n      143\n      147\n      149\n      151\n      153\n      153\n      153\n      154\n      155\n      154\n      157\n      163\n      166\n      160\n      150\n      146\n      148\n      140\n      136\n      130\n      125\n      122\n      124\n      127\n      130\n      130\n      130\n      129\n      129\n      130\n      131\n      130\n      128\n      126\n      129\n      129\n      127\n      127\n      127\n      132\n      140\n      146\n      147\n      150\n      143\n      142\n      147\n      142\n      139\n      134\n      133\n      135\n      135\n      131\n      131\n      133\n      132\n      131\n      131\n      130\n      131\n      136\n      136\n      135\n      138\n      143\n      139\n      137\n      137\n      137\n      136\n      135\n      135\n      140\n      141\n      138\n      134\n      132\n      131\n      138\n      150\n      148\n      144\n      141\n      142\n      144\n      143\n      142\n      142\n      143\n      140\n      141\n      141\n    \n    \n      229\n      155\n      152\n      149\n      147\n      146\n      146\n      147\n      147\n      144\n      146\n      149\n      151\n      150\n      150\n      151\n      153\n      153\n      155\n      158\n      162\n      163\n      161\n      154\n      149\n      141\n      138\n      134\n      131\n      129\n      128\n      127\n      127\n      127\n      129\n      131\n      133\n      134\n      135\n      132\n      128\n      127\n      131\n      130\n      128\n      128\n      130\n      134\n      143\n      142\n      144\n      148\n      146\n      148\n      151\n      143\n      137\n      132\n      131\n      132\n      132\n      128\n      128\n      131\n      130\n      135\n      132\n      128\n      132\n      142\n      142\n      134\n      130\n      137\n      135\n      135\n      136\n      136\n      135\n      134\n      135\n      137\n      138\n      134\n      132\n      136\n      140\n      140\n      143\n      145\n      140\n      139\n      143\n      143\n      138\n      137\n      140\n      143\n      141\n      144\n      148\n    \n    \n      230\n      154\n      153\n      151\n      149\n      149\n      149\n      149\n      149\n      145\n      145\n      145\n      145\n      147\n      149\n      151\n      152\n      151\n      157\n      157\n      153\n      157\n      166\n      163\n      154\n      141\n      139\n      137\n      136\n      136\n      134\n      131\n      128\n      129\n      131\n      132\n      133\n      134\n      135\n      133\n      129\n      131\n      132\n      129\n      126\n      128\n      130\n      132\n      137\n      139\n      140\n      147\n      147\n      148\n      148\n      138\n      133\n      131\n      130\n      132\n      132\n      128\n      129\n      131\n      129\n      134\n      134\n      134\n      139\n      145\n      142\n      134\n      132\n      135\n      135\n      137\n      139\n      139\n      137\n      137\n      138\n      134\n      136\n      134\n      133\n      137\n      138\n      138\n      140\n      138\n      137\n      139\n      141\n      140\n      136\n      134\n      136\n      138\n      138\n      142\n      144\n    \n    \n      231\n      156\n      155\n      155\n      154\n      152\n      148\n      145\n      142\n      148\n      144\n      141\n      141\n      145\n      149\n      152\n      153\n      155\n      153\n      152\n      154\n      156\n      158\n      163\n      168\n      154\n      147\n      139\n      135\n      134\n      136\n      136\n      136\n      130\n      130\n      128\n      127\n      129\n      131\n      131\n      129\n      134\n      133\n      127\n      123\n      127\n      128\n      128\n      130\n      135\n      135\n      143\n      143\n      143\n      142\n      133\n      130\n      131\n      130\n      134\n      135\n      131\n      131\n      133\n      131\n      129\n      137\n      144\n      146\n      144\n      138\n      135\n      140\n      136\n      137\n      141\n      143\n      143\n      140\n      140\n      141\n      131\n      136\n      136\n      135\n      132\n      130\n      133\n      143\n      139\n      143\n      145\n      143\n      139\n      135\n      133\n      131\n      131\n      134\n      138\n      137\n    \n    \n      232\n      156\n      156\n      155\n      153\n      153\n      152\n      150\n      146\n      147\n      146\n      146\n      147\n      147\n      147\n      146\n      145\n      150\n      152\n      153\n      153\n      154\n      156\n      159\n      161\n      170\n      158\n      142\n      133\n      134\n      137\n      136\n      132\n      132\n      134\n      135\n      132\n      129\n      127\n      129\n      131\n      136\n      135\n      134\n      132\n      132\n      133\n      134\n      135\n      135\n      135\n      136\n      139\n      141\n      139\n      136\n      135\n      134\n      131\n      129\n      134\n      140\n      136\n      130\n      132\n      132\n      135\n      142\n      144\n      141\n      143\n      146\n      143\n      147\n      138\n      138\n      142\n      140\n      136\n      135\n      135\n      132\n      129\n      129\n      131\n      131\n      130\n      134\n      140\n      143\n      139\n      136\n      137\n      137\n      135\n      132\n      132\n      133\n      134\n      134\n      133\n    \n    \n      233\n      153\n      157\n      160\n      158\n      153\n      151\n      150\n      149\n      146\n      148\n      149\n      149\n      147\n      145\n      143\n      142\n      144\n      146\n      148\n      150\n      151\n      154\n      156\n      158\n      163\n      165\n      161\n      150\n      139\n      134\n      136\n      139\n      137\n      136\n      135\n      136\n      135\n      133\n      129\n      126\n      129\n      132\n      135\n      139\n      142\n      143\n      144\n      143\n      144\n      141\n      139\n      137\n      134\n      131\n      131\n      133\n      130\n      137\n      140\n      138\n      132\n      126\n      130\n      142\n      131\n      130\n      134\n      139\n      137\n      137\n      140\n      141\n      140\n      134\n      136\n      141\n      140\n      139\n      138\n      137\n      133\n      130\n      129\n      130\n      129\n      127\n      130\n      135\n      141\n      140\n      141\n      142\n      140\n      136\n      134\n      134\n      129\n      130\n      132\n      133\n    \n    \n      234\n      157\n      155\n      154\n      153\n      155\n      156\n      152\n      147\n      144\n      147\n      149\n      148\n      145\n      144\n      144\n      145\n      144\n      145\n      146\n      149\n      152\n      154\n      155\n      155\n      156\n      161\n      166\n      163\n      156\n      148\n      141\n      136\n      137\n      135\n      135\n      136\n      138\n      137\n      134\n      131\n      128\n      128\n      129\n      130\n      133\n      135\n      138\n      140\n      146\n      146\n      148\n      148\n      145\n      141\n      140\n      141\n      131\n      137\n      139\n      137\n      134\n      130\n      130\n      136\n      137\n      131\n      134\n      140\n      139\n      136\n      140\n      144\n      143\n      139\n      139\n      138\n      135\n      135\n      134\n      130\n      132\n      130\n      129\n      131\n      130\n      129\n      131\n      135\n      135\n      138\n      141\n      142\n      138\n      133\n      132\n      134\n      132\n      133\n      133\n      133\n    \n    \n      235\n      159\n      156\n      151\n      151\n      155\n      158\n      155\n      150\n      148\n      149\n      148\n      146\n      144\n      144\n      145\n      147\n      147\n      147\n      146\n      148\n      151\n      153\n      153\n      153\n      154\n      155\n      157\n      161\n      165\n      163\n      153\n      143\n      140\n      139\n      137\n      136\n      135\n      136\n      138\n      139\n      137\n      135\n      132\n      130\n      130\n      131\n      133\n      135\n      135\n      136\n      139\n      142\n      143\n      141\n      141\n      142\n      137\n      136\n      131\n      129\n      135\n      136\n      132\n      129\n      134\n      129\n      130\n      135\n      133\n      132\n      135\n      137\n      140\n      138\n      138\n      135\n      133\n      138\n      141\n      137\n      131\n      129\n      129\n      130\n      130\n      129\n      130\n      132\n      132\n      135\n      138\n      137\n      133\n      130\n      131\n      133\n      137\n      137\n      135\n      134\n    \n    \n      236\n      155\n      159\n      161\n      159\n      155\n      156\n      158\n      160\n      156\n      155\n      153\n      150\n      148\n      147\n      145\n      144\n      144\n      143\n      142\n      143\n      145\n      148\n      150\n      150\n      155\n      156\n      156\n      155\n      157\n      160\n      163\n      164\n      154\n      149\n      143\n      139\n      139\n      140\n      141\n      141\n      138\n      139\n      139\n      139\n      137\n      135\n      133\n      132\n      134\n      129\n      125\n      126\n      128\n      130\n      132\n      134\n      137\n      137\n      131\n      126\n      128\n      131\n      133\n      136\n      130\n      129\n      132\n      132\n      129\n      130\n      132\n      129\n      139\n      139\n      139\n      137\n      135\n      140\n      142\n      137\n      131\n      129\n      128\n      128\n      127\n      125\n      125\n      125\n      136\n      137\n      135\n      133\n      131\n      131\n      132\n      133\n      133\n      134\n      135\n      136\n    \n    \n      237\n      154\n      158\n      162\n      161\n      160\n      159\n      158\n      157\n      157\n      157\n      156\n      156\n      156\n      154\n      150\n      147\n      141\n      140\n      139\n      140\n      143\n      146\n      149\n      150\n      151\n      154\n      155\n      153\n      153\n      156\n      160\n      162\n      162\n      154\n      147\n      146\n      149\n      151\n      148\n      143\n      140\n      140\n      139\n      138\n      136\n      134\n      132\n      131\n      137\n      131\n      126\n      127\n      129\n      130\n      130\n      130\n      133\n      135\n      132\n      131\n      133\n      133\n      131\n      134\n      131\n      134\n      138\n      137\n      133\n      134\n      135\n      131\n      137\n      137\n      139\n      138\n      134\n      135\n      134\n      128\n      131\n      129\n      128\n      128\n      128\n      129\n      129\n      129\n      139\n      138\n      135\n      131\n      130\n      132\n      133\n      132\n      128\n      132\n      135\n      138\n    \n    \n      238\n      160\n      158\n      157\n      157\n      161\n      162\n      158\n      153\n      154\n      155\n      156\n      157\n      157\n      157\n      156\n      156\n      146\n      146\n      145\n      144\n      144\n      146\n      148\n      149\n      148\n      150\n      152\n      153\n      155\n      156\n      154\n      151\n      159\n      156\n      152\n      151\n      151\n      152\n      151\n      150\n      150\n      148\n      145\n      142\n      140\n      138\n      138\n      138\n      132\n      130\n      130\n      132\n      134\n      133\n      131\n      130\n      132\n      130\n      129\n      134\n      139\n      135\n      129\n      128\n      128\n      130\n      136\n      137\n      132\n      129\n      131\n      130\n      130\n      130\n      134\n      136\n      135\n      136\n      137\n      134\n      130\n      130\n      129\n      130\n      133\n      135\n      136\n      136\n      139\n      139\n      136\n      132\n      131\n      131\n      131\n      130\n      129\n      132\n      135\n      136\n    \n    \n      239\n      163\n      163\n      161\n      158\n      157\n      158\n      159\n      159\n      155\n      156\n      156\n      154\n      153\n      154\n      157\n      161\n      153\n      153\n      152\n      149\n      146\n      144\n      144\n      145\n      149\n      152\n      153\n      151\n      150\n      152\n      155\n      157\n      156\n      159\n      159\n      152\n      145\n      142\n      147\n      153\n      147\n      148\n      150\n      150\n      149\n      146\n      142\n      140\n      137\n      135\n      133\n      134\n      135\n      135\n      136\n      138\n      135\n      131\n      127\n      129\n      132\n      128\n      128\n      134\n      128\n      127\n      134\n      139\n      133\n      126\n      129\n      133\n      140\n      137\n      138\n      138\n      135\n      135\n      137\n      137\n      132\n      131\n      130\n      131\n      133\n      136\n      137\n      137\n      139\n      140\n      139\n      135\n      133\n      132\n      131\n      129\n      129\n      130\n      131\n      132\n    \n    \n      240\n      158\n      159\n      162\n      165\n      165\n      163\n      158\n      155\n      166\n      156\n      150\n      153\n      155\n      153\n      153\n      157\n      156\n      153\n      150\n      149\n      150\n      150\n      148\n      146\n      144\n      148\n      151\n      151\n      150\n      150\n      153\n      157\n      162\n      154\n      153\n      158\n      156\n      146\n      141\n      143\n      148\n      147\n      148\n      150\n      150\n      147\n      145\n      146\n      140\n      134\n      131\n      133\n      136\n      136\n      135\n      135\n      141\n      133\n      128\n      129\n      130\n      129\n      128\n      129\n      124\n      132\n      136\n      140\n      132\n      123\n      132\n      140\n      142\n      135\n      140\n      142\n      145\n      131\n      134\n      139\n      133\n      132\n      130\n      129\n      130\n      132\n      133\n      133\n      141\n      140\n      139\n      137\n      135\n      133\n      131\n      130\n      126\n      130\n      131\n      129\n    \n    \n      241\n      156\n      157\n      159\n      161\n      163\n      163\n      161\n      160\n      158\n      162\n      161\n      154\n      151\n      155\n      157\n      156\n      152\n      155\n      157\n      154\n      149\n      147\n      148\n      150\n      147\n      146\n      146\n      148\n      152\n      155\n      156\n      156\n      152\n      160\n      162\n      157\n      157\n      159\n      152\n      141\n      135\n      143\n      146\n      142\n      144\n      149\n      146\n      136\n      140\n      134\n      130\n      130\n      131\n      131\n      132\n      133\n      136\n      130\n      127\n      130\n      133\n      131\n      130\n      130\n      130\n      129\n      126\n      133\n      135\n      132\n      137\n      139\n      127\n      135\n      142\n      137\n      135\n      132\n      134\n      132\n      130\n      130\n      129\n      130\n      132\n      133\n      132\n      132\n      136\n      135\n      134\n      133\n      131\n      130\n      129\n      128\n      131\n      131\n      129\n      128\n    \n    \n      242\n      155\n      155\n      155\n      157\n      160\n      162\n      164\n      164\n      160\n      160\n      161\n      161\n      157\n      152\n      154\n      158\n      155\n      155\n      154\n      154\n      154\n      152\n      150\n      148\n      148\n      146\n      145\n      147\n      151\n      155\n      155\n      153\n      155\n      156\n      156\n      155\n      156\n      157\n      155\n      151\n      135\n      133\n      131\n      131\n      133\n      137\n      140\n      141\n      145\n      140\n      135\n      132\n      131\n      131\n      133\n      135\n      135\n      131\n      130\n      133\n      135\n      132\n      128\n      127\n      130\n      130\n      127\n      130\n      130\n      128\n      137\n      140\n      137\n      135\n      133\n      137\n      135\n      134\n      129\n      127\n      131\n      130\n      129\n      129\n      130\n      130\n      130\n      130\n      135\n      135\n      133\n      132\n      130\n      130\n      129\n      129\n      128\n      129\n      128\n      127\n    \n    \n      243\n      155\n      154\n      153\n      154\n      157\n      160\n      162\n      164\n      164\n      157\n      157\n      162\n      162\n      155\n      153\n      156\n      156\n      153\n      152\n      153\n      156\n      156\n      153\n      150\n      149\n      149\n      149\n      148\n      147\n      147\n      149\n      151\n      157\n      153\n      151\n      154\n      156\n      155\n      157\n      161\n      161\n      152\n      143\n      138\n      134\n      131\n      135\n      142\n      145\n      143\n      140\n      136\n      134\n      132\n      133\n      134\n      134\n      132\n      131\n      134\n      135\n      132\n      128\n      127\n      130\n      133\n      130\n      129\n      126\n      123\n      132\n      135\n      147\n      139\n      130\n      136\n      133\n      135\n      127\n      128\n      131\n      129\n      127\n      126\n      127\n      128\n      130\n      131\n      135\n      135\n      134\n      133\n      132\n      131\n      130\n      130\n      125\n      130\n      131\n      129\n    \n    \n      244\n      155\n      154\n      153\n      154\n      155\n      157\n      159\n      159\n      159\n      163\n      162\n      158\n      158\n      163\n      160\n      153\n      151\n      153\n      155\n      154\n      152\n      151\n      155\n      158\n      153\n      152\n      151\n      149\n      147\n      146\n      147\n      148\n      148\n      152\n      153\n      153\n      156\n      161\n      161\n      159\n      159\n      164\n      161\n      149\n      142\n      143\n      142\n      136\n      140\n      141\n      140\n      138\n      135\n      132\n      131\n      131\n      130\n      128\n      128\n      130\n      132\n      131\n      130\n      129\n      132\n      133\n      128\n      128\n      129\n      128\n      132\n      128\n      134\n      143\n      139\n      133\n      123\n      132\n      132\n      131\n      129\n      128\n      128\n      127\n      127\n      128\n      130\n      131\n      130\n      130\n      130\n      130\n      130\n      129\n      128\n      128\n      129\n      133\n      134\n      130\n    \n    \n      245\n      154\n      153\n      153\n      153\n      154\n      155\n      155\n      155\n      158\n      162\n      164\n      162\n      160\n      161\n      161\n      160\n      154\n      154\n      153\n      152\n      152\n      153\n      155\n      157\n      157\n      154\n      150\n      150\n      151\n      152\n      150\n      148\n      146\n      143\n      143\n      146\n      149\n      150\n      154\n      159\n      152\n      157\n      160\n      157\n      152\n      149\n      146\n      142\n      141\n      143\n      144\n      143\n      141\n      139\n      136\n      134\n      133\n      130\n      128\n      127\n      127\n      127\n      127\n      126\n      126\n      132\n      131\n      131\n      131\n      131\n      137\n      136\n      135\n      139\n      137\n      136\n      130\n      130\n      130\n      131\n      133\n      133\n      133\n      131\n      129\n      127\n      126\n      125\n      127\n      128\n      128\n      128\n      128\n      128\n      128\n      128\n      131\n      131\n      130\n      130\n    \n    \n      246\n      152\n      152\n      152\n      153\n      154\n      154\n      154\n      154\n      159\n      156\n      158\n      164\n      165\n      160\n      159\n      162\n      161\n      156\n      151\n      151\n      154\n      156\n      155\n      152\n      156\n      154\n      152\n      152\n      153\n      154\n      152\n      151\n      152\n      143\n      138\n      141\n      142\n      140\n      146\n      157\n      163\n      159\n      159\n      163\n      160\n      152\n      147\n      149\n      148\n      149\n      149\n      147\n      146\n      145\n      143\n      141\n      140\n      137\n      132\n      128\n      126\n      126\n      125\n      124\n      121\n      132\n      133\n      132\n      129\n      131\n      142\n      144\n      143\n      132\n      131\n      136\n      139\n      129\n      129\n      134\n      138\n      137\n      136\n      133\n      129\n      128\n      127\n      128\n      134\n      133\n      131\n      129\n      128\n      128\n      128\n      128\n      128\n      128\n      129\n      132\n    \n    \n      247\n      150\n      150\n      150\n      152\n      153\n      155\n      155\n      155\n      154\n      156\n      156\n      156\n      162\n      168\n      163\n      152\n      159\n      159\n      158\n      155\n      151\n      150\n      152\n      154\n      152\n      155\n      157\n      155\n      151\n      149\n      152\n      155\n      152\n      156\n      153\n      144\n      142\n      149\n      151\n      147\n      145\n      151\n      154\n      156\n      160\n      164\n      159\n      150\n      149\n      149\n      147\n      145\n      144\n      145\n      144\n      142\n      142\n      139\n      135\n      131\n      129\n      129\n      128\n      128\n      127\n      131\n      127\n      127\n      131\n      137\n      145\n      141\n      133\n      131\n      135\n      129\n      133\n      128\n      136\n      139\n      136\n      135\n      132\n      130\n      129\n      131\n      136\n      140\n      141\n      138\n      134\n      131\n      128\n      127\n      127\n      128\n      127\n      130\n      134\n      135\n    \n    \n      248\n      150\n      151\n      150\n      149\n      150\n      152\n      154\n      153\n      154\n      152\n      152\n      154\n      157\n      160\n      164\n      168\n      159\n      159\n      163\n      164\n      157\n      148\n      147\n      151\n      152\n      154\n      155\n      157\n      157\n      156\n      154\n      153\n      155\n      153\n      153\n      155\n      154\n      149\n      146\n      144\n      147\n      154\n      156\n      154\n      155\n      162\n      166\n      165\n      158\n      152\n      147\n      147\n      149\n      148\n      146\n      145\n      146\n      145\n      141\n      136\n      135\n      135\n      132\n      128\n      130\n      128\n      126\n      126\n      129\n      132\n      132\n      129\n      132\n      133\n      132\n      134\n      136\n      133\n      134\n      140\n      138\n      130\n      128\n      137\n      144\n      142\n      137\n      136\n      133\n      131\n      130\n      129\n      128\n      126\n      127\n      129\n      129\n      130\n      130\n      129\n    \n    \n      249\n      149\n      150\n      150\n      149\n      149\n      151\n      153\n      152\n      154\n      152\n      152\n      153\n      154\n      155\n      158\n      161\n      162\n      161\n      162\n      164\n      166\n      164\n      157\n      150\n      153\n      153\n      153\n      153\n      153\n      153\n      153\n      152\n      155\n      154\n      154\n      154\n      152\n      149\n      151\n      154\n      152\n      144\n      142\n      148\n      151\n      151\n      157\n      165\n      165\n      161\n      157\n      153\n      151\n      150\n      147\n      145\n      146\n      145\n      143\n      141\n      140\n      139\n      136\n      131\n      133\n      132\n      129\n      127\n      129\n      132\n      133\n      132\n      134\n      133\n      131\n      131\n      132\n      130\n      130\n      137\n      138\n      134\n      131\n      133\n      137\n      141\n      144\n      146\n      146\n      137\n      131\n      133\n      135\n      132\n      129\n      129\n      129\n      128\n      128\n      128\n    \n    \n      250\n      147\n      148\n      149\n      148\n      148\n      151\n      151\n      151\n      155\n      154\n      153\n      155\n      155\n      154\n      155\n      156\n      156\n      160\n      160\n      157\n      159\n      164\n      166\n      163\n      155\n      155\n      153\n      152\n      151\n      151\n      151\n      151\n      147\n      152\n      158\n      160\n      157\n      154\n      154\n      156\n      146\n      137\n      135\n      140\n      139\n      135\n      143\n      158\n      161\n      163\n      163\n      159\n      157\n      156\n      154\n      151\n      149\n      150\n      149\n      148\n      147\n      144\n      139\n      135\n      137\n      136\n      134\n      130\n      130\n      132\n      133\n      133\n      133\n      132\n      128\n      128\n      129\n      127\n      128\n      135\n      135\n      135\n      132\n      126\n      126\n      135\n      145\n      150\n      143\n      135\n      129\n      131\n      134\n      131\n      129\n      129\n      129\n      127\n      125\n      126\n    \n    \n      251\n      144\n      146\n      147\n      146\n      148\n      150\n      150\n      150\n      153\n      152\n      153\n      155\n      155\n      154\n      154\n      156\n      153\n      157\n      157\n      153\n      152\n      158\n      164\n      166\n      157\n      158\n      158\n      156\n      152\n      150\n      151\n      152\n      151\n      152\n      153\n      154\n      154\n      155\n      158\n      161\n      154\n      149\n      143\n      141\n      146\n      151\n      150\n      145\n      150\n      158\n      162\n      160\n      159\n      160\n      159\n      155\n      153\n      153\n      154\n      154\n      153\n      150\n      146\n      142\n      139\n      140\n      139\n      136\n      133\n      133\n      133\n      133\n      131\n      130\n      126\n      127\n      129\n      127\n      128\n      135\n      133\n      134\n      132\n      126\n      125\n      131\n      140\n      146\n      138\n      138\n      135\n      133\n      130\n      129\n      129\n      129\n      130\n      128\n      126\n      125\n    \n    \n      252\n      143\n      145\n      145\n      145\n      146\n      149\n      149\n      149\n      149\n      148\n      150\n      152\n      154\n      153\n      153\n      154\n      157\n      156\n      156\n      157\n      159\n      159\n      157\n      156\n      155\n      159\n      162\n      160\n      156\n      153\n      153\n      154\n      149\n      149\n      151\n      154\n      158\n      159\n      156\n      152\n      148\n      158\n      162\n      156\n      152\n      155\n      153\n      148\n      147\n      154\n      157\n      154\n      153\n      156\n      157\n      155\n      153\n      154\n      155\n      157\n      157\n      155\n      153\n      152\n      140\n      143\n      144\n      142\n      138\n      136\n      134\n      132\n      131\n      130\n      127\n      128\n      131\n      129\n      129\n      134\n      133\n      132\n      133\n      135\n      136\n      137\n      141\n      145\n      144\n      148\n      145\n      137\n      133\n      135\n      133\n      128\n      128\n      129\n      129\n      128\n    \n    \n      253\n      143\n      145\n      145\n      144\n      145\n      147\n      148\n      147\n      149\n      148\n      149\n      151\n      152\n      152\n      152\n      154\n      155\n      155\n      156\n      157\n      157\n      155\n      157\n      160\n      153\n      156\n      160\n      160\n      159\n      157\n      156\n      157\n      150\n      150\n      150\n      153\n      156\n      157\n      153\n      147\n      142\n      152\n      161\n      160\n      153\n      148\n      149\n      151\n      148\n      150\n      150\n      147\n      148\n      153\n      156\n      156\n      157\n      156\n      156\n      157\n      156\n      154\n      153\n      153\n      143\n      146\n      148\n      146\n      143\n      140\n      136\n      133\n      133\n      132\n      129\n      131\n      133\n      130\n      127\n      132\n      130\n      128\n      131\n      139\n      145\n      145\n      143\n      143\n      140\n      141\n      137\n      131\n      133\n      139\n      134\n      125\n      126\n      129\n      132\n      132\n    \n    \n      254\n      145\n      146\n      145\n      144\n      144\n      145\n      146\n      146\n      150\n      148\n      148\n      150\n      151\n      151\n      152\n      154\n      150\n      155\n      156\n      152\n      149\n      153\n      160\n      165\n      156\n      155\n      155\n      157\n      160\n      160\n      159\n      157\n      159\n      156\n      150\n      143\n      142\n      148\n      154\n      158\n      160\n      152\n      148\n      153\n      160\n      161\n      156\n      150\n      151\n      149\n      147\n      148\n      151\n      154\n      156\n      156\n      157\n      156\n      156\n      156\n      154\n      151\n      150\n      151\n      148\n      150\n      150\n      148\n      145\n      143\n      140\n      138\n      135\n      133\n      130\n      131\n      133\n      129\n      126\n      130\n      128\n      127\n      129\n      135\n      143\n      146\n      142\n      136\n      132\n      129\n      125\n      125\n      130\n      134\n      133\n      129\n      130\n      131\n      131\n      131\n    \n    \n      255\n      147\n      147\n      146\n      143\n      143\n      144\n      145\n      144\n      147\n      146\n      145\n      147\n      148\n      148\n      150\n      153\n      153\n      156\n      154\n      150\n      153\n      161\n      159\n      151\n      162\n      157\n      153\n      154\n      160\n      163\n      160\n      156\n      147\n      154\n      159\n      156\n      150\n      148\n      148\n      149\n      158\n      160\n      159\n      153\n      150\n      154\n      160\n      163\n      155\n      151\n      150\n      154\n      157\n      155\n      152\n      150\n      151\n      151\n      152\n      154\n      154\n      152\n      151\n      153\n      152\n      152\n      151\n      148\n      146\n      145\n      144\n      142\n      135\n      133\n      129\n      130\n      132\n      129\n      126\n      130\n      129\n      131\n      130\n      130\n      137\n      143\n      139\n      129\n      133\n      129\n      127\n      131\n      131\n      130\n      133\n      140\n      137\n      133\n      128\n      126\n    \n    \n      256\n      146\n      147\n      148\n      147\n      146\n      146\n      148\n      151\n      143\n      146\n      147\n      147\n      147\n      148\n      149\n      148\n      149\n      151\n      149\n      146\n      146\n      149\n      151\n      149\n      157\n      157\n      158\n      156\n      149\n      145\n      156\n      171\n      161\n      151\n      150\n      157\n      161\n      160\n      155\n      148\n      154\n      150\n      151\n      154\n      153\n      150\n      155\n      163\n      163\n      158\n      156\n      159\n      158\n      153\n      150\n      151\n      151\n      152\n      151\n      150\n      152\n      156\n      156\n      154\n      152\n      152\n      152\n      151\n      150\n      148\n      147\n      146\n      138\n      139\n      137\n      132\n      128\n      127\n      127\n      126\n      130\n      130\n      132\n      136\n      138\n      138\n      141\n      144\n      142\n      138\n      133\n      130\n      129\n      129\n      129\n      129\n      140\n      127\n      127\n      128\n    \n    \n      257\n      144\n      145\n      146\n      146\n      145\n      145\n      146\n      148\n      149\n      148\n      146\n      142\n      141\n      143\n      146\n      146\n      146\n      147\n      146\n      145\n      146\n      148\n      150\n      150\n      148\n      153\n      157\n      156\n      151\n      150\n      153\n      156\n      166\n      163\n      159\n      150\n      145\n      152\n      159\n      159\n      143\n      142\n      145\n      152\n      158\n      159\n      154\n      150\n      157\n      161\n      163\n      160\n      160\n      163\n      162\n      158\n      150\n      152\n      153\n      152\n      153\n      153\n      151\n      147\n      154\n      153\n      152\n      150\n      149\n      148\n      148\n      148\n      145\n      143\n      138\n      133\n      130\n      130\n      129\n      128\n      131\n      129\n      129\n      131\n      134\n      136\n      141\n      146\n      144\n      141\n      137\n      133\n      131\n      130\n      129\n      128\n      131\n      124\n      129\n      131\n    \n    \n      258\n      143\n      144\n      146\n      146\n      145\n      145\n      146\n      147\n      149\n      148\n      144\n      139\n      138\n      140\n      142\n      143\n      146\n      144\n      144\n      146\n      147\n      147\n      149\n      151\n      146\n      154\n      157\n      153\n      153\n      158\n      156\n      150\n      158\n      160\n      162\n      158\n      150\n      151\n      155\n      156\n      169\n      157\n      142\n      135\n      143\n      155\n      159\n      156\n      156\n      161\n      162\n      158\n      157\n      160\n      162\n      161\n      162\n      160\n      155\n      149\n      146\n      148\n      151\n      152\n      153\n      153\n      152\n      152\n      151\n      151\n      150\n      150\n      147\n      144\n      138\n      132\n      131\n      132\n      131\n      129\n      130\n      128\n      128\n      129\n      131\n      133\n      138\n      142\n      139\n      137\n      134\n      132\n      130\n      128\n      126\n      124\n      128\n      123\n      132\n      136\n    \n    \n      259\n      143\n      145\n      146\n      146\n      145\n      145\n      145\n      144\n      145\n      146\n      145\n      143\n      142\n      143\n      144\n      143\n      147\n      145\n      145\n      148\n      149\n      147\n      149\n      152\n      151\n      155\n      153\n      148\n      151\n      159\n      161\n      156\n      151\n      146\n      153\n      166\n      165\n      156\n      150\n      149\n      149\n      161\n      170\n      165\n      154\n      148\n      148\n      151\n      154\n      153\n      155\n      160\n      159\n      154\n      153\n      156\n      157\n      159\n      161\n      160\n      158\n      155\n      154\n      153\n      150\n      151\n      153\n      154\n      154\n      153\n      151\n      150\n      144\n      143\n      139\n      135\n      133\n      133\n      132\n      130\n      128\n      128\n      129\n      130\n      130\n      130\n      131\n      133\n      130\n      129\n      128\n      127\n      126\n      125\n      124\n      123\n      128\n      124\n      131\n      139\n    \n    \n      260\n      143\n      146\n      146\n      145\n      144\n      145\n      144\n      142\n      145\n      147\n      149\n      148\n      147\n      148\n      148\n      147\n      149\n      146\n      147\n      149\n      150\n      147\n      147\n      150\n      153\n      153\n      150\n      147\n      149\n      154\n      158\n      158\n      156\n      145\n      145\n      155\n      158\n      157\n      157\n      157\n      159\n      153\n      149\n      155\n      163\n      163\n      157\n      151\n      149\n      148\n      152\n      160\n      162\n      158\n      154\n      154\n      153\n      155\n      159\n      161\n      161\n      160\n      160\n      161\n      152\n      153\n      153\n      154\n      153\n      153\n      152\n      151\n      145\n      147\n      146\n      142\n      138\n      135\n      133\n      131\n      128\n      129\n      130\n      130\n      129\n      128\n      128\n      127\n      127\n      127\n      126\n      125\n      125\n      125\n      126\n      126\n      127\n      123\n      128\n      134\n    \n    \n      261\n      147\n      149\n      148\n      145\n      145\n      147\n      147\n      145\n      146\n      148\n      148\n      146\n      146\n      147\n      148\n      148\n      147\n      147\n      147\n      148\n      148\n      147\n      147\n      147\n      149\n      150\n      152\n      153\n      153\n      152\n      153\n      155\n      159\n      155\n      150\n      144\n      144\n      155\n      163\n      162\n      155\n      154\n      156\n      158\n      156\n      153\n      156\n      161\n      154\n      156\n      153\n      150\n      152\n      158\n      160\n      156\n      161\n      158\n      154\n      153\n      153\n      155\n      160\n      164\n      158\n      157\n      155\n      153\n      151\n      151\n      152\n      153\n      151\n      154\n      155\n      151\n      145\n      141\n      137\n      134\n      134\n      134\n      133\n      130\n      128\n      129\n      130\n      130\n      128\n      128\n      127\n      125\n      124\n      124\n      126\n      127\n      127\n      128\n      129\n      127\n    \n    \n      262\n      147\n      149\n      148\n      144\n      144\n      148\n      149\n      147\n      145\n      146\n      146\n      145\n      144\n      145\n      146\n      145\n      144\n      146\n      147\n      147\n      147\n      148\n      148\n      147\n      146\n      149\n      152\n      155\n      155\n      153\n      153\n      154\n      154\n      159\n      160\n      153\n      148\n      154\n      158\n      152\n      162\n      162\n      159\n      155\n      154\n      155\n      157\n      158\n      161\n      161\n      156\n      148\n      148\n      154\n      157\n      156\n      156\n      155\n      156\n      159\n      159\n      156\n      154\n      154\n      160\n      159\n      157\n      155\n      154\n      153\n      153\n      153\n      153\n      155\n      155\n      152\n      149\n      147\n      144\n      142\n      141\n      141\n      138\n      133\n      130\n      131\n      132\n      132\n      130\n      130\n      129\n      127\n      125\n      124\n      125\n      126\n      128\n      131\n      130\n      124\n    \n    \n      263\n      144\n      145\n      143\n      140\n      141\n      146\n      147\n      145\n      143\n      146\n      148\n      148\n      147\n      147\n      147\n      145\n      142\n      146\n      147\n      146\n      147\n      150\n      150\n      147\n      144\n      148\n      150\n      150\n      151\n      154\n      156\n      156\n      150\n      154\n      162\n      165\n      160\n      154\n      149\n      141\n      148\n      160\n      166\n      162\n      157\n      158\n      159\n      156\n      155\n      155\n      158\n      163\n      160\n      153\n      151\n      153\n      156\n      154\n      153\n      156\n      159\n      159\n      160\n      161\n      157\n      158\n      159\n      159\n      159\n      157\n      154\n      152\n      150\n      150\n      149\n      149\n      150\n      152\n      151\n      149\n      147\n      147\n      144\n      138\n      134\n      134\n      133\n      132\n      132\n      133\n      132\n      131\n      128\n      127\n      127\n      127\n      125\n      128\n      128\n      125\n    \n    \n      264\n      149\n      142\n      139\n      143\n      147\n      147\n      146\n      146\n      146\n      147\n      147\n      146\n      145\n      145\n      147\n      148\n      148\n      146\n      146\n      146\n      146\n      146\n      148\n      151\n      149\n      148\n      148\n      149\n      149\n      149\n      151\n      155\n      155\n      155\n      156\n      159\n      163\n      161\n      152\n      142\n      149\n      150\n      159\n      167\n      163\n      156\n      156\n      158\n      157\n      160\n      159\n      156\n      158\n      162\n      160\n      153\n      155\n      155\n      156\n      156\n      156\n      156\n      156\n      155\n      162\n      160\n      158\n      157\n      158\n      159\n      159\n      158\n      152\n      151\n      151\n      152\n      153\n      154\n      153\n      152\n      150\n      150\n      148\n      144\n      140\n      138\n      137\n      138\n      135\n      135\n      135\n      134\n      131\n      129\n      127\n      127\n      128\n      129\n      130\n      131\n    \n    \n      265\n      145\n      145\n      146\n      146\n      144\n      142\n      144\n      146\n      143\n      144\n      146\n      147\n      146\n      145\n      145\n      145\n      149\n      147\n      146\n      146\n      145\n      144\n      145\n      148\n      148\n      148\n      149\n      151\n      151\n      150\n      151\n      154\n      153\n      153\n      155\n      158\n      163\n      165\n      161\n      155\n      152\n      143\n      144\n      154\n      163\n      165\n      162\n      156\n      156\n      157\n      158\n      158\n      158\n      159\n      159\n      158\n      155\n      155\n      154\n      154\n      154\n      155\n      155\n      155\n      158\n      158\n      158\n      157\n      157\n      157\n      158\n      158\n      162\n      160\n      157\n      154\n      152\n      153\n      155\n      157\n      151\n      152\n      152\n      150\n      146\n      143\n      141\n      141\n      138\n      137\n      137\n      137\n      137\n      134\n      130\n      127\n      129\n      132\n      133\n      131\n    \n    \n      266\n      141\n      144\n      144\n      140\n      137\n      137\n      140\n      142\n      144\n      145\n      146\n      147\n      146\n      146\n      146\n      147\n      148\n      146\n      146\n      145\n      144\n      143\n      144\n      146\n      147\n      147\n      149\n      152\n      152\n      150\n      150\n      152\n      150\n      151\n      151\n      153\n      158\n      163\n      164\n      162\n      160\n      152\n      147\n      146\n      150\n      158\n      165\n      165\n      160\n      157\n      155\n      157\n      158\n      157\n      158\n      162\n      158\n      157\n      156\n      155\n      155\n      154\n      154\n      154\n      156\n      157\n      159\n      158\n      157\n      156\n      157\n      159\n      159\n      157\n      154\n      150\n      146\n      146\n      149\n      152\n      151\n      153\n      154\n      153\n      151\n      148\n      146\n      145\n      145\n      142\n      139\n      139\n      141\n      140\n      135\n      131\n      131\n      130\n      133\n      142\n    \n    \n      267\n      151\n      151\n      147\n      142\n      142\n      146\n      147\n      145\n      146\n      146\n      144\n      143\n      144\n      145\n      147\n      149\n      145\n      144\n      144\n      145\n      145\n      144\n      145\n      148\n      146\n      147\n      149\n      150\n      151\n      150\n      150\n      150\n      150\n      151\n      150\n      149\n      152\n      156\n      159\n      160\n      166\n      166\n      163\n      153\n      144\n      148\n      159\n      163\n      166\n      160\n      155\n      155\n      157\n      158\n      159\n      160\n      161\n      160\n      159\n      158\n      156\n      154\n      152\n      151\n      154\n      156\n      157\n      158\n      158\n      157\n      158\n      159\n      155\n      156\n      156\n      154\n      152\n      150\n      150\n      150\n      150\n      151\n      152\n      152\n      152\n      150\n      149\n      149\n      151\n      147\n      142\n      140\n      141\n      141\n      139\n      137\n      131\n      132\n      132\n      134\n    \n    \n      268\n      146\n      147\n      144\n      139\n      139\n      142\n      141\n      138\n      144\n      143\n      142\n      141\n      142\n      143\n      145\n      145\n      144\n      143\n      143\n      145\n      145\n      145\n      146\n      148\n      146\n      147\n      147\n      147\n      148\n      149\n      149\n      148\n      151\n      152\n      151\n      150\n      150\n      154\n      157\n      158\n      163\n      165\n      168\n      164\n      157\n      155\n      153\n      147\n      161\n      162\n      161\n      157\n      156\n      157\n      158\n      158\n      160\n      160\n      160\n      159\n      157\n      155\n      152\n      150\n      152\n      152\n      153\n      155\n      158\n      159\n      159\n      159\n      155\n      155\n      157\n      158\n      159\n      157\n      154\n      151\n      151\n      150\n      149\n      149\n      150\n      151\n      152\n      152\n      152\n      150\n      146\n      144\n      142\n      142\n      141\n      141\n      140\n      137\n      133\n      128\n    \n    \n      269\n      145\n      149\n      152\n      150\n      146\n      143\n      142\n      141\n      141\n      142\n      143\n      145\n      146\n      145\n      143\n      141\n      144\n      143\n      143\n      145\n      145\n      144\n      145\n      147\n      146\n      147\n      146\n      145\n      146\n      148\n      148\n      147\n      147\n      149\n      149\n      149\n      150\n      153\n      156\n      156\n      156\n      159\n      166\n      168\n      165\n      164\n      160\n      152\n      151\n      160\n      165\n      162\n      157\n      156\n      156\n      157\n      159\n      160\n      161\n      161\n      161\n      159\n      157\n      156\n      151\n      150\n      150\n      152\n      156\n      158\n      159\n      158\n      155\n      155\n      154\n      156\n      157\n      157\n      154\n      152\n      154\n      152\n      150\n      149\n      150\n      152\n      153\n      154\n      149\n      150\n      151\n      150\n      147\n      144\n      143\n      142\n      146\n      143\n      140\n      139\n    \n    \n      270\n      140\n      143\n      146\n      148\n      146\n      141\n      139\n      139\n      143\n      144\n      146\n      148\n      149\n      147\n      143\n      140\n      143\n      142\n      143\n      144\n      144\n      143\n      144\n      146\n      146\n      147\n      146\n      145\n      146\n      148\n      148\n      146\n      145\n      146\n      147\n      147\n      148\n      152\n      154\n      154\n      154\n      156\n      162\n      165\n      162\n      164\n      167\n      166\n      154\n      155\n      157\n      159\n      160\n      157\n      155\n      154\n      159\n      159\n      160\n      160\n      161\n      161\n      161\n      161\n      156\n      155\n      153\n      153\n      153\n      155\n      156\n      157\n      159\n      158\n      158\n      157\n      158\n      158\n      157\n      157\n      157\n      155\n      153\n      152\n      153\n      153\n      153\n      152\n      149\n      151\n      153\n      153\n      151\n      148\n      147\n      146\n      145\n      148\n      149\n      143\n    \n    \n      271\n      147\n      142\n      141\n      146\n      150\n      149\n      145\n      144\n      144\n      144\n      145\n      146\n      147\n      146\n      143\n      140\n      141\n      141\n      142\n      143\n      143\n      143\n      143\n      145\n      146\n      147\n      146\n      145\n      146\n      149\n      148\n      145\n      146\n      147\n      147\n      147\n      148\n      151\n      152\n      151\n      154\n      151\n      155\n      161\n      162\n      164\n      165\n      162\n      166\n      152\n      144\n      152\n      162\n      161\n      155\n      151\n      157\n      157\n      156\n      156\n      156\n      157\n      158\n      159\n      162\n      161\n      159\n      155\n      152\n      151\n      153\n      155\n      154\n      156\n      157\n      156\n      155\n      155\n      156\n      157\n      158\n      157\n      156\n      155\n      155\n      153\n      151\n      149\n      151\n      152\n      152\n      153\n      152\n      151\n      151\n      151\n      151\n      149\n      148\n      148\n    \n    \n      272\n      148\n      147\n      145\n      144\n      145\n      146\n      148\n      149\n      146\n      144\n      142\n      142\n      143\n      145\n      146\n      146\n      143\n      144\n      145\n      145\n      148\n      149\n      147\n      143\n      146\n      145\n      146\n      146\n      143\n      141\n      143\n      147\n      148\n      147\n      147\n      148\n      149\n      150\n      150\n      150\n      152\n      152\n      154\n      158\n      160\n      160\n      161\n      162\n      168\n      167\n      157\n      150\n      155\n      157\n      155\n      158\n      150\n      154\n      157\n      157\n      157\n      157\n      157\n      155\n      159\n      159\n      161\n      162\n      161\n      158\n      155\n      152\n      153\n      154\n      155\n      155\n      154\n      154\n      155\n      155\n      154\n      155\n      155\n      154\n      153\n      153\n      155\n      156\n      153\n      152\n      151\n      151\n      151\n      151\n      151\n      151\n      151\n      150\n      148\n      147\n    \n    \n      273\n      147\n      147\n      146\n      145\n      145\n      146\n      147\n      148\n      144\n      143\n      143\n      144\n      145\n      146\n      145\n      145\n      143\n      144\n      144\n      144\n      145\n      146\n      144\n      141\n      144\n      142\n      141\n      142\n      142\n      141\n      141\n      143\n      144\n      144\n      145\n      147\n      149\n      151\n      152\n      152\n      152\n      151\n      152\n      156\n      159\n      159\n      159\n      159\n      157\n      166\n      170\n      164\n      157\n      152\n      153\n      158\n      151\n      153\n      155\n      156\n      156\n      155\n      156\n      157\n      156\n      156\n      157\n      158\n      159\n      158\n      157\n      156\n      156\n      156\n      156\n      155\n      154\n      154\n      156\n      157\n      155\n      155\n      156\n      156\n      155\n      154\n      155\n      156\n      154\n      153\n      151\n      150\n      149\n      148\n      147\n      145\n      151\n      150\n      149\n      149\n    \n    \n      274\n      145\n      146\n      146\n      146\n      146\n      145\n      145\n      145\n      146\n      145\n      144\n      144\n      143\n      142\n      141\n      140\n      143\n      144\n      143\n      142\n      143\n      144\n      143\n      140\n      146\n      143\n      141\n      143\n      145\n      145\n      144\n      143\n      144\n      144\n      145\n      147\n      147\n      148\n      148\n      148\n      151\n      150\n      151\n      155\n      157\n      158\n      157\n      158\n      160\n      164\n      169\n      169\n      164\n      160\n      157\n      152\n      153\n      151\n      152\n      154\n      155\n      153\n      155\n      158\n      155\n      155\n      154\n      155\n      157\n      158\n      159\n      160\n      157\n      157\n      157\n      156\n      155\n      155\n      155\n      156\n      154\n      154\n      155\n      155\n      154\n      154\n      154\n      154\n      154\n      152\n      151\n      151\n      153\n      153\n      151\n      150\n      150\n      150\n      149\n      149\n    \n    \n      275\n      143\n      144\n      145\n      145\n      145\n      144\n      143\n      143\n      148\n      147\n      145\n      143\n      142\n      142\n      142\n      142\n      143\n      144\n      143\n      142\n      142\n      144\n      143\n      141\n      147\n      145\n      144\n      146\n      148\n      148\n      147\n      146\n      145\n      146\n      147\n      148\n      147\n      147\n      146\n      146\n      150\n      150\n      151\n      154\n      156\n      156\n      157\n      159\n      164\n      157\n      159\n      163\n      166\n      171\n      168\n      157\n      160\n      153\n      150\n      154\n      155\n      153\n      154\n      157\n      157\n      156\n      155\n      156\n      157\n      159\n      160\n      160\n      156\n      158\n      159\n      159\n      158\n      156\n      154\n      154\n      154\n      154\n      154\n      154\n      154\n      154\n      155\n      155\n      152\n      151\n      150\n      152\n      155\n      156\n      154\n      152\n      151\n      151\n      151\n      150\n    \n    \n      276\n      143\n      144\n      145\n      146\n      146\n      145\n      144\n      143\n      145\n      145\n      146\n      146\n      145\n      145\n      145\n      146\n      143\n      144\n      144\n      142\n      143\n      145\n      145\n      144\n      143\n      144\n      144\n      144\n      144\n      144\n      145\n      145\n      142\n      144\n      146\n      148\n      149\n      149\n      150\n      151\n      147\n      148\n      151\n      153\n      153\n      153\n      155\n      159\n      157\n      154\n      158\n      160\n      159\n      166\n      174\n      172\n      167\n      159\n      153\n      156\n      159\n      156\n      155\n      157\n      158\n      157\n      156\n      157\n      158\n      158\n      159\n      159\n      159\n      159\n      160\n      160\n      159\n      157\n      157\n      156\n      157\n      156\n      155\n      155\n      155\n      156\n      157\n      157\n      154\n      152\n      151\n      152\n      153\n      152\n      150\n      148\n      151\n      151\n      152\n      153\n    \n    \n      277\n      144\n      145\n      146\n      147\n      147\n      146\n      146\n      145\n      143\n      145\n      147\n      148\n      146\n      144\n      141\n      140\n      142\n      144\n      144\n      143\n      144\n      146\n      147\n      146\n      140\n      143\n      144\n      143\n      140\n      141\n      142\n      144\n      140\n      142\n      144\n      146\n      146\n      146\n      147\n      149\n      145\n      146\n      149\n      151\n      150\n      150\n      152\n      156\n      156\n      158\n      160\n      159\n      156\n      159\n      166\n      168\n      170\n      164\n      160\n      161\n      162\n      160\n      158\n      158\n      157\n      156\n      156\n      157\n      158\n      158\n      158\n      157\n      162\n      162\n      161\n      159\n      159\n      159\n      160\n      162\n      158\n      156\n      155\n      155\n      155\n      155\n      155\n      155\n      157\n      156\n      155\n      155\n      155\n      154\n      153\n      152\n      153\n      152\n      151\n      151\n    \n    \n      278\n      144\n      145\n      145\n      145\n      146\n      146\n      146\n      146\n      146\n      147\n      148\n      147\n      144\n      141\n      139\n      138\n      141\n      143\n      144\n      143\n      143\n      145\n      146\n      145\n      141\n      145\n      146\n      143\n      140\n      141\n      143\n      144\n      143\n      144\n      145\n      144\n      141\n      140\n      141\n      142\n      147\n      146\n      146\n      149\n      150\n      150\n      150\n      152\n      157\n      159\n      157\n      155\n      157\n      158\n      158\n      158\n      165\n      165\n      164\n      163\n      161\n      159\n      159\n      160\n      158\n      158\n      158\n      158\n      158\n      158\n      157\n      156\n      160\n      160\n      160\n      160\n      160\n      160\n      161\n      162\n      159\n      158\n      157\n      157\n      157\n      156\n      153\n      152\n      155\n      156\n      157\n      157\n      156\n      157\n      158\n      159\n      156\n      154\n      152\n      151\n    \n    \n      279\n      143\n      143\n      143\n      143\n      144\n      145\n      146\n      146\n      149\n      149\n      148\n      146\n      144\n      144\n      146\n      148\n      140\n      142\n      143\n      143\n      143\n      144\n      145\n      144\n      143\n      146\n      146\n      143\n      141\n      143\n      144\n      144\n      146\n      146\n      147\n      145\n      142\n      140\n      141\n      142\n      151\n      147\n      145\n      148\n      151\n      151\n      150\n      149\n      148\n      155\n      154\n      152\n      157\n      158\n      157\n      163\n      158\n      163\n      166\n      163\n      159\n      157\n      159\n      160\n      162\n      161\n      160\n      159\n      159\n      158\n      157\n      156\n      154\n      157\n      160\n      162\n      162\n      160\n      159\n      158\n      164\n      163\n      163\n      162\n      162\n      159\n      155\n      152\n      152\n      153\n      154\n      153\n      151\n      151\n      153\n      155\n      159\n      158\n      157\n      156\n    \n    \n      280\n      148\n      145\n      140\n      138\n      140\n      143\n      143\n      141\n      142\n      143\n      144\n      146\n      147\n      147\n      147\n      146\n      143\n      141\n      142\n      145\n      146\n      145\n      146\n      147\n      143\n      145\n      146\n      145\n      145\n      146\n      144\n      142\n      140\n      142\n      145\n      146\n      146\n      146\n      146\n      146\n      146\n      147\n      148\n      148\n      147\n      148\n      150\n      152\n      152\n      150\n      152\n      156\n      156\n      154\n      155\n      158\n      162\n      158\n      162\n      166\n      164\n      165\n      160\n      146\n      152\n      159\n      162\n      159\n      160\n      164\n      161\n      153\n      151\n      157\n      159\n      159\n      160\n      157\n      155\n      157\n      161\n      159\n      157\n      158\n      160\n      161\n      160\n      159\n      156\n      154\n      152\n      152\n      153\n      155\n      154\n      152\n      154\n      154\n      153\n      154\n    \n    \n      281\n      144\n      144\n      142\n      139\n      138\n      141\n      143\n      144\n      141\n      143\n      144\n      145\n      146\n      146\n      147\n      148\n      147\n      145\n      145\n      146\n      147\n      145\n      146\n      147\n      143\n      144\n      145\n      144\n      144\n      146\n      145\n      144\n      140\n      141\n      142\n      142\n      144\n      145\n      147\n      148\n      144\n      146\n      149\n      149\n      148\n      147\n      147\n      148\n      145\n      146\n      148\n      150\n      152\n      152\n      154\n      156\n      159\n      158\n      162\n      163\n      160\n      165\n      171\n      168\n      145\n      147\n      151\n      155\n      158\n      158\n      158\n      158\n      160\n      158\n      153\n      152\n      158\n      161\n      161\n      162\n      158\n      158\n      158\n      159\n      161\n      161\n      161\n      161\n      161\n      159\n      156\n      154\n      153\n      153\n      154\n      154\n      155\n      153\n      152\n      152\n    \n    \n      282\n      143\n      146\n      146\n      142\n      138\n      139\n      143\n      145\n      143\n      144\n      145\n      146\n      145\n      146\n      147\n      148\n      148\n      147\n      146\n      146\n      145\n      144\n      143\n      144\n      143\n      144\n      144\n      143\n      143\n      145\n      147\n      146\n      146\n      145\n      143\n      143\n      143\n      143\n      143\n      143\n      142\n      144\n      147\n      149\n      148\n      147\n      146\n      146\n      144\n      146\n      148\n      148\n      150\n      153\n      155\n      155\n      153\n      155\n      161\n      163\n      160\n      163\n      167\n      166\n      175\n      158\n      148\n      152\n      154\n      150\n      152\n      160\n      162\n      164\n      161\n      158\n      157\n      156\n      157\n      162\n      159\n      160\n      160\n      160\n      159\n      158\n      158\n      158\n      162\n      162\n      161\n      157\n      153\n      151\n      150\n      150\n      155\n      153\n      152\n      152\n    \n    \n      283\n      148\n      150\n      150\n      146\n      142\n      141\n      142\n      142\n      143\n      145\n      147\n      147\n      145\n      145\n      145\n      147\n      146\n      146\n      147\n      146\n      145\n      144\n      143\n      143\n      144\n      144\n      143\n      142\n      142\n      145\n      147\n      147\n      149\n      148\n      147\n      147\n      146\n      144\n      141\n      139\n      141\n      143\n      145\n      147\n      147\n      147\n      147\n      148\n      147\n      151\n      151\n      149\n      149\n      153\n      154\n      153\n      151\n      153\n      157\n      161\n      162\n      161\n      158\n      154\n      167\n      171\n      159\n      137\n      132\n      147\n      154\n      148\n      154\n      159\n      162\n      163\n      162\n      158\n      156\n      160\n      159\n      160\n      161\n      159\n      157\n      156\n      156\n      157\n      158\n      159\n      160\n      160\n      158\n      155\n      152\n      151\n      152\n      152\n      153\n      153\n    \n    \n      284\n      149\n      149\n      148\n      146\n      147\n      147\n      143\n      139\n      141\n      143\n      145\n      145\n      145\n      144\n      144\n      144\n      144\n      146\n      148\n      149\n      148\n      147\n      146\n      145\n      145\n      146\n      144\n      142\n      142\n      145\n      146\n      147\n      146\n      147\n      147\n      148\n      149\n      148\n      147\n      146\n      144\n      144\n      144\n      144\n      145\n      146\n      148\n      149\n      148\n      151\n      151\n      148\n      147\n      150\n      151\n      150\n      154\n      154\n      154\n      155\n      159\n      160\n      160\n      161\n      155\n      162\n      165\n      155\n      142\n      138\n      143\n      149\n      150\n      148\n      145\n      152\n      163\n      166\n      163\n      164\n      159\n      160\n      161\n      160\n      158\n      157\n      159\n      162\n      156\n      156\n      157\n      159\n      162\n      162\n      161\n      159\n      153\n      154\n      154\n      154\n    \n    \n      285\n      147\n      146\n      144\n      145\n      148\n      149\n      146\n      141\n      139\n      139\n      141\n      142\n      143\n      144\n      143\n      143\n      140\n      144\n      147\n      148\n      147\n      147\n      146\n      145\n      146\n      147\n      145\n      143\n      143\n      145\n      146\n      146\n      146\n      147\n      146\n      146\n      146\n      148\n      150\n      152\n      148\n      146\n      145\n      143\n      143\n      145\n      147\n      148\n      147\n      148\n      149\n      148\n      148\n      149\n      150\n      150\n      150\n      155\n      156\n      156\n      157\n      156\n      158\n      166\n      164\n      156\n      161\n      173\n      168\n      151\n      148\n      161\n      159\n      152\n      144\n      143\n      151\n      155\n      159\n      166\n      163\n      164\n      164\n      163\n      160\n      159\n      160\n      161\n      163\n      160\n      157\n      157\n      159\n      160\n      160\n      158\n      158\n      159\n      158\n      156\n    \n    \n      286\n      147\n      147\n      146\n      145\n      145\n      147\n      146\n      144\n      142\n      141\n      140\n      141\n      143\n      144\n      144\n      143\n      140\n      143\n      146\n      145\n      144\n      145\n      146\n      145\n      146\n      147\n      146\n      144\n      144\n      146\n      146\n      145\n      145\n      146\n      147\n      145\n      143\n      144\n      147\n      150\n      147\n      146\n      145\n      144\n      144\n      145\n      145\n      145\n      145\n      146\n      147\n      149\n      149\n      149\n      150\n      152\n      147\n      154\n      155\n      157\n      160\n      155\n      153\n      159\n      150\n      156\n      158\n      155\n      161\n      171\n      170\n      160\n      165\n      163\n      157\n      152\n      148\n      144\n      148\n      159\n      164\n      165\n      166\n      165\n      163\n      161\n      160\n      160\n      166\n      163\n      160\n      158\n      157\n      156\n      154\n      153\n      160\n      159\n      158\n      158\n    \n    \n      287\n      150\n      152\n      152\n      147\n      142\n      142\n      144\n      146\n      149\n      145\n      142\n      141\n      143\n      145\n      145\n      143\n      142\n      146\n      147\n      145\n      144\n      145\n      147\n      147\n      146\n      147\n      147\n      145\n      145\n      147\n      147\n      145\n      140\n      143\n      146\n      146\n      143\n      143\n      145\n      148\n      144\n      144\n      145\n      145\n      146\n      145\n      144\n      144\n      144\n      142\n      144\n      148\n      149\n      147\n      147\n      151\n      150\n      152\n      149\n      152\n      160\n      158\n      153\n      157\n      152\n      153\n      159\n      162\n      158\n      152\n      155\n      163\n      161\n      162\n      161\n      162\n      161\n      152\n      147\n      150\n      157\n      159\n      162\n      164\n      165\n      164\n      163\n      163\n      161\n      162\n      164\n      163\n      161\n      158\n      156\n      155\n      156\n      154\n      154\n      157\n    \n    \n      288\n      144\n      147\n      151\n      151\n      147\n      143\n      145\n      150\n      144\n      148\n      148\n      144\n      142\n      144\n      144\n      142\n      144\n      144\n      144\n      143\n      143\n      145\n      145\n      144\n      146\n      148\n      147\n      144\n      143\n      147\n      148\n      147\n      144\n      147\n      147\n      145\n      146\n      148\n      147\n      144\n      145\n      144\n      143\n      140\n      137\n      140\n      144\n      141\n      142\n      144\n      146\n      146\n      146\n      146\n      147\n      149\n      148\n      149\n      149\n      149\n      150\n      154\n      156\n      156\n      158\n      152\n      148\n      151\n      155\n      156\n      155\n      155\n      163\n      159\n      155\n      157\n      163\n      165\n      159\n      151\n      149\n      158\n      161\n      161\n      164\n      165\n      164\n      164\n      161\n      164\n      162\n      158\n      159\n      164\n      165\n      161\n      159\n      157\n      156\n      157\n    \n    \n      289\n      148\n      141\n      136\n      140\n      147\n      150\n      148\n      146\n      146\n      150\n      151\n      145\n      140\n      141\n      144\n      144\n      143\n      145\n      144\n      142\n      141\n      143\n      144\n      143\n      141\n      143\n      145\n      144\n      144\n      145\n      145\n      144\n      146\n      146\n      144\n      142\n      143\n      145\n      146\n      145\n      144\n      144\n      146\n      145\n      141\n      142\n      143\n      140\n      143\n      144\n      145\n      145\n      145\n      146\n      148\n      150\n      148\n      149\n      148\n      147\n      148\n      151\n      152\n      152\n      155\n      160\n      156\n      143\n      138\n      147\n      156\n      156\n      154\n      157\n      159\n      158\n      157\n      160\n      163\n      165\n      169\n      159\n      149\n      153\n      162\n      162\n      160\n      163\n      165\n      163\n      165\n      169\n      166\n      158\n      159\n      166\n      165\n      162\n      159\n      158\n    \n    \n      290\n      173\n      165\n      155\n      148\n      145\n      145\n      146\n      148\n      140\n      144\n      147\n      148\n      149\n      150\n      146\n      140\n      146\n      147\n      147\n      144\n      143\n      143\n      144\n      144\n      143\n      143\n      144\n      145\n      145\n      143\n      142\n      143\n      145\n      144\n      145\n      148\n      148\n      147\n      147\n      147\n      145\n      145\n      147\n      147\n      144\n      144\n      145\n      141\n      142\n      142\n      142\n      142\n      143\n      144\n      145\n      146\n      148\n      149\n      148\n      147\n      147\n      149\n      150\n      149\n      150\n      155\n      157\n      152\n      144\n      141\n      145\n      150\n      152\n      155\n      156\n      155\n      155\n      157\n      160\n      161\n      154\n      163\n      163\n      158\n      159\n      160\n      158\n      156\n      161\n      164\n      165\n      164\n      162\n      162\n      163\n      164\n      165\n      162\n      159\n      159\n    \n    \n      291\n      174\n      174\n      171\n      161\n      149\n      142\n      143\n      147\n      151\n      150\n      148\n      145\n      145\n      146\n      146\n      143\n      146\n      148\n      149\n      146\n      144\n      143\n      144\n      143\n      147\n      144\n      143\n      145\n      146\n      145\n      145\n      148\n      143\n      142\n      143\n      146\n      146\n      142\n      142\n      144\n      146\n      145\n      147\n      147\n      144\n      145\n      148\n      146\n      142\n      141\n      141\n      142\n      142\n      142\n      142\n      141\n      148\n      149\n      150\n      149\n      149\n      150\n      149\n      148\n      146\n      147\n      152\n      159\n      158\n      149\n      141\n      139\n      148\n      150\n      152\n      153\n      154\n      156\n      155\n      154\n      160\n      151\n      150\n      164\n      171\n      159\n      152\n      159\n      159\n      158\n      154\n      151\n      156\n      165\n      166\n      162\n      164\n      160\n      158\n      160\n    \n    \n      292\n      168\n      170\n      173\n      173\n      169\n      160\n      148\n      140\n      142\n      146\n      149\n      149\n      147\n      147\n      148\n      150\n      144\n      146\n      147\n      145\n      143\n      142\n      142\n      140\n      145\n      142\n      142\n      145\n      147\n      146\n      147\n      149\n      150\n      146\n      144\n      145\n      143\n      141\n      143\n      148\n      146\n      144\n      147\n      148\n      145\n      145\n      147\n      145\n      141\n      142\n      143\n      144\n      144\n      144\n      143\n      143\n      145\n      148\n      150\n      150\n      150\n      150\n      149\n      147\n      145\n      150\n      151\n      149\n      154\n      159\n      153\n      141\n      139\n      146\n      151\n      151\n      149\n      151\n      155\n      157\n      158\n      161\n      155\n      147\n      151\n      161\n      164\n      163\n      165\n      153\n      147\n      153\n      158\n      157\n      158\n      163\n      167\n      164\n      161\n      162\n    \n    \n      293\n      168\n      170\n      173\n      176\n      178\n      176\n      169\n      161\n      147\n      144\n      142\n      144\n      149\n      152\n      150\n      146\n      145\n      146\n      146\n      144\n      144\n      144\n      143\n      141\n      146\n      144\n      144\n      146\n      146\n      144\n      140\n      139\n      142\n      143\n      146\n      148\n      148\n      147\n      147\n      149\n      145\n      144\n      148\n      151\n      147\n      145\n      144\n      142\n      139\n      141\n      144\n      144\n      144\n      143\n      144\n      144\n      143\n      145\n      147\n      148\n      149\n      150\n      149\n      147\n      147\n      148\n      148\n      148\n      151\n      156\n      156\n      153\n      144\n      144\n      143\n      142\n      144\n      149\n      151\n      152\n      154\n      154\n      155\n      157\n      154\n      147\n      153\n      169\n      163\n      159\n      158\n      159\n      158\n      154\n      154\n      158\n      166\n      163\n      162\n      164\n    \n    \n      294\n      156\n      160\n      162\n      163\n      167\n      174\n      180\n      183\n      181\n      167\n      151\n      143\n      144\n      146\n      146\n      143\n      148\n      147\n      145\n      143\n      144\n      147\n      147\n      145\n      148\n      148\n      147\n      146\n      145\n      144\n      140\n      135\n      132\n      137\n      141\n      143\n      143\n      143\n      144\n      143\n      145\n      143\n      147\n      151\n      148\n      145\n      145\n      143\n      139\n      142\n      144\n      143\n      141\n      140\n      142\n      145\n      143\n      145\n      145\n      145\n      146\n      148\n      149\n      148\n      148\n      145\n      146\n      152\n      153\n      150\n      152\n      157\n      154\n      148\n      141\n      139\n      144\n      149\n      147\n      142\n      143\n      151\n      152\n      149\n      156\n      163\n      156\n      146\n      157\n      162\n      163\n      160\n      158\n      158\n      157\n      154\n      160\n      159\n      159\n      161\n    \n    \n      295\n      158\n      159\n      159\n      158\n      160\n      165\n      169\n      170\n      173\n      177\n      179\n      173\n      161\n      150\n      147\n      148\n      149\n      147\n      142\n      140\n      142\n      147\n      148\n      146\n      146\n      147\n      146\n      144\n      146\n      149\n      148\n      144\n      148\n      149\n      145\n      138\n      136\n      140\n      145\n      146\n      145\n      142\n      145\n      148\n      146\n      145\n      148\n      148\n      143\n      146\n      147\n      144\n      140\n      138\n      142\n      145\n      145\n      145\n      145\n      144\n      145\n      148\n      150\n      149\n      147\n      150\n      149\n      145\n      146\n      152\n      154\n      150\n      154\n      154\n      152\n      148\n      147\n      147\n      148\n      147\n      142\n      147\n      146\n      143\n      148\n      153\n      156\n      159\n      158\n      151\n      152\n      161\n      164\n      159\n      157\n      161\n      157\n      156\n      157\n      158\n    \n    \n      296\n      154\n      156\n      156\n      156\n      155\n      156\n      158\n      161\n      164\n      166\n      169\n      174\n      179\n      179\n      171\n      162\n      147\n      148\n      150\n      149\n      144\n      140\n      143\n      148\n      146\n      145\n      145\n      147\n      147\n      146\n      143\n      141\n      147\n      146\n      145\n      145\n      143\n      140\n      134\n      129\n      140\n      143\n      146\n      146\n      144\n      144\n      147\n      150\n      143\n      144\n      143\n      141\n      139\n      139\n      137\n      135\n      140\n      145\n      147\n      142\n      139\n      142\n      147\n      149\n      144\n      147\n      150\n      150\n      149\n      148\n      149\n      151\n      149\n      153\n      154\n      152\n      149\n      148\n      147\n      146\n      144\n      144\n      146\n      148\n      148\n      146\n      147\n      150\n      169\n      159\n      149\n      148\n      154\n      161\n      161\n      159\n      161\n      159\n      160\n      163\n    \n    \n      297\n      155\n      157\n      158\n      157\n      156\n      157\n      159\n      161\n      164\n      164\n      163\n      164\n      169\n      175\n      178\n      178\n      164\n      152\n      142\n      142\n      148\n      151\n      147\n      143\n      146\n      148\n      146\n      141\n      141\n      145\n      147\n      146\n      146\n      145\n      145\n      146\n      147\n      146\n      145\n      143\n      135\n      138\n      140\n      141\n      142\n      145\n      149\n      153\n      146\n      145\n      142\n      138\n      137\n      139\n      141\n      141\n      144\n      141\n      140\n      141\n      144\n      145\n      145\n      144\n      144\n      145\n      147\n      147\n      147\n      146\n      146\n      147\n      150\n      150\n      149\n      149\n      152\n      154\n      152\n      147\n      146\n      145\n      145\n      147\n      146\n      145\n      147\n      150\n      146\n      152\n      156\n      155\n      151\n      150\n      154\n      158\n      157\n      162\n      163\n      160\n    \n    \n      298\n      161\n      162\n      162\n      162\n      161\n      161\n      163\n      164\n      162\n      163\n      163\n      161\n      160\n      163\n      166\n      167\n      180\n      177\n      171\n      160\n      148\n      141\n      144\n      150\n      142\n      143\n      143\n      142\n      140\n      140\n      143\n      146\n      148\n      148\n      147\n      147\n      147\n      147\n      147\n      148\n      141\n      141\n      141\n      141\n      141\n      143\n      146\n      148\n      147\n      146\n      142\n      138\n      137\n      139\n      142\n      142\n      147\n      140\n      138\n      143\n      147\n      145\n      142\n      141\n      145\n      145\n      146\n      146\n      146\n      146\n      146\n      146\n      148\n      148\n      147\n      148\n      152\n      155\n      154\n      151\n      148\n      147\n      147\n      147\n      147\n      147\n      150\n      154\n      145\n      149\n      155\n      157\n      156\n      155\n      155\n      156\n      154\n      158\n      159\n      156\n    \n    \n      299\n      157\n      158\n      158\n      158\n      157\n      156\n      157\n      158\n      157\n      161\n      163\n      163\n      162\n      164\n      165\n      165\n      157\n      162\n      168\n      171\n      167\n      158\n      150\n      145\n      137\n      139\n      145\n      149\n      142\n      131\n      133\n      142\n      145\n      146\n      146\n      145\n      144\n      144\n      144\n      144\n      149\n      147\n      145\n      144\n      143\n      143\n      144\n      144\n      147\n      148\n      147\n      144\n      143\n      143\n      142\n      140\n      143\n      143\n      144\n      146\n      144\n      141\n      140\n      141\n      144\n      144\n      144\n      145\n      146\n      147\n      147\n      147\n      145\n      148\n      150\n      149\n      147\n      148\n      151\n      154\n      151\n      151\n      150\n      149\n      148\n      148\n      150\n      152\n      156\n      152\n      150\n      153\n      159\n      163\n      161\n      158\n      155\n      151\n      150\n      155\n    \n  \n\n\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      1014\n      1015\n      1016\n      1017\n      1018\n      1019\n      1020\n      1021\n      1022\n      1023\n    \n  \n  \n    \n      0\n      127\n      145\n      149\n      146\n      150\n      144\n      137\n      144\n      127\n      147\n      ...\n      167\n      170\n      170\n      171\n      171\n      170\n      169\n      168\n      167\n      166\n    \n    \n      1\n      165\n      152\n      143\n      141\n      141\n      144\n      146\n      145\n      140\n      135\n      ...\n      166\n      167\n      168\n      166\n      163\n      163\n      166\n      168\n      169\n      168\n    \n    \n      2\n      171\n      145\n      140\n      140\n      128\n      131\n      142\n      138\n      148\n      139\n      ...\n      161\n      163\n      163\n      157\n      151\n      151\n      154\n      156\n      154\n      151\n    \n    \n      3\n      154\n      140\n      146\n      152\n      138\n      135\n      141\n      135\n      146\n      144\n      ...\n      151\n      159\n      156\n      149\n      143\n      143\n      144\n      143\n      138\n      134\n    \n    \n      4\n      137\n      135\n      140\n      145\n      142\n      139\n      136\n      131\n      145\n      140\n      ...\n      143\n      156\n      150\n      145\n      141\n      141\n      142\n      141\n      140\n      139\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      678\n      146\n      134\n      133\n      132\n      130\n      133\n      132\n      134\n      134\n      132\n      ...\n      143\n      143\n      142\n      143\n      144\n      144\n      144\n      144\n      145\n      146\n    \n    \n      679\n      147\n      136\n      135\n      133\n      132\n      133\n      132\n      133\n      136\n      134\n      ...\n      143\n      143\n      142\n      142\n      142\n      143\n      145\n      147\n      147\n      147\n    \n    \n      680\n      147\n      132\n      134\n      131\n      135\n      132\n      131\n      130\n      127\n      123\n      ...\n      145\n      146\n      145\n      145\n      145\n      146\n      146\n      146\n      145\n      144\n    \n    \n      681\n      146\n      130\n      132\n      130\n      136\n      134\n      133\n      131\n      130\n      126\n      ...\n      145\n      146\n      145\n      145\n      145\n      145\n      146\n      146\n      145\n      144\n    \n    \n      682\n      145\n      128\n      129\n      129\n      137\n      136\n      135\n      132\n      132\n      130\n      ...\n      145\n      146\n      144\n      144\n      145\n      145\n      146\n      146\n      145\n      144\n    \n  \n\n683 rows × 1024 columns\n\n\n\n\n결국 이 이미지는 684 x 1024개의 숫자 모임이다.\n\n\nfig = plt.hist(img.flatten(),bins=256,range= [0,256])\n\n\n\n\n\n히스토그램을 그려보니 120 ~ 200 사이에 값들이 몰려있다.\n원래 0 ~ 255까지의 색을 표현할 수 있는데 컴퓨터가 표현가능한 색상보다 적은 조합만을 사용하고 있음.\n아이디어 : 좀 더 많은 색상을 표현하려면, 위 히스토그램을 평평하게, 즉 평활화 작업이 필요하다.\n\n\nimg2 = cv.equalizeHist(img)\n\n\nfig2_1 = plt.hist(img.flatten(), bins = 256, range=[0,256])\n\n\n\n\n\nfig2_2 = plt.hist(img.flatten(), bins = 10, range = [0, 256])\n\n\n\n\n\nplt.imshow(img2, cmap = \"gray\",vmin = 0, vmax = 256)\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x1a503770640>\n\n\n\n\n\n- 비교\n\nimport numpy as np\n\n\n_img = np.hstack((img,img2))\n\nplt.imshow(_img, cmap = \"gray\")\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x1a5033216a0>\n\n\n\n\n\n\n\n\n\nurl2 =  \"https://upload.wikimedia.org/wikipedia/commons/6/6e/Golde33443.jpg\"\nwget.download(url2)\n\n100% [............................................................................] 212652 / 212652\n\n\n'Golde33443.jpg'\n\n\n- 흑백이미지\n\n차원 : 세로픽셀수 x 가로픽셀수\n값 : 0 ~ 255 (값이 클수록 흰색)\n\n- 칼라이미지\n\n차원 : 세로픽셀수 x 가로픽셀수 x 3\n값 : 0 ~ 255 (값이 클수록 진한 빨강, 진한 녹색, 진한 파랑,) \\(\\to\\) 빛의 3원색을 이용하여 모든 색깔을 표현할 수 있다.\n\n\nimport cv2 as cv\n\ndog = cv.imread(\"Golde33443.jpg\")\n\nplt.imshow(dog,vmin=0,vmax=255)\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x1a503ef5100>\n\n\n\n\n\n- 차원을 비교해보면, 흑백 사진과 다르게 샘감이 있어 차이가 발생함을 알 수 있다.\n\nimg.shape\n\n(683, 1024)\n\n\n\ndog.shape\n\n(965, 800, 3)\n\n\n\n\n1 일단 원소값을 모두 0으로 한 array를 만듬\n\nd_red = np.zeros_like(dog)\nd_blue = np.zeros_like(dog)\nd_green = np.zeros_like(dog)\n\n2 빨강, 파랑, 노랑 순으로 데이터를 집어넣음.\n\nd_red[:,:,0] = dog[:,:,0]\nd_green[:,:,1] = dog[:,:,1]\nd_blue[:,:,2] = dog[:,:,2]\n\n\n_img = np.hstack((d_red,d_green,d_blue))\n\nplt.imshow(_img)\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x1a503a0c190>\n\n\n\n\n\n- 빨강 + 녹색\n\nplt.imshow(d_red + d_green)\n\n<matplotlib.image.AxesImage at 0x1a502f27a00>\n\n\n\n\n\n- 모든 색깔을 표현하려면?\n\nplt.imshow(d_red + d_green+ d_blue)\n\n<matplotlib.image.AxesImage at 0x1a50513f6d0>\n\n\n\n\n\n\n\n\n\n이미지 색감에 따른 크기 정도 \\(\\to\\) 빨 > 녹 > 파\n이미지는 단순히 숫자로 이루어진 매트릭스이다."
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-08-02wk.html#예제-1",
    "href": "post/Lecture/STDV/2023-04-08-02wk.html#예제-1",
    "title": "02. Histogram equalization",
    "section": "예제 1",
    "text": "예제 1\n\n키가 큰사람일 수록 몸무게도 많이 나간다.\n즉, 키와 몸궤는 정비례 관계에 있다.\n\n\nx = [44,48,49,58,62,68,69,70,76,79]\ny = [159,160,162,165,167,162,165,175,165,172]\n\n\n상관계수\n1\n\\[r=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\]\n2\n\\[r=\\sum_{i=1}^{n}\\Bigg(\\frac{(x_i-\\bar{x})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}}\\frac{(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\Bigg)\\]\n3\n\\[\\tilde{x}_i = \\frac{(x_i-\\bar{x})}{\\sqrt{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}}\\quad \\tilde{y}_i = \\frac{(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\]\n4\n\\[r=\\sum_{i=1}^{n} \\tilde{x}_i \\tilde{y}_i\\]\n\nx = np.array(x)\ny = np.array(y)\n\n- 각각의 분모자리 계산\n\na = np.sqrt( np.sum ((x-np.mean(x)) **2 ))\nb = np.sqrt( np.sum ((y-np.mean(y)) **2 ))\n\n\na,b\n\n(36.58004920718396, 15.218409903797438)\n\n\n- 분모 자리는 x,y의 분산꼴이므로 \\(a,b\\) 를 비교해보았을 때 \\(\\{x_i\\}\\)가 \\(\\{y_i\\}\\) 보다 평균에서 더 흩어져 있는 것 같다.\n\nxx = (x-np.mean(x))/a\nyy = (y-np.mean(y))/b\n\n\nfig, (ax1,ax2) = plt.subplots(1,2)\n\nax1.plot(x,y)\nax2.plot(xx,yy)\nax1.set_title(\"x,y\")\nax2.set_title(\"xx,yy\")\n\nText(0.5, 1.0, 'xx,yy')\n\n\n\n\n\n- 두 그림이 별로 차이가 없어보이지만 범위가 조정된 결과이다.\n\n즉, \\((x,\\tilde x)\\) \\((y,\\tilde y)\\)는 평균에서부터 떨어진 정도가 거의 비슷하다.\n\n- 질문 1 : \\(r\\)의 값이 양수인가? 음수인가?\n\nplotly를 이용한 산점도를 이용해 관측치를 살펴보자.\n\n\nimport plotly.express as ex\n\n\nfig = ex.scatter(x=xx,y=yy)\nfig.show(renderer=\"notebook\")\n\n\n                                                \n\n\n\n\\((\\tilde x, \\tilde y)\\) 값을 보니 점들이 0을 기준으로 1사분면과 3사분면에 몰려있다.\n\\((\\tilde x \\times \\tilde y)\\) 값이 양수인 것과 음수인 것을 체크해보자.\n양수인쪽이 많은지 음수인쪽이 많은지 보고 \\(r=\\sum_{i=1}^{n}\\tilde{x}_i \\tilde{y}_i\\) 을 직관적으로 생각해볼 수 있다."
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-08-02wk.html#예제-2",
    "href": "post/Lecture/STDV/2023-04-08-02wk.html#예제-2",
    "title": "02. Histogram equalization",
    "section": "예제 2",
    "text": "예제 2\n\n산점도를 이용한 상관계수 파악\n- 질문2 : 아래와 같은 두개 개의 데이터 셋이 있다고 하자.\n\n## set 1\nx1 = np.arange(0,10,0.1)\ny1 = x1 + np.random.normal(loc=0, scale=1, size=len(x1))\n\n## set2\n\nx2 = np.arange(0,10,0.1)\ny2 = x2 + np.random.normal(loc=0, scale=7, size=len(x2))\n\n\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(10,4))\n\nax1.plot(x1,y1,\"o\")\nax2.plot(x2,y2,\"o\")\nax1.set_title(\"scale= 1\")\nax2.set_title(\"scale= 7\")\n\nText(0.5, 1.0, 'scale= 7')\n\n\n\n\n\n- \\((\\tilde x, \\tilde y)\\) 구하기\n\nn = len(x1)\n\nxx1 = (x1-np.mean(x1))/ (np.std(x1)*np.sqrt(n))\nxx2 = (x2-np.mean(x2))/ (np.std(x2)*np.sqrt(n))\nyy1 = (y1-np.mean(y1))/ (np.std(y1)*np.sqrt(n))\nyy2 = (y2-np.mean(y2))/ (np.std(y2)*np.sqrt(n))\n\n\nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(10,4))\n\nax1.plot(xx1,yy1,\"o\")\nax2.plot(xx2,yy2,\"o\")\nax1.set_title(\"scale= 1\")\nax2.set_title(\"scale= 7\")\n\nText(0.5, 1.0, 'scale= 7')\n\n\n\n\n\n\n\\(r_1, r_2\\)의 부호는 양수인가? 음수인가?\n\\(r_1, r_2\\)의 값 중 어떠한 값이 더 절대값이 큰가?\n\n\nsum(xx1*yy1), sum(xx2*yy2)\n\n(0.9431351313287185, 0.32011558760698944)\n\n\n\n\nAncombe’s quartet\n\nx  = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\ny1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\ny2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\ny3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\nx4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\ny4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n\n\nfig, axs = plt.subplots(2,2)\n\naxs[0,0].plot(x,y1,\"o\")\naxs[0,1].plot(x,y2,\"o\")\naxs[1,0].plot(x,y3,\"o\")\naxs[1,1].plot(x4,y4,\"o\")\n\n\n\n\n\n위 4개의 그림은 상관계수가 모두 같은 그림이다 \\(\\to\\) 0.81652\n상관계수는 두 변수의 인과관계를 설명하기에 부적절하다.\n상관계수는 단지 두 변수가 선형관계에 있을 때 그 정도를 나타내는 통계량일 뿐이다.\n교훈: 기본적인 통계량들은 실제자료를 분석하기에 부적절할 수 있다. (= 통계량은 적절한 가정이 동반되어야 의미가 있다.)"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-15-4wk.html",
    "href": "post/Lecture/STDV/2023-04-15-4wk.html",
    "title": "03. 그래프를 그리는 여러 방법",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n\n- 구조 : axis \\(\\subset\\) axes \\(\\subset\\) figure\n- 목표 : plt.plot()을 사용하지 않고 아래 그림을 그려보자.\n\nplt.plot([1,2,3],\"or\")\n\n\n\n\n- 전략 : 그림을 만들고 (도화지를 준비) \\(\\to\\) axis를 만들고(네모틀을 만든다.) \\(\\to\\) axis에 그림을 그린다.\n\nfig = plt.figure() ## step1. 도화지 생성\n\nfig.add_axes([0,0,1,1]) ## step2. 도화지안에 fig(0,0) 위치에 길이가 (1,1)인 네모틀을 만든다.\n\naxis1 = fig.axes[0] ## step3. 첫 번째 axis\n\naxis1.plot([1,2,3],\"or\") ## step4. 첫 번째 axis에 접근하여 그림을 그림\n\n\n\n\n\n\n\n\nfig.add_axes([1,0,1,1]) ## fig (1,0)에 위치에 도화지 생성 세로로 생기는데 그냥 받아들이자\n\nfig\n\n\n\n\n\nax2= fig.axes[1] ## 두 번째 axis\n\n\nax2.plot ([1,2,3],\"ok\")\n\n\nfig\n\n\n\n\n- 첫 번째 axis에 그림추가\n\naxis1.plot([1,2,3],\"--\")\nfig\n\n\n\n\n\n\n\n\nfig = plt.figure()\n\nfig.subplots(1,2)\n\narray([<Axes: >, <Axes: >], dtype=object)\n\n\n\n\n\n\nax1, ax2 = fig.axes\n\n\nax1.plot([1,2,3],\"or\")\nax2.plot([1,2,3],\"ob\")\n\n\nfig\n\n\n\n\n\nfig.tight_layout()\nfig\n\n\n\n\n\nax1.plot([1,2,3],\"--\")\n\n\nfig\n\n\n\n\n\n\n\n\nfig = plt.figure()\nfig.subplots(2,2)\nfig.axes\n\n[<Axes: >, <Axes: >, <Axes: >, <Axes: >]\n\n\n\n\n\n\nax1,ax2,ax3,ax4 = fig.axes\n\n\nax1.plot([1,2,3],'ob')\nax2.plot([1,2,3],'or')\nax3.plot([1,2,3],'ok')\nax4.plot([1,2,3],'oy')\n\n\n\n\n\nx = [1,2,3,4]\ny = [1,2,4,3]\n_,axs = plt.subplots(2,2)\naxs[0,0].plot(x,y,\"o:r\")\naxs[0,1].plot(x,y,\"Xb\")\naxs[1,0].plot(x,y,\"xm\")\naxs[1,1].plot(x,y,\".--k\")\n\n\n\n\n\n\n\n\nx = [1,2,3,4]\ny = [1,2,4,3]\n\nfig, axs = plt.subplots(2,2)\n\n(ax1, ax2), (ax3, ax4) = axs\n\nax1.plot(x,y,\"o:r\")\nax2.plot(x,y,\"Xb\")\nax3.plot(x,y,\"xm\")\nax4.plot(x,y,\"--k\")\n\n\n\n\n\n\n\n\nfig,_ = plt.subplots(2,2)\n\nax1,ax2,ax3,ax4= fig.axes\n\nax1.plot(x,y,\"o:r\")\nax2.plot(x,y,\"Xb\")\nax3.plot(x,y,\"xm\")\nax4.plot(x,y,\"--k\")\n\n\n\n\n\n나한테 편한 방법을 선택해서 그리면 될 것 같당."
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-15-4wk.html#예제-1-plt.plot",
    "href": "post/Lecture/STDV/2023-04-15-4wk.html#예제-1-plt.plot",
    "title": "03. 그래프를 그리는 여러 방법",
    "section": "예제 1 : plt.plot()",
    "text": "예제 1 : plt.plot()\n\nx = [1,2,3]\ny = [1,2,2]\n\nplt.plot(x,y)\nplt.title(\"title\")\n\nText(0.5, 1.0, 'title')"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-15-4wk.html#예제-2-axis를-이용",
    "href": "post/Lecture/STDV/2023-04-15-4wk.html#예제-2-axis를-이용",
    "title": "03. 그래프를 그리는 여러 방법",
    "section": "예제 2 : axis를 이용",
    "text": "예제 2 : axis를 이용\n\nfig = plt.figure()\nfig.subplots()\n\nax1 = fig.axes[0]\nax1.set_title(\"title\")\n\nText(0.5, 1.0, 'title')"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-15-4wk.html#예제-3.-subplot에서-각각의-제목설정",
    "href": "post/Lecture/STDV/2023-04-15-4wk.html#예제-3.-subplot에서-각각의-제목설정",
    "title": "03. 그래프를 그리는 여러 방법",
    "section": "예제 3. subplot에서 각각의 제목설정",
    "text": "예제 3. subplot에서 각각의 제목설정\n\nfig, ax = plt.subplots(2,2)\n(ax1,ax2),(ax3,ax4) =  ax\nax1.set_title(\"title1\")\nax2.set_title(\"title2\")\nax3.set_title(\"title3\")\nax4.set_title(\"title4\")\n\nfig.tight_layout()"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-15-4wk.html#예제-4-axis의-제목-figure",
    "href": "post/Lecture/STDV/2023-04-15-4wk.html#예제-4-axis의-제목-figure",
    "title": "03. 그래프를 그리는 여러 방법",
    "section": "예제 4 : axis의 제목 + figure",
    "text": "예제 4 : axis의 제목 + figure\n\nfig.suptitle(\"sup title\")\n\nfig.tight_layout()\nfig"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-15-4wk.html#예제-1-plt.plot-1",
    "href": "post/Lecture/STDV/2023-04-15-4wk.html#예제-1-plt.plot-1",
    "title": "03. 그래프를 그리는 여러 방법",
    "section": "예제 1 : plt.plot",
    "text": "예제 1 : plt.plot\n\nx = [1,2,3]\ny = [4,5,6]\n\nplt.plot(x,y,'o')\nplt.xlim(-1,5)\nplt.ylim(3,7)\n\n(3.0, 7.0)"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-15-4wk.html#예제-2-subplotsb",
    "href": "post/Lecture/STDV/2023-04-15-4wk.html#예제-2-subplotsb",
    "title": "03. 그래프를 그리는 여러 방법",
    "section": "예제 2: subplotsb",
    "text": "예제 2: subplotsb\n\nimport numpy as np\n\n\nfig = plt.figure()\nfig.subplots()\n\nax1 = fig.axes[0]\n\nax1.plot(np.random.normal(size=100),\"o\")\n\nax1.set_xlim(-10,110)\nax1.set_ylim(-5,5)\n\n(-5.0, 5.0)"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-15-4wk.html#예제-1",
    "href": "post/Lecture/STDV/2023-04-15-4wk.html#예제-1",
    "title": "03. 그래프를 그리는 여러 방법",
    "section": "예제 1",
    "text": "예제 1\n\nimport numpy as np\n\nnp.random.seed(202150256)\n\nx1 = np.linspace(-1,1,100,endpoint=True)\ny1 = x1**2 + np.random.normal(scale=0.1,size=100)\n\nplt.plot(x1,y1,\"o\")\nplt.title(\"y=x**2\")\n\nText(0.5, 1.0, 'y=x**2')\n\n\n\n\n\n\nnp.corrcoef(x1,y1)\n\narray([[1.        , 0.01388119],\n       [0.01388119, 1.        ]])\n\n\n- (표본) 상관계수의 값이 0에 가까운 것은 두 변수의 직선관계가 약한 것을 의미한 것이지 두 변수 사이에 아무런 함수관계가 없다는 것을 의미하는 것이 아니다."
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-15-4wk.html#예졔-2",
    "href": "post/Lecture/STDV/2023-04-15-4wk.html#예졔-2",
    "title": "03. 그래프를 그리는 여러 방법",
    "section": "예졔 2",
    "text": "예졔 2\n\nnp.random.seed(202150256)\n\nx2 = np.random.uniform(low=-1,high=1,size=100000)\ny2 = np.random.uniform(low=-1,high=1,size=100000)\n\nplt.plot(x2,y2,\".\")\nplt.title(\"rect\")\n\nText(0.5, 1.0, 'rect')\n\n\n\n\n\n\nnp.corrcoef(x2,y2)\n\narray([[1.       , 0.0059896],\n       [0.0059896, 1.       ]])"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-15-4wk.html#예제-3",
    "href": "post/Lecture/STDV/2023-04-15-4wk.html#예제-3",
    "title": "03. 그래프를 그리는 여러 방법",
    "section": "예제 3",
    "text": "예제 3\n\nradius = x2**2+ y2**2\n\n\nx3 = x2[radius<1]\ny3 = y2[radius<1]\nplt.plot(x2,y2,\".\")\nplt.plot(x3,y3,\".\")\nplt.title(\"circ\")\nplt.tight_layout()\n\n\n\n\n\nnp.corrcoef(x3,y3)\n\narray([[1.        , 0.00311474],\n       [0.00311474, 1.        ]])"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-15-4wk.html#예제-23으로-알아보는-두-변수의-독립성",
    "href": "post/Lecture/STDV/2023-04-15-4wk.html#예제-23으로-알아보는-두-변수의-독립성",
    "title": "03. 그래프를 그리는 여러 방법",
    "section": "예제 2,3으로 알아보는 두 변수의 독립성",
    "text": "예제 2,3으로 알아보는 두 변수의 독립성\n다음과 같은 경우를 고려하여 보자\n1. $ X $ 일 경우 \\(Y\\) 의 분포를 생각해보자. 그리고 히스토그램을 그려보자.\n2. \\(X \\in [0.9-h, 0.9 + h]\\)일 경우 \\(Y\\)의 분포를 생각해보자. 그리고 히스토그램을 그려보자.\n3. 1 - 2를 비교해보자.\n\nh = 0.05\n\n\nfig, axs = plt.subplots(2,2, figsize=(12,8))\n(ax1,ax2),(ax3,ax4) = axs\nax1.hist(y2[(x2>-h) * (x2<h)])\nax1.set_title(\"x in (-h,h) of example 2\")\nax2.hist(y3[(x3>-h) * (x3<h)])\nax2.set_title(\"x in (-h,h) of example 3\")\nax3.hist(y2[(x2>0.9-h) * (x2<0.9+h)])\nax3.set_title(\"x in (0.9-h,0.9+h) of example 2\")\nax4.hist(y3[(x3>0.9-h) * (x3<0.9+h)])\nax4.set_title(\"x in (0.9-h,0.9+h) of example 3\")\nax4.set_xlim(-1,1)\nfig.tight_layout()\n\n\n\n\n\n예제 3같은 경우 \\(x\\)의 범위에 따라 \\(y\\)에 분포가 달라짐을 볼 수 있는데 이는 독립이 아니라는 것을 뜻한다.\n\n\nnp.corrcoef(x3,y3)\n\narray([[1.        , 0.00311474],\n       [0.00311474, 1.        ]])\n\n\n\n위 처럼 상관계수가 낮은 값을 갖더라도 \\(x\\)의 범위에 따라 \\(y\\)값이 변하므로 독립이 아니라고 판단할 수 있다."
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-15-4wk.html#예제-4-또-다른-예제",
    "href": "post/Lecture/STDV/2023-04-15-4wk.html#예제-4-또-다른-예제",
    "title": "03. 그래프를 그리는 여러 방법",
    "section": "예제 4 : 또 다른 예제",
    "text": "예제 4 : 또 다른 예제\n\nnp.random.seed(202150256)\n\nx4 = np.random.normal(size=10000)\ny4 = np.random.normal(size=10000)\n\nplt.scatter(x4,y4,alpha=0.01)\n\n<matplotlib.collections.PathCollection at 0x7fd1279e44c0>\n\n\n\n\n\n\nnp.corrcoef(x4,y4)\n\narray([[ 1.        , -0.00359161],\n       [-0.00359161,  1.        ]])\n\n\n\n상관계수만 보았을 때는 \\(x,y\\) 관계가 아무런 독립 즉, 아무런 연관이 없는 것 같다.\n전 예제처럼 x 값의 범위를 변경해주며 확인하여 보자\n\n\nk = np.linspace(-2,2,9)\nk\n\narray([-2. , -1.5, -1. , -0.5,  0. ,  0.5,  1. ,  1.5,  2. ])\n\n\n::: {.cell 0=‘c’ 1=‘o’ 2=‘d’ 3=‘e’ 4=‘-’ 5=‘f’ 6=‘o’ 7=‘l’ 8=‘d’ 9=’ ’ 10=‘:’ 11=‘t’ 12=‘r’ 13=‘u’ 14=‘e’ executionInfo=‘{“elapsed”:2172,“status”:“ok”,“timestamp”:1681959577833,“user”:{“displayName”:“이강철”,“userId”:“13507850890638580947”},“user_tz”:-540}’ outputId=‘68ef56b0-eab1-4d70-e031-e491744064c5’ execution_count=61}\nfig,axes = plt.subplots(3,3 , figsize=(10,5))\nfor i in range(9) :\n    fig.axes[i].hist(y4[ (x4>k[i]-h) * (x4< k[i]+h)])\n\n\n\n:::\n\n히스토그램을 살펴본 결과 밀도는 좀 다를 수 있나 분포의 모양이 전부 종모양으로 독립이라고 판단할 수 있다."
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-15-4wk.html#matplotlib-vs-seaborn",
    "href": "post/Lecture/STDV/2023-04-15-4wk.html#matplotlib-vs-seaborn",
    "title": "03. 그래프를 그리는 여러 방법",
    "section": "matplotlib vs seaborn",
    "text": "matplotlib vs seaborn\n- 디자인이 예쁜 패키지를 선택하여 하나만 공부하는 것은 그렇게 좋은 전략이 아니다.\n- 아래와 같이 seaborn 테마를 입혀 matplotlib에 적용할 수 도 있다.\n\nsns.set_theme()\nplt.plot([1,2,3],[4,5,6],\"or\")"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-15-4wk.html#예제-정규분포-체크",
    "href": "post/Lecture/STDV/2023-04-15-4wk.html#예제-정규분포-체크",
    "title": "03. 그래프를 그리는 여러 방법",
    "section": "예제 : 정규분포 체크",
    "text": "예제 : 정규분포 체크\n\nnp.random.seed(202150256)\nx = np.random.normal(size=1000,loc=2,scale=1.5)\n\nplt.hist(x)\n\n(array([  1.,  13.,  44., 129., 231., 278., 181.,  89.,  27.,   7.]),\n array([-3.62829921, -2.57400059, -1.51970198, -0.46540336,  0.58889525,\n         1.64319387,  2.69749249,  3.7517911 ,  4.80608972,  5.86038834,\n         6.91468695]),\n <BarContainer object of 10 artists>)\n\n\n\n\n\n- 기존에 내가배웠던 것 : 종모양이니 정규분포인 듯 하다.\n- + 밀도추정곡선을 그려 정규성을 추정해보자. \\(\\to\\) seaborn을 활용\n\nsns.histplot(x,kde=True)\n\n<Axes: ylabel='Count'>\n\n\n\n\n\n- 그렇다면 아래와 같은 경우는 어떤가? \\(\\to\\) t분포의 경우\n\nnp.random.seed(202150256)\n\nfrom scipy import stats\n\ny = stats.t.rvs(10,size=1000) ## 자유도가 10이고 size가 1000인 t분포를 생성\n\nsns.histplot(y,kde=True)\n\n<Axes: ylabel='Count'>\n\n\n\n\n\n- 생성한 y도 종모양이다.\n- 비교 : 둘다 비슷한 것 같으면서도 scale이 조큼 다르다.\n\n\nCode\nfig, axes = plt.subplots(1,2 , figsize=(10,5))\nax1,ax2 = axes\nsns.histplot(x,ax=ax1,kde=True)\nsns.histplot(y,ax=ax2,kde=True)\n\n\n<Axes: ylabel='Count'>\n\n\n\n\n\n- 표준화 수행\n\n\nCode\nfig, axes = plt.subplots(1,2 , figsize=(10,5))\nax1,ax2 = axes\nxx = (x-np.mean(x))/np.std(x,ddof=1)\nyy = (y-np.mean(y))/np.std(y,ddof=1)\n\nsns.histplot(xx,ax=ax1,kde=True)\nsns.histplot(yy,ax=ax2,kde=True)\n\n\n<Axes: ylabel='Count'>\n\n\n\n\n\n- boxplot과 함께 비교\n\n?sns.boxplot\n\n\nfig, axes = plt.subplots(2,2, figsize=(15,5))\n(ax1,ax2),(ax3,ax4) = axes\nax1.set_title(\"normal distribution\")\nax2.set_title(\"normal distribution\")\n\nax3.set_title(\"t distribution\")\nax4.set_title(\"t distribution\")\n\nsns.boxplot(xx,ax=ax1,orient=\"h\")\nsns.histplot(xx,ax=ax2)\n\nsns.boxplot(yy,ax=ax3,orient=\"h\")\nsns.histplot(yy,ax=ax4)\nfig.tight_layout()\n\n\n\n\n\n정규분포와 다르게 t분포는 이상치라고 판단되는점이 더 많다.\n\n- 난 아직도 이걸보고 정규성? 이라는 것을 판단하기 버거워 - > 종모양을 보고 \\(y\\)는 정규분포야 라고 정확히 판단하는 건 오류가 있는 것 같에\n- 관찰 : boxplot을 그려보니 \\(y\\)의 꼬리가 정규분포보다두꺼워 보인다.\n- 즉 단순히 histogram 만 보고 데이터가 정규성이라고 판단하는 것은 오류의 가능성이 있다…\n- qqplot을 배울거니 걱정하지 말자."
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-29-6wk.html",
    "href": "post/Lecture/STDV/2023-04-29-6wk.html",
    "title": "05. Plotnine & Pandas",
    "section": "",
    "text": "!pip install rpy2==3.5.1"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-29-6wk.html#기본-2차원의-산점도",
    "href": "post/Lecture/STDV/2023-04-29-6wk.html#기본-2차원의-산점도",
    "title": "05. Plotnine & Pandas",
    "section": "기본 2차원의 산점도",
    "text": "기본 2차원의 산점도\n\nmpg.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      manufacturer\n      model\n      displ\n      year\n      cyl\n      trans\n      drv\n      cty\n      hwy\n      fl\n      class\n    \n  \n  \n    \n      0\n      audi\n      a4\n      1.8\n      1999\n      4\n      auto(l5)\n      f\n      18\n      29\n      p\n      compact\n    \n    \n      1\n      audi\n      a4\n      1.8\n      1999\n      4\n      manual(m5)\n      f\n      21\n      29\n      p\n      compact\n    \n    \n      2\n      audi\n      a4\n      2.0\n      2008\n      4\n      manual(m6)\n      f\n      20\n      31\n      p\n      compact\n    \n    \n      3\n      audi\n      a4\n      2.0\n      2008\n      4\n      auto(av)\n      f\n      21\n      30\n      p\n      compact\n    \n    \n      4\n      audi\n      a4\n      2.8\n      1999\n      6\n      auto(l5)\n      f\n      16\n      26\n      p\n      compact\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nggplot(data = mpg) \\\n  + geom_point(aes(x= \"displ\", y = \"hwy\"))\n\n\n\n\n<ggplot: (8759067292480)>\n\n\n- 산점도를 보고 엔진크기와 연료 효율은 반비례 관계임을 알 수 있다."
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-29-6wk.html#산점도-응용-3차원",
    "href": "post/Lecture/STDV/2023-04-29-6wk.html#산점도-응용-3차원",
    "title": "05. Plotnine & Pandas",
    "section": "산점도 응용 (3차원)",
    "text": "산점도 응용 (3차원)\n\nmpg.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      manufacturer\n      model\n      displ\n      year\n      cyl\n      trans\n      drv\n      cty\n      hwy\n      fl\n      class\n    \n  \n  \n    \n      0\n      audi\n      a4\n      1.8\n      1999\n      4\n      auto(l5)\n      f\n      18\n      29\n      p\n      compact\n    \n    \n      1\n      audi\n      a4\n      1.8\n      1999\n      4\n      manual(m5)\n      f\n      21\n      29\n      p\n      compact\n    \n    \n      2\n      audi\n      a4\n      2.0\n      2008\n      4\n      manual(m6)\n      f\n      20\n      31\n      p\n      compact\n    \n    \n      3\n      audi\n      a4\n      2.0\n      2008\n      4\n      auto(av)\n      f\n      21\n      30\n      p\n      compact\n    \n    \n      4\n      audi\n      a4\n      2.8\n      1999\n      6\n      auto(l5)\n      f\n      16\n      26\n      p\n      compact\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- class도 함께 표시하면 데이터를 탐색할 때 좀 더 좋을 것 같다.\n\n산점도 + 점크기 변경\n\nggplot(data = mpg) \\\n  + geom_point(aes(x=\"displ\",y=\"hwy\",size = \"class\"))\n\n/usr/local/lib/python3.10/dist-packages/plotnine/scales/scale_size.py:48: PlotnineWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n<ggplot: (8759061901174)>\n\n\n\n\n산점도 + 투명도 변경\n\nggplot(data = mpg) \\\n  + geom_point(aes(x=\"displ\",y=\"hwy\",alpha = \"class\"))\n\n/usr/local/lib/python3.10/dist-packages/plotnine/scales/scale_alpha.py:68: PlotnineWarning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n<ggplot: (8759061767796)>\n\n\n\n\n산점도 + 투명도 + 점크기 변경\n\nggplot(mpg) \\\n    + geom_point(aes(x=\"displ\",y=\"hwy\",size=\"class\",alpha=\"class\"))\n\n/usr/local/lib/python3.10/dist-packages/plotnine/scales/scale_size.py:48: PlotnineWarning: Using size for a discrete variable is not advised.\n/usr/local/lib/python3.10/dist-packages/plotnine/scales/scale_alpha.py:68: PlotnineWarning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n<ggplot: (8759061696531)>\n\n\n\n\n산점도 + 형태\n\nggplot(mpg) \\\n    + geom_point(aes(x=\"displ\",y=\"hwy\",shape=\"class\"))\n\n\n\n\n<ggplot: (8759061754028)>\n\n\n\n\n산점도 + 색깔\n\nggplot(mpg) \\\n    + geom_point(aes(x=\"displ\",y=\"hwy\",color=\"class\"))\n\n\n\n\n<ggplot: (8759061683729)>\n\n\n\n\n산점도 + 색깔 + 스무딩\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nggplot(mpg,aes(x=\"displ\",y=\"hwy\")) \\\n    + geom_point(aes(color=\"class\")) \\\n    + geom_smooth()\n\n\n\n\n<ggplot: (8759061598184)>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-29-6wk.html#산점도-응용4차원",
    "href": "post/Lecture/STDV/2023-04-29-6wk.html#산점도-응용4차원",
    "title": "05. Plotnine & Pandas",
    "section": "산점도 응용(4차원)",
    "text": "산점도 응용(4차원)\n\n(\nggplot(mpg) \n    +geom_point(aes(x=\"displ\",y=\"hwy\",size=\"class\",color=\"drv\"),alpha=0.2)\n)\n\n\n\n\n<ggplot: (8759061586275)>\n\n\n- 해석 : 모든 \\(x\\)에 대하여 붉은색 점들이 대부분 초록색 점과 보라색 점들에 비해 아래쪽에 위치에 있음 \\(\\to\\) 4륜 구동방식이 연비가 좋지 않음\n\nsmooth 추가\n\n(\nggplot(mpg,aes(x=\"displ\",y=\"hwy\")) \n    +geom_point(aes(color=\"drv\",size=\"class\"),alpha=0.2)\n    +geom_smooth()\n)\n\n\n\n\n<ggplot: (8759103927486)>\n\n\n\n\n구동방식에 따라 smooth 따로 그리기`\n\n(\nggplot(mpg,aes(x=\"displ\",y=\"hwy\")) +\n    geom_point(aes(color=\"drv\",size=\"class\"),alpha=0.3) +\n    geom_smooth(aes(color=\"drv\"))\n)\n\n\n\n\n<ggplot: (8759061774472)>\n\n\n\n구동방식 별로 선의 색깔은 동일시 하되 타입을 변경하기\n\n(\nggplot(mpg,aes(x=\"displ\",y=\"hwy\")) +\n  geom_point(aes(color=\"drv\",size=\"class\"),alpha=0.3) +\n    geom_smooth(aes(linetype=\"drv\"),size=0.5)\n)\n\n\n\n\n<ggplot: (8759059312481)>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-29-6wk.html#summary",
    "href": "post/Lecture/STDV/2023-04-29-6wk.html#summary",
    "title": "05. Plotnine & Pandas",
    "section": "Summary",
    "text": "Summary\n- 고차원 변수를 표현할 수 있는 무기는 다양하다. * 산점도 : 점의 크기, 형태, 색깔, 투명도 * 라인플랏 : 선의 형태, 색깔, 굵기\n- geom과 mapping만 잘 이용해도 아주 다양한 그래프를 그릴 수 있음."
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-29-6wk.html#방법-1",
    "href": "post/Lecture/STDV/2023-04-29-6wk.html#방법-1",
    "title": "05. Plotnine & Pandas",
    "section": "방법 1",
    "text": "방법 1\n\ndf.X1\n\n0   -0.212599\n1    0.079671\n2    0.296181\n3    1.051856\n4   -0.463166\nName: X1, dtype: float64"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-29-6wk.html#방법-2",
    "href": "post/Lecture/STDV/2023-04-29-6wk.html#방법-2",
    "title": "05. Plotnine & Pandas",
    "section": "방법 2",
    "text": "방법 2\n\ndf[\"X1\"]\n\n0   -0.212599\n1    0.079671\n2    0.296181\n3    1.051856\n4   -0.463166\nName: X1, dtype: float64"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-29-6wk.html#방법-3",
    "href": "post/Lecture/STDV/2023-04-29-6wk.html#방법-3",
    "title": "05. Plotnine & Pandas",
    "section": "방법 3",
    "text": "방법 3\n\ndf[[\"X1\"]]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      X1\n    \n  \n  \n    \n      0\n      -0.212599\n    \n    \n      1\n      0.079671\n    \n    \n      2\n      0.296181\n    \n    \n      3\n      1.051856\n    \n    \n      4\n      -0.463166\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- df[\"X1\"]은 serise를 리턴하고 df[[\"X1\"]] 은 데이터프레임을 리턴한다."
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-29-6wk.html#방법-4",
    "href": "post/Lecture/STDV/2023-04-29-6wk.html#방법-4",
    "title": "05. Plotnine & Pandas",
    "section": "방법 4",
    "text": "방법 4\n\ndf.loc[:, \"X1\"]\n\n0   -0.212599\n1    0.079671\n2    0.296181\n3    1.051856\n4   -0.463166\nName: X1, dtype: float64"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-29-6wk.html#방법-5",
    "href": "post/Lecture/STDV/2023-04-29-6wk.html#방법-5",
    "title": "05. Plotnine & Pandas",
    "section": "방법 5",
    "text": "방법 5\n\ndf.loc[:,[\"X1\"]]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      X1\n    \n  \n  \n    \n      0\n      -0.212599\n    \n    \n      1\n      0.079671\n    \n    \n      2\n      0.296181\n    \n    \n      3\n      1.051856\n    \n    \n      4\n      -0.463166"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-29-6wk.html#방법-6",
    "href": "post/Lecture/STDV/2023-04-29-6wk.html#방법-6",
    "title": "05. Plotnine & Pandas",
    "section": "방법 6",
    "text": "방법 6\n\ndf.loc[:, [True, False, False]]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      X1\n    \n  \n  \n    \n      0\n      -0.212599\n    \n    \n      1\n      0.079671\n    \n    \n      2\n      0.296181\n    \n    \n      3\n      1.051856\n    \n    \n      4\n      -0.463166\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.iloc[:,0:2]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      X1\n      X2\n    \n  \n  \n    \n      0\n      -0.212599\n      1.101194\n    \n    \n      1\n      0.079671\n      -0.414452\n    \n    \n      2\n      0.296181\n      0.454763\n    \n    \n      3\n      1.051856\n      -0.070905\n    \n    \n      4\n      -0.463166\n      1.295350"
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-29-6wk.html#참고-열이름이-integer인-경우",
    "href": "post/Lecture/STDV/2023-04-29-6wk.html#참고-열이름이-integer인-경우",
    "title": "05. Plotnine & Pandas",
    "section": "참고 : 열이름이 integer인 경우",
    "text": "참고 : 열이름이 integer인 경우\n\n_df = pd.DataFrame(np.array([[1,2,3],[4,5,6],[7,8,9]]))\n\n\n_df\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      0\n      1\n      2\n      3\n    \n    \n      1\n      4\n      5\n      6\n    \n    \n      2\n      7\n      8\n      9\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n_df.iloc[:,0:1]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      4\n    \n    \n      2\n      7\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n_df.loc[:,0:1]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n      1\n    \n  \n  \n    \n      0\n      1\n      2\n    \n    \n      1\n      4\n      5\n    \n    \n      2\n      7\n      8\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n위를 보고 컬럼명이 intiger일경우 loc 와 iloc의 차이를 구별할 줄 알아야한다."
  },
  {
    "objectID": "post/Lecture/STDV/2023-04-29-6wk.html#예제-movie-data---특정-조건에-맞는-열을-선택",
    "href": "post/Lecture/STDV/2023-04-29-6wk.html#예제-movie-data---특정-조건에-맞는-열을-선택",
    "title": "05. Plotnine & Pandas",
    "section": "예제 : movie data - 특정 조건에 맞는 열을 선택",
    "text": "예제 : movie data - 특정 조건에 맞는 열을 선택\n\nactor라는 단어가 포함된 변수들만 뽑기\n\nimport pandas as pd\n\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/movie.csv\")\n\n\ndf.iloc[:,map(lambda x : \"actor\" in x, df.columns)]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      actor_3_facebook_likes\n      actor_2_name\n      actor_1_facebook_likes\n      actor_1_name\n      actor_3_name\n      actor_2_facebook_likes\n    \n  \n  \n    \n      0\n      855.0\n      Joel David Moore\n      1000.0\n      CCH Pounder\n      Wes Studi\n      936.0\n    \n    \n      1\n      1000.0\n      Orlando Bloom\n      40000.0\n      Johnny Depp\n      Jack Davenport\n      5000.0\n    \n    \n      2\n      161.0\n      Rory Kinnear\n      11000.0\n      Christoph Waltz\n      Stephanie Sigman\n      393.0\n    \n    \n      3\n      23000.0\n      Christian Bale\n      27000.0\n      Tom Hardy\n      Joseph Gordon-Levitt\n      23000.0\n    \n    \n      4\n      NaN\n      Rob Walker\n      131.0\n      Doug Walker\n      NaN\n      12.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4911\n      318.0\n      Daphne Zuniga\n      637.0\n      Eric Mabius\n      Crystal Lowe\n      470.0\n    \n    \n      4912\n      319.0\n      Valorie Curry\n      841.0\n      Natalie Zea\n      Sam Underwood\n      593.0\n    \n    \n      4913\n      0.0\n      Maxwell Moody\n      0.0\n      Eva Boehnke\n      David Chandler\n      0.0\n    \n    \n      4914\n      489.0\n      Daniel Henney\n      946.0\n      Alan Ruck\n      Eliza Coupe\n      719.0\n    \n    \n      4915\n      16.0\n      Brian Herzlinger\n      86.0\n      John August\n      Jon Gunn\n      23.0\n    \n  \n\n4916 rows × 6 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n변수 이름이 s로 끝나는 변수들만 뽑기\n\ndf.iloc[: , map(lambda x : \"s\" in x[-1],df.columns)]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      num_critic_for_reviews\n      director_facebook_likes\n      actor_3_facebook_likes\n      actor_1_facebook_likes\n      gross\n      genres\n      num_voted_users\n      cast_total_facebook_likes\n      plot_keywords\n      num_user_for_reviews\n      actor_2_facebook_likes\n      movie_facebook_likes\n    \n  \n  \n    \n      0\n      723.0\n      0.0\n      855.0\n      1000.0\n      760505847.0\n      Action|Adventure|Fantasy|Sci-Fi\n      886204\n      4834\n      avatar|future|marine|native|paraplegic\n      3054.0\n      936.0\n      33000\n    \n    \n      1\n      302.0\n      563.0\n      1000.0\n      40000.0\n      309404152.0\n      Action|Adventure|Fantasy\n      471220\n      48350\n      goddess|marriage ceremony|marriage proposal|pi...\n      1238.0\n      5000.0\n      0\n    \n    \n      2\n      602.0\n      0.0\n      161.0\n      11000.0\n      200074175.0\n      Action|Adventure|Thriller\n      275868\n      11700\n      bomb|espionage|sequel|spy|terrorist\n      994.0\n      393.0\n      85000\n    \n    \n      3\n      813.0\n      22000.0\n      23000.0\n      27000.0\n      448130642.0\n      Action|Thriller\n      1144337\n      106759\n      deception|imprisonment|lawlessness|police offi...\n      2701.0\n      23000.0\n      164000\n    \n    \n      4\n      NaN\n      131.0\n      NaN\n      131.0\n      NaN\n      Documentary\n      8\n      143\n      NaN\n      NaN\n      12.0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4911\n      1.0\n      2.0\n      318.0\n      637.0\n      NaN\n      Comedy|Drama\n      629\n      2283\n      fraud|postal worker|prison|theft|trial\n      6.0\n      470.0\n      84\n    \n    \n      4912\n      43.0\n      NaN\n      319.0\n      841.0\n      NaN\n      Crime|Drama|Mystery|Thriller\n      73839\n      1753\n      cult|fbi|hideout|prison escape|serial killer\n      359.0\n      593.0\n      32000\n    \n    \n      4913\n      13.0\n      0.0\n      0.0\n      0.0\n      NaN\n      Drama|Horror|Thriller\n      38\n      0\n      NaN\n      3.0\n      0.0\n      16\n    \n    \n      4914\n      14.0\n      0.0\n      489.0\n      946.0\n      10443.0\n      Comedy|Drama|Romance\n      1255\n      2386\n      NaN\n      9.0\n      719.0\n      660\n    \n    \n      4915\n      43.0\n      16.0\n      16.0\n      86.0\n      85222.0\n      Documentary\n      4285\n      163\n      actress name in title|crush|date|four word tit...\n      84.0\n      23.0\n      456\n    \n  \n\n4916 rows × 12 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n변수 이름이 c 혹은 d로 시작하는 변수들만 뽑고싶다.\n\ndf.iloc[:,map(lambda x : \"c\" in x[0] or \"d\" in x[0],df.columns)]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      color\n      director_name\n      duration\n      director_facebook_likes\n      cast_total_facebook_likes\n      country\n      content_rating\n    \n  \n  \n    \n      0\n      Color\n      James Cameron\n      178.0\n      0.0\n      4834\n      USA\n      PG-13\n    \n    \n      1\n      Color\n      Gore Verbinski\n      169.0\n      563.0\n      48350\n      USA\n      PG-13\n    \n    \n      2\n      Color\n      Sam Mendes\n      148.0\n      0.0\n      11700\n      UK\n      PG-13\n    \n    \n      3\n      Color\n      Christopher Nolan\n      164.0\n      22000.0\n      106759\n      USA\n      PG-13\n    \n    \n      4\n      NaN\n      Doug Walker\n      NaN\n      131.0\n      143\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4911\n      Color\n      Scott Smith\n      87.0\n      2.0\n      2283\n      Canada\n      NaN\n    \n    \n      4912\n      Color\n      NaN\n      43.0\n      NaN\n      1753\n      USA\n      TV-14\n    \n    \n      4913\n      Color\n      Benjamin Roberds\n      76.0\n      0.0\n      0\n      USA\n      NaN\n    \n    \n      4914\n      Color\n      Daniel Hsia\n      100.0\n      0.0\n      2386\n      USA\n      PG-13\n    \n    \n      4915\n      Color\n      Jon Gunn\n      90.0\n      16.0\n      163\n      USA\n      PG\n    \n  \n\n4916 rows × 7 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.iloc[:, map(lambda  x: (\"c\" in x[0]) | (\"d\" in x[0]), df.columns  )]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      color\n      director_name\n      duration\n      director_facebook_likes\n      cast_total_facebook_likes\n      country\n      content_rating\n    \n  \n  \n    \n      0\n      Color\n      James Cameron\n      178.0\n      0.0\n      4834\n      USA\n      PG-13\n    \n    \n      1\n      Color\n      Gore Verbinski\n      169.0\n      563.0\n      48350\n      USA\n      PG-13\n    \n    \n      2\n      Color\n      Sam Mendes\n      148.0\n      0.0\n      11700\n      UK\n      PG-13\n    \n    \n      3\n      Color\n      Christopher Nolan\n      164.0\n      22000.0\n      106759\n      USA\n      PG-13\n    \n    \n      4\n      NaN\n      Doug Walker\n      NaN\n      131.0\n      143\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4911\n      Color\n      Scott Smith\n      87.0\n      2.0\n      2283\n      Canada\n      NaN\n    \n    \n      4912\n      Color\n      NaN\n      43.0\n      NaN\n      1753\n      USA\n      TV-14\n    \n    \n      4913\n      Color\n      Benjamin Roberds\n      76.0\n      0.0\n      0\n      USA\n      NaN\n    \n    \n      4914\n      Color\n      Daniel Hsia\n      100.0\n      0.0\n      2386\n      USA\n      PG-13\n    \n    \n      4915\n      Color\n      Jon Gunn\n      90.0\n      16.0\n      163\n      USA\n      PG\n    \n  \n\n4916 rows × 7 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nface라는 단어가 포함된 변수열을 선택하라.\n\ndf.iloc[:, map(lambda x : \"face\" in x, df.columns)]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      director_facebook_likes\n      actor_3_facebook_likes\n      actor_1_facebook_likes\n      cast_total_facebook_likes\n      facenumber_in_poster\n      actor_2_facebook_likes\n      movie_facebook_likes\n    \n  \n  \n    \n      0\n      0.0\n      855.0\n      1000.0\n      4834\n      0.0\n      936.0\n      33000\n    \n    \n      1\n      563.0\n      1000.0\n      40000.0\n      48350\n      0.0\n      5000.0\n      0\n    \n    \n      2\n      0.0\n      161.0\n      11000.0\n      11700\n      1.0\n      393.0\n      85000\n    \n    \n      3\n      22000.0\n      23000.0\n      27000.0\n      106759\n      0.0\n      23000.0\n      164000\n    \n    \n      4\n      131.0\n      NaN\n      131.0\n      143\n      0.0\n      12.0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4911\n      2.0\n      318.0\n      637.0\n      2283\n      2.0\n      470.0\n      84\n    \n    \n      4912\n      NaN\n      319.0\n      841.0\n      1753\n      1.0\n      593.0\n      32000\n    \n    \n      4913\n      0.0\n      0.0\n      0.0\n      0\n      0.0\n      0.0\n      16\n    \n    \n      4914\n      0.0\n      489.0\n      946.0\n      2386\n      5.0\n      719.0\n      660\n    \n    \n      4915\n      16.0\n      16.0\n      86.0\n      163\n      0.0\n      23.0\n      456\n    \n  \n\n4916 rows × 7 columns"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-06-7wk.html",
    "href": "post/Lecture/STDV/2023-05-06-7wk.html",
    "title": "06. Partial Correlation & Pandas Handling",
    "section": "",
    "text": "- 내가 알고있는 것 * 특정 변수간에 상관계수를 구할 때 제 3의 요인을 통제하여 상관계수를 계산한다.\n\n\n\n여름철에 수영장을 많이가서 소아마비에 빈도가 늘었다.\n여름철엔 아이스크림을 많이 먹는다.\n“아이스크림과 소아마비는 상관관계가 높을 것이다” 라는 것이 이 문제의 핵심 \\(\\to\\) 사실은 전혀 그렇지 않음\n\n\nimport pandas as pd\nfrom plotnine import *\nimport numpy as np\n\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/extremum.csv')\n\n\ndf.columns\n\nIndex(['지점번호', '지점명', '일시', '평균기온(℃)', '최고기온(℃)', '최고기온시각', '최저기온(℃)',\n       '최저기온시각일교차', 'Unnamed: 8'],\n      dtype='object')\n\n\n- 전처리 작업\n\ntemp = df.iloc[:,3]\n\n\\[\\text{diseas} = \\text{temp} \\times 0.5 + 40 + ɛ \\quad \\varepsilon_i \\sim N(0,\\sigma^2)\\]\n\ne1 = np.random.normal(size=len(df),scale=10)\nicecream = 30 + temp*2+e1\nϵ2=np.random.normal(size=656,scale=1)\ndisease=temp*0.5 + 40 +ϵ2\ndf1 = pd.DataFrame({\"temp\" : temp, \"disease\" : disease, \"icecream\" : icecream})\n\n\ndef f(x): \n    if x<0: \n        y='group0'\n    elif x<5: \n        y='group5'\n    elif x<10: \n        y='group10'\n    elif x<15: \n        y='group15'\n    elif x<20:\n        y='group20'\n    elif x<25: \n        y='group25'\n    else: \n        y='group30'\n    return y \n\n\ndf1[\"temp2\"] = list(map(lambda x : f(x) ,df1.temp))\n\n\n(\nggplot(df1,aes(x=\"icecream\",y=\"disease\")) +\n    geom_point()\n)\n\n\n\n\n<ggplot: (8728308521369)>\n\n\n\n(\nggplot(df1,aes(x=\"icecream\",y=\"disease\")) +\n    geom_point(aes(color=\"temp2\"))\n)\n\n\n\n\n<ggplot: (8728307658002)>\n\n\n\n온도라는 변수를 통제하니 아이스크림과 소아마비 간에는 연관성이 없어보인다.\n\n\n\n\nnp.random.seed(1)\nϵ1=np.random.normal(size=656, scale=10) \nicecream=temp*2 + 30 + ϵ1 \n\n\nnp.random.seed(2) \nϵ2=np.random.normal(size=656,scale=1)\ndisease= 30+ temp*0.0 + icecream*0.15 +ϵ2*2\n\n\ndf2=pd.DataFrame({'temp':temp,'icecream':icecream,'disease':disease})\ndf2['temp2']=list(map(f,df2.temp))\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n(\nggplot(df2, aes(x=\"icecream\",y=\"disease\")) +\n    geom_point(aes(color=\"temp2\")) + \n    geom_smooth(aes(color=\"temp2\"),linetype=\"dashed\")\n)\n\n\n\n\n<ggplot: (8728303924011)>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-06-7wk.html#loc",
    "href": "post/Lecture/STDV/2023-05-06-7wk.html#loc",
    "title": "06. Partial Correlation & Pandas Handling",
    "section": "loc",
    "text": "loc\n\nnp.random.seed(1)\n_df= pd.DataFrame(np.random.normal(size=(20,4)), columns=list('ABCD'), index=pd.date_range('20201225',periods=20))\n\n\n_df.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      2020-12-25\n      1.624345\n      -0.611756\n      -0.528172\n      -1.072969\n    \n    \n      2020-12-26\n      0.865408\n      -2.301539\n      1.744812\n      -0.761207\n    \n    \n      2020-12-27\n      0.319039\n      -0.249370\n      1.462108\n      -2.060141\n    \n    \n      2020-12-28\n      -0.322417\n      -0.384054\n      1.133769\n      -1.099891\n    \n    \n      2020-12-29\n      -0.172428\n      -0.877858\n      0.042214\n      0.582815\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- 1월 5일부터 1월 8일 까지의 자료만 보고싶다.\n\n_df.loc[\"20210105\":\"20210108\",:]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      2021-01-05\n      0.050808\n      -0.636996\n      0.190915\n      2.100255\n    \n    \n      2021-01-06\n      0.120159\n      0.617203\n      0.300170\n      -0.352250\n    \n    \n      2021-01-07\n      -1.142518\n      -0.349343\n      -0.208894\n      0.586623\n    \n    \n      2021-01-08\n      0.838983\n      0.931102\n      0.285587\n      0.885141\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nloc와 lloc는 행과 더 친하고 열과 친하지 않다. 따라서 열을 선택할때는 그렇게 좋은 방법은 아니다.\n\n- 아래의 경우는 인덱스를 2칸씩 띄어서 불러옴\n\n_df.iloc[::2]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      2020-12-25\n      1.624345\n      -0.611756\n      -0.528172\n      -1.072969\n    \n    \n      2020-12-27\n      0.319039\n      -0.249370\n      1.462108\n      -2.060141\n    \n    \n      2020-12-29\n      -0.172428\n      -0.877858\n      0.042214\n      0.582815\n    \n    \n      2020-12-31\n      0.900856\n      -0.683728\n      -0.122890\n      -0.935769\n    \n    \n      2021-01-02\n      -0.687173\n      -0.845206\n      -0.671246\n      -0.012665\n    \n    \n      2021-01-04\n      -0.191836\n      -0.887629\n      -0.747158\n      1.692455\n    \n    \n      2021-01-06\n      0.120159\n      0.617203\n      0.300170\n      -0.352250\n    \n    \n      2021-01-08\n      0.838983\n      0.931102\n      0.285587\n      0.885141\n    \n    \n      2021-01-10\n      0.488518\n      -0.075572\n      1.131629\n      1.519817\n    \n    \n      2021-01-12\n      0.160037\n      0.876169\n      0.315635\n      -2.022201\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- 컬럼에도 적용 가능하다.\n\n_df.iloc[:,::3]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      D\n    \n  \n  \n    \n      2020-12-25\n      1.624345\n      -1.072969\n    \n    \n      2020-12-26\n      0.865408\n      -0.761207\n    \n    \n      2020-12-27\n      0.319039\n      -2.060141\n    \n    \n      2020-12-28\n      -0.322417\n      -1.099891\n    \n    \n      2020-12-29\n      -0.172428\n      0.582815\n    \n    \n      2020-12-30\n      -1.100619\n      0.502494\n    \n    \n      2020-12-31\n      0.900856\n      -0.935769\n    \n    \n      2021-01-01\n      -0.267888\n      -0.396754\n    \n    \n      2021-01-02\n      -0.687173\n      -0.012665\n    \n    \n      2021-01-03\n      -1.117310\n      0.742044\n    \n    \n      2021-01-04\n      -0.191836\n      1.692455\n    \n    \n      2021-01-05\n      0.050808\n      2.100255\n    \n    \n      2021-01-06\n      0.120159\n      -0.352250\n    \n    \n      2021-01-07\n      -1.142518\n      0.586623\n    \n    \n      2021-01-08\n      0.838983\n      0.885141\n    \n    \n      2021-01-09\n      -0.754398\n      -0.298093\n    \n    \n      2021-01-10\n      0.488518\n      1.519817\n    \n    \n      2021-01-11\n      2.185575\n      -0.504466\n    \n    \n      2021-01-12\n      0.160037\n      -2.022201\n    \n    \n      2021-01-13\n      -0.306204\n      0.762011\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- 위 방법은 컬럼에는 자주 사용하지 않는다. 행의 경우 특정 간격으로 뽑는 데이터를 뽑는 일이 빈번함"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-06-7wk.html#lambda-map",
    "href": "post/Lecture/STDV/2023-05-06-7wk.html#lambda-map",
    "title": "06. Partial Correlation & Pandas Handling",
    "section": "lambda + map",
    "text": "lambda + map\n\nnp.random.seed(1)\ndf2= pd.DataFrame(np.random.normal(size=(10,4)),columns=list('ABCD'))\ndf2\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      1.624345\n      -0.611756\n      -0.528172\n      -1.072969\n    \n    \n      1\n      0.865408\n      -2.301539\n      1.744812\n      -0.761207\n    \n    \n      2\n      0.319039\n      -0.249370\n      1.462108\n      -2.060141\n    \n    \n      3\n      -0.322417\n      -0.384054\n      1.133769\n      -1.099891\n    \n    \n      4\n      -0.172428\n      -0.877858\n      0.042214\n      0.582815\n    \n    \n      5\n      -1.100619\n      1.144724\n      0.901591\n      0.502494\n    \n    \n      6\n      0.900856\n      -0.683728\n      -0.122890\n      -0.935769\n    \n    \n      7\n      -0.267888\n      0.530355\n      -0.691661\n      -0.396754\n    \n    \n      8\n      -0.687173\n      -0.845206\n      -0.671246\n      -0.012665\n    \n    \n      9\n      -1.117310\n      0.234416\n      1.659802\n      0.742044\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n컬럼 A의 값이 0보다 큰경우만\n\ndf2.iloc[map(lambda x : x>0, df2.A),:]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      1.624345\n      -0.611756\n      -0.528172\n      -1.072969\n    \n    \n      1\n      0.865408\n      -2.301539\n      1.744812\n      -0.761207\n    \n    \n      2\n      0.319039\n      -0.249370\n      1.462108\n      -2.060141\n    \n    \n      6\n      0.900856\n      -0.683728\n      -0.122890\n      -0.935769\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.loc[map(lambda x : x > 0 , df2.A),: ]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      1.624345\n      -0.611756\n      -0.528172\n      -1.072969\n    \n    \n      1\n      0.865408\n      -2.301539\n      1.744812\n      -0.761207\n    \n    \n      2\n      0.319039\n      -0.249370\n      1.462108\n      -2.060141\n    \n    \n      6\n      0.900856\n      -0.683728\n      -0.122890\n      -0.935769\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n컬럼 A와 B가 둘다 0보다 큰경우만\n\ndf2.loc[ map(lambda x,y : (x>0) and (y>0) , df2.A,df2.C),:]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      1\n      0.865408\n      -2.301539\n      1.744812\n      -0.761207\n    \n    \n      2\n      0.319039\n      -0.249370\n      1.462108\n      -2.060141\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.loc[ map(lambda x,y : (x>0)  | (y>0) , df2.A,df2.C),:]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      1.624345\n      -0.611756\n      -0.528172\n      -1.072969\n    \n    \n      1\n      0.865408\n      -2.301539\n      1.744812\n      -0.761207\n    \n    \n      2\n      0.319039\n      -0.249370\n      1.462108\n      -2.060141\n    \n    \n      3\n      -0.322417\n      -0.384054\n      1.133769\n      -1.099891\n    \n    \n      4\n      -0.172428\n      -0.877858\n      0.042214\n      0.582815\n    \n    \n      5\n      -1.100619\n      1.144724\n      0.901591\n      0.502494\n    \n    \n      6\n      0.900856\n      -0.683728\n      -0.122890\n      -0.935769\n    \n    \n      9\n      -1.117310\n      0.234416\n      1.659802\n      0.742044\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.loc[ map(lambda x,y : (x>0)  or (y>0) , df2.A,df2.C),:]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      1.624345\n      -0.611756\n      -0.528172\n      -1.072969\n    \n    \n      1\n      0.865408\n      -2.301539\n      1.744812\n      -0.761207\n    \n    \n      2\n      0.319039\n      -0.249370\n      1.462108\n      -2.060141\n    \n    \n      3\n      -0.322417\n      -0.384054\n      1.133769\n      -1.099891\n    \n    \n      4\n      -0.172428\n      -0.877858\n      0.042214\n      0.582815\n    \n    \n      5\n      -1.100619\n      1.144724\n      0.901591\n      0.502494\n    \n    \n      6\n      0.900856\n      -0.683728\n      -0.122890\n      -0.935769\n    \n    \n      9\n      -1.117310\n      0.234416\n      1.659802\n      0.742044"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-06-7wk.html#query",
    "href": "post/Lecture/STDV/2023-05-06-7wk.html#query",
    "title": "06. Partial Correlation & Pandas Handling",
    "section": "query",
    "text": "query\n\nA>0 and B<0 인 행을 선택\n\nnp.random.seed(1)\ndf=pd.DataFrame(np.random.normal(size=(15,4)),columns=list('ABCD'))\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      1.624345\n      -0.611756\n      -0.528172\n      -1.072969\n    \n    \n      1\n      0.865408\n      -2.301539\n      1.744812\n      -0.761207\n    \n    \n      2\n      0.319039\n      -0.249370\n      1.462108\n      -2.060141\n    \n    \n      3\n      -0.322417\n      -0.384054\n      1.133769\n      -1.099891\n    \n    \n      4\n      -0.172428\n      -0.877858\n      0.042214\n      0.582815\n    \n    \n      5\n      -1.100619\n      1.144724\n      0.901591\n      0.502494\n    \n    \n      6\n      0.900856\n      -0.683728\n      -0.122890\n      -0.935769\n    \n    \n      7\n      -0.267888\n      0.530355\n      -0.691661\n      -0.396754\n    \n    \n      8\n      -0.687173\n      -0.845206\n      -0.671246\n      -0.012665\n    \n    \n      9\n      -1.117310\n      0.234416\n      1.659802\n      0.742044\n    \n    \n      10\n      -0.191836\n      -0.887629\n      -0.747158\n      1.692455\n    \n    \n      11\n      0.050808\n      -0.636996\n      0.190915\n      2.100255\n    \n    \n      12\n      0.120159\n      0.617203\n      0.300170\n      -0.352250\n    \n    \n      13\n      -1.142518\n      -0.349343\n      -0.208894\n      0.586623\n    \n    \n      14\n      0.838983\n      0.931102\n      0.285587\n      0.885141\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.query('A>0 and B<0')\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      1.624345\n      -0.611756\n      -0.528172\n      -1.072969\n    \n    \n      1\n      0.865408\n      -2.301539\n      1.744812\n      -0.761207\n    \n    \n      2\n      0.319039\n      -0.249370\n      1.462108\n      -2.060141\n    \n    \n      6\n      0.900856\n      -0.683728\n      -0.122890\n      -0.935769\n    \n    \n      11\n      0.050808\n      -0.636996\n      0.190915\n      2.100255\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.query('A>0 & B<0')\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      1.624345\n      -0.611756\n      -0.528172\n      -1.072969\n    \n    \n      1\n      0.865408\n      -2.301539\n      1.744812\n      -0.761207\n    \n    \n      2\n      0.319039\n      -0.249370\n      1.462108\n      -2.060141\n    \n    \n      6\n      0.900856\n      -0.683728\n      -0.122890\n      -0.935769\n    \n    \n      11\n      0.050808\n      -0.636996\n      0.190915\n      2.100255\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nA < B< C인 행을선택\n\ndf.query('A<B<C')\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      9\n      -1.117310\n      0.234416\n      1.659802\n      0.742044\n    \n    \n      13\n      -1.142518\n      -0.349343\n      -0.208894\n      0.586623\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nA>mean(A)인 행을 선택\n\nmeanA =np.mean(df.A)\nmeanA\n\n-0.018839420539994597\n\n\n\ndf.query(\"A>@meanA\")\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      1.624345\n      -0.611756\n      -0.528172\n      -1.072969\n    \n    \n      1\n      0.865408\n      -2.301539\n      1.744812\n      -0.761207\n    \n    \n      2\n      0.319039\n      -0.249370\n      1.462108\n      -2.060141\n    \n    \n      6\n      0.900856\n      -0.683728\n      -0.122890\n      -0.935769\n    \n    \n      11\n      0.050808\n      -0.636996\n      0.190915\n      2.100255\n    \n    \n      12\n      0.120159\n      0.617203\n      0.300170\n      -0.352250\n    \n    \n      14\n      0.838983\n      0.931102\n      0.285587\n      0.885141\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nA > mean(A)이고, A<0.8 인 것을 선택\n\ndf.query(\"A> @meanA and A<0.8\")\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      2\n      0.319039\n      -0.249370\n      1.462108\n      -2.060141\n    \n    \n      11\n      0.050808\n      -0.636996\n      0.190915\n      2.100255\n    \n    \n      12\n      0.120159\n      0.617203\n      0.300170\n      -0.352250\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n단순인덱싱\n- 0,3:5, 9:11 에 해당하는 row를 뽑고 싶다 \\(\\to\\) 칼럼이름을 index로 받아서 사용한다.\n\ndf.query(\"index==0 or 3<=index<=5 or 9<=index<=11\")\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      0\n      1.624345\n      -0.611756\n      -0.528172\n      -1.072969\n    \n    \n      3\n      -0.322417\n      -0.384054\n      1.133769\n      -1.099891\n    \n    \n      4\n      -0.172428\n      -0.877858\n      0.042214\n      0.582815\n    \n    \n      5\n      -1.100619\n      1.144724\n      0.901591\n      0.502494\n    \n    \n      9\n      -1.117310\n      0.234416\n      1.659802\n      0.742044\n    \n    \n      10\n      -0.191836\n      -0.887629\n      -0.747158\n      1.692455\n    \n    \n      11\n      0.050808\n      -0.636996\n      0.190915\n      2.100255\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- 아래와 같은 시계열 자료에서 특히 유용함.\n\ndf2=pd.DataFrame(np.random.normal(size=(10,4)), columns=list('ABCD'), index=pd.date_range('20201226',periods=10))\ndf2\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      2020-12-26\n      -0.754398\n      1.252868\n      0.512930\n      -0.298093\n    \n    \n      2020-12-27\n      0.488518\n      -0.075572\n      1.131629\n      1.519817\n    \n    \n      2020-12-28\n      2.185575\n      -1.396496\n      -1.444114\n      -0.504466\n    \n    \n      2020-12-29\n      0.160037\n      0.876169\n      0.315635\n      -2.022201\n    \n    \n      2020-12-30\n      -0.306204\n      0.827975\n      0.230095\n      0.762011\n    \n    \n      2020-12-31\n      -0.222328\n      -0.200758\n      0.186561\n      0.410052\n    \n    \n      2021-01-01\n      0.198300\n      0.119009\n      -0.670662\n      0.377564\n    \n    \n      2021-01-02\n      0.121821\n      1.129484\n      1.198918\n      0.185156\n    \n    \n      2021-01-03\n      -0.375285\n      -0.638730\n      0.423494\n      0.077340\n    \n    \n      2021-01-04\n      -0.343854\n      0.043597\n      -0.620001\n      0.698032\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.query(\"20201226 <=index <=20210101\")\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      2020-12-26\n      -0.754398\n      1.252868\n      0.512930\n      -0.298093\n    \n    \n      2020-12-27\n      0.488518\n      -0.075572\n      1.131629\n      1.519817\n    \n    \n      2020-12-28\n      2.185575\n      -1.396496\n      -1.444114\n      -0.504466\n    \n    \n      2020-12-29\n      0.160037\n      0.876169\n      0.315635\n      -2.022201\n    \n    \n      2020-12-30\n      -0.306204\n      0.827975\n      0.230095\n      0.762011\n    \n    \n      2020-12-31\n      -0.222328\n      -0.200758\n      0.186561\n      0.410052\n    \n    \n      2021-01-01\n      0.198300\n      0.119009\n      -0.670662\n      0.377564\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.query(\"20201226 <=index <=20210101 and A+B < C\")\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      2020-12-26\n      -0.754398\n      1.252868\n      0.512930\n      -0.298093\n    \n    \n      2020-12-27\n      0.488518\n      -0.075572\n      1.131629\n      1.519817\n    \n    \n      2020-12-31\n      -0.222328\n      -0.200758\n      0.186561\n      0.410052"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-13-8wk.html",
    "href": "post/Lecture/STDV/2023-05-13-8wk.html",
    "title": "07. tidydata",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\ndf=pd.read_csv('https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/flights.csv')\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      MONTH\n      DAY\n      WEEKDAY\n      AIRLINE\n      ORG_AIR\n      DEST_AIR\n      SCHED_DEP\n      DEP_DELAY\n      AIR_TIME\n      DIST\n      SCHED_ARR\n      ARR_DELAY\n      DIVERTED\n      CANCELLED\n    \n  \n  \n    \n      0\n      1\n      1\n      4\n      WN\n      LAX\n      SLC\n      1625\n      58.0\n      94.0\n      590\n      1905\n      65.0\n      0\n      0\n    \n    \n      1\n      1\n      1\n      4\n      UA\n      DEN\n      IAD\n      823\n      7.0\n      154.0\n      1452\n      1333\n      -13.0\n      0\n      0\n    \n    \n      2\n      1\n      1\n      4\n      MQ\n      DFW\n      VPS\n      1305\n      36.0\n      85.0\n      641\n      1453\n      35.0\n      0\n      0\n    \n    \n      3\n      1\n      1\n      4\n      AA\n      DFW\n      DCA\n      1555\n      7.0\n      126.0\n      1192\n      1935\n      -7.0\n      0\n      0\n    \n    \n      4\n      1\n      1\n      4\n      WN\n      LAX\n      MCI\n      1720\n      48.0\n      166.0\n      1363\n      2225\n      39.0\n      0\n      0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n- 방법 1\n\ndf.groupby(\"AIRLINE\").aggregate({\"ARR_DELAY\" : \"mean\"})\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      ARR_DELAY\n    \n    \n      AIRLINE\n      \n    \n  \n  \n    \n      AA\n      5.542661\n    \n    \n      AS\n      -0.833333\n    \n    \n      B6\n      8.692593\n    \n    \n      DL\n      0.339691\n    \n    \n      EV\n      7.034580\n    \n    \n      F9\n      13.630651\n    \n    \n      HA\n      4.972973\n    \n    \n      MQ\n      6.860591\n    \n    \n      NK\n      18.436070\n    \n    \n      OO\n      7.593463\n    \n    \n      UA\n      7.765755\n    \n    \n      US\n      1.681105\n    \n    \n      VX\n      5.348884\n    \n    \n      WN\n      6.397353\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- 방법 2\n\ndf.groupby(\"AIRLINE\").aggregate({\"ARR_DELAY\":np.mean})\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      ARR_DELAY\n    \n    \n      AIRLINE\n      \n    \n  \n  \n    \n      AA\n      5.542661\n    \n    \n      AS\n      -0.833333\n    \n    \n      B6\n      8.692593\n    \n    \n      DL\n      0.339691\n    \n    \n      EV\n      7.034580\n    \n    \n      F9\n      13.630651\n    \n    \n      HA\n      4.972973\n    \n    \n      MQ\n      6.860591\n    \n    \n      NK\n      18.436070\n    \n    \n      OO\n      7.593463\n    \n    \n      UA\n      7.765755\n    \n    \n      US\n      1.681105\n    \n    \n      VX\n      5.348884\n    \n    \n      WN\n      6.397353\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- 방법 3\n\ndf.groupby(\"AIRLINE\")[\"ARR_DELAY\"].agg(\"mean\")\n\nAIRLINE\nAA     5.542661\nAS    -0.833333\nB6     8.692593\nDL     0.339691\nEV     7.034580\nF9    13.630651\nHA     4.972973\nMQ     6.860591\nNK    18.436070\nOO     7.593463\nUA     7.765755\nUS     1.681105\nVX     5.348884\nWN     6.397353\nName: ARR_DELAY, dtype: float64\n\n\n- 방법 4\n\ndf.groupby(\"AIRLINE\")[\"ARR_DELAY\"].agg(np.mean)\n\nAIRLINE\nAA     5.542661\nAS    -0.833333\nB6     8.692593\nDL     0.339691\nEV     7.034580\nF9    13.630651\nHA     4.972973\nMQ     6.860591\nNK    18.436070\nOO     7.593463\nUA     7.765755\nUS     1.681105\nVX     5.348884\nWN     6.397353\nName: ARR_DELAY, dtype: float64\n\n\n- 방법 5\n\ndf.groupby(\"AIRLINE\")[\"ARR_DELAY\"].mean()\n\nAIRLINE\nAA     5.542661\nAS    -0.833333\nB6     8.692593\nDL     0.339691\nEV     7.034580\nF9    13.630651\nHA     4.972973\nMQ     6.860591\nNK    18.436070\nOO     7.593463\nUA     7.765755\nUS     1.681105\nVX     5.348884\nWN     6.397353\nName: ARR_DELAY, dtype: float64\n\n\n- 방법 2와 방법 4는 사용자정의 함수를 쓸 수 있다는 장점이 있음.\n\ndef f(x) : return -np.mean(x)\n\n\ndf.groupby(\"AIRLINE\").agg({\"ARR_DELAY\" : f})\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      ARR_DELAY\n    \n    \n      AIRLINE\n      \n    \n  \n  \n    \n      AA\n      -5.542661\n    \n    \n      AS\n      0.833333\n    \n    \n      B6\n      -8.692593\n    \n    \n      DL\n      -0.339691\n    \n    \n      EV\n      -7.034580\n    \n    \n      F9\n      -13.630651\n    \n    \n      HA\n      -4.972973\n    \n    \n      MQ\n      -6.860591\n    \n    \n      NK\n      -18.436070\n    \n    \n      OO\n      -7.593463\n    \n    \n      UA\n      -7.765755\n    \n    \n      US\n      -1.681105\n    \n    \n      VX\n      -5.348884\n    \n    \n      WN\n      -6.397353\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.groupby(by='AIRLINE').agg({'ARR_DELAY':lambda x: -np.mean(x)})\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      ARR_DELAY\n    \n    \n      AIRLINE\n      \n    \n  \n  \n    \n      AA\n      -5.542661\n    \n    \n      AS\n      0.833333\n    \n    \n      B6\n      -8.692593\n    \n    \n      DL\n      -0.339691\n    \n    \n      EV\n      -7.034580\n    \n    \n      F9\n      -13.630651\n    \n    \n      HA\n      -4.972973\n    \n    \n      MQ\n      -6.860591\n    \n    \n      NK\n      -18.436070\n    \n    \n      OO\n      -7.593463\n    \n    \n      UA\n      -7.765755\n    \n    \n      US\n      -1.681105\n    \n    \n      VX\n      -5.348884\n    \n    \n      WN\n      -6.397353\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- 입력이 여러개인 사용자 정의 함수도 사용가능함\n\ndef f(x,y): return np.mean(x)**y \n\n\ndf.groupby(by='AIRLINE')['ARR_DELAY'].agg(f,2)\n\nAIRLINE\nAA     30.721086\nAS      0.694444\nB6     75.561166\nDL      0.115390\nEV     49.485310\nF9    185.794656\nHA     24.730460\nMQ     47.067715\nNK    339.888677\nOO     57.660681\nUA     60.306954\nUS      2.826113\nVX     28.610564\nWN     40.926120\nName: ARR_DELAY, dtype: float64\n\n\n\ndf.groupby(by='AIRLINE').agg({'ARR_DELAY': lambda x: f(x,2)})\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      ARR_DELAY\n    \n    \n      AIRLINE\n      \n    \n  \n  \n    \n      AA\n      30.721086\n    \n    \n      AS\n      0.694444\n    \n    \n      B6\n      75.561166\n    \n    \n      DL\n      0.115390\n    \n    \n      EV\n      49.485310\n    \n    \n      F9\n      185.794656\n    \n    \n      HA\n      24.730460\n    \n    \n      MQ\n      47.067715\n    \n    \n      NK\n      339.888677\n    \n    \n      OO\n      57.660681\n    \n    \n      UA\n      60.306954\n    \n    \n      US\n      2.826113\n    \n    \n      VX\n      28.610564\n    \n    \n      WN\n      40.926120\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\n\ndf.groupby([\"AIRLINE\",\"WEEKDAY\"]).agg({\"CANCELLED\":sum})\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      \n      CANCELLED\n    \n    \n      AIRLINE\n      WEEKDAY\n      \n    \n  \n  \n    \n      AA\n      1\n      41\n    \n    \n      2\n      9\n    \n    \n      3\n      16\n    \n    \n      4\n      20\n    \n    \n      5\n      18\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      WN\n      3\n      18\n    \n    \n      4\n      10\n    \n    \n      5\n      7\n    \n    \n      6\n      10\n    \n    \n      7\n      7\n    \n  \n\n98 rows × 1 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\n\ndf.groupby([\"AIRLINE\",\"WEEKDAY\"]).agg({\"CANCELLED\" : [sum,np.mean],\n                                      \"DIVERTED\" : [sum,np.mean],\n                                      })\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      \n      CANCELLED\n      DIVERTED\n    \n    \n      \n      \n      sum\n      mean\n      sum\n      mean\n    \n    \n      AIRLINE\n      WEEKDAY\n      \n      \n      \n      \n    \n  \n  \n    \n      AA\n      1\n      41\n      0.032106\n      6\n      0.004699\n    \n    \n      2\n      9\n      0.007341\n      2\n      0.001631\n    \n    \n      3\n      16\n      0.011949\n      2\n      0.001494\n    \n    \n      4\n      20\n      0.015004\n      5\n      0.003751\n    \n    \n      5\n      18\n      0.014151\n      1\n      0.000786\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      WN\n      3\n      18\n      0.014118\n      2\n      0.001569\n    \n    \n      4\n      10\n      0.007911\n      4\n      0.003165\n    \n    \n      5\n      7\n      0.005828\n      0\n      0.000000\n    \n    \n      6\n      10\n      0.010132\n      3\n      0.003040\n    \n    \n      7\n      7\n      0.006066\n      3\n      0.002600\n    \n  \n\n98 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\n\ndf.groupby([\"AIRLINE\",\"WEEKDAY\"]).agg({ \"CANCELLED\" : [sum,np.mean,len],\n                                         \"AIR_TIME\" : [np.mean,np.var]})\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      \n      CANCELLED\n      AIR_TIME\n    \n    \n      \n      \n      sum\n      mean\n      len\n      mean\n      var\n    \n    \n      AIRLINE\n      WEEKDAY\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      AA\n      1\n      41\n      0.032106\n      1277\n      147.610569\n      5393.806723\n    \n    \n      2\n      9\n      0.007341\n      1226\n      143.851852\n      5359.890719\n    \n    \n      3\n      16\n      0.011949\n      1339\n      144.514005\n      5378.854539\n    \n    \n      4\n      20\n      0.015004\n      1333\n      141.124618\n      4791.524627\n    \n    \n      5\n      18\n      0.014151\n      1272\n      145.430966\n      5884.592076\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      WN\n      3\n      18\n      0.014118\n      1275\n      104.219920\n      2901.873447\n    \n    \n      4\n      10\n      0.007911\n      1264\n      107.200800\n      2966.568935\n    \n    \n      5\n      7\n      0.005828\n      1201\n      107.893635\n      3268.717093\n    \n    \n      6\n      10\n      0.010132\n      987\n      109.247433\n      3152.753719\n    \n    \n      7\n      7\n      0.006066\n      1154\n      107.602273\n      3183.126889\n    \n  \n\n98 rows × 5 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\n- 목표 : DIST를 적당한 구간으로 나누어 카테고리화 하고 그것을 바탕으로 groupby를 수행하자.\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndf.DIST.hist()\n\n<Axes: >\n\n\n\n\n\n\ndf.DIST.describe()\n\ncount    58492.000000\nmean       872.900072\nstd        624.996805\nmin         67.000000\n25%        391.000000\n50%        690.000000\n75%       1199.000000\nmax       4502.000000\nName: DIST, dtype: float64\n\n\n- 구간을 아래와 같이 설정한다.\n\nbins = [-np.inf, 400,700,1200,np.inf]\n\n\nbins\n\n[-inf, 400, 700, 1200, inf]\n\n\n\ncuts= pd.cut(df.DIST,bins=bins).to_list()\n\n\ndf[\"cuts\"] = cuts\n\n\ndf.groupby([\"cuts\",\"AIRLINE\"]).agg({\"DIVERTED\":sum})\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      \n      DIVERTED\n    \n    \n      cuts\n      AIRLINE\n      \n    \n  \n  \n    \n      (-inf, 400.0]\n      AA\n      0\n    \n    \n      B6\n      0\n    \n    \n      DL\n      1\n    \n    \n      EV\n      3\n    \n    \n      F9\n      0\n    \n    \n      MQ\n      0\n    \n    \n      NK\n      0\n    \n    \n      OO\n      5\n    \n    \n      UA\n      2\n    \n    \n      US\n      0\n    \n    \n      VX\n      0\n    \n    \n      WN\n      1\n    \n    \n      (400.0, 700.0]\n      AA\n      3\n    \n    \n      AS\n      0\n    \n    \n      B6\n      0\n    \n    \n      DL\n      12\n    \n    \n      EV\n      8\n    \n    \n      F9\n      1\n    \n    \n      MQ\n      4\n    \n    \n      NK\n      1\n    \n    \n      OO\n      7\n    \n    \n      UA\n      1\n    \n    \n      US\n      0\n    \n    \n      VX\n      0\n    \n    \n      WN\n      2\n    \n    \n      (700.0, 1200.0]\n      AA\n      10\n    \n    \n      AS\n      0\n    \n    \n      B6\n      1\n    \n    \n      DL\n      6\n    \n    \n      EV\n      4\n    \n    \n      F9\n      0\n    \n    \n      MQ\n      1\n    \n    \n      NK\n      1\n    \n    \n      OO\n      5\n    \n    \n      UA\n      4\n    \n    \n      US\n      0\n    \n    \n      VX\n      0\n    \n    \n      WN\n      4\n    \n    \n      (1200.0, inf]\n      AA\n      13\n    \n    \n      AS\n      0\n    \n    \n      B6\n      1\n    \n    \n      DL\n      5\n    \n    \n      EV\n      0\n    \n    \n      F9\n      1\n    \n    \n      HA\n      1\n    \n    \n      MQ\n      0\n    \n    \n      NK\n      3\n    \n    \n      OO\n      4\n    \n    \n      UA\n      12\n    \n    \n      US\n      1\n    \n    \n      VX\n      1\n    \n    \n      WN\n      8\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.groupby(['AIRLINE']).agg({'AIRLINE':len})\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      AIRLINE\n    \n    \n      AIRLINE\n      \n    \n  \n  \n    \n      AA\n      8900\n    \n    \n      AS\n      768\n    \n    \n      B6\n      543\n    \n    \n      DL\n      10601\n    \n    \n      EV\n      5858\n    \n    \n      F9\n      1317\n    \n    \n      HA\n      112\n    \n    \n      MQ\n      3471\n    \n    \n      NK\n      1516\n    \n    \n      OO\n      6588\n    \n    \n      UA\n      7792\n    \n    \n      US\n      1615\n    \n    \n      VX\n      993\n    \n    \n      WN\n      8418\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- cuts을 이용하여 추가그룹핑을 하면 조금 다른 특징들을 데이터에서 발견할 수 있다. - AA항공사와 DL항공사는 모두 비슷한 우회횟수를 가지고 있음. - AA항공사는 700회이상의 구간에서 우회를 많이하고 DL항공사는 400~700사이에서 우회를 많이 한다. (패턴이 다름)\n- 구간이름에 label을 붙이는 방법\n\nbins\n\n[-inf, 400, 700, 1200, inf]\n\n\n\ncuts2=pd.cut(df.DIST,bins=bins,labels=['Q1','Q2','Q3','Q4'])\ncuts2\n\n0        Q2\n1        Q4\n2        Q2\n3        Q3\n4        Q4\n         ..\n58487    Q4\n58488    Q2\n58489    Q1\n58490    Q3\n58491    Q2\nName: DIST, Length: 58492, dtype: category\nCategories (4, object): ['Q1' < 'Q2' < 'Q3' < 'Q4']\n\n\n\ndf.groupby(by=[cuts2,'AIRLINE']).agg({'DIVERTED':sum})\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      \n      DIVERTED\n    \n    \n      DIST\n      AIRLINE\n      \n    \n  \n  \n    \n      Q1\n      AA\n      0\n    \n    \n      AS\n      0\n    \n    \n      B6\n      0\n    \n    \n      DL\n      1\n    \n    \n      EV\n      3\n    \n    \n      F9\n      0\n    \n    \n      HA\n      0\n    \n    \n      MQ\n      0\n    \n    \n      NK\n      0\n    \n    \n      OO\n      5\n    \n    \n      UA\n      2\n    \n    \n      US\n      0\n    \n    \n      VX\n      0\n    \n    \n      WN\n      1\n    \n    \n      Q2\n      AA\n      3\n    \n    \n      AS\n      0\n    \n    \n      B6\n      0\n    \n    \n      DL\n      12\n    \n    \n      EV\n      8\n    \n    \n      F9\n      1\n    \n    \n      HA\n      0\n    \n    \n      MQ\n      4\n    \n    \n      NK\n      1\n    \n    \n      OO\n      7\n    \n    \n      UA\n      1\n    \n    \n      US\n      0\n    \n    \n      VX\n      0\n    \n    \n      WN\n      2\n    \n    \n      Q3\n      AA\n      10\n    \n    \n      AS\n      0\n    \n    \n      B6\n      1\n    \n    \n      DL\n      6\n    \n    \n      EV\n      4\n    \n    \n      F9\n      0\n    \n    \n      HA\n      0\n    \n    \n      MQ\n      1\n    \n    \n      NK\n      1\n    \n    \n      OO\n      5\n    \n    \n      UA\n      4\n    \n    \n      US\n      0\n    \n    \n      VX\n      0\n    \n    \n      WN\n      4\n    \n    \n      Q4\n      AA\n      13\n    \n    \n      AS\n      0\n    \n    \n      B6\n      1\n    \n    \n      DL\n      5\n    \n    \n      EV\n      0\n    \n    \n      F9\n      1\n    \n    \n      HA\n      1\n    \n    \n      MQ\n      0\n    \n    \n      NK\n      3\n    \n    \n      OO\n      4\n    \n    \n      UA\n      12\n    \n    \n      US\n      1\n    \n    \n      VX\n      1\n    \n    \n      WN\n      8\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.groupby(cuts2).agg({'DIVERTED':len})\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      DIVERTED\n    \n    \n      DIST\n      \n    \n  \n  \n    \n      Q1\n      15027\n    \n    \n      Q2\n      14697\n    \n    \n      Q3\n      14417\n    \n    \n      Q4\n      14351"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-13-8wk.html#melt",
    "href": "post/Lecture/STDV/2023-05-13-8wk.html#melt",
    "title": "07. tidydata",
    "section": "melt",
    "text": "melt\n\nurl = 'https://raw.githubusercontent.com/PacktPublishing/Pandas-Cookbook/master/data/state_fruit.csv'\ndf2=pd.read_csv(url)\ndf2\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Unnamed: 0\n      Apple\n      Orange\n      Banana\n    \n  \n  \n    \n      0\n      Texas\n      12\n      10\n      40\n    \n    \n      1\n      Arizona\n      9\n      7\n      12\n    \n    \n      2\n      Florida\n      0\n      14\n      190\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.rename(columns={\"Unnamed: 0\":\"group1\"})\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      group1\n      Apple\n      Orange\n      Banana\n    \n  \n  \n    \n      0\n      Texas\n      12\n      10\n      40\n    \n    \n      1\n      Arizona\n      9\n      7\n      12\n    \n    \n      2\n      Florida\n      0\n      14\n      190\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.rename(columns={\"Unnamed: 0\":\"group1\"}).melt(id_vars=\"group1\")\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      group1\n      variable\n      value\n    \n  \n  \n    \n      0\n      Texas\n      Apple\n      12\n    \n    \n      1\n      Arizona\n      Apple\n      9\n    \n    \n      2\n      Florida\n      Apple\n      0\n    \n    \n      3\n      Texas\n      Orange\n      10\n    \n    \n      4\n      Arizona\n      Orange\n      7\n    \n    \n      5\n      Florida\n      Orange\n      14\n    \n    \n      6\n      Texas\n      Banana\n      40\n    \n    \n      7\n      Arizona\n      Banana\n      12\n    \n    \n      8\n      Florida\n      Banana\n      190\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.rename(columns={\"Unnamed: 0\":\"group1\"}).melt(id_vars=\"group1\")\\\n.rename(columns={\"variable\":\"group2\",\"value\":\"X\"})\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      group1\n      group2\n      X\n    \n  \n  \n    \n      0\n      Texas\n      Apple\n      12\n    \n    \n      1\n      Arizona\n      Apple\n      9\n    \n    \n      2\n      Florida\n      Apple\n      0\n    \n    \n      3\n      Texas\n      Orange\n      10\n    \n    \n      4\n      Arizona\n      Orange\n      7\n    \n    \n      5\n      Florida\n      Orange\n      14\n    \n    \n      6\n      Texas\n      Banana\n      40\n    \n    \n      7\n      Arizona\n      Banana\n      12\n    \n    \n      8\n      Florida\n      Banana\n      190\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Apple\n      Orange\n      Banana\n    \n  \n  \n    \n      Texas\n      12\n      10\n      40\n    \n    \n      Arizona\n      9\n      7\n      12\n    \n    \n      Florida\n      0\n      14\n      190\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- step 1\n\ndf.reset_index()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      index\n      Apple\n      Orange\n      Banana\n    \n  \n  \n    \n      0\n      Texas\n      12\n      10\n      40\n    \n    \n      1\n      Arizona\n      9\n      7\n      12\n    \n    \n      2\n      Florida\n      0\n      14\n      190\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- step 2\n\ndf.reset_index().melt(id_vars=\"index\",var_name=\"fruit\",value_name=\"X\")\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      index\n      fruit\n      X\n    \n  \n  \n    \n      0\n      Texas\n      Apple\n      12\n    \n    \n      1\n      Arizona\n      Apple\n      9\n    \n    \n      2\n      Florida\n      Apple\n      0\n    \n    \n      3\n      Texas\n      Orange\n      10\n    \n    \n      4\n      Arizona\n      Orange\n      7\n    \n    \n      5\n      Florida\n      Orange\n      14\n    \n    \n      6\n      Texas\n      Banana\n      40\n    \n    \n      7\n      Arizona\n      Banana\n      12\n    \n    \n      8\n      Florida\n      Banana\n      190"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-13-8wk.html#다양한-요약값을-이용해보기",
    "href": "post/Lecture/STDV/2023-05-13-8wk.html#다양한-요약값을-이용해보기",
    "title": "07. tidydata",
    "section": "다양한 요약값을 이용해보기",
    "text": "다양한 요약값을 이용해보기\n\ndf.groupby(\"g\").agg({\"y\" : [np.mean,np.median,np.std, lambda x : max(x)-min(x)]})\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      y\n    \n    \n      \n      mean\n      median\n      std\n      <lambda_0>\n    \n    \n      g\n      \n      \n      \n      \n    \n  \n  \n    \n      A\n      2.275751\n      2.144761\n      2.109639\n      12.169683\n    \n    \n      B\n      2.991754\n      3.007031\n      1.034663\n      5.466597\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ntd=df.groupby(\"g\").agg({\"y\" : [np.mean,np.median,np.std, lambda x : max(x)-min(x)]})\\\n.rename(columns={\"<lambda_0>\": \"range\"})\\\n.reset_index().melt(id_vars=\"g\").iloc[:,[0,2,3]].rename(columns={\"variable_1\":\"stat\"})\n\n\ntd\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      g\n      stat\n      value\n    \n  \n  \n    \n      0\n      A\n      mean\n      2.275751\n    \n    \n      1\n      B\n      mean\n      2.991754\n    \n    \n      2\n      A\n      median\n      2.144761\n    \n    \n      3\n      B\n      median\n      3.007031\n    \n    \n      4\n      A\n      std\n      2.109639\n    \n    \n      5\n      B\n      std\n      1.034663\n    \n    \n      6\n      A\n      range\n      12.169683\n    \n    \n      7\n      B\n      range\n      5.466597\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n(\n    ggplot(td) +\n    geom_bar(aes(x=\"stat\",y=\"value\",fill=\"g\"),stat=\"identity\",position=\"dodge\")\n)\n\n\n\n\n<ggplot: (8779086833301)>\n\n\n\ncoord_filp()\n\n(\n    ggplot(td) +\n    geom_bar(aes(x=\"stat\",y=\"value\",fill=\"g\"),stat=\"identity\",position=\"dodge\") +\n    coord_flip()\n)\n\n\n\n\n<ggplot: (8779086820064)>\n\n\n\n\nfacet_wrap()\n\n(\n    ggplot(td) +\n    geom_bar(aes(x=\"stat\",y=\"value\",fill=\"g\"),stat=\"identity\",position=\"dodge\") +\n    coord_flip()+\n    facet_wrap(\"stat\")\n)\n\n\n\n\n<ggplot: (8779086783419)>\n\n\n\n(\n    ggplot(td) +\n    geom_bar(aes(x=\"g\",y=\"value\",fill=\"g\"),stat=\"identity\",position=\"dodge\") +\n    coord_flip()+\n    facet_wrap(\"stat\")\n)\n\n\n\n\n<ggplot: (8779086782516)>\n\n\n\n(\n    ggplot(td) + \n    geom_bar(aes(x=\"g\",y=\"value\",fill=\"g\"),stat=\"identity\",position=\"dodge\") +\n    coord_flip() +\n    facet_grid(\"stat~g\") \n)\n\n\n\n\n<ggplot: (8779086478141)>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-13-8wk.html#시각화-1",
    "href": "post/Lecture/STDV/2023-05-13-8wk.html#시각화-1",
    "title": "07. tidydata",
    "section": "시각화 1",
    "text": "시각화 1\n\nDEP=(['A1']*2+['A2']*2+['B1']*2+['B2']*2)*2 \nGEN=['M']*8+['F']*8\nSTATE=['PASS','FAIL']*8\nCOUNT=[1,9,2,8,80,20,85,15,5,5,5,5,9,1,9,1]\n\n\ndf=pd.DataFrame({'DEP':DEP,'STATE':STATE,'GEN':GEN,'COUNT':COUNT})\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      DEP\n      STATE\n      GEN\n      COUNT\n    \n  \n  \n    \n      0\n      A1\n      PASS\n      M\n      1\n    \n    \n      1\n      A1\n      FAIL\n      M\n      9\n    \n    \n      2\n      A2\n      PASS\n      M\n      2\n    \n    \n      3\n      A2\n      FAIL\n      M\n      8\n    \n    \n      4\n      B1\n      PASS\n      M\n      80\n    \n    \n      5\n      B1\n      FAIL\n      M\n      20\n    \n    \n      6\n      B2\n      PASS\n      M\n      85\n    \n    \n      7\n      B2\n      FAIL\n      M\n      15\n    \n    \n      8\n      A1\n      PASS\n      F\n      5\n    \n    \n      9\n      A1\n      FAIL\n      F\n      5\n    \n    \n      10\n      A2\n      PASS\n      F\n      5\n    \n    \n      11\n      A2\n      FAIL\n      F\n      5\n    \n    \n      12\n      B1\n      PASS\n      F\n      9\n    \n    \n      13\n      B1\n      FAIL\n      F\n      1\n    \n    \n      14\n      B2\n      PASS\n      F\n      9\n    \n    \n      15\n      B2\n      FAIL\n      F\n      1\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf1=df.groupby(['GEN','STATE']).agg({'COUNT':np.sum}).reset_index()\ng_sum =df1.groupby(\"GEN\")[\"COUNT\"].sum().to_list()\ng_sum =list(np.repeat(g_sum,2))\ndf1[\"prop\"] = df1[\"COUNT\"]/g_sum\ndf1\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      GEN\n      STATE\n      COUNT\n      prop\n    \n  \n  \n    \n      0\n      F\n      FAIL\n      12\n      0.300000\n    \n    \n      1\n      F\n      PASS\n      28\n      0.700000\n    \n    \n      2\n      M\n      FAIL\n      52\n      0.236364\n    \n    \n      3\n      M\n      PASS\n      168\n      0.763636\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n(\n    ggplot(df1.query(\"STATE =='PASS'\")) +\n    geom_bar(aes(x=\"GEN\",y=\"prop\",fill=\"GEN\"),stat=\"identity\")\n)\n\n\n\n\n<ggplot: (8779086857075)>\n\n\n- 남학생의 합격률이 더 높아 보인다. \\(\\to\\) 남녀차별인가?"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-13-8wk.html#시각화-2-학과별-합격률",
    "href": "post/Lecture/STDV/2023-05-13-8wk.html#시각화-2-학과별-합격률",
    "title": "07. tidydata",
    "section": "시각화 2: 학과별 합격률",
    "text": "시각화 2: 학과별 합격률\n\ntd =df.groupby([\"DEP\",\"GEN\"]).agg({\"COUNT\":sum})\\\n          .reset_index().\\\n            rename(columns={\"COUNT\": \"SUM\"}).merge(df)\n\n\ntd[\"prop\"] = td.COUNT/td.SUM\n\n\ntd\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      DEP\n      GEN\n      SUM\n      STATE\n      COUNT\n      prop\n    \n  \n  \n    \n      0\n      A1\n      F\n      10\n      PASS\n      5\n      0.50\n    \n    \n      1\n      A1\n      F\n      10\n      FAIL\n      5\n      0.50\n    \n    \n      2\n      A1\n      M\n      10\n      PASS\n      1\n      0.10\n    \n    \n      3\n      A1\n      M\n      10\n      FAIL\n      9\n      0.90\n    \n    \n      4\n      A2\n      F\n      10\n      PASS\n      5\n      0.50\n    \n    \n      5\n      A2\n      F\n      10\n      FAIL\n      5\n      0.50\n    \n    \n      6\n      A2\n      M\n      10\n      PASS\n      2\n      0.20\n    \n    \n      7\n      A2\n      M\n      10\n      FAIL\n      8\n      0.80\n    \n    \n      8\n      B1\n      F\n      10\n      PASS\n      9\n      0.90\n    \n    \n      9\n      B1\n      F\n      10\n      FAIL\n      1\n      0.10\n    \n    \n      10\n      B1\n      M\n      100\n      PASS\n      80\n      0.80\n    \n    \n      11\n      B1\n      M\n      100\n      FAIL\n      20\n      0.20\n    \n    \n      12\n      B2\n      F\n      10\n      PASS\n      9\n      0.90\n    \n    \n      13\n      B2\n      F\n      10\n      FAIL\n      1\n      0.10\n    \n    \n      14\n      B2\n      M\n      100\n      PASS\n      85\n      0.85\n    \n    \n      15\n      B2\n      M\n      100\n      FAIL\n      15\n      0.15\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n(\nggplot(td.query(\"STATE=='PASS'\")) +\n    geom_bar(aes(x=\"GEN\",y=\"prop\",fill=\"GEN\"),stat=\"identity\") +\n    facet_wrap(\"DEP\")\n)\n\n\n\n\n<ggplot: (8779086903858)>\n\n\n\n즉 전체적으로 보았을 때는 남학생의 합격률이 더 높은 것 같지만, 학과별로 변수를 나누어서 살펴본 결과 여학생의 합격률이 더 높았다는 것이다."
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-14-midsol.html",
    "href": "post/Lecture/STDV/2023-05-14-midsol.html",
    "title": "08. mid term",
    "section": "",
    "text": "import matplotlib.pyplot as plt \nimport seaborn as sns \nfrom plotnine import * \nimport cv2 \nimport numpy as np \nimport pandas as pd \n\n\n\n\n예시 a-c, b-d\n\n\n\n\na-c, b-d\n\n\n\n\n자료\nx=[1,2,3,4]\ny=[1,2,4,3]\n시각화예시\n\n\n\n\nx = [1,2,3,4]\ny = [1,2,4,3]\n\nfig, axes = plt.subplots(2,2)\n(ax1,ax2),(ax3,ax4) = axes\n\nax1.plot(x,y,\"bo:\")\nax2.plot(x,y,\"rs\")\nax3.plot(x,y,\"xk\")\nax4.plot(x,y,\"m.--\")\n\n\n\n\n\n\n\n\n\n(하니) 그림 (a)-(d)는 모두 양의 상관계수를 가진다.\n(나애리) 그림 (b)는 산점도가 직선이 아니라 곡선의 모양을 띄고 있으므로 상관계수는 0이다.\n(홍두깨) 그림 (c)에서 상단의 이상치를 제외하면 상관계수는 1이다.\n(고은애) 그림 (d)의 우측 이상치의 값을 적절하게 바꾸면 (d)의 상관계수를 음수로 만드는 것이 가능하다.\n(이창수) 그림 (c)역시 상단의 이상치 값을 적절하게 바꾸면 (c)의 상관계수를 음수로 만드는 것이 가능하다.\n\n\n하니, 홍두깨, 고은애, 이창수\n\n\n\n\n\n(하니) 아이스크림과 소아마비는 양의 상관관계에 있다.\n(나애리) 상관계수의 값이 1에 가까울수록 아이스크림과 소아마비의 인과성이 명확하다고 볼 수 있다.\n(홍두깨) 비슷한 온도를 가진 관측치에서는 아이스크림과 소아마비의 상관계수가 0에 가깝다.\n(고은애) 온도를 통제하였을 경우 아이스크림과 소아마비의 상관계수가 0이므로 둘 사이의 인과성이 있다고 보긴 어렵다. (단, 온도를 통제하였을 경우에는 아이스크림은 랜덤으로 먹었다고 가정한다.)\n\n\n하니, 홍두깨, 고은애\n\n\n\n\n아래의 코드를 활용하여 FIFA22의 자료를 불러온뒤 물음에 답하라.\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-10-25-FIFA22_official_data.csv')\n\nimport pandas as pd\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-10-25-FIFA22_official_data.csv')\n\n\n\n\ndf.loc[:,['Loaned From','Marking']]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Loaned From\n      Marking\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      16705\n      NaN\n      5.0\n    \n    \n      16706\n      NaN\n      NaN\n    \n    \n      16707\n      NaN\n      NaN\n    \n    \n      16708\n      NaN\n      NaN\n    \n    \n      16709\n      NaN\n      15.0\n    \n  \n\n16710 rows × 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\n\ndf.iloc[:,map(lambda x: 'Loaned From' != x and 'Marking' != x ,df.columns )]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      ID\n      Name\n      Age\n      Photo\n      Nationality\n      Flag\n      Overall\n      Potential\n      Club\n      Club Logo\n      ...\n      SlidingTackle\n      GKDiving\n      GKHandling\n      GKKicking\n      GKPositioning\n      GKReflexes\n      Best Position\n      Best Overall Rating\n      Release Clause\n      DefensiveAwareness\n    \n  \n  \n    \n      0\n      212198\n      Bruno Fernandes\n      26\n      https://cdn.sofifa.com/players/212/198/22_60.png\n      Portugal\n      https://cdn.sofifa.com/flags/pt.png\n      88\n      89\n      Manchester United\n      https://cdn.sofifa.com/teams/11/30.png\n      ...\n      65.0\n      12.0\n      14.0\n      15.0\n      8.0\n      14.0\n      CAM\n      88.0\n      €206.9M\n      72.0\n    \n    \n      1\n      209658\n      L. Goretzka\n      26\n      https://cdn.sofifa.com/players/209/658/22_60.png\n      Germany\n      https://cdn.sofifa.com/flags/de.png\n      87\n      88\n      FC Bayern München\n      https://cdn.sofifa.com/teams/21/30.png\n      ...\n      77.0\n      13.0\n      8.0\n      15.0\n      11.0\n      9.0\n      CM\n      87.0\n      €160.4M\n      74.0\n    \n    \n      2\n      176580\n      L. Suárez\n      34\n      https://cdn.sofifa.com/players/176/580/22_60.png\n      Uruguay\n      https://cdn.sofifa.com/flags/uy.png\n      88\n      88\n      Atlético de Madrid\n      https://cdn.sofifa.com/teams/240/30.png\n      ...\n      38.0\n      27.0\n      25.0\n      31.0\n      33.0\n      37.0\n      ST\n      88.0\n      €91.2M\n      42.0\n    \n    \n      3\n      192985\n      K. De Bruyne\n      30\n      https://cdn.sofifa.com/players/192/985/22_60.png\n      Belgium\n      https://cdn.sofifa.com/flags/be.png\n      91\n      91\n      Manchester City\n      https://cdn.sofifa.com/teams/10/30.png\n      ...\n      53.0\n      15.0\n      13.0\n      5.0\n      10.0\n      13.0\n      CM\n      91.0\n      €232.2M\n      68.0\n    \n    \n      4\n      224334\n      M. Acuña\n      29\n      https://cdn.sofifa.com/players/224/334/22_60.png\n      Argentina\n      https://cdn.sofifa.com/flags/ar.png\n      84\n      84\n      Sevilla FC\n      https://cdn.sofifa.com/teams/481/30.png\n      ...\n      82.0\n      8.0\n      14.0\n      13.0\n      13.0\n      14.0\n      LB\n      84.0\n      €77.7M\n      80.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      16705\n      240558\n      18 L. Clayton\n      17\n      https://cdn.sofifa.com/players/240/558/18_60.png\n      England\n      https://cdn.sofifa.com/flags/gb-eng.png\n      53\n      70\n      Cheltenham Town\n      https://cdn.sofifa.com/teams/1936/30.png\n      ...\n      12.0\n      55.0\n      54.0\n      52.0\n      50.0\n      59.0\n      GK\n      52.0\n      €238K\n      NaN\n    \n    \n      16706\n      262846\n      �. Dobre\n      20\n      https://cdn.sofifa.com/players/262/846/22_60.png\n      Romania\n      https://cdn.sofifa.com/flags/ro.png\n      53\n      63\n      FC Academica Clinceni\n      https://cdn.sofifa.com/teams/113391/30.png\n      ...\n      12.0\n      57.0\n      52.0\n      53.0\n      48.0\n      58.0\n      GK\n      53.0\n      €279K\n      5.0\n    \n    \n      16707\n      241317\n      21 Xue Qinghao\n      19\n      https://cdn.sofifa.com/players/241/317/21_60.png\n      China PR\n      https://cdn.sofifa.com/flags/cn.png\n      47\n      60\n      Shanghai Shenhua FC\n      https://cdn.sofifa.com/teams/110955/30.png\n      ...\n      9.0\n      49.0\n      48.0\n      45.0\n      38.0\n      52.0\n      GK\n      47.0\n      €223K\n      21.0\n    \n    \n      16708\n      259646\n      A. Shaikh\n      18\n      https://cdn.sofifa.com/players/259/646/22_60.png\n      India\n      https://cdn.sofifa.com/flags/in.png\n      47\n      67\n      ATK Mohun Bagan FC\n      https://cdn.sofifa.com/teams/113146/30.png\n      ...\n      13.0\n      49.0\n      41.0\n      39.0\n      45.0\n      49.0\n      GK\n      47.0\n      €259K\n      7.0\n    \n    \n      16709\n      178453\n      07 A. Censori\n      17\n      https://cdn.sofifa.com/players/178/453/07_60.png\n      Italy\n      https://cdn.sofifa.com/flags/it.png\n      28\n      38\n      Arezzo\n      https://cdn.sofifa.com/teams/110907/30.png\n      ...\n      NaN\n      7.0\n      1.0\n      36.0\n      6.0\n      9.0\n      ST\n      36.0\n      NaN\n      NaN\n    \n  \n\n16710 rows × 63 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\n\ndf.iloc[:,map(lambda x: 'Loaned From' != x and 'Marking' != x ,df.columns )].dropna()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      ID\n      Name\n      Age\n      Photo\n      Nationality\n      Flag\n      Overall\n      Potential\n      Club\n      Club Logo\n      ...\n      SlidingTackle\n      GKDiving\n      GKHandling\n      GKKicking\n      GKPositioning\n      GKReflexes\n      Best Position\n      Best Overall Rating\n      Release Clause\n      DefensiveAwareness\n    \n  \n  \n    \n      0\n      212198\n      Bruno Fernandes\n      26\n      https://cdn.sofifa.com/players/212/198/22_60.png\n      Portugal\n      https://cdn.sofifa.com/flags/pt.png\n      88\n      89\n      Manchester United\n      https://cdn.sofifa.com/teams/11/30.png\n      ...\n      65.0\n      12.0\n      14.0\n      15.0\n      8.0\n      14.0\n      CAM\n      88.0\n      €206.9M\n      72.0\n    \n    \n      1\n      209658\n      L. Goretzka\n      26\n      https://cdn.sofifa.com/players/209/658/22_60.png\n      Germany\n      https://cdn.sofifa.com/flags/de.png\n      87\n      88\n      FC Bayern München\n      https://cdn.sofifa.com/teams/21/30.png\n      ...\n      77.0\n      13.0\n      8.0\n      15.0\n      11.0\n      9.0\n      CM\n      87.0\n      €160.4M\n      74.0\n    \n    \n      2\n      176580\n      L. Suárez\n      34\n      https://cdn.sofifa.com/players/176/580/22_60.png\n      Uruguay\n      https://cdn.sofifa.com/flags/uy.png\n      88\n      88\n      Atlético de Madrid\n      https://cdn.sofifa.com/teams/240/30.png\n      ...\n      38.0\n      27.0\n      25.0\n      31.0\n      33.0\n      37.0\n      ST\n      88.0\n      €91.2M\n      42.0\n    \n    \n      3\n      192985\n      K. De Bruyne\n      30\n      https://cdn.sofifa.com/players/192/985/22_60.png\n      Belgium\n      https://cdn.sofifa.com/flags/be.png\n      91\n      91\n      Manchester City\n      https://cdn.sofifa.com/teams/10/30.png\n      ...\n      53.0\n      15.0\n      13.0\n      5.0\n      10.0\n      13.0\n      CM\n      91.0\n      €232.2M\n      68.0\n    \n    \n      4\n      224334\n      M. Acuña\n      29\n      https://cdn.sofifa.com/players/224/334/22_60.png\n      Argentina\n      https://cdn.sofifa.com/flags/ar.png\n      84\n      84\n      Sevilla FC\n      https://cdn.sofifa.com/teams/481/30.png\n      ...\n      82.0\n      8.0\n      14.0\n      13.0\n      13.0\n      14.0\n      LB\n      84.0\n      €77.7M\n      80.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      16703\n      259718\n      F. Gebhardt\n      19\n      https://cdn.sofifa.com/players/259/718/22_60.png\n      Germany\n      https://cdn.sofifa.com/flags/de.png\n      52\n      66\n      FC Basel 1893\n      https://cdn.sofifa.com/teams/896/30.png\n      ...\n      10.0\n      53.0\n      45.0\n      47.0\n      52.0\n      57.0\n      GK\n      52.0\n      €361K\n      6.0\n    \n    \n      16704\n      251433\n      B. Voll\n      20\n      https://cdn.sofifa.com/players/251/433/22_60.png\n      Germany\n      https://cdn.sofifa.com/flags/de.png\n      58\n      69\n      F.C. Hansa Rostock\n      https://cdn.sofifa.com/teams/27/30.png\n      ...\n      10.0\n      59.0\n      60.0\n      56.0\n      55.0\n      61.0\n      GK\n      58.0\n      €656K\n      5.0\n    \n    \n      16706\n      262846\n      �. Dobre\n      20\n      https://cdn.sofifa.com/players/262/846/22_60.png\n      Romania\n      https://cdn.sofifa.com/flags/ro.png\n      53\n      63\n      FC Academica Clinceni\n      https://cdn.sofifa.com/teams/113391/30.png\n      ...\n      12.0\n      57.0\n      52.0\n      53.0\n      48.0\n      58.0\n      GK\n      53.0\n      €279K\n      5.0\n    \n    \n      16707\n      241317\n      21 Xue Qinghao\n      19\n      https://cdn.sofifa.com/players/241/317/21_60.png\n      China PR\n      https://cdn.sofifa.com/flags/cn.png\n      47\n      60\n      Shanghai Shenhua FC\n      https://cdn.sofifa.com/teams/110955/30.png\n      ...\n      9.0\n      49.0\n      48.0\n      45.0\n      38.0\n      52.0\n      GK\n      47.0\n      €223K\n      21.0\n    \n    \n      16708\n      259646\n      A. Shaikh\n      18\n      https://cdn.sofifa.com/players/259/646/22_60.png\n      India\n      https://cdn.sofifa.com/flags/in.png\n      47\n      67\n      ATK Mohun Bagan FC\n      https://cdn.sofifa.com/teams/113146/30.png\n      ...\n      13.0\n      49.0\n      41.0\n      39.0\n      45.0\n      49.0\n      GK\n      47.0\n      €259K\n      7.0\n    \n  \n\n14398 rows × 63 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\n### 코드1 \ndef convert_currency(value):\n    floatvalue = 0.0\n    strvalue=\"\"\n    if \"M\" in value:\n        strvalue=value.replace(\"M\",\"\").replace(\"€\",\"\")\n        floatvalue=float(float(strvalue)*1000000)\n    elif \"K\" in value:\n        strvalue=value.replace(\"K\",\"\").replace(\"€\",\"\")\n        floatvalue=float(float(strvalue)*1000)\n    else:\n        floatvalue=value.replace(\"€\",\"\")\n    return floatvalue\n\n코드출처: https://www.kaggle.com/talhademirezen/cost-effective-youth-players-fifa22\n\n\ndef convert_currency(value):\n    floatvalue = 0.0\n    strvalue=\"\"\n    if \"M\" in value:\n        strvalue=value.replace(\"M\",\"\").replace(\"€\",\"\")\n        floatvalue=float(float(strvalue)*1000000)\n    elif \"K\" in value:\n        strvalue=value.replace(\"K\",\"\").replace(\"€\",\"\")\n        floatvalue=float(float(strvalue)*1000)\n    else:\n        floatvalue=value.replace(\"€\",\"\")\n    return floatvalue\n\n\ndf=df.iloc[:,map(lambda x: 'Loaned From' != x and 'Marking' != x ,df.columns )].dropna()\ndf['Wage']=list(map(convert_currency,df.Wage))\n\n\n\n\n\nx축을 Best Position으로 하고 y축은 Value의 평균으로 할 것\nValue가 가장 높은 3개의 포지션을 다른색으로 하이라이팅 할 것\n\n시각화예시\n\ndf['Value']=list(map(convert_currency,df.Value))\n\n\nimport numpy as np\n\n\n_df=df.groupby('Best Position').agg({'Value':np.mean})\\\n.rename(columns={'Value':'mean(Value)'})\\\n.sort_values('mean(Value)',ascending=False)\\\n.reset_index()\n_df['Highlight']=_df['mean(Value)']>=_df['mean(Value)'][2]\n_df\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Best Position\n      mean(Value)\n      Highlight\n    \n  \n  \n    \n      0\n      CF\n      9.122222e+06\n      True\n    \n    \n      1\n      LW\n      6.443137e+06\n      True\n    \n    \n      2\n      CM\n      5.630414e+06\n      True\n    \n    \n      3\n      CAM\n      4.356162e+06\n      False\n    \n    \n      4\n      RW\n      3.977832e+06\n      False\n    \n    \n      5\n      CDM\n      3.539740e+06\n      False\n    \n    \n      6\n      LWB\n      3.451340e+06\n      False\n    \n    \n      7\n      LM\n      3.439977e+06\n      False\n    \n    \n      8\n      ST\n      3.295080e+06\n      False\n    \n    \n      9\n      RB\n      3.203283e+06\n      False\n    \n    \n      10\n      LB\n      3.051887e+06\n      False\n    \n    \n      11\n      CB\n      3.038834e+06\n      False\n    \n    \n      12\n      RWB\n      3.023522e+06\n      False\n    \n    \n      13\n      GK\n      2.703686e+06\n      False\n    \n    \n      14\n      RM\n      2.550153e+06\n      False\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nfrom plotnine import *\n\n\n(\n  ggplot(_df)\\\n    + geom_bar(aes(x='Best Position',y='mean(Value)',fill='Highlight'),stat='identity')\n)\n\n\n\n\n<ggplot: (8787037455895)>\n\n\n\n\n\n세부사항\n\nBest Position의 값을 바탕으로 면분할을 하라.\nAge를 색으로 표현하라.\n산점도의 투명도는 alpha=0.5로 size=0.5로 설정할 것.\n\n\nggplot(df)\\\n+geom_point(aes(x='Dribbling',y='SlidingTackle',color='Age'),alpha=0.5,size=0.5)\\\n+facet_wrap('Best Position')\n\n\n\n\n<ggplot: (8787037504353)>\n\n\n\n\n\n(하니) 포지션 GK에 있는 선수는 Dribbling, SlidingTackle 값이 다른포지션대비 상대적으로 낮다.\n(나애리) 모든 포지션에서 Dribbling, SlidingTackle은 서로 독립이라 볼 수 있다.\n(홍두깨) 포지션 CAM은 나이와 Dribbling 사이에 양의 상관관계에 있다.\n(고은애) 포지션 CB은 나이와 Dribbling 사이에 상관계수가 거의 0이다.\n\n\n하니, 홍두깨, 고은애\n\n\n\n\n세부사항\n\nx축: Dribbling, y축: SlidingTackle 로 설정\nValue에 따라 점의 크기를 다르게 설정\n나이에 따라 색깔을 다르게 설정\n\n시각화예시\n\nggplot(df.loc[(df['Best Position']=='CAM') |(df['Best Position']=='CB')])\\\n+geom_point(aes(x='Dribbling',y='SlidingTackle',color='Age',size='Value'),alpha=0.5)\\\n+facet_wrap('Best Position')\n\n\n\n\n<ggplot: (8787037463105)>\n\n\n\n\n\n(하니) AGE와 Value는 양의 상관관계에 있다.\n(나애리) 따라서 축구선수는 AGE가 증가함 따라 Value가 올라가는 것을 알 수 있다. 즉 AGE와 Value사이에는 인과성이 있다.\n(홍두깨) 포지션 CAM은 Dribbiling 능력과 Value가 양의 상관관계에 있어보인다.\n(고은애) 반면에 포지션 CB는 Dribbling 능력보다는 SlidingTackle이 Value와 양의 상관관계에 있다고 볼 수 있다.\n\n\n하니, 홍두깨, 고은애\n\n\n\n\n\n공원에서 뛰는것을 좋아하는 강아지 하니가 있다. 아래는 강아지 하니가 주인과 함께 공원을 산책한 경로이다. 산책코스는 아래와 같이 집에서 공원으로 가는 A코스와 공원에서 집으로 오는 B코스로 나누어진다.\n- A코스: 집 \\(\\to\\) 카페 \\(\\to\\) 초등학교 정문 \\(\\to\\) 공원\n- B코스: 공원 \\(\\to\\) 초등학교 후문 \\(\\to\\) 동물병원 \\(\\to\\) 집\n각 위치의 좌표는 아래와 같다.\n\n집: (0,0)\n카페: (1,2)\n초등학교 정문: (4,3)\n공원: (5,5)\n초등학교 후문: (4.1,3)\n동물병원: (1,0.5)\n\n집에서 출발시에 하니의 체력은 100이며, 각 중간지점에서 하니의 체력은 이동거리에 비례하여 감소한다고 하자. 예를들어 A코스-카페에서 하니의 체력은 아래와 같이 계산할 수 있다.\n\n\\(100- \\sqrt{1^2+2^2}\\)\n\n또한 하니는 공원정문에서 달리기를 시작하였고 이후에 70의 체력을 소진한뒤 공원후문에 도착하였다고 하자. (즉 공원정문에서하니의 체력이 \\(x\\) 라면 공원후문에서 하니의 체력은 \\(x-60\\) 이다.)\n하니의 이동경로에 따른 체력의 변화를 시각화 하라.\n(풀이)\n\nx=[0, 1, 4, 5,  5, 4.1, 1, 0] \ny=[0, 2, 3, 5,  5, 3, 0.5, 0] \ncourse=['A']*4 + ['B']*4 \n_delta = [np.sqrt((x[i]-x[i-1])**2 + (y[i]-y[i-1])**2) for i in range(len(x))]\nstamina = np.array([100,100,100,100, 40, 40, 40, 40]) - [sum(_delta[:(i+1)]) for i in range(8)]\n\n\nggplot(pd.DataFrame({'x':x, 'y':y, 'course':course, 'stamina':stamina}))+\\\ngeom_point(aes(x='x',y='y'))+\\\ngeom_line(aes(x='x',y='y',size='stamina',color='course'))\n\n\n\n\n<ggplot: (8787037463066)>\n\n\n\n\n\n다음은 농구선수 A,B의 시즌별 자유투 성공률이다.\n\n\n\n\n시즌1\n시즌2\n\n\n\n\nA선수\n7/10\n?\n\n\nB선수\n?\n4/4\n\n\n\n\n표안의 값은 성공횟수/총자유투시도\n\n?에 적절한 값을 채워 시즌 1,2 모두 B선수의 자유투 성공률이 높지만 시즌1-2를 전체 합치면 A선수의 자유투 성공률이 더 높도록 하라. (즉 ?에 적절한 값을 채워 심슨의 역설을 설명하기 위한 자료를 구성하라.)\n만들어진 자료를 바탕으로 심슨의 역설을 시각화하라. (즉 시즌 1,2의 자유투 성공률과 전체 자유투 성공률을 barplot으로 시각화하라)\n\n\n\ns1 = [7,999999,8,4]\ns2 = [10,1000000,10,4]\nplayer = [\"A\",\"A\",\"B\",\"B\"]\n\n\ntotal_A = [s2[0] + s2[1]]*2 ## A가 총 던진횟수\ntotal_B = [s2[2] + s2[3]]*2 ## B가 총 던진횟수\n\ntotal  = total_A + total_B\n\n\ntotal\n\n[1000010, 1000010, 14, 14]\n\n\n\nseson_prob = [ s1[i]/s2[i]  for i in range(len(total))]\n\ntotal_prob1 = [((s1[0] + s1[1])/total[0])]*2\ntotal_prob2 = [((s1[2] + s1[3])/total[2])]*2\ntotal_prob = total_prob1 + total_prob2\n\n\nplayer\n\n['A', 'A', 'B', 'B']\n\n\n\nseson_prob\n\n[0.7, 0.999999, 0.8, 1.0]\n\n\n\ndata = pd.DataFrame({\"player\" : player, \"season_prob\" : seson_prob,\n              \"toal_prob\": total_prob})\n\n\ndata[\"season\"] = [\"1\",\"2\",\"1\",\"2\"]\ndata = data.iloc[:, [0,3,1,2]]\ndata\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      player\n      toal_prob\n      season\n      season_prob\n    \n  \n  \n    \n      0\n      A\n      0.999996\n      1\n      0.700000\n    \n    \n      1\n      A\n      0.999996\n      2\n      0.999999\n    \n    \n      2\n      B\n      0.857143\n      1\n      0.800000\n    \n    \n      3\n      B\n      0.857143\n      2\n      1.000000\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n(\n    ggplot(data)\\\n      + geom_bar(aes(x = \"player\",y = \"seson_prob\",fill=\"player\"),stat = \"identity\") \\\n      + facet_wrap(\"season\")\n)\n\n\n\n\n<ggplot: (8787038724640)>\n\n\n\n(\n    ggplot(data)\\\n      + geom_bar(aes(x = \"player\",y = \"total_prob\",fill=\"player\"),stat = \"identity\")\n)\n\n\n\n\n<ggplot: (8787033849340)>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-15-11wk.html",
    "href": "post/Lecture/STDV/2023-05-15-11wk.html",
    "title": "09. folium",
    "section": "",
    "text": "- 통계힉자의 시각화 목적은 분석에 대한 근거를 시각화로 표현하는 것이다.\n- 예쁜 그림을 그리려다가 메시지가 흐려지지 않도록 해야한다.\n- 시간경과에 따른 변화를 보여주고 싶으면 “라인플랏”, 비교를 하고 싶다면 “바플랏”, 변수간의 관계를 알고 싶다면 “산점도”가 일반적이다.\n- 버블차트의 경우 선거와 갗이 특정 지역에 어떠한 표가 몰려있는지 확인할 때 유용하다.\n- 다시 심슨의 역설 : 어떠한 현상을 살펴볼 때, 부분집합들 역시 그러한지 살펴보는 것이 기본이다."
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-15-11wk.html#pd.concat",
    "href": "post/Lecture/STDV/2023-05-15-11wk.html#pd.concat",
    "title": "09. folium",
    "section": "pd.concat",
    "text": "pd.concat\n- R의 rbind와 유사한 느낌\n\npd.concat([df2016,df2017,df2018])\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Symbol\n      Shares\n      Low\n      High\n    \n  \n  \n    \n      0\n      AAPL\n      80\n      95\n      110\n    \n    \n      1\n      TSLA\n      50\n      80\n      130\n    \n    \n      2\n      WMT\n      40\n      55\n      70\n    \n    \n      0\n      AAPL\n      50\n      120\n      140\n    \n    \n      1\n      GE\n      100\n      30\n      40\n    \n    \n      2\n      IBM\n      87\n      75\n      95\n    \n    \n      3\n      SLB\n      20\n      55\n      85\n    \n    \n      4\n      TXN\n      500\n      15\n      23\n    \n    \n      5\n      TSLA\n      100\n      100\n      300\n    \n    \n      0\n      AAPL\n      40\n      135\n      170\n    \n    \n      1\n      AMZN\n      8\n      900\n      1125\n    \n    \n      2\n      TSLA\n      50\n      220\n      400\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.concat([df2016,df2017,df2018]).reset_index()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      index\n      Symbol\n      Shares\n      Low\n      High\n    \n  \n  \n    \n      0\n      0\n      AAPL\n      80\n      95\n      110\n    \n    \n      1\n      1\n      TSLA\n      50\n      80\n      130\n    \n    \n      2\n      2\n      WMT\n      40\n      55\n      70\n    \n    \n      3\n      0\n      AAPL\n      50\n      120\n      140\n    \n    \n      4\n      1\n      GE\n      100\n      30\n      40\n    \n    \n      5\n      2\n      IBM\n      87\n      75\n      95\n    \n    \n      6\n      3\n      SLB\n      20\n      55\n      85\n    \n    \n      7\n      4\n      TXN\n      500\n      15\n      23\n    \n    \n      8\n      5\n      TSLA\n      100\n      100\n      300\n    \n    \n      9\n      0\n      AAPL\n      40\n      135\n      170\n    \n    \n      10\n      1\n      AMZN\n      8\n      900\n      1125\n    \n    \n      11\n      2\n      TSLA\n      50\n      220\n      400\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.concat([df2016, df2017, df2018]).reset_index(drop=True) ## 기존의 생성된 인덱스를 제거하여 고유한 인덱스를 부여함\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Symbol\n      Shares\n      Low\n      High\n    \n  \n  \n    \n      0\n      AAPL\n      80\n      95\n      110\n    \n    \n      1\n      TSLA\n      50\n      80\n      130\n    \n    \n      2\n      WMT\n      40\n      55\n      70\n    \n    \n      3\n      AAPL\n      50\n      120\n      140\n    \n    \n      4\n      GE\n      100\n      30\n      40\n    \n    \n      5\n      IBM\n      87\n      75\n      95\n    \n    \n      6\n      SLB\n      20\n      55\n      85\n    \n    \n      7\n      TXN\n      500\n      15\n      23\n    \n    \n      8\n      TSLA\n      100\n      100\n      300\n    \n    \n      9\n      AAPL\n      40\n      135\n      170\n    \n    \n      10\n      AMZN\n      8\n      900\n      1125\n    \n    \n      11\n      TSLA\n      50\n      220\n      400\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- rbind와의 차이점 : 합치려는 데이터 프렘임의 컬럼들이 동일할 필요가 없음.\n\nNA값을 채워가며 데이터를 결합해줌\n\n\npd.concat([df2016,df2017.iloc[:,1:]])\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Symbol\n      Shares\n      Low\n      High\n    \n  \n  \n    \n      0\n      AAPL\n      80\n      95\n      110\n    \n    \n      1\n      TSLA\n      50\n      80\n      130\n    \n    \n      2\n      WMT\n      40\n      55\n      70\n    \n    \n      0\n      NaN\n      50\n      120\n      140\n    \n    \n      1\n      NaN\n      100\n      30\n      40\n    \n    \n      2\n      NaN\n      87\n      75\n      95\n    \n    \n      3\n      NaN\n      20\n      55\n      85\n    \n    \n      4\n      NaN\n      500\n      15\n      23\n    \n    \n      5\n      NaN\n      100\n      100\n      300\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.concat([],axis=“column”)\n- R에서 cbind와 비슷한 느낌. (그런데 row의 숫자가 서로 달라도 괜찮음)\n\npd.concat([df2016,df2017,df2018],axis=1) ## 역시 비어있는 값들은 NA로 채워준다.\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Symbol\n      Shares\n      Low\n      High\n      Symbol\n      Shares\n      Low\n      High\n      Symbol\n      Shares\n      Low\n      High\n    \n  \n  \n    \n      0\n      AAPL\n      80.0\n      95.0\n      110.0\n      AAPL\n      50\n      120\n      140\n      AAPL\n      40.0\n      135.0\n      170.0\n    \n    \n      1\n      TSLA\n      50.0\n      80.0\n      130.0\n      GE\n      100\n      30\n      40\n      AMZN\n      8.0\n      900.0\n      1125.0\n    \n    \n      2\n      WMT\n      40.0\n      55.0\n      70.0\n      IBM\n      87\n      75\n      95\n      TSLA\n      50.0\n      220.0\n      400.0\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      SLB\n      20\n      55\n      85\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      TXN\n      500\n      15\n      23\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      5\n      NaN\n      NaN\n      NaN\n      NaN\n      TSLA\n      100\n      100\n      300\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\npd.concat([…],keys=[…])\n\npd.concat([df2016,df2017,df2018],keys=[2016,2017,2018])\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      \n      Symbol\n      Shares\n      Low\n      High\n    \n  \n  \n    \n      2016\n      0\n      AAPL\n      80\n      95\n      110\n    \n    \n      1\n      TSLA\n      50\n      80\n      130\n    \n    \n      2\n      WMT\n      40\n      55\n      70\n    \n    \n      2017\n      0\n      AAPL\n      50\n      120\n      140\n    \n    \n      1\n      GE\n      100\n      30\n      40\n    \n    \n      2\n      IBM\n      87\n      75\n      95\n    \n    \n      3\n      SLB\n      20\n      55\n      85\n    \n    \n      4\n      TXN\n      500\n      15\n      23\n    \n    \n      5\n      TSLA\n      100\n      100\n      300\n    \n    \n      2018\n      0\n      AAPL\n      40\n      135\n      170\n    \n    \n      1\n      AMZN\n      8\n      900\n      1125\n    \n    \n      2\n      TSLA\n      50\n      220\n      400\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- 안좋은 예시\n\n키와 리스트간 1대1 ㅐ칭이 이루어져야 데이터의 훼손이 일어나지 않는다.\n\n\npd.concat([df2016,df2017,df2018],keys=[2016,2018])\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      \n      Symbol\n      Shares\n      Low\n      High\n    \n  \n  \n    \n      2016\n      0\n      AAPL\n      80\n      95\n      110\n    \n    \n      1\n      TSLA\n      50\n      80\n      130\n    \n    \n      2\n      WMT\n      40\n      55\n      70\n    \n    \n      2018\n      0\n      AAPL\n      50\n      120\n      140\n    \n    \n      1\n      GE\n      100\n      30\n      40\n    \n    \n      2\n      IBM\n      87\n      75\n      95\n    \n    \n      3\n      SLB\n      20\n      55\n      85\n    \n    \n      4\n      TXN\n      500\n      15\n      23\n    \n    \n      5\n      TSLA\n      100\n      100\n      300\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- 데이터를 합친후 생성한 인덱스의 이름변경\n\npd.concat([df2016,df2017,df2018],keys=[2016,2017,2018]).reset_index().rename({\"level_0\":\"year\"},axis=1).drop(\"level_1\",axis=1)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      year\n      Symbol\n      Shares\n      Low\n      High\n    \n  \n  \n    \n      0\n      2016\n      AAPL\n      80\n      95\n      110\n    \n    \n      1\n      2016\n      TSLA\n      50\n      80\n      130\n    \n    \n      2\n      2016\n      WMT\n      40\n      55\n      70\n    \n    \n      3\n      2017\n      AAPL\n      50\n      120\n      140\n    \n    \n      4\n      2017\n      GE\n      100\n      30\n      40\n    \n    \n      5\n      2017\n      IBM\n      87\n      75\n      95\n    \n    \n      6\n      2017\n      SLB\n      20\n      55\n      85\n    \n    \n      7\n      2017\n      TXN\n      500\n      15\n      23\n    \n    \n      8\n      2017\n      TSLA\n      100\n      100\n      300\n    \n    \n      9\n      2018\n      AAPL\n      40\n      135\n      170\n    \n    \n      10\n      2018\n      AMZN\n      8\n      900\n      1125\n    \n    \n      11\n      2018\n      TSLA\n      50\n      220\n      400\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.concat([df2016,df2017,df2018],keys = [2016,2017,2018])\\\n    .reset_index().rename({\"level_0\" : \"year\"},axis=1)\\\n    .drop(\"level_1\",axis=1)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      year\n      Symbol\n      Shares\n      Low\n      High\n    \n  \n  \n    \n      0\n      2016\n      AAPL\n      80\n      95\n      110\n    \n    \n      1\n      2016\n      TSLA\n      50\n      80\n      130\n    \n    \n      2\n      2016\n      WMT\n      40\n      55\n      70\n    \n    \n      3\n      2017\n      AAPL\n      50\n      120\n      140\n    \n    \n      4\n      2017\n      GE\n      100\n      30\n      40\n    \n    \n      5\n      2017\n      IBM\n      87\n      75\n      95\n    \n    \n      6\n      2017\n      SLB\n      20\n      55\n      85\n    \n    \n      7\n      2017\n      TXN\n      500\n      15\n      23\n    \n    \n      8\n      2017\n      TSLA\n      100\n      100\n      300\n    \n    \n      9\n      2018\n      AAPL\n      40\n      135\n      170\n    \n    \n      10\n      2018\n      AMZN\n      8\n      900\n      1125\n    \n    \n      11\n      2018\n      TSLA\n      50\n      220\n      400"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-15-11wk.html#순서-1-foluum.map",
    "href": "post/Lecture/STDV/2023-05-15-11wk.html#순서-1-foluum.map",
    "title": "09. folium",
    "section": "순서 1: foluum.map",
    "text": "순서 1: foluum.map\n- globar view : 일단 맵이라는 객체를 생성\n- scrollWheelZoom = False : zoom in-out를 막음\n\nm = folium.Map(scrollWheelZoom = False)\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-15-11wk.html#순서-2-위도-경도-zoom_start",
    "href": "post/Lecture/STDV/2023-05-15-11wk.html#순서-2-위도-경도-zoom_start",
    "title": "09. folium",
    "section": "순서 2 : 위도, 경도, zoom_start",
    "text": "순서 2 : 위도, 경도, zoom_start\n\nfolium.Map(\n           scrollWheelZoom=False,\n            location = (35.84676618432067, 127.12936405235232),## 튜플로 넣어도 상관없음, 근데 그냥 리스트로 넣자\n            zoom_start=16 ## 얼마나 가깝게 볼건지\n)\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-15-11wk.html#순서-3-외형변경",
    "href": "post/Lecture/STDV/2023-05-15-11wk.html#순서-3-외형변경",
    "title": "09. folium",
    "section": "순서 3 : 외형변경",
    "text": "순서 3 : 외형변경\n- tiles 옵션을 주어서 지도의 외형을 변경하여 보자. tiles는 ggplot에 theme라고 생각하자 - “OpenStreetMap” -> default 값으로 선정되어 있음 - “Stamen Terrain”, “Stamen Toner”, “Stamen Watercolor”, “CartoDB positron”, “CartoDB dark_matter”\n\nm=folium.Map(scrollWheelZoom=False,\n             location=[35.84676618432067, 127.12936405235232],\n             zoom_start=14,\n             tiles=\"Stamen Toner\"\n            )\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-15-11wk.html#순서-4-object-추가",
    "href": "post/Lecture/STDV/2023-05-15-11wk.html#순서-4-object-추가",
    "title": "09. folium",
    "section": "순서 4 : Object 추가",
    "text": "순서 4 : Object 추가\n- 지도위에 무엇인가 오브젝트를 추가하는데, 가장 기본적인 것이 마커임 (점이라 생각해도 괜찮음)\n- 마커안에 내용을 넣을 수 있음. (경우에 따라서는 유용하다)\n\nicon=‘street-view’\nicon=‘tree’\nicon=‘plane’\nicon=‘bell’\n…\n\nhttps://getbootstrap.com/docs/3.3/components/\n\nm=folium.Map(scrollWheelZoom=False,\n             location=[35.84676618432067, 127.12936405235232],\n             zoom_start=15,\n             tiles=\"CartoDB positron\"\n            )\nfolium.Marker(location=[35.84676618432067, 127.12936405235232],\n              tooltip='Click me',\n              popup='JBNU', ## text-message\n              icon=folium.Icon(color='red',icon='university',prefix='fa') ## prefix=\"fa\"를 안하면 그림이 안보임 \n             ).add_to(m) \n\nfolium.Marker(location=[35.837688889992634, 127.11212714586104],\n              tooltip='Click me',\n              popup='HOME'              \n             ).add_to(m) \nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nm=folium.Map(scrollWheelZoom=False,\n             location=[35.84676618432067, 127.12936405235232],\n             zoom_start=14,\n             tiles=\"CartoDB positron\"\n            )\nfolium.Marker(location=[35.84676618432067, 127.12936405235232],\n              #tooltip='Click me',\n              popup='JBNU',\n              icon=folium.Icon(color='red',icon='music',prefix='fa')\n             ).add_to(m) \n\nfolium.Marker(location=[35.837688889992634, 127.11212714586104],\n              #tooltip='Click me',\n              popup='HOME'              \n             ).add_to(m) \nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n- marker에 html 넣기\n\nm=folium.Map(scrollWheelZoom=False,\n             location=[35.84676618432067, 127.12936405235232],\n             zoom_start=14,\n             tiles=\"CartoDB positron\"\n            )\nfolium.Marker(location=[35.84676618432067, 127.12936405235232],\n              #tooltip='Click me',\n              popup='<h2> JBNU </h2><br>', ## 글씨 폰트도 조정 가능\n              icon=folium.Icon(color='red',icon='music',prefix='fa')\n             ).add_to(m) \n\nfolium.Marker(location=[35.837688889992634, 127.11212714586104],\n              #tooltip='Click me',\n              popup='<h5> HOME </h5><br>'              \n             ).add_to(m) \nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n- 마커에 데이터프렘임을 html로 바꾸어서 넣기\n\ndf= pd.DataFrame(data=[[2019,35],[2020,35],[2021,33]],columns=['Year','Enrollment'])\ndf\ndf.to_html()\n\n\n\n\n\n\n\n\n\n\n\n\nYear\n\n\n\nEnrollment\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\n2019\n\n\n\n35\n\n\n\n\n\n\n\n1\n\n\n\n2020\n\n\n\n35\n\n\n\n\n\n\n\n2\n\n\n\n2021\n\n\n\n33\n\n\n\n\n\n\n\n\n\n\n\nm=folium.Map(scrollWheelZoom=False,\n             location=[35.84676618432067, 127.12936405235232],\n             zoom_start=14,\n             tiles=\"CartoDB positron\"\n            )\nfolium.Marker(location=[35.84676618432067, 127.12936405235232],\n              #tooltip='Click me',\n              popup='<h2> JBNU </h2><br>'+df.to_html(),\n              icon=folium.Icon(color='red',icon='music',prefix='fa')\n             ).add_to(m) \n\nfolium.Marker(location=[35.837688889992634, 127.11212714586104],\n              #tooltip='Click me',\n              popup='<h5> HOME </h5><br>'              \n             ).add_to(m) \nm\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-15-11wk.html#중간요약",
    "href": "post/Lecture/STDV/2023-05-15-11wk.html#중간요약",
    "title": "09. folium",
    "section": "중간요약",
    "text": "중간요약\n- 논리 구조상 html 오브제긏를 아무거나 넣을 수 있다.\n\n그림도 넣을 수 있나?\n\n\nimport base64\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfig = plt.figure()\nplt.plot([1,2,3,4],[2,3,4,3],\"o--r\")\nfig.savefig(\"temp.png\")\n\n\n\n\n- 인터넷에서 긁어온 코드\n\n_encoded = base64.b64encode(open('temp.png','rb').read())\n_myhtml = '<img src=\"data:image/png;base64,{}\">'.format\n_iframe = folium.IFrame(_myhtml(_encoded.decode('UTF-8')),width=400,height=300)\n_popup = folium.Popup(_iframe)\n\n\nm=folium.Map(scrollWheelZoom=False,\n             location=[35.84676618432067, 127.12936405235232],\n             zoom_start=13,\n             tiles=\"CartoDB positron\"\n            )\nfolium.Marker(location=[35.84676618432067, 127.12936405235232],\n              #tooltip='Click me',\n              popup=_popup, ## 이부분에서 이미지가 들어가는 것 같다.\n              icon=folium.Icon(color='red',icon='music',prefix='fa')\n             ).add_to(m) \n\nfolium.Marker(location=[35.837688889992634, 127.11212714586104],\n              #tooltip='Click me',\n              popup='<h5> HOME </h5>'              \n             ).add_to(m) \nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n## folium.CircleMarker\n\nm=folium.Map(scrollWheelZoom=False,\n             location=[35.84676618432067, 127.12936405235232],\n             zoom_start=13,\n             tiles=\"CartoDB positron\"\n            )\nfolium.Marker(location=[35.84676618432067, 127.12936405235232],\n              #tooltip='Click me',\n              popup=_popup,\n              icon=folium.Icon(color='red',icon='music',prefix='fa')\n             ).add_to(m) \n\nfolium.CircleMarker(location=[35.84676618432067, 127.12936405235232], \n                    radius=40, ## 반지름\n                    color=\"red\", ## 이건 원 테두리 컬러\n                    fill=True,\n                    fill_color='red', ## 이건 원 안에 컬러\n                    popup='JBNU').add_to(m)\n\nfolium.Marker(location=[35.837688889992634, 127.11212714586104],\n              #tooltip='Click me',\n              popup='<h5> HOME </h5><br>'              \n             ).add_to(m) \nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n- 다양한 마커를 그냥 지옴처럼 생각해도 괜찮아보임\n- 동그라미 하나 그리는데 너무 많이 노력을 하는것 같기도 함.\n- Circle이 아니라 라인, 사각형, 폴리곤등도 지도에 추가할수 있으나 더 많은 노력이 필요하다."
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-15-11wk.html#heatmap",
    "href": "post/Lecture/STDV/2023-05-15-11wk.html#heatmap",
    "title": "09. folium",
    "section": "Heatmap",
    "text": "Heatmap\n- Heatmap은 Folium에서 데이터 시각화를 하기에 적합한 기본도구임\n- 지도위에 밀도를 표현하기 적합한 시각\n- heatmap은 음수값이 표현이 안되기 때문에 뒤에 만큼 더해 준거임\n- 또한 heatmap은 list형태로 입력을 받기때문에 list로 변경해준 것임`\n\nfrom folium import plugins \ndata=(np.random.normal(size=(100,3)) + np.array([[28,77,5]])).tolist() ## 차원이 3인 이유는 2개는 좌표 1개는 weight 즉 밀도 값임\n\n\nm= folium.Map(location=[28,77],\n             zoom_start = 6,\n             scrollWheelZoom=False)\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n의미가 없는 데이터이나 가운데에 데이터가 많이 밀집해있는 것을 표현하는 예제이다.\n\n\nplugins.HeatMap(data).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nplugins.HeatMapWith Time\n- 입자의 위치\n\\[\\begin{cases}\nx_t = x_{t-1} +\\epsilon_t \\\\\ny_t = y_{t-1} +\\eta_t\n\\end{cases}\\]\n단 \\((x_0,y_0)=(35.837688889992634, 127.11212714586104)\\) 라고 하고 \\(\\epsilon_t\\)와 \\(\\eta_t\\)는 서로 독립인 정규분포이다.\n\n최종적인 차원: 프레임수(시점) * 점의수 * 2\n\n\nt0 = np.array([[35.837688889992634,127.11212714586104]]*3000) ##(x0.y0)\nwalk = np.random.normal(size=(200,3000,2)) * 0.0001 ## 프레임수(시점) x 점의수 x 2, 분산은 매우 작게 , 에러텀이라고 생각하자.\n\n\ndata = [(t0+walk[:i,:,:].sum(axis=0)).tolist() for i in range(200)]\n\n\nm=folium.Map(scrollWheelZoom=False,\n             location=[35.837688889992634,127.11212714586104],\n             zoom_start=16\n            )\nplugins.HeatMapWithTime(data,radius=10).add_to(m)\nm\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\n예제 : earthquakes\n\nstep1. pandas\n\ndf=pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Date\n      Latitude\n      Longitude\n      Magnitude\n    \n  \n  \n    \n      0\n      01/02/1965\n      19.246\n      145.616\n      6.0\n    \n    \n      1\n      01/04/1965\n      1.863\n      127.352\n      5.8\n    \n    \n      2\n      01/05/1965\n      -20.579\n      -173.972\n      6.2\n    \n    \n      3\n      01/08/1965\n      -59.076\n      -23.557\n      5.8\n    \n    \n      4\n      01/09/1965\n      11.938\n      126.427\n      5.8\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- assign은 r의 mutate라고 생각하자.\n\ndf.assign(Year = \n            list(map(lambda x : x.split(\"/\")[-1], df.Date)))\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Date\n      Latitude\n      Longitude\n      Magnitude\n      Year\n    \n  \n  \n    \n      0\n      01/02/1965\n      19.2460\n      145.6160\n      6.0\n      1965\n    \n    \n      1\n      01/04/1965\n      1.8630\n      127.3520\n      5.8\n      1965\n    \n    \n      2\n      01/05/1965\n      -20.5790\n      -173.9720\n      6.2\n      1965\n    \n    \n      3\n      01/08/1965\n      -59.0760\n      -23.5570\n      5.8\n      1965\n    \n    \n      4\n      01/09/1965\n      11.9380\n      126.4270\n      5.8\n      1965\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      23407\n      12/28/2016\n      38.3917\n      -118.8941\n      5.6\n      2016\n    \n    \n      23408\n      12/28/2016\n      38.3777\n      -118.8957\n      5.5\n      2016\n    \n    \n      23409\n      12/28/2016\n      36.9179\n      140.4262\n      5.9\n      2016\n    \n    \n      23410\n      12/29/2016\n      -9.0283\n      118.6639\n      6.3\n      2016\n    \n    \n      23411\n      12/30/2016\n      37.3973\n      141.4103\n      5.5\n      2016\n    \n  \n\n23412 rows × 5 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- pipe를 이용하여 리스트 구조로 그룹화한 데이터를 리스트로 변경\n\n_df = df.assign(Year = \n            list(map(lambda x : x.split(\"/\")[-1], df.Date))).\\\n            groupby(\"Year\").\\\n            pipe(list)\n\n- 1965년 데이터 색출\n\n_df[0]\n\n('1965',\n            Date  Latitude  Longitude  Magnitude  Year\n 0    01/02/1965    19.246    145.616        6.0  1965\n 1    01/04/1965     1.863    127.352        5.8  1965\n 2    01/05/1965   -20.579   -173.972        6.2  1965\n 3    01/08/1965   -59.076    -23.557        5.8  1965\n 4    01/09/1965    11.938    126.427        5.8  1965\n ..          ...       ...        ...        ...   ...\n 334  12/25/1965   -18.096   -178.979        6.2  1965\n 335  12/26/1965    -5.201    151.583        5.9  1965\n 336  12/28/1965    27.863    141.992        5.8  1965\n 337  12/30/1965    54.210   -164.476        5.6  1965\n 338  12/30/1965   -16.673    -71.111        5.8  1965\n \n [339 rows x 5 columns])\n\n\n\n첫번째 인덱스 1965는 메타데이터, 그 속에 위치한 하위 인덱스는 실제 데이터라고 생각하자\n\n\n_df[0][0]\n\n'1965'\n\n\n\n_df[0][1]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Date\n      Latitude\n      Longitude\n      Magnitude\n      Year\n    \n  \n  \n    \n      0\n      01/02/1965\n      19.246\n      145.616\n      6.0\n      1965\n    \n    \n      1\n      01/04/1965\n      1.863\n      127.352\n      5.8\n      1965\n    \n    \n      2\n      01/05/1965\n      -20.579\n      -173.972\n      6.2\n      1965\n    \n    \n      3\n      01/08/1965\n      -59.076\n      -23.557\n      5.8\n      1965\n    \n    \n      4\n      01/09/1965\n      11.938\n      126.427\n      5.8\n      1965\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      334\n      12/25/1965\n      -18.096\n      -178.979\n      6.2\n      1965\n    \n    \n      335\n      12/26/1965\n      -5.201\n      151.583\n      5.9\n      1965\n    \n    \n      336\n      12/28/1965\n      27.863\n      141.992\n      5.8\n      1965\n    \n    \n      337\n      12/30/1965\n      54.210\n      -164.476\n      5.6\n      1965\n    \n    \n      338\n      12/30/1965\n      -16.673\n      -71.111\n      5.8\n      1965\n    \n  \n\n339 rows × 5 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- 데이터 프레임을 리스트 구조로 변경시 1차적으로 넘파이 구조로 변경해주어야한다.\n\n_df[0][1].iloc[:,1:4].\\\nto_numpy().tolist()\n\n[[19.246, 145.616, 6.0],\n [1.863, 127.352, 5.8],\n [-20.579, -173.972, 6.2],\n [-59.076, -23.557, 5.8],\n [11.938, 126.427, 5.8],\n [-13.405, 166.62900000000002, 6.7],\n [27.357, 87.867, 5.9],\n [-13.309, 166.21200000000002, 6.0],\n [-56.452, -27.043000000000003, 6.0],\n [-24.563, 178.487, 5.8],\n [-6.807, 108.988, 5.9],\n [-2.608, 125.952, 8.2],\n [54.636, 161.703, 5.5],\n [-18.697, -177.864, 5.6],\n [37.523, 73.251, 6.0],\n [-51.84, 139.741, 6.1],\n [51.251000000000005, 178.715, 8.7],\n [51.639, 175.055, 6.0],\n [52.528, 172.007, 5.7],\n [51.626000000000005, 175.74599999999998, 5.8],\n [51.037, 177.84799999999998, 5.9],\n [51.73, 173.975, 5.9],\n [51.775, 173.058, 5.7],\n [52.611, 172.588, 5.7],\n [51.831, 174.368, 5.7],\n [51.948, 173.96900000000002, 5.6],\n [51.443000000000005, 179.605, 7.3],\n [52.773, 171.97400000000002, 6.5],\n [51.772, 174.696, 5.6],\n [52.975, 171.09099999999998, 6.4],\n [52.99, 170.87400000000002, 5.8],\n [51.536, 175.045, 5.8],\n [13.245, -44.922, 5.8],\n [51.812, 174.206, 5.7],\n [51.762, 174.84099999999998, 5.7],\n [52.438, 174.321, 6.3],\n [51.946000000000005, 173.84, 5.7],\n [51.738, 174.56599999999997, 6.0],\n [51.487, 176.558, 5.6],\n [53.008, -162.00799999999998, 6.4],\n [52.184, 175.505, 6.2],\n [52.076, 172.918, 5.6],\n [51.744, 175.213, 5.7],\n [52.057, 174.116, 5.7],\n [53.191, -161.859, 6.3],\n [51.447, 176.46900000000002, 5.8],\n [51.258, 173.393, 5.7],\n [52.031000000000006, 175.41099999999997, 5.7],\n [51.294, 179.092, 5.8],\n [55.223, 165.426, 5.9],\n [-18.718, 169.386, 5.6],\n [52.815, 171.90400000000002, 6.0],\n [52.188, 172.752, 5.8],\n [51.00899999999999, 179.325, 5.8],\n [3.026, 125.951, 5.9],\n [38.908, 142.095, 5.7],\n [51.694, 176.446, 5.7],\n [21.527, 143.08100000000002, 5.6],\n [25.011, 94.186, 5.6],\n [-7.251, 126.715, 5.9],\n [51.415, 179.358, 5.8],\n [-15.343, -172.889, 5.8],\n [-25.633000000000003, -70.679, 7.0],\n [-5.461, 151.98, 6.2],\n [51.88399999999999, 173.072, 6.2],\n [6.746, -72.971, 5.6],\n [28.133000000000003, -112.208, 6.0],\n [-5.377000000000001, 152.115, 5.9],\n [21.104, 121.218, 5.7],\n [-5.371, 152.055, 5.5],\n [21.122, 121.148, 5.6],\n [52.20399999999999, 173.91099999999997, 5.7],\n [15.404000000000002, -92.623, 5.6],\n [38.399, 28.226, 5.8],\n [-27.145, -177.262, 5.6],\n [-5.513999999999999, 151.819, 6.7],\n [53.04600000000001, 171.308, 6.3],\n [-5.487, 146.993, 5.7],\n [50.928, 179.511, 5.5],\n [52.39, 174.937, 5.6],\n [-27.027, -63.214, 5.8],\n [52.416, 174.239, 5.6],\n [-18.425, -132.923, 5.6],\n [20.13, 121.461, 5.8],\n [-29.998, -177.968, 6.0],\n [39.197, 23.86, 6.1],\n [53.292, -162.33100000000002, 5.7],\n [36.405, 70.72399999999999, 7.4],\n [22.635, 121.117, 5.8],\n [40.697, 143.032, 6.4],\n [52.785, 171.821, 6.0],\n [-20.036, -175.89, 6.3],\n [-55.857, -26.758000000000003, 5.7],\n [-1.887, 119.836, 6.0],\n [-1.547, 126.623, 6.3],\n [-15.262, -173.25400000000002, 6.5],\n [-31.974, -71.57300000000001, 6.2],\n [-15.382, -173.19400000000002, 6.2],\n [8.418, 126.553, 5.7],\n [55.26, 161.904, 5.8],\n [-32.522, -71.233, 7.4],\n [40.687, 142.915, 6.4],\n [-20.502, -173.701, 5.9],\n [50.282, 177.959, 7.6],\n [38.365, 22.405, 6.8],\n [9.986, 125.896, 5.9],\n [15.8, -98.067, 5.8],\n [51.865, 175.172, 5.7],\n [-27.088, -176.046, 5.6],\n [37.505, 22.067, 6.1],\n [44.812, 150.871, 5.7],\n [36.083, 139.968, 5.7],\n [-0.4629999999999999, 120.039, 6.2],\n [52.192, 173.437, 6.1],\n [-32.707, -178.207, 6.0],\n [35.047, 24.318, 6.2],\n [-20.455, -173.62099999999998, 5.7],\n [-17.857, -178.646, 6.5],\n [-13.409, 170.37599999999998, 6.3],\n [-42.692, 174.205, 5.9],\n [-26.168000000000003, 178.628, 5.7],\n [30.317, 138.702, 5.6],\n [25.08, 122.897, 5.6],\n [64.572, -160.375, 6.0],\n [41.46, -127.416, 5.9],\n [-59.687, -26.454, 6.1],\n [-59.586000000000006, -26.372, 6.1],\n [34.848, 138.332, 6.0],\n [11.451, 140.231, 5.8],\n [24.409, 142.776, 6.0],\n [-1.733, 126.568, 5.7],\n [54.157, -162.59, 5.6],\n [21.045, 120.798, 6.1],\n [-6.822, 129.504, 6.0],\n [35.626, 23.425, 6.1],\n [1.3330000000000002, -85.075, 5.8],\n [47.288, -122.406, 6.7],\n [-5.547999999999999, 110.332, 6.0],\n [60.35, -146.17600000000002, 5.6],\n [28.927, 128.769, 5.8],\n [-32.537, -70.569, 5.8],\n [13.683, -89.07, 5.9],\n [-24.281, -67.92699999999999, 5.7],\n [41.801, 79.414, 6.2],\n [-3.523, 137.924, 5.7],\n [-6.174, 130.358, 6.1],\n [-4.131, 134.946, 5.8],\n [5.268, 125.467, 5.9],\n [22.336, 121.248, 6.7],\n [-8.937999999999999, 159.083, 5.6],\n [-6.575, 105.311, 5.6],\n [-4.819, 152.436, 5.5],\n [-14.921, 167.34, 7.7],\n [1.3090000000000002, 126.239, 5.7],\n [-21.126, -178.537, 6.1],\n [52.149, 175.11900000000003, 6.0],\n [13.174, 124.604, 5.9],\n [51.227, 178.882, 5.8],\n [-55.957, -27.875, 6.0],\n [-7.566, 128.578, 5.9],\n [-23.54, -179.917, 5.7],\n [15.674, -46.712, 5.9],\n [-1.649, 126.552, 5.6],\n [51.88399999999999, 174.015, 5.6],\n [44.608, 149.02200000000002, 7.0],\n [44.578, 148.69899999999998, 7.2],\n [44.133, 149.255, 5.5],\n [44.112, 149.539, 5.6],\n [44.299, 149.032, 5.7],\n [44.167, 149.87, 5.6],\n [43.775, 149.44799999999998, 5.7],\n [43.804, 149.351, 5.7],\n [43.885, 149.339, 5.5],\n [-20.38, -68.944, 5.9],\n [41.705, 143.727, 6.1],\n [37.718, 29.378, 5.9],\n [-39.902, 45.55, 5.6],\n [-38.08, 177.442, 5.9],\n [-21.025, 173.574, 6.2],\n [32.052, 87.78399999999999, 5.9],\n [44.505, 149.218, 5.6],\n [42.973, -126.652, 5.5],\n [28.134, 55.891000000000005, 5.9],\n [7.175, 123.613, 6.3],\n [56.543, -152.94799999999998, 6.5],\n [7.034, 126.25, 5.8],\n [-54.437, 4.965, 5.7],\n [60.281000000000006, -141.418, 6.5],\n [23.764, 121.56, 5.9],\n [-5.01, 153.06, 6.0],\n [-1.734, 126.53, 5.8],\n [51.813, 176.53, 5.7],\n [-63.159, -163.707, 6.1],\n [52.99, -167.739, 7.8],\n [22.338, 101.496, 5.9],\n [-22.567, 172.93900000000002, 5.8],\n [38.384, 22.545, 6.3],\n [46.591, 152.467, 5.6],\n [-4.408, 155.062, 6.3],\n [-6.819, 105.505, 5.7],\n [-28.428, -68.25, 5.7],\n [7.74, 123.811, 6.0],\n [-9.72, 159.888, 5.9],\n [7.56, 124.247, 5.7],\n [53.318000000000005, 170.365, 5.5],\n [1.813, 99.276, 5.5],\n [41.278, 146.68200000000002, 5.6],\n [51.472, 175.894, 5.7],\n [-2.335, 101.767, 5.6],\n [51.05, -171.28900000000002, 6.8],\n [50.799, -171.607, 5.7],\n [51.076, -171.46599999999998, 5.6],\n [6.789, -72.907, 5.6],\n [-32.441, -178.803, 5.9],\n [-56.04600000000001, 157.922, 7.3],\n [-7.526, -81.179, 5.7],\n [-5.212999999999999, 151.701, 6.5],\n [-15.449000000000002, 166.98, 7.2],\n [-15.67, 167.21599999999998, 5.8],\n [-15.685, 167.097, 6.9],\n [-15.741, 167.26, 6.0],\n [-15.861, 167.092, 7.6],\n [-15.817, 167.399, 6.9],\n [-5.36, 152.116, 6.8],\n [-15.931, 167.392, 5.9],\n [-15.97, 167.43900000000002, 5.8],\n [-15.995, 166.96, 6.1],\n [-15.871, 166.96, 7.4],\n [-16.198, 167.607, 7.2],\n [-16.801, 167.602, 6.2],\n [-6.38, 148.543, 6.0],\n [-15.819, 166.794, 6.0],\n [-11.385, 166.332, 5.7],\n [-0.53, -19.951, 6.2],\n [4.872, 95.836, 6.1],\n [-15.185, 166.67, 5.8],\n [-15.945, 166.99900000000002, 6.2],\n [-5.714, 128.493, 6.7],\n [-18.954, -68.82, 6.7],\n [-22.887, -176.19400000000002, 6.1],\n [-5.921, 103.993, 5.6],\n [40.459, 26.206, 6.0],\n [16.081, -95.867, 7.4],\n [15.837, -96.235, 5.8],\n [44.453, 149.06, 5.7],\n [-16.930999999999997, 167.454, 5.8],\n [-6.428, 104.667, 5.7],\n [-34.596, 179.922, 5.8],\n [51.867, 175.451, 5.5],\n [46.515, 153.387, 5.9],\n [58.09, -152.525, 7.0],\n [57.513000000000005, -152.263, 5.7],\n [6.47, -84.40799999999999, 6.2],\n [-5.349, 152.955, 6.2],\n [-6.209, 151.651, 6.1],\n [-6.416, 70.726, 6.3],\n [55.338, 165.731, 5.9],\n [-36.448, -97.6, 6.1],\n [8.382, 126.895, 5.8],\n [7.156000000000001, 126.522, 6.0],\n [-1.443, -77.476, 6.5],\n [36.269, 141.533, 5.8],\n [36.245, 141.53, 5.9],\n [36.27, 141.361, 6.8],\n [8.187999999999999, 126.874, 6.0],\n [28.952, 128.227, 6.7],\n [20.682, 99.326, 5.9],\n [-5.391, 151.631, 5.9],\n [36.395, 141.46, 6.3],\n [13.092, 145.167, 5.6],\n [39.626, 143.357, 5.7],\n [-54.757, -38.244, 5.8],\n [51.752, 175.5, 5.6],\n [-28.278, -177.99900000000002, 6.5],\n [59.28, -144.06, 6.1],\n [49.937, 178.166, 6.8],\n [-19.921, 174.48, 6.2],\n [-60.468, -24.551, 6.0],\n [49.426, 156.417, 5.9],\n [-42.969, -75.225, 5.9],\n [12.347, 114.485, 5.9],\n [-59.225, -25.091, 5.8],\n [56.19, -153.615, 5.6],\n [3.113, 128.558, 5.7],\n [-7.981, 156.287, 5.7],\n [-1.28, 127.725, 6.3],\n [52.36600000000001, 174.25400000000002, 6.1],\n [53.901, -165.403, 5.6],\n [4.203, 125.787, 5.9],\n [49.646, 156.116, 5.6],\n [44.12, 145.389, 6.7],\n [-1.257, 127.735, 5.6],\n [-9.031, -71.139, 6.7],\n [-22.457, -113.788, 6.1],\n [-3.144, 143.789, 5.9],\n [-22.126, -113.928, 6.2],\n [30.628, 140.23, 6.4],\n [43.745, 87.84299999999999, 6.5],\n [-29.414, -68.166, 5.7],\n [36.514, 141.22899999999998, 5.7],\n [-0.2239999999999999, -18.645, 6.1],\n [6.553, 126.903, 5.9],\n [30.887, -41.479, 6.0],\n [25.449, 125.319, 5.7],\n [-7.058, 129.347, 5.6],\n [-18.855, -177.72099999999998, 6.5],\n [53.909, 160.579, 6.0],\n [-7.225, 129.12, 5.8],\n [-6.193, 130.40200000000002, 6.5],\n [51.93600000000001, -176.092, 5.7],\n [51.361, -179.71599999999998, 5.9],\n [2.983, 124.831, 6.2],\n [51.42, -179.7, 5.6],\n [-8.666, 111.103, 5.8],\n [33.011, 141.079, 5.6],\n [-9.611, 159.797, 5.8],\n [-45.847, -72.958, 6.2],\n [36.096, 27.318, 5.9],\n [-4.841, 103.103, 5.8],\n [51.101000000000006, -170.604, 5.8],\n [52.612, 173.14, 5.6],\n [18.919, -107.176, 6.7],\n [-6.457999999999999, 146.298, 6.3],\n [-37.199, 177.558, 6.0],\n [17.135, -100.126, 6.4],\n [-18.108, -178.062, 6.2],\n [-11.292, 166.143, 5.9],\n [44.688, 149.939, 6.0],\n [44.757, 150.195, 5.6],\n [-56.722, -141.439, 5.8],\n [7.65, -81.913, 6.3],\n [52.293, 160.356, 5.8],\n [58.335, -153.303, 6.1],\n [60.481, -140.766, 5.6],\n [-18.096, -178.979, 6.2],\n [-5.2010000000000005, 151.583, 5.9],\n [27.863000000000003, 141.99200000000002, 5.8],\n [54.21, -164.476, 5.6],\n [-16.673, -71.111, 5.8]]\n\n\n- 각 연도별 지진 위도, 경도, 강도, 데이터를 받기(1965 ~ 2016)\n\nlst=[df.assign(Year=list(map(lambda x: x.split('/')[-1], df.Date))).\\\ngroupby('Year').\\\npipe(list)[i][1].iloc[:,[1,2,3]].\\\nto_numpy().tolist() for i in range(2017-1965)]\n\n\nm=folium.Map(scrollWheelZoom=False)\nplugins.HeatMapWithTime(lst,radius=7,index=list(range(1965,2017))).add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-16-12wk.html",
    "href": "post/Lecture/STDV/2023-05-16-12wk.html",
    "title": "10. Choropleth Map",
    "section": "",
    "text": "import folium \nimport pandas as pd \nimport json ## json python 상에서 쓸 수 있게하는 패키지\nimport requests  ## 인터넷상의 자료를 긁어오는 패키지\n\n- 아래 같은지도에 Choropleth Map 처럼 선을 그린다.\n- github에 선들을 그려주는 위도, 경도 데이터가 저장되어 있음\n\nm = folium.Map([36,128],zoom_start=7,scrollWheelZoom=False)\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n- 링크 저장\n\nlocal_distriction_jsonurl='https://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-municipalities-2018-geo.json'\nglobal_distriction_jsonurl='https://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-provinces-2018-geo.json'\n\n- json 파일 load \\(\\to\\) 복잡한 형채의 dic 형태로 저장되어 있음\n\nlocal_dict = json.loads(requests.get(local_distriction_jsonurl).text) ## 시군구별 json 파일\nglobal_dict = json.loads(requests.get(global_distriction_jsonurl).text) ## 도별 json 파일\n\n\n#global_dict\n\n\n\n\nm = folium.Map(\n                location = [36,128],\n                zoom_start =7,\n                scrollWheelZoom = False\n                )\nfolium.Choropleth(geo_data=global_dict).add_to(m)\nm\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\n\n- local_dict를 살펴보자.\n\nlocal_dict?\n\n- 아래와 같이 4개의 원소를 가지는 딕셔너리 임을 확인\n\nlocal_dict.keys()\n\ndict_keys(['type', 'features', 'name', 'crs'])\n\n\n\nlocal_dict[\"type\"]\n\n'FeatureCollection'\n\n\n\nlocal_dict['name']\n\n'sgg'\n\n\n\nlocal_dict['crs']\n\n{'type': 'name', 'properties': {'name': 'urn:ogc:def:crs:OGC:1.3:CRS84'}}\n\n\n- 위 3개는 별거 없어보임\n\nfeatures 길이가 250인 리스트형태이다.\n리스트내의 요소들은각 지역의 위도 경도 데이터를 가지고 있는것 같다.\n\n\n_features =local_dict[\"features\"]\n\n\n_features[0]\n\n{'type': 'Feature',\n 'geometry': {'type': 'MultiPolygon',\n  'coordinates': [[[[126.97468086053324, 37.629805433945975],\n     [126.97496550641705, 37.62902584732434],\n     [126.97619200024447, 37.629086831818235],\n     [126.97841861289852, 37.62929009924553],\n     [126.97952791872238, 37.62889299772442],\n     [126.97993254143336, 37.62855138674378],\n     [126.97997245663913, 37.62845560448294],\n     [126.98004544991201, 37.628280435760445],\n     [126.97960294035302, 37.626371002121644],\n     [126.98090237068868, 37.625391798020395],\n     [126.98168803207568, 37.62449661776251],\n     [126.98184774907679, 37.62431463061319],\n     [126.98310978892584, 37.622104640078],\n     [126.98365101056845, 37.62115092912986],\n     [126.98355888565348, 37.620963267437894],\n     [126.98317999257362, 37.62018285183479],\n     [126.9831620682276, 37.6192347829173],\n     [126.9837143460292, 37.61775856638039],\n     [126.98453643307462, 37.61598061074333],\n     [126.985901535693, 37.612194740823796],\n     [126.9857804843237, 37.61090698776889],\n     [126.98586523076781, 37.60946995153532],\n     [126.98623831468637, 37.607736782107125],\n     [126.98675598705915, 37.60608704965549],\n     [126.98677074181907, 37.60458661863664],\n     [126.9866691435352, 37.604023824468506],\n     [126.98589778740545, 37.60363176153766],\n     [126.98259603020061, 37.60224271097029],\n     [126.9806843648502, 37.60161031225276],\n     [126.97880665420949, 37.60101042316433],\n     [126.97852169405631, 37.60099248777431],\n     [126.97841693568347, 37.600914296754326],\n     [126.97830701720335, 37.600830447643865],\n     [126.97768193317589, 37.60024002437466],\n     [126.9774909383084, 37.599649166074286],\n     [126.97719804274786, 37.59864847380904],\n     [126.97672044958631, 37.59790077442683],\n     [126.97713557023776, 37.59768356016227],\n     [126.97874017601505, 37.59663493476611],\n     [126.97911091252222, 37.596394270916285],\n     [126.98148823597326, 37.59533576215007],\n     [126.98292059990969, 37.594775982193624],\n     [126.98384405286397, 37.59443543461552],\n     [126.98432317109803, 37.59437806733553],\n     [126.98450416009975, 37.59437471448865],\n     [126.9846156223434, 37.59438909275369],\n     [126.98463785016311, 37.59439408049888],\n     [126.98464216134765, 37.59439557572296],\n     [126.98466445483236, 37.59440401859687],\n     [126.98474441343951, 37.594434717703415],\n     [126.98479058424006, 37.594451906863895],\n     [126.98487868712496, 37.59446656147479],\n     [126.98514831673403, 37.594501790677405],\n     [126.98542946037894, 37.59444945443447],\n     [126.98577892249222, 37.59425495254234],\n     [126.98851705023388, 37.59194143989685],\n     [126.98886704210804, 37.59164472228141],\n     [126.98907495209343, 37.59142090222344],\n     [126.98908433025534, 37.59140908294205],\n     [126.98962720754243, 37.59139763837295],\n     [126.99366158210871, 37.59149362412968],\n     [126.99744340490047, 37.59221011675905],\n     [126.99845395445548, 37.59247126210828],\n     [127.00182654761254, 37.59117745103071],\n     [127.00408062474574, 37.58773357887768],\n     [127.00656012457411, 37.58553460128624],\n     [127.00863797169242, 37.58045993171883],\n     [127.01051131122689, 37.58025727035416],\n     [127.0111260694579, 37.58104392936677],\n     [127.0111791729006, 37.58109176738164],\n     [127.01169858419944, 37.58150458110694],\n     [127.0117885968101, 37.58157458058202],\n     [127.0120845128743, 37.581585674737156],\n     [127.01222457400928, 37.581550873062085],\n     [127.0124187027769, 37.58150235525185],\n     [127.01253278245557, 37.58147102693477],\n     [127.01262598550882, 37.58144402349896],\n     [127.01274334585422, 37.58141185855377],\n     [127.01285982629548, 37.58138609392204],\n     [127.01314825156626, 37.58143362114355],\n     [127.01324571819538, 37.58145032393419],\n     [127.0141191759168, 37.581820171723166],\n     [127.01474563580459, 37.582305831258346],\n     [127.01483334205895, 37.58233145649853],\n     [127.01491713941482, 37.582244907556685],\n     [127.01638580146992, 37.58193775580977],\n     [127.01673363020068, 37.58169269831286],\n     [127.01829142498933, 37.579049416348774],\n     [127.01830026223921, 37.57903270179077],\n     [127.0187770633223, 37.57751723332606],\n     [127.02179265953254, 37.57888146291368],\n     [127.02313849621643, 37.57802087910892],\n     [127.02336168058478, 37.57191110395814],\n     [127.0231368263208, 37.57196081106164],\n     [127.01902025943656, 37.57101323682973],\n     [127.01733386994712, 37.570076865408346],\n     [127.01003508740953, 37.569770822171066],\n     [127.00983517715241, 37.569764939352325],\n     [127.00667501959796, 37.5696322906642],\n     [127.00331554021028, 37.56962809025019],\n     [127.00166446511412, 37.56954949223042],\n     [126.99997987347145, 37.5692153342546],\n     [126.99979663625149, 37.569179012484],\n     [126.99919901238287, 37.569061584986855],\n     [126.99894910731591, 37.56901456760983],\n     [126.99872661605082, 37.56897317063658],\n     [126.99854409537699, 37.568939937981746],\n     [126.99823033791377, 37.56888559029638],\n     [126.99795832632955, 37.56884081225306],\n     [126.99734249535118, 37.56876646165091],\n     [126.99683684418163, 37.568709851806034],\n     [126.99646755692298, 37.56866901280384],\n     [126.99624859929914, 37.56864873037851],\n     [126.9908841263958, 37.568202068530944],\n     [126.99015096670833, 37.56813614100136],\n     [126.9899958620981, 37.568128802881496],\n     [126.98961154520899, 37.5681113107368],\n     [126.98956785452701, 37.56810990322031],\n     [126.98946917417102, 37.5681067923079],\n     [126.98894530081218, 37.56810280048161],\n     [126.98892726902085, 37.56810279903639],\n     [126.98892019626068, 37.56810279906341],\n     [126.98838782923946, 37.56812610223998],\n     [126.9882334327857, 37.568133130541256],\n     [126.98821751956565, 37.56813396831202],\n     [126.98663917909906, 37.568232328822624],\n     [126.97973830842082, 37.5690421486377],\n     [126.97762523789515, 37.569298907580105],\n     [126.97566360541524, 37.56938088043951],\n     [126.96909869121171, 37.56824768293371],\n     [126.96669861216496, 37.565820355825934],\n     [126.9657113705709, 37.56677550324494],\n     [126.95940037405296, 37.57304934989536],\n     [126.95491745681615, 37.577149174890465],\n     [126.95355819355073, 37.578748855084044],\n     [126.95362029368313, 37.57880909633655],\n     [126.95720618763337, 37.5798655310825],\n     [126.9575728370125, 37.58230278085636],\n     [126.95799889706633, 37.58412468840079],\n     [126.95799717111329, 37.58444209574531],\n     [126.95790145320846, 37.584703977736275],\n     [126.95782691437041, 37.58485252016721],\n     [126.95784782389684, 37.585019116837834],\n     [126.95783833056504, 37.58504093891924],\n     [126.9578273214224, 37.58506624584095],\n     [126.95747215846923, 37.5860469085176],\n     [126.95737395766116, 37.587076849872744],\n     [126.9573886107174, 37.58906602474426],\n     [126.95749035641607, 37.59007828277017],\n     [126.95779750312285, 37.592573592797144],\n     [126.95852639147067, 37.59418568474202],\n     [126.95902399975766, 37.59479744400614],\n     [126.95903333866578, 37.59505902103998],\n     [126.95826848335163, 37.597788688618316],\n     [126.9580418874916, 37.598155874167304],\n     [126.95771415677586, 37.59836707719559],\n     [126.9573078320294, 37.5985310319704],\n     [126.95654122604887, 37.598827579847494],\n     [126.95634147535245, 37.59892143950143],\n     [126.95414105350225, 37.59909827963582],\n     [126.95338041139009, 37.599531288469684],\n     [126.95259689745177, 37.60122903220133],\n     [126.95250806229828, 37.60156792706982],\n     [126.95237511989355, 37.602476036294],\n     [126.95356668581537, 37.60447934849368],\n     [126.95376271651638, 37.60469036467159],\n     [126.95388420830209, 37.60480196572351],\n     [126.95254683214502, 37.60665255755602],\n     [126.95220372203077, 37.60688813942989],\n     [126.95056465460655, 37.60835968464961],\n     [126.95033780725277, 37.60909261594867],\n     [126.95036280527036, 37.610622635741834],\n     [126.94982875644766, 37.61165624620594],\n     [126.94986821104538, 37.611730018151725],\n     [126.95047793576704, 37.6128699970444],\n     [126.95073164301206, 37.61352828204463],\n     [126.95079658268352, 37.613849365294406],\n     [126.95072414848573, 37.615537943529716],\n     [126.95046953204748, 37.616099153508785],\n     [126.94986337153628, 37.61872668218891],\n     [126.94889796158519, 37.623262743943236],\n     [126.94900026337734, 37.6243351349276],\n     [126.94971290787633, 37.625344069539224],\n     [126.94981195859722, 37.625497586539126],\n     [126.95045372101613, 37.626418006443956],\n     [126.95556524710337, 37.62828290175954],\n     [126.95841841842733, 37.629469751140356],\n     [126.96011573909414, 37.62977556572832],\n     [126.96708433443806, 37.63070658019315],\n     [126.97302977391784, 37.6323747679206],\n     [126.97538835725105, 37.63173308759023],\n     [126.97536907543345, 37.631698170595676],\n     [126.97508476623943, 37.63118349920217],\n     [126.97468086053324, 37.629805433945975]]]]},\n 'properties': {'name': '종로구',\n  'base_year': '2018',\n  'name_eng': 'Jongno-gu',\n  'code': '11010'}}\n\n\n\nfeatures 의 내부구조를 또 살펴보니\ngeometry는 위도 경도, type은 별거 없고, properties 의 경우 이름, 연도, 영문 이름, 시도코드가 저장되어있다.\n\n\n_features[0].keys()\n\ndict_keys(['type', 'geometry', 'properties'])\n\n\n\n_features[0][\"properties\"]\n\n{'name': '종로구', 'base_year': '2018', 'name_eng': 'Jongno-gu', 'code': '11010'}\n\n\n\n\n\n- 지역 이름을 가지는 리스트를 생성\n\n_lst = [_features[i][\"properties\"][\"name\"] for i in range(250)]\n\n\n_lst[:10]\n\n['종로구', '중구', '용산구', '성동구', '광진구', '동대문구', '중랑구', '성북구', '강북구', '도봉구']\n\n\n- index 함수 이용\n\n_lst.index(\"전주시덕진구\")\n\n166\n\n\n\n_lst.index(\"전주시완산구\")\n\n165\n\n\n\n_keywords = [\"전주시덕진구\",\"전주시완산구\"]\n_jj = [_lst.index(keyword) for keyword in _keywords]\n_jj\n\n[166, 165]\n\n\n- view vs copy\n\n딕셔너리의 경우 copy를 명시하지 않으면 두 개의 객체가 동시의 변경된다.\n따라서 딕셔너리를 이용할 경우 카피를 명시하고 사용하자\n\n\n\n\ndf = pd.DataFrame({'key':['전주시덕진구','전주시완산구'],'value':[20,30]})\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      key\n      value\n    \n  \n  \n    \n      0\n      전주시덕진구\n      20\n    \n    \n      1\n      전주시완산구\n      30\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nlocal_dict2 = local_dict.copy()\n\n\nlocal_dict2[\"features\"] = [local_dict[\"features\"][j] for j in _jj]\n\n\nm = folium.Map([35.84195368311022, 127.1155556693179],zoom_start=11,scrollWheelZoom=False)\nchoro = folium.Choropleth(\n    data=df, \n    geo_data=local_dict2,\n    columns=['key','value'],\n    key_on = 'feature.properties.name'\n)\nchoro.add_to(m) \nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n\n\n\n- folium.Choropleth의 사용법\n\n2개의 데이터를 연결해야함 : 하나는 일반 데이터 프레임, 하나는 json에서 나온 dict\n데이터 연결을 위해서는 공유가능한 연결의 매체가 필요하다.\n연결의 매개체는 df-key, local_dct2[”],local_dict2['features'][?]['properties']['name']을 이용했다.\n연결의 매개체를 name 이아닌 다른 것들도 이용할 수 있다.\n\n\nm = folium.Map([35.84195368311022, 127.1155556693179],zoom_start=11,scrollWheelZoom=False)\n\nchoro = folium.Choropleth(\n    data=df,  ## data1 \n    geo_data=local_dict2, ## data2 (이 시점에서 폴리곤에 대한 정보가 choro 인스턴스에 전달) \n    columns=['key','value'], ## data1에서 중요한것들을 나열. 항상 [key,value]의 조합으로 써야한다. \n        # 이때 ['key','value']는 [data2와의 매개체,지도에서 색의 단계를 표현하는 변수]로 해석가능\n    key_on = 'feature.properties.name' ## data2에서 중요한것: 즉 data1과의 연결매개체 \n)\nchoro.add_to(m)\nm\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv')\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      행정구역(시군구)별\n      총인구수 (명)\n    \n  \n  \n    \n      0\n      서울특별시\n      9532428\n    \n    \n      1\n      부산광역시\n      3356311\n    \n    \n      2\n      대구광역시\n      2390721\n    \n    \n      3\n      인천광역시\n      2945009\n    \n    \n      4\n      광주광역시\n      1442454\n    \n    \n      5\n      대전광역시\n      1454228\n    \n    \n      6\n      울산광역시\n      1122566\n    \n    \n      7\n      세종특별자치시\n      368276\n    \n    \n      8\n      경기도\n      13549577\n    \n    \n      9\n      강원도\n      1537717\n    \n    \n      10\n      충청북도\n      1596948\n    \n    \n      11\n      충청남도\n      2118977\n    \n    \n      12\n      전라북도\n      1789770\n    \n    \n      13\n      전라남도\n      1834653\n    \n    \n      14\n      경상북도\n      2627925\n    \n    \n      15\n      경상남도\n      3318161\n    \n    \n      16\n      제주특별자치도\n      676569\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nm = folium.Map([35.84195368311022, 127.1155556693179],zoom_start=7,scrollWheelZoom=False)\nchoro= folium.Choropleth(\n    data=df, \n    geo_data= global_dict, \n    columns=['행정구역(시군구)별','총인구수 (명)'], \n    key_on='feature.properties.name' ## geo_data에 연결매개체\n)\nchoro.add_to(m) \nm\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\n\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-muni.csv')\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      행정구역(시군구)별\n      총인구수 (명)\n    \n  \n  \n    \n      0\n      종로구\n      145346\n    \n    \n      1\n      중구\n      122781\n    \n    \n      2\n      용산구\n      223713\n    \n    \n      3\n      성동구\n      287174\n    \n    \n      4\n      광진구\n      340814\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      269\n      함양군\n      38475\n    \n    \n      270\n      거창군\n      61242\n    \n    \n      271\n      합천군\n      43029\n    \n    \n      272\n      제주시\n      493225\n    \n    \n      273\n      서귀포시\n      183344\n    \n  \n\n274 rows × 2 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nm = folium.Map([35.84195368311022, 127.1155556693179],zoom_start=7,scrollWheelZoom=False)\nchoro= folium.Choropleth(\n    data=df, \n    geo_data= local_dict, \n    columns=['행정구역(시군구)별','총인구수 (명)'], \n    key_on='feature.properties.name'\n)\nchoro.add_to(m) \nm\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\n\n\nimport plotly.express as px\nfrom IPython.display import HTML\n\ndf = px.data.election()\ngeojson = px.data.election_geojson()\n\nfig = px.choropleth_mapbox(df, ## 데이터 1 \n                           geojson=geojson, ## 데이터 2\n                           color=\"Bergeron\",\n                           locations=\"district\", featureidkey=\"properties.district\",\n                           center={\"lat\": 45.5517, \"lon\": -73.7073},\n                           mapbox_style=\"carto-positron\", zoom=9)\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nHTML(fig.to_html(include_plotlyjs='cdn',include_mathjax=False, config=dict({'scrollZoom':False})))\nfig.show()\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      district\n      Coderre\n      Bergeron\n      Joly\n      total\n      winner\n      result\n      district_id\n    \n  \n  \n    \n      0\n      101-Bois-de-Liesse\n      2481\n      1829\n      3024\n      7334\n      Joly\n      plurality\n      101\n    \n    \n      1\n      102-Cap-Saint-Jacques\n      2525\n      1163\n      2675\n      6363\n      Joly\n      plurality\n      102\n    \n    \n      2\n      11-Sault-au-Récollet\n      3348\n      2770\n      2532\n      8650\n      Coderre\n      plurality\n      11\n    \n    \n      3\n      111-Mile-End\n      1734\n      4782\n      2514\n      9030\n      Bergeron\n      majority\n      111\n    \n    \n      4\n      112-DeLorimier\n      1770\n      5933\n      3044\n      10747\n      Bergeron\n      majority\n      112\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndata2 즉 위치 정보 데이터는 properties의 district 혹은 id를 key로 한다.\n\n\ngeojson['features']\n\n[{'type': 'Feature',\n  'geometry': {'type': 'MultiPolygon',\n   'coordinates': [[[[-73.6363215300962, 45.5759177646435],\n      [-73.6362833815582, 45.5758266113331],\n      [-73.6446417578686, 45.5658132919643],\n      [-73.6453511352974, 45.5647725775888],\n      [-73.648867564748, 45.5586898267402],\n      [-73.6513170845065, 45.5545659435652],\n      [-73.6515658357324, 45.5554439857955],\n      [-73.6660837831645, 45.5596724837829],\n      [-73.6706609041685, 45.5610978251999],\n      [-73.6676019919116, 45.5632340862888],\n      [-73.6645385824068, 45.5642716484367],\n      [-73.663663123697, 45.5654269638586],\n      [-73.663336397858, 45.5666288247853],\n      [-73.6637764768649, 45.5678900619231],\n      [-73.6625073244826, 45.5688479494114],\n      [-73.6624620526633, 45.5708304456346],\n      [-73.6620201425015, 45.5713925326191],\n      [-73.6616100197742, 45.5737924780218],\n      [-73.6612199500215, 45.5747171555678],\n      [-73.6625087613399, 45.5748980132699],\n      [-73.6639172423219, 45.5730041908097],\n      [-73.6654358660443, 45.5729040009532],\n      [-73.6661069174428, 45.5737928224235],\n      [-73.6657870687343, 45.574385118162],\n      [-73.6636711124334, 45.577018676761],\n      [-73.6620928771361, 45.578161887933],\n      [-73.6611738698168, 45.5798517041392],\n      [-73.660404649744, 45.5806752214364],\n      [-73.659760079306, 45.5804007503503],\n      [-73.6604711314507, 45.5793767269523],\n      [-73.660479658497, 45.5787710701802],\n      [-73.6613549575147, 45.5780184952852],\n      [-73.6617871566128, 45.5758213640561],\n      [-73.6607445476436, 45.5764832852254],\n      [-73.6608243059909, 45.5774702934068],\n      [-73.6592236423267, 45.5793785624903],\n      [-73.6570262958283, 45.5810509513563],\n      [-73.6552694168748, 45.5819333817794],\n      [-73.6543058972308, 45.5836262624158],\n      [-73.652557313063, 45.5826892716542],\n      [-73.6363215300962, 45.5759177646435]]],\n    [[[-73.6561004885273, 45.5841347974261],\n      [-73.656376117147, 45.5845383929424],\n      [-73.6559939844354, 45.585868003125],\n      [-73.6552168328229, 45.5855392416017],\n      [-73.6561004885273, 45.5841347974261]]]]},\n  'properties': {'district': '11-Sault-au-Récollet'},\n  'id': '11'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.6217484540132, 45.5544783077209],\n     [-73.6235005117779, 45.5536358848324],\n     [-73.6278096771011, 45.5513024018691],\n     [-73.6301686544477, 45.5503989164938],\n     [-73.6325600663803, 45.5499795021129],\n     [-73.6342899238824, 45.5494628161417],\n     [-73.6363671046978, 45.548169415833],\n     [-73.6398673671325, 45.5441488267699],\n     [-73.6407295659042, 45.5429686686779],\n     [-73.6485543103777, 45.5308320827376],\n     [-73.6510270564502, 45.5263874543882],\n     [-73.6555330494037, 45.5277342274232],\n     [-73.6698852350516, 45.5318835785726],\n     [-73.6726124365382, 45.5322997327559],\n     [-73.6747555535799, 45.5323081056009],\n     [-73.6764129283619, 45.5321264506516],\n     [-73.683124268159, 45.5305991300384],\n     [-73.6807721370907, 45.534331320177],\n     [-73.6785046215606, 45.5379069417581],\n     [-73.6752852955733, 45.5369404343695],\n     [-73.6740837310465, 45.5378868187786],\n     [-73.6687347480061, 45.5363337408676],\n     [-73.667891862525, 45.5387409623539],\n     [-73.664945183105, 45.5382590909533],\n     [-73.6646235398403, 45.5394847807605],\n     [-73.662249203322, 45.543633408253],\n     [-73.6616100449503, 45.5441416421638],\n     [-73.6604302515405, 45.5460790656787],\n     [-73.6568659704249, 45.5450525873044],\n     [-73.6513170845065, 45.5545659435652],\n     [-73.648867564748, 45.5586898267402],\n     [-73.6453511352974, 45.5647725775888],\n     [-73.6240413737876, 45.5555253903511],\n     [-73.6217484540132, 45.5544783077209]]]},\n  'properties': {'district': '12-Saint-Sulpice'},\n  'id': '12'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.6513170845065, 45.5545659435652],\n     [-73.6568659704249, 45.5450525873044],\n     [-73.6604302515405, 45.5460790656787],\n     [-73.6616100449503, 45.5441416421638],\n     [-73.662249203322, 45.543633408253],\n     [-73.6646235398403, 45.5394847807605],\n     [-73.664945183105, 45.5382590909533],\n     [-73.667891862525, 45.5387409623539],\n     [-73.6687347480061, 45.5363337408676],\n     [-73.6740837310465, 45.5378868187786],\n     [-73.6752852955733, 45.5369404343695],\n     [-73.6785046215606, 45.5379069417581],\n     [-73.6807721370907, 45.534331320177],\n     [-73.6920199187781, 45.5394683031028],\n     [-73.702520410412, 45.5431766227664],\n     [-73.7042065154565, 45.5438906316964],\n     [-73.7043014929187, 45.5441905861876],\n     [-73.7027027602765, 45.5451406840713],\n     [-73.7023335988111, 45.5457621950639],\n     [-73.7012226911876, 45.5459702396396],\n     [-73.7004635154173, 45.5466167282154],\n     [-73.6996123824321, 45.5464852570484],\n     [-73.6992033294858, 45.5475781625301],\n     [-73.6968595536907, 45.5481581755357],\n     [-73.6949311086011, 45.5474604427764],\n     [-73.6924688517172, 45.5472280284581],\n     [-73.6891281157722, 45.5477334249253],\n     [-73.6879273316976, 45.5481318231022],\n     [-73.6865595013798, 45.5491308196528],\n     [-73.6852748086046, 45.5488162600605],\n     [-73.6827845213391, 45.5489146412483],\n     [-73.6801283681886, 45.549462773374],\n     [-73.6791182211476, 45.5500801562043],\n     [-73.6772981030339, 45.5516675115113],\n     [-73.6765545558035, 45.552766523393],\n     [-73.6744642269172, 45.5548591969672],\n     [-73.6730096784532, 45.5582339830541],\n     [-73.6706609041685, 45.5610978251999],\n     [-73.6660837831645, 45.5596724837829],\n     [-73.6515658357324, 45.5554439857955],\n     [-73.6513170845065, 45.5545659435652]]]},\n  'properties': {'district': '13-Ahuntsic'},\n  'id': '13'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.7043014929187, 45.5441905861876],\n     [-73.7042065154565, 45.5438906316964],\n     [-73.702520410412, 45.5431766227664],\n     [-73.6920199187781, 45.5394683031028],\n     [-73.6807721370907, 45.534331320177],\n     [-73.683124268159, 45.5305991300384],\n     [-73.685195685208, 45.5301022578388],\n     [-73.6870922504792, 45.5286619976995],\n     [-73.6897803429098, 45.5288476157777],\n     [-73.6939612003867, 45.528347826243],\n     [-73.6963296087078, 45.5276805484038],\n     [-73.7091218538247, 45.5230954877394],\n     [-73.7112612170074, 45.5243023856458],\n     [-73.7131122096208, 45.5251637887575],\n     [-73.7145584630039, 45.5239315629596],\n     [-73.7172924358234, 45.5256928911099],\n     [-73.7185119695016, 45.5247508136954],\n     [-73.7217031155859, 45.5267447688191],\n     [-73.7280602953464, 45.5213554616816],\n     [-73.7317546325275, 45.5236815365784],\n     [-73.7358987648866, 45.5207291683311],\n     [-73.7285097856203, 45.5160943522463],\n     [-73.7350991221023, 45.5137867482303],\n     [-73.7550630650544, 45.5065238038199],\n     [-73.7551669017747, 45.5064862077527],\n     [-73.7612752776246, 45.5104175090183],\n     [-73.7638683329059, 45.5120860463668],\n     [-73.762390301954, 45.5126791739444],\n     [-73.7600609683013, 45.5141288858482],\n     [-73.7568442082942, 45.5171884494623],\n     [-73.7548593171474, 45.5180889539846],\n     [-73.752606980088, 45.5185523798859],\n     [-73.749151955166, 45.519018390863],\n     [-73.7452110015187, 45.5215286261207],\n     [-73.7437662661847, 45.5231373686573],\n     [-73.7411211602605, 45.526837900694],\n     [-73.7394306858002, 45.5285277381045],\n     [-73.7360589393138, 45.5302671129156],\n     [-73.7343347000828, 45.5309824138834],\n     [-73.7311072638005, 45.5320095049916],\n     [-73.7283844986554, 45.533203827821],\n     [-73.7249729419404, 45.5359178137401],\n     [-73.7231098233772, 45.5369689627545],\n     [-73.7202103870331, 45.5381794076214],\n     [-73.7181375140605, 45.5387109207903],\n     [-73.7158043157039, 45.5388996570908],\n     [-73.7135388429561, 45.5396848689224],\n     [-73.7111454931799, 45.541340948266],\n     [-73.7091038651233, 45.5430898357363],\n     [-73.7078456046825, 45.5423008892929],\n     [-73.7069875096913, 45.5436077713277],\n     [-73.705614345008, 45.5441267025504],\n     [-73.7043014929187, 45.5441905861876]]]},\n  'properties': {'district': '14-Bordeaux-Cartierville'},\n  'id': '14'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5576871015794, 45.5932166140624],\n     [-73.5694206600629, 45.5971486400825],\n     [-73.5696609688417, 45.5965806032278],\n     [-73.5726733802096, 45.5977479911912],\n     [-73.5745938220503, 45.5986717630315],\n     [-73.576232424293, 45.5992577290381],\n     [-73.5771288919308, 45.599852413767],\n     [-73.5889961999073, 45.6050220349527],\n     [-73.5971103983983, 45.6085989887758],\n     [-73.6013378366857, 45.6104025414741],\n     [-73.6028907661658, 45.6112216102358],\n     [-73.601559767966, 45.6118073578371],\n     [-73.6023660828945, 45.6125718329314],\n     [-73.5984976040034, 45.6140840514745],\n     [-73.5954830428467, 45.6158123480652],\n     [-73.5740068269183, 45.6304929149516],\n     [-73.5683379250343, 45.6341956640577],\n     [-73.5514095863784, 45.6269444197056],\n     [-73.5481773494253, 45.6255197854529],\n     [-73.5585223912755, 45.6137280036474],\n     [-73.5640615699407, 45.6074316710882],\n     [-73.564986439275, 45.60561620283],\n     [-73.5662995431109, 45.6039974029119],\n     [-73.5631517029922, 45.6031634368599],\n     [-73.5614050931716, 45.6025489377902],\n     [-73.5567925816714, 45.6005393547562],\n     [-73.5589887922886, 45.5979945383903],\n     [-73.5595164040348, 45.5961661000645],\n     [-73.5564557704867, 45.5951543577536],\n     [-73.5576871015794, 45.5932166140624]]]},\n  'properties': {'district': '21-Ouest'},\n  'id': '21'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5452799498143, 45.5959627917351],\n     [-73.5491007054645, 45.5975302159963],\n     [-73.5479841638628, 45.5989434662401],\n     [-73.5468145864499, 45.6013644519429],\n     [-73.5461968110206, 45.6046211322422],\n     [-73.549913736884, 45.605150517886],\n     [-73.5533688744189, 45.605412630486],\n     [-73.5524302953106, 45.6080081462088],\n     [-73.5507759763854, 45.6103339985918],\n     [-73.5585223912755, 45.6137280036474],\n     [-73.5481773494253, 45.6255197854529],\n     [-73.5358870172169, 45.620207093019],\n     [-73.5374747721378, 45.6166252950092],\n     [-73.5375256945614, 45.6160673327401],\n     [-73.5381703589619, 45.6139517855176],\n     [-73.5437218072693, 45.6149096422408],\n     [-73.5452777806501, 45.6141657083913],\n     [-73.5462767505068, 45.6127238289344],\n     [-73.5444977198785, 45.6121760838565],\n     [-73.5453373826653, 45.6107633861198],\n     [-73.5441363974969, 45.6102716461918],\n     [-73.5412098839782, 45.6095033241271],\n     [-73.5416434070878, 45.6081797015109],\n     [-73.540900931842, 45.6078533012629],\n     [-73.5416213388458, 45.606939681157],\n     [-73.5375401492111, 45.6052359766051],\n     [-73.5365038947396, 45.6026457300995],\n     [-73.5365958888719, 45.602162446611],\n     [-73.5394321608651, 45.6030910243993],\n     [-73.5421763850909, 45.6042454438226],\n     [-73.5429289535666, 45.6032316351511],\n     [-73.539729971205, 45.6018186221009],\n     [-73.5407418924341, 45.6006998521118],\n     [-73.5447576674512, 45.6019410417126],\n     [-73.5451108447268, 45.6000061818095],\n     [-73.5445859643042, 45.5999308464328],\n     [-73.5442050076359, 45.5981948225546],\n     [-73.5438572127564, 45.5980784963673],\n     [-73.5452799498143, 45.5959627917351]]]},\n  'properties': {'district': '22-Est'},\n  'id': '22'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5576871015794, 45.5932166140624],\n     [-73.5564557704867, 45.5951543577536],\n     [-73.5595164040348, 45.5961661000645],\n     [-73.5589887922886, 45.5979945383903],\n     [-73.5567925816714, 45.6005393547562],\n     [-73.5614050931716, 45.6025489377902],\n     [-73.5631517029922, 45.6031634368599],\n     [-73.5662995431109, 45.6039974029119],\n     [-73.564986439275, 45.60561620283],\n     [-73.5640615699407, 45.6074316710882],\n     [-73.5585223912755, 45.6137280036474],\n     [-73.5507759763854, 45.6103339985918],\n     [-73.5524302953106, 45.6080081462088],\n     [-73.5533688744189, 45.605412630486],\n     [-73.549913736884, 45.605150517886],\n     [-73.5461968110206, 45.6046211322422],\n     [-73.5468145864499, 45.6013644519429],\n     [-73.5479841638628, 45.5989434662401],\n     [-73.5491007054645, 45.5975302159963],\n     [-73.5452799498143, 45.5959627917351],\n     [-73.5455045323429, 45.5956272275911],\n     [-73.545930102715, 45.5949937733284],\n     [-73.5473484500099, 45.5955355370728],\n     [-73.5481396292151, 45.5946360429193],\n     [-73.5465394629102, 45.5940712033384],\n     [-73.5489394765731, 45.5903029802044],\n     [-73.5576871015794, 45.5932166140624]]]},\n  'properties': {'district': '23-Centre'},\n  'id': '23'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.6207592308166, 45.5138993727882],\n     [-73.6236560898078, 45.5106541632808],\n     [-73.624384898555, 45.5091582170891],\n     [-73.6260875230251, 45.507277208983],\n     [-73.6279506225702, 45.5061120243158],\n     [-73.628749836468, 45.5048079525531],\n     [-73.6304494685866, 45.5028490467289],\n     [-73.6256440443835, 45.500662130816],\n     [-73.6331323407225, 45.49234917306],\n     [-73.6480439724158, 45.4990451424615],\n     [-73.6458072174417, 45.5016289991394],\n     [-73.6422399591459, 45.5054987858353],\n     [-73.6423036889254, 45.5062221151428],\n     [-73.6415508188144, 45.5069829054595],\n     [-73.6416660769228, 45.5076101030023],\n     [-73.6385805807369, 45.5112159439382],\n     [-73.6365911851412, 45.5146566965285],\n     [-73.6357295158589, 45.5143963845137],\n     [-73.6346269691423, 45.5156528191911],\n     [-73.6289040380069, 45.5140823858644],\n     [-73.6267223236567, 45.5165801475772],\n     [-73.6207592308166, 45.5138993727882]]]},\n  'properties': {'district': '31-Darlington'},\n  'id': '31'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5956108742759, 45.504064067218],\n     [-73.5941401789655, 45.5033436344744],\n     [-73.594737745289, 45.5028667176493],\n     [-73.5977240363362, 45.5016310280474],\n     [-73.5997960499359, 45.5001439911523],\n     [-73.6009279740317, 45.4988800346583],\n     [-73.6033750499038, 45.4967309718248],\n     [-73.6044775189475, 45.4952226776326],\n     [-73.6068399722065, 45.4952769144878],\n     [-73.6083680626652, 45.493592693099],\n     [-73.6093606035265, 45.4939788081012],\n     [-73.6115925457235, 45.4916920833656],\n     [-73.6147351668377, 45.4930299583055],\n     [-73.6184160919695, 45.4889467204633],\n     [-73.618423454854, 45.4881800607925],\n     [-73.6153515138627, 45.4867956883177],\n     [-73.6169038962737, 45.4851011880198],\n     [-73.6331323407225, 45.49234917306],\n     [-73.6256440443835, 45.500662130816],\n     [-73.6304494685866, 45.5028490467289],\n     [-73.628749836468, 45.5048079525531],\n     [-73.6279506225702, 45.5061120243158],\n     [-73.6260875230251, 45.507277208983],\n     [-73.624384898555, 45.5091582170891],\n     [-73.6236560898078, 45.5106541632808],\n     [-73.6207592308166, 45.5138993727882],\n     [-73.6176645335398, 45.5125000231349],\n     [-73.617959519301, 45.5121686212918],\n     [-73.61888001694, 45.5111346532789],\n     [-73.6169256431203, 45.5102737338336],\n     [-73.6178928803835, 45.5091750557282],\n     [-73.6160933747766, 45.5082725969653],\n     [-73.6189454361275, 45.5051234940594],\n     [-73.6182356279483, 45.504722156916],\n     [-73.6130513861854, 45.5104478685544],\n     [-73.6063535452531, 45.5074911306599],\n     [-73.6061344132343, 45.5068772235294],\n     [-73.6035337540631, 45.5057001623398],\n     [-73.6024762927983, 45.5068574351344],\n     [-73.5993805679324, 45.5053898653966],\n     [-73.5987846893921, 45.5056180649425],\n     [-73.5956108742759, 45.504064067218]]]},\n  'properties': {'district': '32-Côte-des-Neiges'},\n  'id': '32'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.6458072174417, 45.5016289991394],\n     [-73.6480439724158, 45.4990451424615],\n     [-73.6331323407225, 45.49234917306],\n     [-73.6169038962737, 45.4851011880198],\n     [-73.6171481421592, 45.4848346166979],\n     [-73.6143642141962, 45.4835868912093],\n     [-73.6159143530659, 45.4828328944331],\n     [-73.6178740199084, 45.4828487105026],\n     [-73.6188871149392, 45.4826094451865],\n     [-73.6212759199926, 45.4802684345737],\n     [-73.6225662166532, 45.4809098124242],\n     [-73.6236079242847, 45.479859834089],\n     [-73.6247644644914, 45.4792722536745],\n     [-73.6274376104247, 45.4787022686984],\n     [-73.6296262215988, 45.478422071886],\n     [-73.6309839979771, 45.4790336481084],\n     [-73.6303804074798, 45.4796761714723],\n     [-73.6511475576679, 45.4890232837666],\n     [-73.6486185977978, 45.4918874029923],\n     [-73.6505027283703, 45.4926843456098],\n     [-73.6558244313911, 45.4867907549359],\n     [-73.6554964724779, 45.4865593791057],\n     [-73.6559518311015, 45.4851214751334],\n     [-73.6560231568666, 45.4789400952457],\n     [-73.6611067166692, 45.4811529055447],\n     [-73.6618618884249, 45.4803203895973],\n     [-73.6695855765435, 45.483802365291],\n     [-73.6746342471328, 45.4819683440264],\n     [-73.6763145414839, 45.4831298265322],\n     [-73.6776271543493, 45.4823345919293],\n     [-73.677264286648, 45.4836591822082],\n     [-73.6751680903633, 45.4912833590877],\n     [-73.6678730495106, 45.4869534898132],\n     [-73.6660964808822, 45.4866186123515],\n     [-73.665349200462, 45.4870183659149],\n     [-73.6562276688947, 45.49432722353],\n     [-73.6637510731136, 45.4992450684359],\n     [-73.6617471598017, 45.4992191258018],\n     [-73.6623623016746, 45.5022066512214],\n     [-73.6605369493135, 45.5024306141327],\n     [-73.6600841825714, 45.50331916613],\n     [-73.659999506549, 45.5047080114556],\n     [-73.6566453499119, 45.5037588241059],\n     [-73.6516126661051, 45.504311872331],\n     [-73.6458072174417, 45.5016289991394]]]},\n  'properties': {'district': '33-Snowdon'},\n  'id': '33'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.6143642141962, 45.4835868912093],\n     [-73.6123997707291, 45.4826898974257],\n     [-73.6143764334436, 45.4805885224868],\n     [-73.6054346259738, 45.4765925587108],\n     [-73.605024536716, 45.4770080719998],\n     [-73.5985471562789, 45.4740708829885],\n     [-73.5951347718664, 45.4764466331536],\n     [-73.5966486659517, 45.4732210790839],\n     [-73.5995923966142, 45.4714885118876],\n     [-73.6015380888098, 45.4698449656495],\n     [-73.6025897276388, 45.4690525247808],\n     [-73.6031619796725, 45.4691032161751],\n     [-73.6056266168094, 45.4676031109374],\n     [-73.6058459619009, 45.4672881102352],\n     [-73.6116004847967, 45.4647691120724],\n     [-73.6121904900053, 45.4648542087469],\n     [-73.6157015928197, 45.4632907897894],\n     [-73.6167538282606, 45.4637150194085],\n     [-73.6260868871444, 45.4678594502274],\n     [-73.626514031426, 45.4682540051731],\n     [-73.6317531402478, 45.4705324186308],\n     [-73.6328791824832, 45.4692477650227],\n     [-73.6375349663401, 45.4712784001916],\n     [-73.636638362928, 45.4717833758261],\n     [-73.6429334550997, 45.4745543212637],\n     [-73.6397624769414, 45.4766952953752],\n     [-73.6393702983563, 45.4768105158646],\n     [-73.6354876412699, 45.4768957315274],\n     [-73.6324931664858, 45.477871983287],\n     [-73.6296262215988, 45.478422071886],\n     [-73.6274376104247, 45.4787022686984],\n     [-73.6247644644914, 45.4792722536745],\n     [-73.6236079242847, 45.479859834089],\n     [-73.6225662166532, 45.4809098124242],\n     [-73.6212759199926, 45.4802684345737],\n     [-73.6188871149392, 45.4826094451865],\n     [-73.6178740199084, 45.4828487105026],\n     [-73.6159143530659, 45.4828328944331],\n     [-73.6143642141962, 45.4835868912093]]]},\n  'properties': {'district': '34-Notre-Dame-de-Grâce'},\n  'id': '34'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.6157015928197, 45.4632907897894],\n     [-73.6182352042185, 45.4622268121902],\n     [-73.6200562870233, 45.4616683026575],\n     [-73.6243236842049, 45.4586553508057],\n     [-73.6262003043123, 45.4574918787775],\n     [-73.6314544395632, 45.4527593013115],\n     [-73.6350181512615, 45.4509158064615],\n     [-73.6351722497066, 45.4507225431116],\n     [-73.6579831562151, 45.4595353374805],\n     [-73.6571683135528, 45.4601081011059],\n     [-73.6581848068946, 45.4614565130998],\n     [-73.6556103303451, 45.4629114310011],\n     [-73.652126146378, 45.4676895103383],\n     [-73.650271326417, 45.4688055451965],\n     [-73.6491060800072, 45.4705082201931],\n     [-73.6456903840605, 45.4732294857318],\n     [-73.6429334550997, 45.4745543212637],\n     [-73.636638362928, 45.4717833758261],\n     [-73.6375349663401, 45.4712784001916],\n     [-73.6328791824832, 45.4692477650227],\n     [-73.6317531402478, 45.4705324186308],\n     [-73.626514031426, 45.4682540051731],\n     [-73.6260868871444, 45.4678594502274],\n     [-73.6167538282606, 45.4637150194085],\n     [-73.6157015928197, 45.4632907897894]]]},\n  'properties': {'district': '35-Loyola'},\n  'id': '35'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.6827804258142, 45.4635658825978],\n     [-73.680902143784, 45.4623754221733],\n     [-73.6819797751037, 45.4611465117243],\n     [-73.6828103840958, 45.4609562889736],\n     [-73.6837901612144, 45.4594789107516],\n     [-73.6850645006026, 45.4581450364168],\n     [-73.6872613190858, 45.45743059602],\n     [-73.6874478222607, 45.4558195821286],\n     [-73.6872205285223, 45.4552707229765],\n     [-73.6763900162006, 45.4523637929963],\n     [-73.67377108286, 45.4530336717393],\n     [-73.6720573706002, 45.4528472783153],\n     [-73.6708095323057, 45.454567942777],\n     [-73.6685657847746, 45.4567309452231],\n     [-73.6569820780073, 45.4523202088578],\n     [-73.6448369147901, 45.4475941198865],\n     [-73.6433550395347, 45.4477889354918],\n     [-73.6432919538684, 45.4483609262331],\n     [-73.6389029083366, 45.4466926413334],\n     [-73.6367835977829, 45.447838031415],\n     [-73.6320301276656, 45.4495010416609],\n     [-73.6290926270452, 45.4483946176344],\n     [-73.6320762130349, 45.4466558317026],\n     [-73.6351699979365, 45.445393422083],\n     [-73.6392793914912, 45.4438396460105],\n     [-73.6470945537851, 45.4407073290916],\n     [-73.648051977364, 45.4402561224796],\n     [-73.6537698617598, 45.4370900482024],\n     [-73.6593235959903, 45.4348597630844],\n     [-73.6621186235209, 45.433443013701],\n     [-73.6659361085031, 45.4320787105657],\n     [-73.665989308902, 45.4284342302665],\n     [-73.6689564175307, 45.4291767414944],\n     [-73.6731193300976, 45.4280636694712],\n     [-73.6759423999609, 45.4276813808521],\n     [-73.6833563260685, 45.4282185950861],\n     [-73.6865003978824, 45.4283124979178],\n     [-73.6878687931234, 45.4286162358336],\n     [-73.6882161207604, 45.4292815314123],\n     [-73.6856210014114, 45.4290337944536],\n     [-73.6824907873074, 45.4293268197374],\n     [-73.6802412950549, 45.4292313975545],\n     [-73.675637946869, 45.4284197115606],\n     [-73.6730699666729, 45.4286396172162],\n     [-73.6691630663163, 45.429896298712],\n     [-73.6694459537283, 45.4304807516148],\n     [-73.6730208497195, 45.4292965595349],\n     [-73.6750647146261, 45.4289965544438],\n     [-73.6767010508474, 45.4291739328909],\n     [-73.6788386194388, 45.4302144484015],\n     [-73.6788136864397, 45.4304304732339],\n     [-73.6733659453582, 45.4293230346261],\n     [-73.6691401095123, 45.4307511873639],\n     [-73.6696391042462, 45.4309484342688],\n     [-73.6714905342855, 45.4303967061942],\n     [-73.6716716548405, 45.4310983881409],\n     [-73.6737798875814, 45.4309332101671],\n     [-73.6775256263951, 45.431305335836],\n     [-73.6792911320404, 45.4318695414462],\n     [-73.6820917599341, 45.4323959865103],\n     [-73.6847244511365, 45.4323557615008],\n     [-73.6843819981847, 45.4331481730083],\n     [-73.6862880355843, 45.433675955614],\n     [-73.6893786382238, 45.43297800821],\n     [-73.6919766084407, 45.4328477403871],\n     [-73.6920222789549, 45.4344181753399],\n     [-73.6924064617963, 45.4372834135916],\n     [-73.6919192684033, 45.4375875837079],\n     [-73.6922359492324, 45.4397539275822],\n     [-73.6790976648257, 45.4399597709378],\n     [-73.6718763889968, 45.4401679518281],\n     [-73.6702870397047, 45.440019142569],\n     [-73.6635037895282, 45.4400929494508],\n     [-73.6629212228917, 45.4401596573971],\n     [-73.6629008949467, 45.4422188385912],\n     [-73.6635315763833, 45.4422179374171],\n     [-73.6636442247504, 45.4445171206371],\n     [-73.6650313754737, 45.4449769741284],\n     [-73.6688238611265, 45.4465533440431],\n     [-73.6703899048248, 45.4470018886664],\n     [-73.6723760891366, 45.4472832888363],\n     [-73.6862003751106, 45.4475701793018],\n     [-73.6929749468618, 45.4479566183894],\n     [-73.6931235366321, 45.44814625489],\n     [-73.6960704968967, 45.4482114291558],\n     [-73.6970275555854, 45.4488217409105],\n     [-73.6970986158494, 45.4534198522438],\n     [-73.6970604739367, 45.4567907245283],\n     [-73.6967667127171, 45.4587187165095],\n     [-73.6956922188653, 45.4626538256577],\n     [-73.6956163827684, 45.4640262681568],\n     [-73.69619836572, 45.4654974169452],\n     [-73.6947785817616, 45.4645178045181],\n     [-73.6933043566279, 45.4703396575946],\n     [-73.6827804258142, 45.4635658825978]]]},\n  'properties': {'district': '41-du Canal'},\n  'id': '41'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.6922359492324, 45.4397539275822],\n     [-73.6930646838521, 45.4455152024534],\n     [-73.6929749468618, 45.4479566183894],\n     [-73.6862003751106, 45.4475701793018],\n     [-73.6723760891366, 45.4472832888363],\n     [-73.6703899048248, 45.4470018886664],\n     [-73.6688238611265, 45.4465533440431],\n     [-73.6650313754737, 45.4449769741284],\n     [-73.6636442247504, 45.4445171206371],\n     [-73.6635315763833, 45.4422179374171],\n     [-73.6629008949467, 45.4422188385912],\n     [-73.6629212228917, 45.4401596573971],\n     [-73.6635037895282, 45.4400929494508],\n     [-73.6702870397047, 45.440019142569],\n     [-73.6718763889968, 45.4401679518281],\n     [-73.6790976648257, 45.4399597709378],\n     [-73.6922359492324, 45.4397539275822]]]},\n  'properties': {'district': '42-J.-Émery-Provost'},\n  'id': '42'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.6929749468618, 45.4479566183894],\n     [-73.6930646838521, 45.4455152024534],\n     [-73.6922359492324, 45.4397539275822],\n     [-73.6919192684033, 45.4375875837079],\n     [-73.6924064617963, 45.4372834135916],\n     [-73.6920222789549, 45.4344181753399],\n     [-73.6919766084407, 45.4328477403871],\n     [-73.6932876958108, 45.4324224970616],\n     [-73.694195347338, 45.4324839527568],\n     [-73.6949136010573, 45.4332296803625],\n     [-73.6988288457718, 45.4344466726102],\n     [-73.7010791173413, 45.434694716853],\n     [-73.7030235835285, 45.4351501569008],\n     [-73.7052464981121, 45.4348762919984],\n     [-73.7084847193563, 45.4361211648559],\n     [-73.7108619082078, 45.4360808035114],\n     [-73.7131770697603, 45.436535476583],\n     [-73.7141004237697, 45.4373526055584],\n     [-73.7197027055463, 45.4383497204873],\n     [-73.7205977591364, 45.438420079974],\n     [-73.7206259030039, 45.439094891288],\n     [-73.7192721714824, 45.4393943883616],\n     [-73.7222362640027, 45.4487922160595],\n     [-73.7206936272739, 45.4487451257208],\n     [-73.722831053433, 45.4557427179281],\n     [-73.7245717150603, 45.4610335488204],\n     [-73.7198081628308, 45.4644866016957],\n     [-73.7170519619499, 45.4666015017259],\n     [-73.7125173690129, 45.4699350201585],\n     [-73.7080097103994, 45.47334246671],\n     [-73.7047665432935, 45.4712980390786],\n     [-73.69619836572, 45.4654974169452],\n     [-73.6956163827684, 45.4640262681568],\n     [-73.6956922188653, 45.4626538256577],\n     [-73.6967667127171, 45.4587187165095],\n     [-73.6970604739367, 45.4567907245283],\n     [-73.6970986158494, 45.4534198522438],\n     [-73.6970275555854, 45.4488217409105],\n     [-73.6960704968967, 45.4482114291558],\n     [-73.6931235366321, 45.44814625489],\n     [-73.6929749468618, 45.4479566183894]]]},\n  'properties': {'district': '43-Fort-Rolland'},\n  'id': '43'},\n {'type': 'Feature',\n  'geometry': {'type': 'MultiPolygon',\n   'coordinates': [[[[-73.5878943026224, 45.4214667320274],\n      [-73.5863736733671, 45.4214319841574],\n      [-73.5869474028608, 45.4206395990966],\n      [-73.5878943026224, 45.4214667320274]]],\n    [[[-73.5725281473542, 45.4260939579768],\n      [-73.5704318906483, 45.4257263831213],\n      [-73.5705580062435, 45.4244124795508],\n      [-73.5739539624556, 45.4219626672329],\n      [-73.5766876209735, 45.4212859978906],\n      [-73.5793575905544, 45.4208612013169],\n      [-73.5824241611882, 45.4207510361274],\n      [-73.5840593827098, 45.4204618626093],\n      [-73.585056506724, 45.4207940136185],\n      [-73.586554348269, 45.4226195923359],\n      [-73.5860576024637, 45.4236638110559],\n      [-73.584295514885, 45.4245289791408],\n      [-73.5827488367791, 45.424251218775],\n      [-73.5809343733788, 45.424360511388],\n      [-73.5784175573386, 45.4248212373305],\n      [-73.5770508918231, 45.425290048099],\n      [-73.5731288926102, 45.4262015674713],\n      [-73.5725281473542, 45.4260939579768]]],\n    [[[-73.5765170514597, 45.4274230889043],\n      [-73.5749707241727, 45.4273611190412],\n      [-73.5723756663881, 45.4267869197677],\n      [-73.5752500087582, 45.4259752044009],\n      [-73.5797854469441, 45.4251621784139],\n      [-73.5803981192377, 45.4246848695548],\n      [-73.5816888803182, 45.4247468599426],\n      [-73.5837470045383, 45.4252493371248],\n      [-73.5811925680258, 45.4261690319939],\n      [-73.5773858946539, 45.42731451108],\n      [-73.5765170514597, 45.4274230889043]]],\n    [[[-73.5993656298056, 45.437522488597],\n      [-73.5837871545249, 45.4350036826887],\n      [-73.5864937818087, 45.4330669729378],\n      [-73.5889455796285, 45.4317602654679],\n      [-73.5892899771004, 45.4312921174991],\n      [-73.5917282376663, 45.4295893857394],\n      [-73.5930673492969, 45.4278875931486],\n      [-73.5941659028554, 45.4276167102135],\n      [-73.5970103055467, 45.4244378040521],\n      [-73.6024582557665, 45.4197537949939],\n      [-73.6050240641787, 45.418374641288],\n      [-73.6071441080738, 45.4178597046428],\n      [-73.6089187571527, 45.4171291621745],\n      [-73.6111012955212, 45.4159032271],\n      [-73.6144997511617, 45.4158098763183],\n      [-73.6157904203163, 45.4159075684946],\n      [-73.6190093657475, 45.4154992857685],\n      [-73.6211671693205, 45.4148130602082],\n      [-73.6256397226365, 45.4151411168221],\n      [-73.6290762426954, 45.4149482971955],\n      [-73.6295864341622, 45.4145878316083],\n      [-73.6326788478802, 45.4147641808428],\n      [-73.6324382282295, 45.4157003113135],\n      [-73.6338310649383, 45.4157616841636],\n      [-73.6373466145775, 45.4154299316521],\n      [-73.6378211847585, 45.4157853047214],\n      [-73.6349821404871, 45.4216213986168],\n      [-73.630914321986, 45.4207058504903],\n      [-73.629580513036, 45.4205264796284],\n      [-73.6269794842437, 45.4219064890514],\n      [-73.6221765288238, 45.4233556809216],\n      [-73.6235576102796, 45.4251452650735],\n      [-73.625860660556, 45.4262630893683],\n      [-73.6213932515199, 45.4290799524068],\n      [-73.6189096270924, 45.431198716026],\n      [-73.6153394945845, 45.4349173246406],\n      [-73.6147722148194, 45.4363801178349],\n      [-73.6185038157741, 45.4371719975162],\n      [-73.6203669519898, 45.4381561591494],\n      [-73.6292793360419, 45.4422437520764],\n      [-73.6304212122284, 45.4412800353944],\n      [-73.6319594346838, 45.4420660970196],\n      [-73.6341339482992, 45.4434436213567],\n      [-73.6334152486196, 45.4438343169323],\n      [-73.6348120456809, 45.444546590121],\n      [-73.6351699979365, 45.445393422083],\n      [-73.6320762130349, 45.4466558317026],\n      [-73.6290926270452, 45.4483946176344],\n      [-73.6288149375869, 45.4482900651074],\n      [-73.6269986444794, 45.4501430927141],\n      [-73.6255515041789, 45.4513172221793],\n      [-73.622324477466, 45.4535901866228],\n      [-73.6201147014597, 45.4548257159799],\n      [-73.6136079881888, 45.4576954341346],\n      [-73.6068239912076, 45.4545470690524],\n      [-73.6048092799479, 45.4489130784869],\n      [-73.6052123848584, 45.4479821667775],\n      [-73.6094089858961, 45.4442773154325],\n      [-73.6104503124607, 45.4429793199455],\n      [-73.6118008558715, 45.4394499728596],\n      [-73.5996082895714, 45.4375187309259],\n      [-73.5993656298056, 45.437522488597]]]]},\n  'properties': {'district': '51-Sault-Saint-Louis'},\n  'id': '51'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.6373466145775, 45.4154299316521],\n     [-73.6398482479311, 45.4154574940498],\n     [-73.6413054445293, 45.4157165943723],\n     [-73.6441055865314, 45.4165049565242],\n     [-73.6469598857207, 45.418436018764],\n     [-73.6496982979192, 45.4199531866296],\n     [-73.6528057037257, 45.4208669233735],\n     [-73.6563386875677, 45.4231117299686],\n     [-73.6574132128538, 45.4235151379827],\n     [-73.6590899773487, 45.4245026614551],\n     [-73.6597563681528, 45.4251856881017],\n     [-73.6622132461692, 45.4263249714976],\n     [-73.6626617007921, 45.4267473070838],\n     [-73.6639788872218, 45.4270423138518],\n     [-73.665989308902, 45.4284342302665],\n     [-73.6659361085031, 45.4320787105657],\n     [-73.6621186235209, 45.433443013701],\n     [-73.6593235959903, 45.4348597630844],\n     [-73.6537698617598, 45.4370900482024],\n     [-73.648051977364, 45.4402561224796],\n     [-73.6470945537851, 45.4407073290916],\n     [-73.6392793914912, 45.4438396460105],\n     [-73.6351699979365, 45.445393422083],\n     [-73.6348120456809, 45.444546590121],\n     [-73.6334152486196, 45.4438343169323],\n     [-73.6341339482992, 45.4434436213567],\n     [-73.6319594346838, 45.4420660970196],\n     [-73.6304212122284, 45.4412800353944],\n     [-73.6292793360419, 45.4422437520764],\n     [-73.6203669519898, 45.4381561591494],\n     [-73.6185038157741, 45.4371719975162],\n     [-73.6147722148194, 45.4363801178349],\n     [-73.6153394945845, 45.4349173246406],\n     [-73.6189096270924, 45.431198716026],\n     [-73.6213932515199, 45.4290799524068],\n     [-73.625860660556, 45.4262630893683],\n     [-73.6235576102796, 45.4251452650735],\n     [-73.6221765288238, 45.4233556809216],\n     [-73.6269794842437, 45.4219064890514],\n     [-73.629580513036, 45.4205264796284],\n     [-73.630914321986, 45.4207058504903],\n     [-73.6349821404871, 45.4216213986168],\n     [-73.6378211847585, 45.4157853047214],\n     [-73.6373466145775, 45.4154299316521]]]},\n  'properties': {'district': '52-Cecil-P.-Newman'},\n  'id': '52'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.8688974470136, 45.4882449524107],\n     [-73.871602833053, 45.4905720481716],\n     [-73.8694139090223, 45.4919469541754],\n     [-73.870532930773, 45.4928625205596],\n     [-73.8716913646707, 45.4933306815312],\n     [-73.872233047857, 45.4949468141821],\n     [-73.8714042058561, 45.4965567000138],\n     [-73.872571305249, 45.4969659674767],\n     [-73.871134091344, 45.4990564163597],\n     [-73.8701130707197, 45.5000549627491],\n     [-73.8690288298613, 45.5007201896345],\n     [-73.8722903935919, 45.503568954282],\n     [-73.8716174921312, 45.5042888396132],\n     [-73.8756305607743, 45.5078228770541],\n     [-73.8783878189261, 45.5059406633388],\n     [-73.8803094438111, 45.5042458334118],\n     [-73.8803341428118, 45.5033993667444],\n     [-73.879759682167, 45.5029518530572],\n     [-73.8770152857708, 45.501806378212],\n     [-73.8789156358527, 45.5017299402712],\n     [-73.879779371889, 45.5004798588748],\n     [-73.8810568286333, 45.4999384356033],\n     [-73.8830127770248, 45.4996741535333],\n     [-73.884412047715, 45.4990183531015],\n     [-73.8853262749786, 45.4998100023704],\n     [-73.8840175555813, 45.5020031308338],\n     [-73.8845885513537, 45.503100834689],\n     [-73.8882584256138, 45.5047130635017],\n     [-73.8902948397667, 45.5048242389134],\n     [-73.8931298332185, 45.5045492632818],\n     [-73.8938090092703, 45.5051466368411],\n     [-73.8930883375795, 45.5061095241892],\n     [-73.8912989165002, 45.5060276552235],\n     [-73.8907763512179, 45.5069642008183],\n     [-73.8894862335895, 45.5075924382318],\n     [-73.8893630543063, 45.5086347066791],\n     [-73.8880808924888, 45.5083547817147],\n     [-73.8870687665439, 45.5091995603904],\n     [-73.8852521620131, 45.5102059725257],\n     [-73.8881290541888, 45.5127359235277],\n     [-73.8890287067442, 45.5122302528857],\n     [-73.8911436774502, 45.5127840458479],\n     [-73.8937551506541, 45.5125006850816],\n     [-73.8953141801776, 45.5130744053201],\n     [-73.896331119406, 45.5139759395892],\n     [-73.8972364759115, 45.5131483697101],\n     [-73.8989649328704, 45.5146837440746],\n     [-73.9002254469086, 45.5131179581327],\n     [-73.9008379207362, 45.5136630574361],\n     [-73.9005919569215, 45.5144536068957],\n     [-73.9021261241608, 45.5155955053506],\n     [-73.9009914493894, 45.5161347116661],\n     [-73.9020712706953, 45.5167877731109],\n     [-73.9020876361723, 45.5172918560237],\n     [-73.8999623185899, 45.5172091442281],\n     [-73.8983416967041, 45.5160989860003],\n     [-73.8970501718221, 45.5162744685233],\n     [-73.896506711813, 45.5172571885447],\n     [-73.8931760247634, 45.5179232635692],\n     [-73.8918830801879, 45.5186858504077],\n     [-73.89060443538, 45.5188882146005],\n     [-73.8899347125325, 45.5182876407975],\n     [-73.8875283162043, 45.5182958298662],\n     [-73.8871271342177, 45.5195300009394],\n     [-73.8860195052025, 45.5203975900766],\n     [-73.8849337116296, 45.5207252282394],\n     [-73.8820294235971, 45.5209418712909],\n     [-73.8801807487187, 45.5201561927542],\n     [-73.8790398509879, 45.5199080044884],\n     [-73.8757241818541, 45.5198469639429],\n     [-73.8712540724591, 45.51942970446],\n     [-73.8693729830596, 45.5195167954401],\n     [-73.867959836772, 45.5187475084215],\n     [-73.8660852612371, 45.518916733747],\n     [-73.8650447273518, 45.5192607308197],\n     [-73.8636752254303, 45.5192742783684],\n     [-73.8642825006245, 45.5181743422849],\n     [-73.8612862477059, 45.5160062120045],\n     [-73.8573480782361, 45.5146328898613],\n     [-73.8576368219668, 45.5137501468995],\n     [-73.8563777974017, 45.5129982318504],\n     [-73.8561039739004, 45.5121981492155],\n     [-73.8550238065929, 45.5113916654122],\n     [-73.8556577823538, 45.5104448573143],\n     [-73.8585654378245, 45.508816075481],\n     [-73.8591760216501, 45.5082382022867],\n     [-73.8597941402612, 45.5068235473577],\n     [-73.8595897032533, 45.5048715239796],\n     [-73.8603509384135, 45.5038432949756],\n     [-73.8628677193202, 45.5031873776418],\n     [-73.8634772725348, 45.5024655856932],\n     [-73.863229031203, 45.4996768609889],\n     [-73.8624925888829, 45.4985813745818],\n     [-73.8617450446894, 45.4983353640287],\n     [-73.8610656712915, 45.4975420638734],\n     [-73.8614902756333, 45.4959120252572],\n     [-73.8631347750672, 45.4949799702698],\n     [-73.8643010381037, 45.4932934742402],\n     [-73.8637682240978, 45.4920174491202],\n     [-73.8642488213344, 45.4911520725992],\n     [-73.8664440380781, 45.4903261139859],\n     [-73.8688974470136, 45.4882449524107]]]},\n  'properties': {'district': '61-Pierre-Foretier'},\n  'id': '61'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.871602833053, 45.4905720481716],\n     [-73.8758099807102, 45.4943016403121],\n     [-73.8832708288195, 45.4900124335721],\n     [-73.893915982267, 45.4991295138785],\n     [-73.8978204833524, 45.5021042916793],\n     [-73.8993079427873, 45.5021890950386],\n     [-73.901198690394, 45.5019296796061],\n     [-73.9032166411352, 45.5013944047771],\n     [-73.9072471667436, 45.4990521662397],\n     [-73.9097687478128, 45.498420306124],\n     [-73.910763113645, 45.4979785583287],\n     [-73.9131442528934, 45.5001505549281],\n     [-73.9117306033429, 45.5014828251295],\n     [-73.9105682077776, 45.502919788446],\n     [-73.9125029822642, 45.5046603019441],\n     [-73.9125904080641, 45.5059790139203],\n     [-73.9115367282405, 45.5071346344323],\n     [-73.9098260610793, 45.5077167397986],\n     [-73.9095376253027, 45.5082630419981],\n     [-73.9075790301516, 45.5084626314966],\n     [-73.9064637954127, 45.5082146250766],\n     [-73.9052902228377, 45.5087497533613],\n     [-73.9048304811418, 45.5093648655927],\n     [-73.9028980165401, 45.5106748854948],\n     [-73.9025706032313, 45.513213659007],\n     [-73.9021033388663, 45.5139081604197],\n     [-73.9026005080814, 45.5156160928628],\n     [-73.9021261241608, 45.5155955053506],\n     [-73.9005919569215, 45.5144536068957],\n     [-73.9008379207362, 45.5136630574361],\n     [-73.9002254469086, 45.5131179581327],\n     [-73.8989649328704, 45.5146837440746],\n     [-73.8972364759115, 45.5131483697101],\n     [-73.896331119406, 45.5139759395892],\n     [-73.8953141801776, 45.5130744053201],\n     [-73.8937551506541, 45.5125006850816],\n     [-73.8911436774502, 45.5127840458479],\n     [-73.8890287067442, 45.5122302528857],\n     [-73.8881290541888, 45.5127359235277],\n     [-73.8852521620131, 45.5102059725257],\n     [-73.8870687665439, 45.5091995603904],\n     [-73.8880808924888, 45.5083547817147],\n     [-73.8893630543063, 45.5086347066791],\n     [-73.8894862335895, 45.5075924382318],\n     [-73.8907763512179, 45.5069642008183],\n     [-73.8912989165002, 45.5060276552235],\n     [-73.8930883375795, 45.5061095241892],\n     [-73.8938090092703, 45.5051466368411],\n     [-73.8931298332185, 45.5045492632818],\n     [-73.8902948397667, 45.5048242389134],\n     [-73.8882584256138, 45.5047130635017],\n     [-73.8845885513537, 45.503100834689],\n     [-73.8840175555813, 45.5020031308338],\n     [-73.8853262749786, 45.4998100023704],\n     [-73.884412047715, 45.4990183531015],\n     [-73.8830127770248, 45.4996741535333],\n     [-73.8810568286333, 45.4999384356033],\n     [-73.879779371889, 45.5004798588748],\n     [-73.8789156358527, 45.5017299402712],\n     [-73.8770152857708, 45.501806378212],\n     [-73.879759682167, 45.5029518530572],\n     [-73.8803341428118, 45.5033993667444],\n     [-73.8803094438111, 45.5042458334118],\n     [-73.8783878189261, 45.5059406633388],\n     [-73.8756305607743, 45.5078228770541],\n     [-73.8716174921312, 45.5042888396132],\n     [-73.8722903935919, 45.503568954282],\n     [-73.8690288298613, 45.5007201896345],\n     [-73.8701130707197, 45.5000549627491],\n     [-73.871134091344, 45.4990564163597],\n     [-73.872571305249, 45.4969659674767],\n     [-73.8714042058561, 45.4965567000138],\n     [-73.872233047857, 45.4949468141821],\n     [-73.8716913646707, 45.4933306815312],\n     [-73.870532930773, 45.4928625205596],\n     [-73.8694139090223, 45.4919469541754],\n     [-73.871602833053, 45.4905720481716]]]},\n  'properties': {'district': '62-Denis-Benjamin-Viger'},\n  'id': '62'},\n {'type': 'Feature',\n  'geometry': {'type': 'MultiPolygon',\n   'coordinates': [[[[-73.876179389015, 45.484463443982],\n      [-73.8748623836763, 45.4845487678604],\n      [-73.8746531555376, 45.4838655945021],\n      [-73.8752754756586, 45.4831976914281],\n      [-73.875332125701, 45.4820996763162],\n      [-73.8773551113916, 45.4823719006883],\n      [-73.8773585206136, 45.4838386904758],\n      [-73.8770492982827, 45.4844695752153],\n      [-73.876179389015, 45.484463443982]]],\n    [[[-73.871602833053, 45.4905720481716],\n      [-73.8688974470136, 45.4882449524107],\n      [-73.8709851008353, 45.487059190004],\n      [-73.8725145833884, 45.4854652904542],\n      [-73.8761955543809, 45.4864641220707],\n      [-73.8773897323982, 45.4856292741431],\n      [-73.8778041545601, 45.4844850797772],\n      [-73.8785921129696, 45.4837086183771],\n      [-73.8781898959115, 45.4828731025615],\n      [-73.8784365392884, 45.4814954771698],\n      [-73.8796440398513, 45.4803396456993],\n      [-73.8808564171004, 45.4799307290984],\n      [-73.8818738698332, 45.4790634862107],\n      [-73.8828567898793, 45.4787722169973],\n      [-73.8847419788084, 45.4776140567433],\n      [-73.8874400837697, 45.4756252617452],\n      [-73.8890406450115, 45.4740361165489],\n      [-73.8896093964888, 45.473044339843],\n      [-73.8920158379205, 45.4714793088166],\n      [-73.8939883829402, 45.4719134901468],\n      [-73.8962921181107, 45.4721394139777],\n      [-73.8981449394396, 45.4719079872095],\n      [-73.8992850543273, 45.4721559893786],\n      [-73.9014271769693, 45.4712486357827],\n      [-73.9038056762196, 45.4712042547101],\n      [-73.9051695937615, 45.4705605613393],\n      [-73.9045774306121, 45.4700226989345],\n      [-73.9028488295322, 45.4697318295333],\n      [-73.9012801121118, 45.4703403082789],\n      [-73.8996924472568, 45.4700938928558],\n      [-73.8993825392247, 45.469663097207],\n      [-73.8999775060713, 45.4688151666323],\n      [-73.9017348723624, 45.4677561646305],\n      [-73.9023924422294, 45.468509706018],\n      [-73.9043441969034, 45.4696096448133],\n      [-73.9084347070547, 45.4693341104744],\n      [-73.9120964530199, 45.4682153872984],\n      [-73.9126431874804, 45.4694449753644],\n      [-73.9136278859403, 45.4694413239075],\n      [-73.9154106009438, 45.4701278046013],\n      [-73.9159006769417, 45.4706838913767],\n      [-73.9168728992832, 45.4707163448971],\n      [-73.9159791650457, 45.4726632836215],\n      [-73.9164275963913, 45.4727606419858],\n      [-73.9171840487468, 45.4712821683374],\n      [-73.9179119052825, 45.4711355184565],\n      [-73.9186742261816, 45.4721675623649],\n      [-73.9196472003342, 45.4740176773167],\n      [-73.9200930017441, 45.4737730563005],\n      [-73.921822149022, 45.4741086594329],\n      [-73.9226284972793, 45.4752714551831],\n      [-73.924738453321, 45.4775172443067],\n      [-73.9253150364798, 45.4776590879844],\n      [-73.9266000233535, 45.476745452941],\n      [-73.9277434225804, 45.477398128666],\n      [-73.9295423138299, 45.4767974221727],\n      [-73.9302919142255, 45.4778114366407],\n      [-73.9314189351491, 45.4779961715469],\n      [-73.9326541103686, 45.4772716425334],\n      [-73.9335030743869, 45.4762245897937],\n      [-73.9350412733002, 45.4766507100519],\n      [-73.9370692452005, 45.4762391230436],\n      [-73.9406266477915, 45.4761163385087],\n      [-73.942911201225, 45.475477638423],\n      [-73.9443213415212, 45.4758861323891],\n      [-73.9441879968396, 45.4768134931837],\n      [-73.9428248046939, 45.478697804247],\n      [-73.9416119001885, 45.4794240216934],\n      [-73.9405851561781, 45.4789960185208],\n      [-73.9387460246999, 45.4793540720446],\n      [-73.9383403023619, 45.4798235952081],\n      [-73.9385185477879, 45.4813526185292],\n      [-73.9403810268872, 45.4823442895009],\n      [-73.9410419166311, 45.4834215699972],\n      [-73.942256795601, 45.483380875313],\n      [-73.9426871284147, 45.4827943413787],\n      [-73.9436978356457, 45.482808418644],\n      [-73.9433844250482, 45.4836104864635],\n      [-73.9412104480622, 45.4837088979371],\n      [-73.941186300091, 45.4855356756957],\n      [-73.9409258043702, 45.4865805138247],\n      [-73.940155183621, 45.4878253029672],\n      [-73.9400375236355, 45.4891394779763],\n      [-73.939084522942, 45.4899799926108],\n      [-73.9355316222878, 45.4921352533586],\n      [-73.9349775409211, 45.4932801876613],\n      [-73.9328555975538, 45.4935401391563],\n      [-73.9323151123675, 45.4931193083499],\n      [-73.9298370141484, 45.4936595300698],\n      [-73.9291104700923, 45.4940132354185],\n      [-73.9270348943367, 45.4936430419454],\n      [-73.925473026234, 45.4952145713303],\n      [-73.9244092344479, 45.4966763272855],\n      [-73.9244257212294, 45.4971711381462],\n      [-73.9233876507023, 45.4982360094313],\n      [-73.9226520258841, 45.4995263547275],\n      [-73.9218373692027, 45.5000962326717],\n      [-73.9209670436555, 45.5000814447197],\n      [-73.918541557808, 45.5008552086516],\n      [-73.9174837992266, 45.5014440345453],\n      [-73.9145252844175, 45.5010948373465],\n      [-73.9134946418312, 45.5018904208046],\n      [-73.9127623770829, 45.5032518742003],\n      [-73.9125029822642, 45.5046603019441],\n      [-73.9105682077776, 45.502919788446],\n      [-73.9117306033429, 45.5014828251295],\n      [-73.9131442528934, 45.5001505549281],\n      [-73.910763113645, 45.4979785583287],\n      [-73.9097687478128, 45.498420306124],\n      [-73.9072471667436, 45.4990521662397],\n      [-73.9032166411352, 45.5013944047771],\n      [-73.901198690394, 45.5019296796061],\n      [-73.8993079427873, 45.5021890950386],\n      [-73.8978204833524, 45.5021042916793],\n      [-73.893915982267, 45.4991295138785],\n      [-73.8832708288195, 45.4900124335721],\n      [-73.8758099807102, 45.4943016403121],\n      [-73.871602833053, 45.4905720481716]]]]},\n  'properties': {'district': '63-Jacques-Bizard'},\n  'id': '63'},\n {'type': 'Feature',\n  'geometry': {'type': 'MultiPolygon',\n   'coordinates': [[[[-73.8795251463835, 45.4721694456373],\n      [-73.8770259047514, 45.4733055656905],\n      [-73.8771362722445, 45.474146820144],\n      [-73.8758315302677, 45.4744852213893],\n      [-73.8761694823241, 45.475285050858],\n      [-73.8753710533004, 45.4763945070991],\n      [-73.8754013038998, 45.4771052655959],\n      [-73.8745581040502, 45.4791867014648],\n      [-73.873401680597, 45.4803243146019],\n      [-73.8714778204625, 45.4814913213621],\n      [-73.8706822810873, 45.4830686993857],\n      [-73.8688129680033, 45.4847754445001],\n      [-73.8690502967072, 45.4858455083571],\n      [-73.8658510539776, 45.4876644845598],\n      [-73.8647484711707, 45.4872900384824],\n      [-73.8624368975074, 45.4879093508046],\n      [-73.8598077012489, 45.4888850004049],\n      [-73.8587614558506, 45.4873090890375],\n      [-73.8613316931676, 45.4851413285652],\n      [-73.8624629228902, 45.4840039194648],\n      [-73.8650419471552, 45.4821657726431],\n      [-73.8728715855598, 45.4756017517064],\n      [-73.871209821762, 45.4733162451191],\n      [-73.8733213850942, 45.4725990666647],\n      [-73.8782862116718, 45.4705088548296],\n      [-73.8795251463835, 45.4721694456373]]],\n    [[[-73.8598857733827, 45.4911029074784],\n      [-73.8591675296752, 45.4908171897388],\n      [-73.8600571674373, 45.4898965476815],\n      [-73.8612991016732, 45.4900456152652],\n      [-73.8605639387492, 45.4911097682054],\n      [-73.8598857733827, 45.4911029074784]]]]},\n  'properties': {'district': '64-Sainte-Geneviève'},\n  'id': '64'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.505845657669, 45.5915064148139],\n     [-73.5103359282538, 45.5921124591547],\n     [-73.5206880075519, 45.5952083313336],\n     [-73.5204207506441, 45.5957326455424],\n     [-73.5227698757162, 45.5964405193219],\n     [-73.5231408135446, 45.5957591991736],\n     [-73.533521772969, 45.5988762603839],\n     [-73.5369160899836, 45.5931626565659],\n     [-73.5411715382662, 45.5939767034301],\n     [-73.5423606148624, 45.5943108573749],\n     [-73.5455045323429, 45.5956272275911],\n     [-73.5452799498143, 45.5959627917351],\n     [-73.5438572127564, 45.5980784963673],\n     [-73.5442050076359, 45.5981948225546],\n     [-73.5445859643042, 45.5999308464328],\n     [-73.5451108447268, 45.6000061818095],\n     [-73.5447576674512, 45.6019410417126],\n     [-73.5407418924341, 45.6006998521118],\n     [-73.539729971205, 45.6018186221009],\n     [-73.5429289535666, 45.6032316351511],\n     [-73.5421763850909, 45.6042454438226],\n     [-73.5394321608651, 45.6030910243993],\n     [-73.5365958888719, 45.602162446611],\n     [-73.5365038947396, 45.6026457300995],\n     [-73.5375401492111, 45.6052359766051],\n     [-73.5416213388458, 45.606939681157],\n     [-73.540900931842, 45.6078533012629],\n     [-73.5416434070878, 45.6081797015109],\n     [-73.5412098839782, 45.6095033241271],\n     [-73.5441363974969, 45.6102716461918],\n     [-73.5453373826653, 45.6107633861198],\n     [-73.5444977198785, 45.6121760838565],\n     [-73.5462767505068, 45.6127238289344],\n     [-73.5452777806501, 45.6141657083913],\n     [-73.5437218072693, 45.6149096422408],\n     [-73.5381703589619, 45.6139517855176],\n     [-73.5375256945614, 45.6160673327401],\n     [-73.5252332402243, 45.6137951013372],\n     [-73.5145702896503, 45.6119902043961],\n     [-73.5073926872162, 45.6107104987953],\n     [-73.507104042043, 45.6103305736701],\n     [-73.5088445389914, 45.6059056683277],\n     [-73.5094016237031, 45.6029289790696],\n     [-73.5089844073373, 45.5984312397903],\n     [-73.5085623510741, 45.5964354408978],\n     [-73.5073994472774, 45.5937585441643],\n     [-73.505845657669, 45.5915064148139]]]},\n  'properties': {'district': '71-Tétreaultville'},\n  'id': '71'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.505845657669, 45.5915064148139],\n     [-73.5052335547608, 45.5914035841385],\n     [-73.5040074172822, 45.5877509637108],\n     [-73.50548477383, 45.5842108995057],\n     [-73.5046727489859, 45.5839523001102],\n     [-73.5060139625714, 45.5806625893987],\n     [-73.5058561540639, 45.5806068926751],\n     [-73.5091666956111, 45.5757730349861],\n     [-73.5141197194594, 45.5708695926608],\n     [-73.5190039768335, 45.5682773867463],\n     [-73.5197939453882, 45.5669670591355],\n     [-73.5180270972663, 45.5661485153046],\n     [-73.5253430489297, 45.5574887203531],\n     [-73.5250635489053, 45.5568993328574],\n     [-73.5236813870747, 45.5568097261785],\n     [-73.521480081669, 45.5591100901826],\n     [-73.5209547766993, 45.5588573091504],\n     [-73.5218702090576, 45.5578763137034],\n     [-73.5209413390244, 45.5570999459563],\n     [-73.5191167026398, 45.5588496230043],\n     [-73.518595216105, 45.5586040300451],\n     [-73.522051635704, 45.5552304196808],\n     [-73.5269225613603, 45.549446926412],\n     [-73.5335765932332, 45.5515757481653],\n     [-73.5389816134656, 45.5533773670715],\n     [-73.5382239872366, 45.5549283797349],\n     [-73.5486183023616, 45.5582491708623],\n     [-73.5460296824187, 45.562265422129],\n     [-73.5459659905086, 45.5625254142127],\n     [-73.5456328946765, 45.5629337545645],\n     [-73.5413632157982, 45.5709610549351],\n     [-73.5386884000436, 45.5749269433291],\n     [-73.5320531965098, 45.5852349268307],\n     [-73.540035463506, 45.5878529169711],\n     [-73.5369160899836, 45.5931626565659],\n     [-73.533521772969, 45.5988762603839],\n     [-73.5231408135446, 45.5957591991736],\n     [-73.5227698757162, 45.5964405193219],\n     [-73.5204207506441, 45.5957326455424],\n     [-73.5206880075519, 45.5952083313336],\n     [-73.5103359282538, 45.5921124591547],\n     [-73.505845657669, 45.5915064148139]]]},\n  'properties': {'district': '72-MaisonneuveLongue-Pointe'},\n  'id': '72'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5269225613603, 45.549446926412],\n     [-73.5289560474797, 45.5470320263359],\n     [-73.5277739913219, 45.547187016061],\n     [-73.5277290513663, 45.5464490003045],\n     [-73.5313590128093, 45.5457750092113],\n     [-73.5321039500724, 45.5447229718789],\n     [-73.5291969726971, 45.5451540050291],\n     [-73.5296700173696, 45.5442239930694],\n     [-73.5329650126455, 45.5436089818848],\n     [-73.5339429567644, 45.5424780308809],\n     [-73.5315039994787, 45.5426720333775],\n     [-73.5316359736222, 45.5420080334177],\n     [-73.5344850464319, 45.5413149877237],\n     [-73.5404589754615, 45.5356759920904],\n     [-73.5417879567955, 45.529933001055],\n     [-73.5428587889529, 45.5304546205586],\n     [-73.5439636509721, 45.5334697006227],\n     [-73.5473499781753, 45.5376709059424],\n     [-73.5482541001656, 45.5383569002539],\n     [-73.550216365092, 45.5389758922042],\n     [-73.5592280432661, 45.5399028690768],\n     [-73.5552605710227, 45.5474166761255],\n     [-73.5545655421497, 45.55467023726],\n     [-73.5542787320438, 45.5568764086233],\n     [-73.5546496932451, 45.5649641797921],\n     [-73.5544730757919, 45.5652953442798],\n     [-73.5459659905086, 45.5625254142127],\n     [-73.5460296824187, 45.562265422129],\n     [-73.5486183023616, 45.5582491708623],\n     [-73.5382239872366, 45.5549283797349],\n     [-73.5389816134656, 45.5533773670715],\n     [-73.5335765932332, 45.5515757481653],\n     [-73.5269225613603, 45.549446926412]]]},\n  'properties': {'district': '73-Hochelaga'},\n  'id': '73'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5455045323429, 45.5956272275911],\n     [-73.5423606148624, 45.5943108573749],\n     [-73.5411715382662, 45.5939767034301],\n     [-73.5369160899836, 45.5931626565659],\n     [-73.540035463506, 45.5878529169711],\n     [-73.5320531965098, 45.5852349268307],\n     [-73.5386884000436, 45.5749269433291],\n     [-73.5413632157982, 45.5709610549351],\n     [-73.5456328946765, 45.5629337545645],\n     [-73.5459659905086, 45.5625254142127],\n     [-73.5544730757919, 45.5652953442798],\n     [-73.5489772535198, 45.5734435433853],\n     [-73.5557298683366, 45.5757065336466],\n     [-73.5584435150771, 45.578098690279],\n     [-73.5683754804235, 45.5814045757918],\n     [-73.5648413846768, 45.5830623920281],\n     [-73.566054883333, 45.5843176844033],\n     [-73.563949176176, 45.5849798671748],\n     [-73.5642198577817, 45.5854079588427],\n     [-73.5642246834943, 45.5872483268508],\n     [-73.5663609324088, 45.5883396268265],\n     [-73.5686767639537, 45.5922969983003],\n     [-73.5690474086577, 45.5937353194892],\n     [-73.5687432258054, 45.5949521683749],\n     [-73.5696609688417, 45.5965806032278],\n     [-73.5694206600629, 45.5971486400825],\n     [-73.5576871015794, 45.5932166140624],\n     [-73.5489394765731, 45.5903029802044],\n     [-73.5465394629102, 45.5940712033384],\n     [-73.5481396292151, 45.5946360429193],\n     [-73.5473484500099, 45.5955355370728],\n     [-73.545930102715, 45.5949937733284],\n     [-73.5455045323429, 45.5956272275911]]]},\n  'properties': {'district': '74-Louis-Riel'},\n  'id': '74'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.6202427208594, 45.5951205335532],\n     [-73.6296215435427, 45.5838130726522],\n     [-73.6288186186035, 45.5832571469721],\n     [-73.631962332515, 45.579640175342],\n     [-73.6327012962769, 45.5799802985646],\n     [-73.6363215300962, 45.5759177646435],\n     [-73.652557313063, 45.5826892716542],\n     [-73.6543058972308, 45.5836262624158],\n     [-73.6531900096361, 45.5859698880497],\n     [-73.6526947978849, 45.5876532157306],\n     [-73.6523937374702, 45.5900742122073],\n     [-73.6505401287885, 45.5918403260598],\n     [-73.6490983475598, 45.5943078055034],\n     [-73.646028354957, 45.5966872776611],\n     [-73.6450204850692, 45.598497300652],\n     [-73.6434244270991, 45.6009558130627],\n     [-73.6432738605823, 45.6022427894953],\n     [-73.6418446065649, 45.6048360555341],\n     [-73.6415278334524, 45.606276219979],\n     [-73.6400469319338, 45.6087976232294],\n     [-73.6388534618155, 45.6101933725153],\n     [-73.6374274156649, 45.6122509805631],\n     [-73.6349651825723, 45.6101401159366],\n     [-73.6284179810453, 45.603912893307],\n     [-73.6227414555439, 45.5985713755102],\n     [-73.6209924104659, 45.5968531497608],\n     [-73.6216736698201, 45.5964861898013],\n     [-73.6202427208594, 45.5951205335532]]]},\n  'properties': {'district': '81-Marie-Clarac'},\n  'id': '81'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.6028907661658, 45.6112216102358],\n     [-73.6041139394664, 45.6106174089674],\n     [-73.6035616092243, 45.6101078733853],\n     [-73.6048043091085, 45.6094292879636],\n     [-73.6062298901187, 45.610759685103],\n     [-73.6079197060538, 45.6094530924621],\n     [-73.6092890313816, 45.6081470965116],\n     [-73.6125635803426, 45.604373796832],\n     [-73.61467650443, 45.601820884038],\n     [-73.6202427208594, 45.5951205335532],\n     [-73.6216736698201, 45.5964861898013],\n     [-73.6209924104659, 45.5968531497608],\n     [-73.6227414555439, 45.5985713755102],\n     [-73.6284179810453, 45.603912893307],\n     [-73.6349651825723, 45.6101401159366],\n     [-73.6374274156649, 45.6122509805631],\n     [-73.6357315212, 45.6162353900055],\n     [-73.6334060135125, 45.6196665233391],\n     [-73.6310514588113, 45.6217838603777],\n     [-73.6289543944547, 45.6234221648674],\n     [-73.6269636907653, 45.6247707822286],\n     [-73.6250472436735, 45.6262897836938],\n     [-73.6235959602566, 45.6271766849645],\n     [-73.6206953088035, 45.6298453145986],\n     [-73.6168081007299, 45.6262083345707],\n     [-73.6071490341973, 45.6170493269392],\n     [-73.6023660828945, 45.6125718329314],\n     [-73.601559767966, 45.6118073578371],\n     [-73.6028907661658, 45.6112216102358]]]},\n  'properties': {'district': '82-Ovide-Clermont'},\n  'id': '82'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5989854515795, 45.5160358124743],\n     [-73.5986563507028, 45.5163499572059],\n     [-73.601437737118, 45.5176238014654],\n     [-73.6020308024605, 45.5177034122021],\n     [-73.613010925262, 45.5225880512056],\n     [-73.6108850612038, 45.5249602486287],\n     [-73.5966746401772, 45.5186407402299],\n     [-73.5982202946245, 45.5169177573452],\n     [-73.5969109664106, 45.5161364855643],\n     [-73.5989854515795, 45.5160358124743]]]},\n  'properties': {'district': '91-Claude-Ryan'},\n  'id': '91'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5989854515795, 45.5160358124743],\n     [-73.6044730454934, 45.5155769251068],\n     [-73.6089843852315, 45.5146703511952],\n     [-73.6091799069916, 45.5148927825],\n     [-73.6186655613026, 45.5191467549529],\n     [-73.6148105187034, 45.5234843904785],\n     [-73.6155485100061, 45.52374152426],\n     [-73.6253373412254, 45.518414467689],\n     [-73.6244833769123, 45.5195443162251],\n     [-73.6208637881685, 45.5236565043515],\n     [-73.617235572865, 45.5277835686717],\n     [-73.6170925681517, 45.5277209056734],\n     [-73.6124143323658, 45.525647330045],\n     [-73.6108850612038, 45.5249602486287],\n     [-73.613010925262, 45.5225880512056],\n     [-73.6020308024605, 45.5177034122021],\n     [-73.601437737118, 45.5176238014654],\n     [-73.5986563507028, 45.5163499572059],\n     [-73.5989854515795, 45.5160358124743]]]},\n  'properties': {'district': '92-Joseph-Beaubien'},\n  'id': '92'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5956108742759, 45.504064067218],\n     [-73.5987846893921, 45.5056180649425],\n     [-73.5993805679324, 45.5053898653966],\n     [-73.6024762927983, 45.5068574351344],\n     [-73.6035337540631, 45.5057001623398],\n     [-73.6061344132343, 45.5068772235294],\n     [-73.6063535452531, 45.5074911306599],\n     [-73.6130513861854, 45.5104478685544],\n     [-73.6182356279483, 45.504722156916],\n     [-73.6189454361275, 45.5051234940594],\n     [-73.6160933747766, 45.5082725969653],\n     [-73.6178928803835, 45.5091750557282],\n     [-73.6169256431203, 45.5102737338336],\n     [-73.61888001694, 45.5111346532789],\n     [-73.617959519301, 45.5121686212918],\n     [-73.6171142113946, 45.5117885655948],\n     [-73.6163442345691, 45.5125462730753],\n     [-73.615038686563, 45.511951761593],\n     [-73.6111115129373, 45.5140796366228],\n     [-73.6089843852315, 45.5146703511952],\n     [-73.6044730454934, 45.5155769251068],\n     [-73.5989854515795, 45.5160358124743],\n     [-73.5969109664106, 45.5161364855643],\n     [-73.5982202946245, 45.5169177573452],\n     [-73.5966746401772, 45.5186407402299],\n     [-73.5902604532774, 45.5157813641729],\n     [-73.5911992183544, 45.5147110287094],\n     [-73.5918451342772, 45.5144729450241],\n     [-73.5968586638591, 45.5145623065118],\n     [-73.5978614314752, 45.5141637733071],\n     [-73.598896296438, 45.5131127563816],\n     [-73.5969405107098, 45.5130415385056],\n     [-73.5969012890154, 45.5117790625741],\n     [-73.5949672180747, 45.5109069459258],\n     [-73.5935088435771, 45.5105257693584],\n     [-73.594191031848, 45.5097864575692],\n     [-73.591464574106, 45.508070751295],\n     [-73.5956108742759, 45.504064067218]]]},\n  'properties': {'district': '93-Robert-Bourassa'},\n  'id': '93'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.617959519301, 45.5121686212918],\n     [-73.6176645335398, 45.5125000231349],\n     [-73.6207592308166, 45.5138993727882],\n     [-73.6267223236567, 45.5165801475772],\n     [-73.6253373412254, 45.518414467689],\n     [-73.6155485100061, 45.52374152426],\n     [-73.6148105187034, 45.5234843904785],\n     [-73.6186655613026, 45.5191467549529],\n     [-73.6091799069916, 45.5148927825],\n     [-73.6089843852315, 45.5146703511952],\n     [-73.6111115129373, 45.5140796366228],\n     [-73.615038686563, 45.511951761593],\n     [-73.6163442345691, 45.5125462730753],\n     [-73.6171142113946, 45.5117885655948],\n     [-73.617959519301, 45.5121686212918]]]},\n  'properties': {'district': '94-Jeanne-Sauvé'},\n  'id': '94'},\n {'type': 'Feature',\n  'geometry': {'type': 'MultiPolygon',\n   'coordinates': [[[[-73.8187679895316, 45.5140812084438],\n      [-73.8182383369059, 45.5132188436317],\n      [-73.8194262685108, 45.5128016511853],\n      [-73.8210527525633, 45.5129859861948],\n      [-73.8211602697308, 45.5138855228726],\n      [-73.8187679895316, 45.5140812084438]]],\n    [[[-73.7612752776246, 45.5104175090183],\n      [-73.7551669017747, 45.5064862077527],\n      [-73.7550630650544, 45.5065238038199],\n      [-73.7516832079437, 45.5044227485347],\n      [-73.7530750881715, 45.5037537247243],\n      [-73.7547114610262, 45.5033812068779],\n      [-73.756652358581, 45.5024589334263],\n      [-73.7582371682807, 45.5020414841587],\n      [-73.7594306007504, 45.5027766325504],\n      [-73.7609994664832, 45.5016752336666],\n      [-73.7638615520544, 45.5034953365559],\n      [-73.764601206777, 45.5029446682134],\n      [-73.7653841918245, 45.5034648112556],\n      [-73.7671979733428, 45.5027497284985],\n      [-73.7692002551043, 45.5040137486001],\n      [-73.7709616412032, 45.5034073001873],\n      [-73.7725018897261, 45.5025715611262],\n      [-73.770851931212, 45.5015263075263],\n      [-73.7738796981111, 45.5005112554049],\n      [-73.7716579558132, 45.4991257181769],\n      [-73.7735738316828, 45.4981363521851],\n      [-73.7779880975343, 45.5009521734749],\n      [-73.780555316633, 45.4997211145292],\n      [-73.7811328878805, 45.5000886664279],\n      [-73.7828691809435, 45.4992834417083],\n      [-73.7866716942594, 45.5016560785918],\n      [-73.7833876826593, 45.5031694957223],\n      [-73.7921892811359, 45.505162983993],\n      [-73.790378705595, 45.5039707845666],\n      [-73.7971312330344, 45.5007319392105],\n      [-73.800845778582, 45.4990215282087],\n      [-73.8020719362353, 45.500313400533],\n      [-73.8090622022287, 45.4969839281454],\n      [-73.8103152495861, 45.4968275421823],\n      [-73.8148195197041, 45.5014854076037],\n      [-73.814130519647, 45.5018291882481],\n      [-73.8151983502239, 45.5028700758232],\n      [-73.8158749418108, 45.5025712297953],\n      [-73.8192989400152, 45.5061341618076],\n      [-73.8239273064692, 45.5039359215182],\n      [-73.8260247686459, 45.5028472072531],\n      [-73.8336495886976, 45.4992478732958],\n      [-73.8354476552771, 45.4983089444422],\n      [-73.835851882975, 45.4987134333303],\n      [-73.8376510464643, 45.4977036685397],\n      [-73.8352748755507, 45.4946201233283],\n      [-73.8359064469498, 45.4941865754856],\n      [-73.8445138674112, 45.4899447538175],\n      [-73.8461670237813, 45.488985999861],\n      [-73.8503699985263, 45.4936842455575],\n      [-73.853113099403, 45.4984292547447],\n      [-73.8558121436329, 45.5017030001578],\n      [-73.8554973469563, 45.506068266442],\n      [-73.8541507320505, 45.5071387755787],\n      [-73.8547842282295, 45.5078848003326],\n      [-73.8541503989792, 45.5088568468323],\n      [-73.8514364847285, 45.5107998956924],\n      [-73.8509728924279, 45.5113681864054],\n      [-73.8497265304179, 45.5116149897809],\n      [-73.8487575861648, 45.5122208147549],\n      [-73.8486390859537, 45.5137509650076],\n      [-73.8470767467479, 45.515690388096],\n      [-73.8459043806884, 45.5165397868316],\n      [-73.8447612809957, 45.5165252439303],\n      [-73.8416070645454, 45.5171016749459],\n      [-73.8379192636275, 45.5168516646645],\n      [-73.8362532214568, 45.51650560879],\n      [-73.8321180123678, 45.5163557197529],\n      [-73.8313656471532, 45.5168348664122],\n      [-73.8297408583824, 45.5169655584571],\n      [-73.8282028168338, 45.5166190156169],\n      [-73.8268661171387, 45.5156510162477],\n      [-73.8250222653614, 45.5155213777536],\n      [-73.8246734151281, 45.5149644141917],\n      [-73.8233629812572, 45.5141042906802],\n      [-73.8234733111558, 45.5132581353231],\n      [-73.8218473252627, 45.5131637356642],\n      [-73.8209366065744, 45.5128154164364],\n      [-73.8192176588039, 45.5126139694502],\n      [-73.8179057909873, 45.5132557668608],\n      [-73.8178065811604, 45.5138229980126],\n      [-73.8160507514445, 45.5154287971508],\n      [-73.814770537422, 45.5156310808765],\n      [-73.8131059190362, 45.5155096653326],\n      [-73.810217503155, 45.5162914840575],\n      [-73.8084551833961, 45.5162217432523],\n      [-73.8064888710304, 45.5155636081875],\n      [-73.8057027099487, 45.5147919349551],\n      [-73.8040410010469, 45.5149672884696],\n      [-73.801546777817, 45.5149829442516],\n      [-73.8001619454196, 45.5148065602758],\n      [-73.7995790764703, 45.5135123735559],\n      [-73.7995988098213, 45.5123965010493],\n      [-73.7992059489796, 45.5107148435346],\n      [-73.7978204799485, 45.5101065121228],\n      [-73.7969591683437, 45.5093799021214],\n      [-73.7939384028061, 45.509297721884],\n      [-73.79058311435, 45.5088654314285],\n      [-73.7870155334484, 45.5094594051816],\n      [-73.7852614432657, 45.5093198098856],\n      [-73.7829719822765, 45.5095864846083],\n      [-73.7806008657296, 45.5088903884048],\n      [-73.7769513427547, 45.5084674192311],\n      [-73.7745708564459, 45.5084461679299],\n      [-73.7719486126198, 45.5087044209407],\n      [-73.7697639799538, 45.5095104479827],\n      [-73.7673058990371, 45.5093183322757],\n      [-73.7625356657639, 45.5100672469129],\n      [-73.7612752776246, 45.5104175090183]]]]},\n  'properties': {'district': '101-Bois-de-Liesse'},\n  'id': '101'},\n {'type': 'Feature',\n  'geometry': {'type': 'MultiPolygon',\n   'coordinates': [[[[-73.8461670237813, 45.488985999861],\n      [-73.850532069583, 45.486758681857],\n      [-73.8458397013725, 45.4794951647217],\n      [-73.8472019880053, 45.4788779608994],\n      [-73.8518367097832, 45.4764514219245],\n      [-73.8573573288562, 45.4736627559579],\n      [-73.8545371967343, 45.4706120339916],\n      [-73.8603908221258, 45.4659954945452],\n      [-73.859889484827, 45.4655831049747],\n      [-73.8669852747875, 45.4613941823855],\n      [-73.8659406037748, 45.4600298096328],\n      [-73.8814395340419, 45.4526362439631],\n      [-73.8847216555087, 45.4524908762247],\n      [-73.8890536732734, 45.4491166876217],\n      [-73.8915462525593, 45.4480217199892],\n      [-73.8938116724967, 45.4465652096353],\n      [-73.8981246578432, 45.4475293470406],\n      [-73.9003739737275, 45.4471542652552],\n      [-73.9000666991504, 45.4467285005871],\n      [-73.9015060886188, 45.446358434947],\n      [-73.9023709077117, 45.4475251321732],\n      [-73.9043599589048, 45.4467892477034],\n      [-73.9055608807492, 45.4484676964261],\n      [-73.9104202259452, 45.4468936252547],\n      [-73.9231679445166, 45.4420154242323],\n      [-73.9228839309917, 45.4416385017936],\n      [-73.9245143656332, 45.4408765943703],\n      [-73.924830550628, 45.4404255381026],\n      [-73.9348115551979, 45.4490797759617],\n      [-73.9369647858857, 45.4508355286695],\n      [-73.9367305115467, 45.4519612579974],\n      [-73.936084813848, 45.45278257894],\n      [-73.9363214019941, 45.4536095287366],\n      [-73.9384314066949, 45.45366444434],\n      [-73.93849022267, 45.4546540611182],\n      [-73.9401870518575, 45.4558173634308],\n      [-73.9432583349177, 45.4561564624145],\n      [-73.9444816415584, 45.4572495419421],\n      [-73.9470042484352, 45.4577165755588],\n      [-73.9475358331527, 45.4586413391725],\n      [-73.9466703677291, 45.4591486916182],\n      [-73.9457347700682, 45.4588823651705],\n      [-73.9452919598077, 45.4594780510929],\n      [-73.9438715446993, 45.4593485885153],\n      [-73.9418275850641, 45.461228202182],\n      [-73.9418734970443, 45.4621908676335],\n      [-73.9433996704037, 45.4627518229434],\n      [-73.9458662537823, 45.4625712338861],\n      [-73.9452835656909, 45.4632754571652],\n      [-73.9455865563861, 45.4644081059436],\n      [-73.9444336707811, 45.464151615889],\n      [-73.9428244875956, 45.4644188471942],\n      [-73.9420560006039, 45.4659065384758],\n      [-73.9397320642846, 45.4663563878557],\n      [-73.9401770020335, 45.4676594913386],\n      [-73.9394768022606, 45.4680671581924],\n      [-73.9395814637767, 45.4700104369658],\n      [-73.9382622094761, 45.4714102256228],\n      [-73.9356853954855, 45.4722749319843],\n      [-73.9346437792633, 45.4731967435679],\n      [-73.9329374951772, 45.4725193630933],\n      [-73.9323111235668, 45.4725667323419],\n      [-73.9299152373346, 45.4737006039578],\n      [-73.928842473854, 45.4739115313467],\n      [-73.9275236887696, 45.4737184885069],\n      [-73.9265293732015, 45.4741631743875],\n      [-73.925796243829, 45.4736259485905],\n      [-73.9248000500235, 45.4721089475954],\n      [-73.9222818122356, 45.4705794857248],\n      [-73.922965231979, 45.4696141155523],\n      [-73.9226524205748, 45.4688324194201],\n      [-73.9212201297683, 45.4688467094977],\n      [-73.9216571330551, 45.4674143796986],\n      [-73.921550552687, 45.4668568126713],\n      [-73.9195312526606, 45.4635798400557],\n      [-73.9178201887521, 45.4621912996059],\n      [-73.9148789978459, 45.4619144443475],\n      [-73.9126660861929, 45.4603383473932],\n      [-73.91114607451, 45.4605598133876],\n      [-73.9103819033438, 45.4609764429342],\n      [-73.9100549453511, 45.4617335639878],\n      [-73.9082146339227, 45.4617969415567],\n      [-73.9075688375165, 45.4609865148744],\n      [-73.9058658349117, 45.4609563776965],\n      [-73.9051876779266, 45.4605991035458],\n      [-73.9038323289561, 45.4606219020727],\n      [-73.9020889859002, 45.4618068799119],\n      [-73.9017130574761, 45.4628880358479],\n      [-73.9003152261538, 45.4623259838994],\n      [-73.899767819995, 45.462669876326],\n      [-73.8976775036577, 45.4654577069673],\n      [-73.8962483592814, 45.4658946426154],\n      [-73.8957916608649, 45.4664270982997],\n      [-73.8945384007686, 45.4664224345534],\n      [-73.8936214304942, 45.4669655597388],\n      [-73.8921732932478, 45.4665295724673],\n      [-73.8916127412336, 45.4668464725107],\n      [-73.8913265706041, 45.4679902821467],\n      [-73.8900901349216, 45.4685793797869],\n      [-73.8888217813327, 45.4686035027193],\n      [-73.8860812311329, 45.4695648402732],\n      [-73.8847899934361, 45.4696321930072],\n      [-73.8828866868638, 45.4718612485547],\n      [-73.8823610038276, 45.4716560245278],\n      [-73.8811747849457, 45.4721459539451],\n      [-73.8806356311152, 45.471850773116],\n      [-73.8795251463835, 45.4721694456373],\n      [-73.8782862116718, 45.4705088548296],\n      [-73.8733213850942, 45.4725990666647],\n      [-73.871209821762, 45.4733162451191],\n      [-73.8728715855598, 45.4756017517064],\n      [-73.8650419471552, 45.4821657726431],\n      [-73.8624629228902, 45.4840039194648],\n      [-73.8613316931676, 45.4851413285652],\n      [-73.8587614558506, 45.4873090890375],\n      [-73.8598077012489, 45.4888850004049],\n      [-73.8592579904622, 45.4889452259737],\n      [-73.8578565654384, 45.4898674655412],\n      [-73.8569952719841, 45.4912379676731],\n      [-73.8545791042202, 45.4935760467931],\n      [-73.8531479621556, 45.4959201221199],\n      [-73.8544902230577, 45.4977606434992],\n      [-73.8548560501467, 45.4989743240793],\n      [-73.8555516665994, 45.4997100150003],\n      [-73.8578400891255, 45.4995048836594],\n      [-73.8585024501129, 45.4997829560785],\n      [-73.8591539006867, 45.5007425753891],\n      [-73.8588895885858, 45.501436262307],\n      [-73.8580145103082, 45.5020213087385],\n      [-73.8578513100126, 45.5031672055572],\n      [-73.85712842707, 45.5040318263201],\n      [-73.8570843291783, 45.505329258001],\n      [-73.8555961994083, 45.5077731597575],\n      [-73.8547842282295, 45.5078848003326],\n      [-73.8541507320505, 45.5071387755787],\n      [-73.8554973469563, 45.506068266442],\n      [-73.8558121436329, 45.5017030001578],\n      [-73.853113099403, 45.4984292547447],\n      [-73.8503699985263, 45.4936842455575],\n      [-73.8461670237813, 45.488985999861]]],\n    [[[-73.8565163230435, 45.5065638456799],\n      [-73.8570490281172, 45.5058063309797],\n      [-73.8583878278331, 45.5070438611748],\n      [-73.8567674874305, 45.5078408713637],\n      [-73.8559107285537, 45.5079515359158],\n      [-73.8565163230435, 45.5065638456799]]]]},\n  'properties': {'district': '102-Cap-Saint-Jacques'},\n  'id': '102'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5792047395127, 45.5280064404383],\n     [-73.581506814336, 45.5255527662198],\n     [-73.5893088980696, 45.5167596569974],\n     [-73.5902604532774, 45.5157813641729],\n     [-73.5966746401772, 45.5186407402299],\n     [-73.6108850612038, 45.5249602486287],\n     [-73.6124143323658, 45.525647330045],\n     [-73.6088433154766, 45.5276253868022],\n     [-73.6061835826899, 45.5283650193777],\n     [-73.6023604559616, 45.5284000664849],\n     [-73.5992862274652, 45.5288774305058],\n     [-73.5962116234987, 45.5301487739359],\n     [-73.5947764680621, 45.5311589735711],\n     [-73.5863837119054, 45.5382742004886],\n     [-73.5871054297936, 45.5372069067238],\n     [-73.5899093482658, 45.5348049045751],\n     [-73.5847595146863, 45.5324079438366],\n     [-73.5859421309178, 45.5310790335786],\n     [-73.5792047395127, 45.5280064404383]]]},\n  'properties': {'district': '111-Mile-End'},\n  'id': '111'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5655614721314, 45.5213643879447],\n     [-73.5740572626836, 45.5252747203744],\n     [-73.5745923823609, 45.5259374612073],\n     [-73.5792047395127, 45.5280064404383],\n     [-73.5859421309178, 45.5310790335786],\n     [-73.5847595146863, 45.5324079438366],\n     [-73.5899093482658, 45.5348049045751],\n     [-73.5871054297936, 45.5372069067238],\n     [-73.5863837119054, 45.5382742004886],\n     [-73.5862704189394, 45.5383757481773],\n     [-73.5838512015266, 45.5400861300747],\n     [-73.5810607519196, 45.5411249464667],\n     [-73.5786195284797, 45.5415422864787],\n     [-73.5764926058406, 45.5415739191208],\n     [-73.5735822926994, 45.5413565189643],\n     [-73.5592280432661, 45.5399028690768],\n     [-73.5613401818778, 45.5359226324552],\n     [-73.5641429566138, 45.5273492053093],\n     [-73.5652383566418, 45.5238165260966],\n     [-73.5655614721314, 45.5213643879447]]]},\n  'properties': {'district': '112-De Lorimier'},\n  'id': '112'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5734800018833, 45.5049709717948],\n     [-73.580113792401, 45.5081921313855],\n     [-73.5785542203746, 45.5104511791594],\n     [-73.5786781018666, 45.5117608495828],\n     [-73.582430798457, 45.5136019767056],\n     [-73.5893088980696, 45.5167596569974],\n     [-73.581506814336, 45.5255527662198],\n     [-73.5792047395127, 45.5280064404383],\n     [-73.5745923823609, 45.5259374612073],\n     [-73.5740572626836, 45.5252747203744],\n     [-73.5655614721314, 45.5213643879447],\n     [-73.5655904084691, 45.5210639273018],\n     [-73.571279665339, 45.5083636436696],\n     [-73.5729588302122, 45.5055494196855],\n     [-73.5734800018833, 45.5049709717948]]]},\n  'properties': {'district': '113-Jeanne-Mance'},\n  'id': '113'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.4868984819315, 45.6668163755988],\n     [-73.4909285596849, 45.6670796662378],\n     [-73.4943692754526, 45.6676107175883],\n     [-73.5052117189194, 45.6694506091685],\n     [-73.5066485322791, 45.66770609928],\n     [-73.5086661140009, 45.6660376032574],\n     [-73.514071369879, 45.663410883372],\n     [-73.5226608995211, 45.6590189476592],\n     [-73.5409160537594, 45.6501644528664],\n     [-73.5441930168941, 45.6487914226698],\n     [-73.544738106063, 45.6482242645147],\n     [-73.5524611738286, 45.6444112188767],\n     [-73.5555002141554, 45.6426177060947],\n     [-73.5652962272313, 45.6362003202316],\n     [-73.5713393674169, 45.6418436992789],\n     [-73.5731131247475, 45.6438386857375],\n     [-73.5764976851296, 45.641270099185],\n     [-73.5866413495341, 45.6508496604014],\n     [-73.5898582112291, 45.6537703909976],\n     [-73.5838459058531, 45.6567113166436],\n     [-73.5767899673498, 45.6602649640823],\n     [-73.5719339731853, 45.6626357744781],\n     [-73.539067309963, 45.678979317943],\n     [-73.5369552091684, 45.6802676902485],\n     [-73.5352474248791, 45.6816148433586],\n     [-73.532629555028, 45.6840504841943],\n     [-73.5308427113654, 45.6858730099174],\n     [-73.5292925000853, 45.6880318303705],\n     [-73.5284527492428, 45.6897324941851],\n     [-73.5262321777264, 45.695068712866],\n     [-73.5248704855113, 45.696745979989],\n     [-73.5234829925507, 45.6977084455985],\n     [-73.521300714461, 45.6988491350421],\n     [-73.518072949079, 45.7000844428825],\n     [-73.5135468623237, 45.7013546459954],\n     [-73.5051625304534, 45.7033317792418],\n     [-73.498947493721, 45.7045252458975],\n     [-73.4954757567513, 45.7049490135942],\n     [-73.4932103740932, 45.7047553331643],\n     [-73.4921323200277, 45.7041126467728],\n     [-73.4909405407531, 45.7027073672294],\n     [-73.4900498047694, 45.7021797732479],\n     [-73.4887664394737, 45.7021177409386],\n     [-73.4856763927507, 45.7030727269477],\n     [-73.4829186491029, 45.7036523330194],\n     [-73.4803835661343, 45.7046724098692],\n     [-73.4783219605948, 45.7052709191606],\n     [-73.4766385432002, 45.7054709950549],\n     [-73.4751651064534, 45.7048699119281],\n     [-73.4745824263264, 45.7033866618514],\n     [-73.4749382311583, 45.7024425674455],\n     [-73.4763824131897, 45.7006700752533],\n     [-73.4784777403736, 45.697652858064],\n     [-73.4790042639298, 45.6961901697848],\n     [-73.4792384579055, 45.6941308824961],\n     [-73.4796140010016, 45.6929943766718],\n     [-73.4803640884928, 45.6919258774639],\n     [-73.483643872129, 45.6885943328445],\n     [-73.4851764024197, 45.6867120396751],\n     [-73.4857156558079, 45.6857399456762],\n     [-73.4866448398145, 45.682296765324],\n     [-73.486914072058, 45.6801843677801],\n     [-73.4872497030629, 45.6755976940944],\n     [-73.4873520249609, 45.6730343323445],\n     [-73.4868984819315, 45.6668163755988]]]},\n  'properties': {'district': '121-La Pointe-aux-Prairies'},\n  'id': '121'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.544738106063, 45.6482242645147],\n     [-73.5441930168941, 45.6487914226698],\n     [-73.5409160537594, 45.6501644528664],\n     [-73.5226608995211, 45.6590189476592],\n     [-73.514071369879, 45.663410883372],\n     [-73.5086661140009, 45.6660376032574],\n     [-73.5066485322791, 45.66770609928],\n     [-73.5052117189194, 45.6694506091685],\n     [-73.4943692754526, 45.6676107175883],\n     [-73.4909285596849, 45.6670796662378],\n     [-73.4868984819315, 45.6668163755988],\n     [-73.4865306288933, 45.6640279113745],\n     [-73.486175917176, 45.6627454155844],\n     [-73.4851106606772, 45.6602901608252],\n     [-73.4846809037125, 45.6588306383762],\n     [-73.4832382097687, 45.6554954757732],\n     [-73.4821383232798, 45.6536179248291],\n     [-73.4787636354681, 45.6491486823782],\n     [-73.4780610185114, 45.6476510142289],\n     [-73.4774799728854, 45.6454905837529],\n     [-73.4872757738587, 45.6469779321365],\n     [-73.4879707187836, 45.6443924831145],\n     [-73.4878427542795, 45.6430795950772],\n     [-73.4879726577335, 45.6412664739826],\n     [-73.4872874543207, 45.6393119798027],\n     [-73.489854112659, 45.6368295683212],\n     [-73.4903943370184, 45.6355618234685],\n     [-73.4916720124889, 45.6345001210246],\n     [-73.4910397713222, 45.6338422392127],\n     [-73.491927215926, 45.6328678840833],\n     [-73.5098459767589, 45.6358225966319],\n     [-73.5202077969535, 45.6374981237725],\n     [-73.5214492864595, 45.6377899174742],\n     [-73.5431729267196, 45.6475814296488],\n     [-73.544738106063, 45.6482242645147]]]},\n  'properties': {'district': '122-Pointe-aux-Trembles'},\n  'id': '122'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5652962272313, 45.6362003202316],\n     [-73.5683379250343, 45.6341956640577],\n     [-73.5740068269183, 45.6304929149516],\n     [-73.5954830428467, 45.6158123480652],\n     [-73.5984976040034, 45.6140840514745],\n     [-73.6023660828945, 45.6125718329314],\n     [-73.6071490341973, 45.6170493269392],\n     [-73.6168081007299, 45.6262083345707],\n     [-73.6206953088035, 45.6298453145986],\n     [-73.6241232556812, 45.6330522649771],\n     [-73.5943845703787, 45.6513674636385],\n     [-73.5898582112291, 45.6537703909976],\n     [-73.5866413495341, 45.6508496604014],\n     [-73.5764976851296, 45.641270099185],\n     [-73.5731131247475, 45.6438386857375],\n     [-73.5713393674169, 45.6418436992789],\n     [-73.5652962272313, 45.6362003202316]]]},\n  'properties': {'district': '123-Rivière-des-Prairies'},\n  'id': '123'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5864681894816, 45.538468592349],\n     [-73.5862704189394, 45.5383757481773],\n     [-73.5863837119054, 45.5382742004886],\n     [-73.5947764680621, 45.5311589735711],\n     [-73.5962116234987, 45.5301487739359],\n     [-73.5992862274652, 45.5288774305058],\n     [-73.6023604559616, 45.5284000664849],\n     [-73.6061835826899, 45.5283650193777],\n     [-73.6088433154766, 45.5276253868022],\n     [-73.6124143323658, 45.525647330045],\n     [-73.6170925681517, 45.5277209056734],\n     [-73.6170629786343, 45.5290836882431],\n     [-73.6174856875378, 45.5297416402976],\n     [-73.6190955864224, 45.5306548374863],\n     [-73.6215351178101, 45.5311083503899],\n     [-73.6182756954315, 45.5346855542873],\n     [-73.6168611349556, 45.5357046144078],\n     [-73.6144250141163, 45.5383032859687],\n     [-73.6123252903368, 45.5421333051385],\n     [-73.6086846729618, 45.5461409843801],\n     [-73.6070387029669, 45.5479557550382],\n     [-73.6035398110948, 45.5463469119155],\n     [-73.5864681894816, 45.538468592349]]]},\n  'properties': {'district': '131-Saint-Édouard'},\n  'id': '131'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5864681894816, 45.538468592349],\n     [-73.6035398110948, 45.5463469119155],\n     [-73.6019471163738, 45.5481178990012],\n     [-73.5975914187124, 45.553207322408],\n     [-73.5910006798707, 45.5607570868859],\n     [-73.5774327397367, 45.5565544731088],\n     [-73.5794549693978, 45.5542318437368],\n     [-73.5755593123915, 45.5530615312237],\n     [-73.5800875168702, 45.5456157629748],\n     [-73.5813297515111, 45.5438139922411],\n     [-73.5837910525824, 45.541331668411],\n     [-73.5864681894816, 45.538468592349]]]},\n  'properties': {'district': '132-Étienne-Desmarteau'},\n  'id': '132'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5592280432661, 45.5399028690768],\n     [-73.5735822926994, 45.5413565189643],\n     [-73.5764926058406, 45.5415739191208],\n     [-73.5786195284797, 45.5415422864787],\n     [-73.5810607519196, 45.5411249464667],\n     [-73.5838512015266, 45.5400861300747],\n     [-73.5862704189394, 45.5383757481773],\n     [-73.5864681894816, 45.538468592349],\n     [-73.5837910525824, 45.541331668411],\n     [-73.5813297515111, 45.5438139922411],\n     [-73.5800875168702, 45.5456157629748],\n     [-73.5755593123915, 45.5530615312237],\n     [-73.5794549693978, 45.5542318437368],\n     [-73.5774327397367, 45.5565544731088],\n     [-73.573735635149, 45.5607975285183],\n     [-73.5588620237975, 45.5560173236498],\n     [-73.5545655421497, 45.55467023726],\n     [-73.5552605710227, 45.5474166761255],\n     [-73.5592280432661, 45.5399028690768]]]},\n  'properties': {'district': '133-Vieux-Rosemont'},\n  'id': '133'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5694653361432, 45.5816630209033],\n     [-73.5683754804235, 45.5814045757918],\n     [-73.5584435150771, 45.578098690279],\n     [-73.5557298683366, 45.5757065336466],\n     [-73.5489772535198, 45.5734435433853],\n     [-73.5544730757919, 45.5652953442798],\n     [-73.5546496932451, 45.5649641797921],\n     [-73.5542787320438, 45.5568764086233],\n     [-73.5545655421497, 45.55467023726],\n     [-73.5588620237975, 45.5560173236498],\n     [-73.573735635149, 45.5607975285183],\n     [-73.5774327397367, 45.5565544731088],\n     [-73.5910006798707, 45.5607570868859],\n     [-73.5863094536503, 45.5661499995185],\n     [-73.5871520545273, 45.566412729231],\n     [-73.584731908233, 45.5691066724291],\n     [-73.5772385119147, 45.5776818920023],\n     [-73.578130955202, 45.5780689341974],\n     [-73.5752604702031, 45.5805388907561],\n     [-73.5739953109801, 45.5819266091566],\n     [-73.5709328233077, 45.5809046758787],\n     [-73.5705509760525, 45.5814521659595],\n     [-73.5694653361432, 45.5816630209033]]]},\n  'properties': {'district': '134-Marie-Victorin'},\n  'id': '134'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.7091218538247, 45.5230954877394],\n     [-73.6918870159802, 45.5123542857052],\n     [-73.6881524418874, 45.5100541386763],\n     [-73.6827840991507, 45.5143258168654],\n     [-73.6744134053365, 45.5090285090174],\n     [-73.6696337564668, 45.5063911079337],\n     [-73.6673976326861, 45.5049604588181],\n     [-73.6660099573454, 45.5044347648104],\n     [-73.6644157884018, 45.5050492992242],\n     [-73.6646042365099, 45.504734675849],\n     [-73.6681641565442, 45.501660999668],\n     [-73.67484741122, 45.4963374151867],\n     [-73.680502634204, 45.4920408059609],\n     [-73.6825483439594, 45.4907732405381],\n     [-73.6860100016197, 45.4896024241141],\n     [-73.677264286648, 45.4836591822082],\n     [-73.6776271543493, 45.4823345919293],\n     [-73.6827804258142, 45.4635658825978],\n     [-73.6933043566279, 45.4703396575946],\n     [-73.6947785817616, 45.4645178045181],\n     [-73.69619836572, 45.4654974169452],\n     [-73.7047665432935, 45.4712980390786],\n     [-73.7080097103994, 45.47334246671],\n     [-73.7221390583544, 45.4826358450705],\n     [-73.7241021941398, 45.4814064808358],\n     [-73.7315756591541, 45.4757155112906],\n     [-73.7423991292426, 45.4671531227456],\n     [-73.74247528623, 45.4670360365519],\n     [-73.7505933570667, 45.4606744381013],\n     [-73.7626492898165, 45.4684579890562],\n     [-73.7623181805669, 45.4687557278511],\n     [-73.7684606113614, 45.4749503791392],\n     [-73.7661156464615, 45.4767286467613],\n     [-73.7690019457786, 45.4785035754768],\n     [-73.7741729540726, 45.4818386741628],\n     [-73.7681746120908, 45.4847594283289],\n     [-73.7678463791609, 45.485678039566],\n     [-73.7664813956616, 45.486482172257],\n     [-73.7666703097771, 45.4885693048636],\n     [-73.7671205072924, 45.4890902137182],\n     [-73.7645934137213, 45.4903739088715],\n     [-73.7681822983003, 45.4944507802049],\n     [-73.767927191601, 45.4946223655038],\n     [-73.7708534417468, 45.4965141396676],\n     [-73.7735738316828, 45.4981363521851],\n     [-73.7716579558132, 45.4991257181769],\n     [-73.7738796981111, 45.5005112554049],\n     [-73.770851931212, 45.5015263075263],\n     [-73.7725018897261, 45.5025715611262],\n     [-73.7709616412032, 45.5034073001873],\n     [-73.7692002551043, 45.5040137486001],\n     [-73.7671979733428, 45.5027497284985],\n     [-73.7653841918245, 45.5034648112556],\n     [-73.764601206777, 45.5029446682134],\n     [-73.7638615520544, 45.5034953365559],\n     [-73.7609994664832, 45.5016752336666],\n     [-73.7594306007504, 45.5027766325504],\n     [-73.7582371682807, 45.5020414841587],\n     [-73.756652358581, 45.5024589334263],\n     [-73.7547114610262, 45.5033812068779],\n     [-73.7530750881715, 45.5037537247243],\n     [-73.7516832079437, 45.5044227485347],\n     [-73.7550630650544, 45.5065238038199],\n     [-73.7350991221023, 45.5137867482303],\n     [-73.7285097856203, 45.5160943522463],\n     [-73.7358987648866, 45.5207291683311],\n     [-73.7317546325275, 45.5236815365784],\n     [-73.7280602953464, 45.5213554616816],\n     [-73.7217031155859, 45.5267447688191],\n     [-73.7185119695016, 45.5247508136954],\n     [-73.7172924358234, 45.5256928911099],\n     [-73.7145584630039, 45.5239315629596],\n     [-73.7131122096208, 45.5251637887575],\n     [-73.7112612170074, 45.5243023856458],\n     [-73.7091218538247, 45.5230954877394]]]},\n  'properties': {'district': '141-Côte-de-Liesse'},\n  'id': '141'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.6510270564502, 45.5263874543882],\n     [-73.6529551695687, 45.5229726150982],\n     [-73.6580588118226, 45.5152954959574],\n     [-73.6620316014577, 45.509044066038],\n     [-73.6644157884018, 45.5050492992242],\n     [-73.6660099573454, 45.5044347648104],\n     [-73.6673976326861, 45.5049604588181],\n     [-73.6696337564668, 45.5063911079337],\n     [-73.6744134053365, 45.5090285090174],\n     [-73.6827840991507, 45.5143258168654],\n     [-73.6881524418874, 45.5100541386763],\n     [-73.6918870159802, 45.5123542857052],\n     [-73.7091218538247, 45.5230954877394],\n     [-73.6963296087078, 45.5276805484038],\n     [-73.6939612003867, 45.528347826243],\n     [-73.6897803429098, 45.5288476157777],\n     [-73.6870922504792, 45.5286619976995],\n     [-73.685195685208, 45.5301022578388],\n     [-73.683124268159, 45.5305991300384],\n     [-73.6764129283619, 45.5321264506516],\n     [-73.6747555535799, 45.5323081056009],\n     [-73.6726124365382, 45.5322997327559],\n     [-73.6698852350516, 45.5318835785726],\n     [-73.6555330494037, 45.5277342274232],\n     [-73.6510270564502, 45.5263874543882]]]},\n  'properties': {'district': '142-Norman-McLaren'},\n  'id': '142'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5696609688417, 45.5965806032278],\n     [-73.5687432258054, 45.5949521683749],\n     [-73.5690474086577, 45.5937353194892],\n     [-73.5686767639537, 45.5922969983003],\n     [-73.5663609324088, 45.5883396268265],\n     [-73.5642246834943, 45.5872483268508],\n     [-73.5642198577817, 45.5854079588427],\n     [-73.563949176176, 45.5849798671748],\n     [-73.566054883333, 45.5843176844033],\n     [-73.5648413846768, 45.5830623920281],\n     [-73.5683754804235, 45.5814045757918],\n     [-73.5694653361432, 45.5816630209033],\n     [-73.5732723109658, 45.5829252463303],\n     [-73.5742252628096, 45.5830975713343],\n     [-73.5790395581623, 45.5832311484519],\n     [-73.5817361693186, 45.5828435232896],\n     [-73.5850508708636, 45.5817485572606],\n     [-73.5872765077617, 45.5817600646972],\n     [-73.5896208961855, 45.5825234015516],\n     [-73.5927968176961, 45.5840511668496],\n     [-73.5944959844225, 45.585037876555],\n     [-73.5961671053026, 45.5863788582573],\n     [-73.5985181690096, 45.5893118445279],\n     [-73.6020871279873, 45.5931683856944],\n     [-73.6032485083726, 45.5942192930034],\n     [-73.6086990780645, 45.5965789895409],\n     [-73.6101417757025, 45.5974474797099],\n     [-73.6116347298964, 45.5986698542549],\n     [-73.6130354491493, 45.6003781128039],\n     [-73.61467650443, 45.601820884038],\n     [-73.6125635803426, 45.604373796832],\n     [-73.6092890313816, 45.6081470965116],\n     [-73.6079197060538, 45.6094530924621],\n     [-73.6062298901187, 45.610759685103],\n     [-73.6048043091085, 45.6094292879636],\n     [-73.6035616092243, 45.6101078733853],\n     [-73.6041139394664, 45.6106174089674],\n     [-73.6028907661658, 45.6112216102358],\n     [-73.6013378366857, 45.6104025414741],\n     [-73.5971103983983, 45.6085989887758],\n     [-73.5889961999073, 45.6050220349527],\n     [-73.5771288919308, 45.599852413767],\n     [-73.576232424293, 45.5992577290381],\n     [-73.5745938220503, 45.5986717630315],\n     [-73.5726733802096, 45.5977479911912],\n     [-73.5696609688417, 45.5965806032278]]]},\n  'properties': {'district': '151-Saint-Léonard-Est'},\n  'id': '151'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5694653361432, 45.5816630209033],\n     [-73.5705509760525, 45.5814521659595],\n     [-73.5709328233077, 45.5809046758787],\n     [-73.5739953109801, 45.5819266091566],\n     [-73.5752604702031, 45.5805388907561],\n     [-73.578130955202, 45.5780689341974],\n     [-73.5772385119147, 45.5776818920023],\n     [-73.584731908233, 45.5691066724291],\n     [-73.5871520545273, 45.566412729231],\n     [-73.5874046777632, 45.5664890958155],\n     [-73.5986381445569, 45.5714628841478],\n     [-73.6016424297946, 45.5727835347079],\n     [-73.6005459884454, 45.5740149891614],\n     [-73.6023753660902, 45.5742891313383],\n     [-73.6043589520305, 45.5755705351656],\n     [-73.6113135304111, 45.5785547523195],\n     [-73.6148320474836, 45.580191303428],\n     [-73.6208358233192, 45.5828408672289],\n     [-73.623269111324, 45.5839925896586],\n     [-73.6242513055513, 45.5846473290482],\n     [-73.6267569494518, 45.5854624669655],\n     [-73.6280749026379, 45.5838243220039],\n     [-73.6288186186035, 45.5832571469721],\n     [-73.6296215435427, 45.5838130726522],\n     [-73.6202427208594, 45.5951205335532],\n     [-73.61467650443, 45.601820884038],\n     [-73.6130354491493, 45.6003781128039],\n     [-73.6116347298964, 45.5986698542549],\n     [-73.6101417757025, 45.5974474797099],\n     [-73.6086990780645, 45.5965789895409],\n     [-73.6032485083726, 45.5942192930034],\n     [-73.6020871279873, 45.5931683856944],\n     [-73.5985181690096, 45.5893118445279],\n     [-73.5961671053026, 45.5863788582573],\n     [-73.5944959844225, 45.585037876555],\n     [-73.5927968176961, 45.5840511668496],\n     [-73.5896208961855, 45.5825234015516],\n     [-73.5872765077617, 45.5817600646972],\n     [-73.5850508708636, 45.5817485572606],\n     [-73.5817361693186, 45.5828435232896],\n     [-73.5790395581623, 45.5832311484519],\n     [-73.5742252628096, 45.5830975713343],\n     [-73.5732723109658, 45.5829252463303],\n     [-73.5694653361432, 45.5816630209033]]]},\n  'properties': {'district': '152-Saint-Léonard-Ouest'},\n  'id': '152'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5390758626817, 45.4883382585102],\n     [-73.5377390087046, 45.4867781311819],\n     [-73.5371875080419, 45.4847716425103],\n     [-73.5390896331107, 45.4789579782257],\n     [-73.5400605004001, 45.4771849523131],\n     [-73.5409045352268, 45.4770676622637],\n     [-73.544790989561, 45.4746546504853],\n     [-73.547897885736, 45.4732945786535],\n     [-73.5519632604747, 45.4713851205092],\n     [-73.5535995299322, 45.4704935184097],\n     [-73.5561183847095, 45.4699434381887],\n     [-73.5571517699907, 45.4699658746605],\n     [-73.5630845502664, 45.4721519332886],\n     [-73.5629773154921, 45.472291406797],\n     [-73.5680238353449, 45.4742185784751],\n     [-73.5698754985218, 45.474526598925],\n     [-73.5723445217071, 45.4741023764959],\n     [-73.5731337395658, 45.4754020852012],\n     [-73.5751431743146, 45.4755655583636],\n     [-73.5776045371727, 45.4761591293053],\n     [-73.5787522413781, 45.4747754151352],\n     [-73.5814009760351, 45.4720006793341],\n     [-73.5831499688592, 45.4704494618721],\n     [-73.586192167734, 45.4685776819367],\n     [-73.5902980753151, 45.4669303074751],\n     [-73.593418397905, 45.4661730419256],\n     [-73.5952982354664, 45.4653094409894],\n     [-73.5969113989909, 45.4662290782584],\n     [-73.6006750017194, 45.4693265503939],\n     [-73.6015380888098, 45.4698449656495],\n     [-73.5995923966142, 45.4714885118876],\n     [-73.5966486659517, 45.4732210790839],\n     [-73.5951347718664, 45.4764466331536],\n     [-73.5861251156067, 45.4827228896224],\n     [-73.5851119123332, 45.4822361060294],\n     [-73.5842729287511, 45.4830658950147],\n     [-73.5806528881796, 45.4855782409323],\n     [-73.5812577295116, 45.4863226865868],\n     [-73.5788051804801, 45.4879299741214],\n     [-73.5726247033677, 45.4927252183027],\n     [-73.5671882558807, 45.4901263927885],\n     [-73.5653550896585, 45.4921968620631],\n     [-73.5630011277184, 45.4954107671203],\n     [-73.561988917097, 45.497173589984],\n     [-73.5607477815647, 45.4979125426916],\n     [-73.5594748545018, 45.4973055470605],\n     [-73.5559728136711, 45.4959454567625],\n     [-73.5545777017417, 45.4947339282606],\n     [-73.5533130105218, 45.4921853756885],\n     [-73.5524160214136, 45.4912899988865],\n     [-73.5507109567109, 45.4904150448375],\n     [-73.5496180493079, 45.4900890295113],\n     [-73.5411879980439, 45.4884799718781],\n     [-73.5398824970348, 45.4880751564896],\n     [-73.5390758626817, 45.4883382585102]]]},\n  'properties': {'district': '161-Saint-HenriPetite-BourgognePointe-Saint-Charles'},\n  'id': '161'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5803978450664, 45.4607242060317],\n     [-73.580383848446, 45.45869859258],\n     [-73.5808415940198, 45.4566805659982],\n     [-73.5822612868352, 45.4543201865436],\n     [-73.5841878662571, 45.4525126975338],\n     [-73.5873026203887, 45.4501968887305],\n     [-73.5895426922804, 45.4479760865095],\n     [-73.590317536462, 45.4476444830976],\n     [-73.5946364116012, 45.4432356001591],\n     [-73.5993656298056, 45.437522488597],\n     [-73.5996082895714, 45.4375187309259],\n     [-73.6118008558715, 45.4394499728596],\n     [-73.6104503124607, 45.4429793199455],\n     [-73.6094089858961, 45.4442773154325],\n     [-73.6052123848584, 45.4479821667775],\n     [-73.6048092799479, 45.4489130784869],\n     [-73.6068239912076, 45.4545470690524],\n     [-73.6136079881888, 45.4576954341346],\n     [-73.6201147014597, 45.4548257159799],\n     [-73.622324477466, 45.4535901866228],\n     [-73.6255515041789, 45.4513172221793],\n     [-73.6269986444794, 45.4501430927141],\n     [-73.6288149375869, 45.4482900651074],\n     [-73.6290926270452, 45.4483946176344],\n     [-73.6320301276656, 45.4495010416609],\n     [-73.6351722497066, 45.4507225431116],\n     [-73.6350181512615, 45.4509158064615],\n     [-73.6314544395632, 45.4527593013115],\n     [-73.6262003043123, 45.4574918787775],\n     [-73.6243236842049, 45.4586553508057],\n     [-73.6200562870233, 45.4616683026575],\n     [-73.6182352042185, 45.4622268121902],\n     [-73.6157015928197, 45.4632907897894],\n     [-73.6121904900053, 45.4648542087469],\n     [-73.6116004847967, 45.4647691120724],\n     [-73.6058459619009, 45.4672881102352],\n     [-73.6056266168094, 45.4676031109374],\n     [-73.6031619796725, 45.4691032161751],\n     [-73.6025897276388, 45.4690525247808],\n     [-73.6015380888098, 45.4698449656495],\n     [-73.6006750017194, 45.4693265503939],\n     [-73.5969113989909, 45.4662290782584],\n     [-73.5952982354664, 45.4653094409894],\n     [-73.593418397905, 45.4661730419256],\n     [-73.5902980753151, 45.4669303074751],\n     [-73.586192167734, 45.4685776819367],\n     [-73.5831499688592, 45.4704494618721],\n     [-73.5814009760351, 45.4720006793341],\n     [-73.5787522413781, 45.4747754151352],\n     [-73.5776045371727, 45.4761591293053],\n     [-73.5751431743146, 45.4755655583636],\n     [-73.5731337395658, 45.4754020852012],\n     [-73.5723445217071, 45.4741023764959],\n     [-73.5717046710668, 45.4728345777635],\n     [-73.5722263249864, 45.4666080942859],\n     [-73.5725692397967, 45.4663779997673],\n     [-73.5760636235492, 45.4665267227639],\n     [-73.57595438059, 45.467731089093],\n     [-73.5774466724098, 45.4677934094142],\n     [-73.5787521792962, 45.467573327191],\n     [-73.5788688512642, 45.4664809032158],\n     [-73.5806776025411, 45.4665598624633],\n     [-73.5810988106852, 45.464472934997],\n     [-73.5809592544789, 45.4629192039784],\n     [-73.5803978450664, 45.4607242060317]]]},\n  'properties': {'district': '162-Saint-PaulÉmard'},\n  'id': '162'},\n {'type': 'Feature',\n  'geometry': {'type': 'MultiPolygon',\n   'coordinates': [[[[-73.561675647957, 45.457026812229],\n      [-73.5636503131163, 45.4570986393357],\n      [-73.5631589565038, 45.4599505630657],\n      [-73.5803978450664, 45.4607242060317],\n      [-73.5809592544789, 45.4629192039784],\n      [-73.5810988106852, 45.464472934997],\n      [-73.5806776025411, 45.4665598624633],\n      [-73.5788688512642, 45.4664809032158],\n      [-73.5787521792962, 45.467573327191],\n      [-73.5774466724098, 45.4677934094142],\n      [-73.57595438059, 45.467731089093],\n      [-73.5760636235492, 45.4665267227639],\n      [-73.5725692397967, 45.4663779997673],\n      [-73.5722263249864, 45.4666080942859],\n      [-73.5717046710668, 45.4728345777635],\n      [-73.5723445217071, 45.4741023764959],\n      [-73.5698754985218, 45.474526598925],\n      [-73.5680238353449, 45.4742185784751],\n      [-73.5629773154921, 45.472291406797],\n      [-73.5630845502664, 45.4721519332886],\n      [-73.5571517699907, 45.4699658746605],\n      [-73.5566742286267, 45.4697998242277],\n      [-73.5564729236705, 45.4697250295537],\n      [-73.5568919723525, 45.4692509155842],\n      [-73.5584979993156, 45.4688652320405],\n      [-73.5594741255912, 45.4680886833354],\n      [-73.5600314607336, 45.4630762897765],\n      [-73.5602070432843, 45.4599716856435],\n      [-73.5604361685249, 45.458927727929],\n      [-73.561675647957, 45.457026812229]]],\n    [[[-73.531939269639, 45.4668762388698],\n      [-73.533282030383, 45.4668218592047],\n      [-73.5391619813113, 45.4629235420104],\n      [-73.5399777236207, 45.4591888832885],\n      [-73.5484320046815, 45.4470555292603],\n      [-73.5539781715369, 45.4450013623675],\n      [-73.5598960265295, 45.4441434701074],\n      [-73.5620234184536, 45.4489835563647],\n      [-73.5598289224776, 45.4530789952144],\n      [-73.5597287567806, 45.4550947277761],\n      [-73.5567932176119, 45.4601984189426],\n      [-73.5563771935053, 45.4660746682549],\n      [-73.5556877795518, 45.4672628311292],\n      [-73.5517365614674, 45.4676966587134],\n      [-73.5498323828317, 45.4691552202777],\n      [-73.5458565074551, 45.4710106620399],\n      [-73.544425597308, 45.472837915256],\n      [-73.541638672543, 45.4745667491612],\n      [-73.539337021061, 45.4752874231563],\n      [-73.5374306118892, 45.4744062177182],\n      [-73.5344866609104, 45.4708347638261],\n      [-73.5344988591904, 45.4699079767891],\n      [-73.5334499941273, 45.4695573443126],\n      [-73.531939269639, 45.4668762388698]]]]},\n  'properties': {'district': \"171-ChamplainL'Île-des-Soeurs\"},\n  'id': '171'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5837871545249, 45.4350036826887],\n     [-73.5993656298056, 45.437522488597],\n     [-73.5946364116012, 45.4432356001591],\n     [-73.590317536462, 45.4476444830976],\n     [-73.5895426922804, 45.4479760865095],\n     [-73.5873026203887, 45.4501968887305],\n     [-73.5841878662571, 45.4525126975338],\n     [-73.5822612868352, 45.4543201865436],\n     [-73.5808415940198, 45.4566805659982],\n     [-73.580383848446, 45.45869859258],\n     [-73.5803978450664, 45.4607242060317],\n     [-73.5631589565038, 45.4599505630657],\n     [-73.5636503131163, 45.4570986393357],\n     [-73.561675647957, 45.457026812229],\n     [-73.5644316339364, 45.4531214887569],\n     [-73.5653629828168, 45.4514382552214],\n     [-73.5666188947215, 45.4498773089445],\n     [-73.5684785830554, 45.4481879229679],\n     [-73.5692699550472, 45.4471436069185],\n     [-73.5705853638418, 45.446107961992],\n     [-73.5722579304274, 45.444469116852],\n     [-73.5739182313887, 45.4433612189646],\n     [-73.5756800507555, 45.4416593600233],\n     [-73.5779148011138, 45.4400830529182],\n     [-73.5837871545249, 45.4350036826887]]]},\n  'properties': {'district': '172-Desmarchais-Crawford'},\n  'id': '172'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5734800018833, 45.5049709717948],\n     [-73.5674300859224, 45.5019457643141],\n     [-73.5651563545969, 45.5009257536286],\n     [-73.5607477815647, 45.4979125426916],\n     [-73.561988917097, 45.497173589984],\n     [-73.5630011277184, 45.4954107671203],\n     [-73.5653550896585, 45.4921968620631],\n     [-73.5671882558807, 45.4901263927885],\n     [-73.5726247033677, 45.4927252183027],\n     [-73.5788051804801, 45.4879299741214],\n     [-73.5812577295116, 45.4863226865868],\n     [-73.583103429338, 45.4880924257338],\n     [-73.5822957867321, 45.4883263628527],\n     [-73.5956809652028, 45.4926003378767],\n     [-73.5964675380297, 45.491704867985],\n     [-73.6012074615523, 45.4937019439569],\n     [-73.6009266859408, 45.4942697485875],\n     [-73.6045082688859, 45.4947060857692],\n     [-73.6068399722065, 45.4952769144878],\n     [-73.6044775189475, 45.4952226776326],\n     [-73.6033750499038, 45.4967309718248],\n     [-73.6009279740317, 45.4988800346583],\n     [-73.5997960499359, 45.5001439911523],\n     [-73.5977240363362, 45.5016310280474],\n     [-73.594737745289, 45.5028667176493],\n     [-73.5941401789655, 45.5033436344744],\n     [-73.5956108742759, 45.504064067218],\n     [-73.591464574106, 45.508070751295],\n     [-73.594191031848, 45.5097864575692],\n     [-73.5935088435771, 45.5105257693584],\n     [-73.5949672180747, 45.5109069459258],\n     [-73.5969012890154, 45.5117790625741],\n     [-73.5969405107098, 45.5130415385056],\n     [-73.598896296438, 45.5131127563816],\n     [-73.5978614314752, 45.5141637733071],\n     [-73.5968586638591, 45.5145623065118],\n     [-73.5918451342772, 45.5144729450241],\n     [-73.5911992183544, 45.5147110287094],\n     [-73.5902604532774, 45.5157813641729],\n     [-73.5893088980696, 45.5167596569974],\n     [-73.582430798457, 45.5136019767056],\n     [-73.5786781018666, 45.5117608495828],\n     [-73.5785542203746, 45.5104511791594],\n     [-73.580113792401, 45.5081921313855],\n     [-73.5734800018833, 45.5049709717948]]]},\n  'properties': {'district': '181-Peter-McGill'},\n  'id': '181'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5448806808084, 45.5191907535652],\n     [-73.5464379266303, 45.5162854363298],\n     [-73.5468620115755, 45.513857976795],\n     [-73.5492190367874, 45.5102959685844],\n     [-73.5477210054261, 45.5103219824598],\n     [-73.5469030036515, 45.5117340389128],\n     [-73.54657799908, 45.5129270270646],\n     [-73.5456969917881, 45.512357998283],\n     [-73.5463640090514, 45.5093239803744],\n     [-73.549695020088, 45.5092129682665],\n     [-73.550625003396, 45.5079019928894],\n     [-73.5467100456003, 45.5077980094498],\n     [-73.5469790182482, 45.5067930379886],\n     [-73.5514338758453, 45.5065970940784],\n     [-73.5520469804644, 45.5053720127641],\n     [-73.5473670408895, 45.5056550070783],\n     [-73.5477130465639, 45.5046509913685],\n     [-73.552496564402, 45.5042657932143],\n     [-73.5528089694944, 45.5031050102093],\n     [-73.5479779947401, 45.5032960364571],\n     [-73.5483780292475, 45.5021840045336],\n     [-73.5528640439438, 45.5019529839854],\n     [-73.5523299208625, 45.4996958153214],\n     [-73.5495469612169, 45.4995170102135],\n     [-73.5501069846685, 45.4916750357777],\n     [-73.5489559611269, 45.4916509673111],\n     [-73.5478249505202, 45.4998100412319],\n     [-73.5461249747178, 45.4997069784511],\n     [-73.5466960251749, 45.494926043877],\n     [-73.5457370062228, 45.4948960144495],\n     [-73.5450679617308, 45.4999819687279],\n     [-73.5437740015677, 45.5074910365882],\n     [-73.5427789476457, 45.5078289768043],\n     [-73.542713028629, 45.5014819775582],\n     [-73.5425669725168, 45.4977269821455],\n     [-73.5411679652957, 45.4913989983256],\n     [-73.5390758626817, 45.4883382585102],\n     [-73.5398824970348, 45.4880751564896],\n     [-73.5411879980439, 45.4884799718781],\n     [-73.5496180493079, 45.4900890295113],\n     [-73.5507109567109, 45.4904150448375],\n     [-73.5524160214136, 45.4912899988865],\n     [-73.5533130105218, 45.4921853756885],\n     [-73.5545777017417, 45.4947339282606],\n     [-73.5559728136711, 45.4959454567625],\n     [-73.5594748545018, 45.4973055470605],\n     [-73.5607477815647, 45.4979125426916],\n     [-73.5651563545969, 45.5009257536286],\n     [-73.5674300859224, 45.5019457643141],\n     [-73.5734800018833, 45.5049709717948],\n     [-73.5729588302122, 45.5055494196855],\n     [-73.571279665339, 45.5083636436696],\n     [-73.5655904084691, 45.5210639273018],\n     [-73.5655614721314, 45.5213643879447],\n     [-73.5652383566418, 45.5238165260966],\n     [-73.5568838251035, 45.5199246700318],\n     [-73.5537687235941, 45.5232551201153],\n     [-73.5497109051325, 45.521356149553],\n     [-73.5448806808084, 45.5191907535652]]]},\n  'properties': {'district': '182-Saint-Jacques'},\n  'id': '182'},\n {'type': 'Feature',\n  'geometry': {'type': 'MultiPolygon',\n   'coordinates': [[[[-73.5274147442424, 45.5235135910537],\n      [-73.5251840405205, 45.5163154069544],\n      [-73.5190996853698, 45.4964289566498],\n      [-73.5196839019087, 45.4963133966792],\n      [-73.5192316055091, 45.4951826206437],\n      [-73.5235780132462, 45.4965380018334],\n      [-73.5262219855313, 45.4982100373889],\n      [-73.5284680362731, 45.4999489748907],\n      [-73.5298469696156, 45.5015420394984],\n      [-73.5305050225488, 45.5047999722322],\n      [-73.5301920008631, 45.5074609662696],\n      [-73.5284829714113, 45.5111469746062],\n      [-73.5295139475797, 45.521700975721],\n      [-73.5275250227419, 45.5218729979917],\n      [-73.5274147442424, 45.5235135910537]]],\n    [[[-73.5323979026789, 45.5309116588315],\n      [-73.5322539821252, 45.5289053155386],\n      [-73.532650406906, 45.5287379966455],\n      [-73.535473289372, 45.5289601566106],\n      [-73.5360719733226, 45.5280409644584],\n      [-73.535776948517, 45.527560047342],\n      [-73.5344149864457, 45.5272360422621],\n      [-73.5339490789414, 45.5259734056912],\n      [-73.5318552691702, 45.5248370417729],\n      [-73.5314333871381, 45.5222881159041],\n      [-73.5315020312839, 45.5206100031596],\n      [-73.5304254498686, 45.5175627808954],\n      [-73.5300105388493, 45.5149954869969],\n      [-73.530089354595, 45.5141148861583],\n      [-73.5320378226962, 45.5082952174682],\n      [-73.5331764353799, 45.5063562405992],\n      [-73.5349739836122, 45.5062349788193],\n      [-73.5356400145159, 45.5066940066737],\n      [-73.5374959977644, 45.5096790170141],\n      [-73.5387299917373, 45.5139430070273],\n      [-73.5384300301647, 45.5176120247025],\n      [-73.5393770324368, 45.5206029708317],\n      [-73.5399389592119, 45.523607991237],\n      [-73.5390530375969, 45.5256939673896],\n      [-73.5354039767675, 45.5305800445252],\n      [-73.5342920245728, 45.5315010077074],\n      [-73.5329380014176, 45.53135795749],\n      [-73.5323979026789, 45.5309116588315]]],\n    [[[-73.5448806808084, 45.5191907535652],\n      [-73.5497109051325, 45.521356149553],\n      [-73.5537687235941, 45.5232551201153],\n      [-73.5568838251035, 45.5199246700318],\n      [-73.5652383566418, 45.5238165260966],\n      [-73.5641429566138, 45.5273492053093],\n      [-73.5613401818778, 45.5359226324552],\n      [-73.5592280432661, 45.5399028690768],\n      [-73.550216365092, 45.5389758922042],\n      [-73.5482541001656, 45.5383569002539],\n      [-73.5473499781753, 45.5376709059424],\n      [-73.5439636509721, 45.5334697006227],\n      [-73.5428587889529, 45.5304546205586],\n      [-73.5417879567955, 45.529933001055],\n      [-73.542631029933, 45.5272240023092],\n      [-73.5433079570219, 45.5259280339674],\n      [-73.5450449609183, 45.5202409952069],\n      [-73.5448806808084, 45.5191907535652]]]]},\n  'properties': {'district': '183-Sainte-Marie'},\n  'id': '183'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5986381445569, 45.5714628841478],\n     [-73.6027105259893, 45.5669595870386],\n     [-73.6076346034193, 45.5613579100341],\n     [-73.6093881709895, 45.5598575160123],\n     [-73.6142388370954, 45.5543735967318],\n     [-73.6161654573353, 45.5519916303917],\n     [-73.6217484540132, 45.5544783077209],\n     [-73.6240413737876, 45.5555253903511],\n     [-73.6453511352974, 45.5647725775888],\n     [-73.6446417578686, 45.5658132919643],\n     [-73.6362833815582, 45.5758266113331],\n     [-73.6363215300962, 45.5759177646435],\n     [-73.6327012962769, 45.5799802985646],\n     [-73.631962332515, 45.579640175342],\n     [-73.6288186186035, 45.5832571469721],\n     [-73.6280749026379, 45.5838243220039],\n     [-73.6267569494518, 45.5854624669655],\n     [-73.6242513055513, 45.5846473290482],\n     [-73.623269111324, 45.5839925896586],\n     [-73.6208358233192, 45.5828408672289],\n     [-73.6148320474836, 45.580191303428],\n     [-73.6113135304111, 45.5785547523195],\n     [-73.6043589520305, 45.5755705351656],\n     [-73.6023753660902, 45.5742891313383],\n     [-73.6005459884454, 45.5740149891614],\n     [-73.6016424297946, 45.5727835347079],\n     [-73.5986381445569, 45.5714628841478]]]},\n  'properties': {'district': '191-Saint-Michel'},\n  'id': '191'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.5871520545273, 45.566412729231],\n     [-73.5863094536503, 45.5661499995185],\n     [-73.5910006798707, 45.5607570868859],\n     [-73.5975914187124, 45.553207322408],\n     [-73.6019471163738, 45.5481178990012],\n     [-73.6035398110948, 45.5463469119155],\n     [-73.6070387029669, 45.5479557550382],\n     [-73.6086846729618, 45.5461409843801],\n     [-73.6205880272051, 45.5514560807313],\n     [-73.6201064769685, 45.5520190517324],\n     [-73.6235005117779, 45.5536358848324],\n     [-73.6217484540132, 45.5544783077209],\n     [-73.6161654573353, 45.5519916303917],\n     [-73.6142388370954, 45.5543735967318],\n     [-73.6093881709895, 45.5598575160123],\n     [-73.6076346034193, 45.5613579100341],\n     [-73.6027105259893, 45.5669595870386],\n     [-73.5986381445569, 45.5714628841478],\n     [-73.5874046777632, 45.5664890958155],\n     [-73.5871520545273, 45.566412729231]]]},\n  'properties': {'district': '192-François-Perrault'},\n  'id': '192'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.6168611349556, 45.5357046144078],\n     [-73.618922206967, 45.5366652760056],\n     [-73.6407295659042, 45.5429686686779],\n     [-73.6398673671325, 45.5441488267699],\n     [-73.6363671046978, 45.548169415833],\n     [-73.6342899238824, 45.5494628161417],\n     [-73.6325600663803, 45.5499795021129],\n     [-73.6301686544477, 45.5503989164938],\n     [-73.6278096771011, 45.5513024018691],\n     [-73.6235005117779, 45.5536358848324],\n     [-73.6201064769685, 45.5520190517324],\n     [-73.6205880272051, 45.5514560807313],\n     [-73.6086846729618, 45.5461409843801],\n     [-73.6123252903368, 45.5421333051385],\n     [-73.6144250141163, 45.5383032859687],\n     [-73.6168611349556, 45.5357046144078]]]},\n  'properties': {'district': '193-Villeray'},\n  'id': '193'},\n {'type': 'Feature',\n  'geometry': {'type': 'Polygon',\n   'coordinates': [[[-73.6170925681517, 45.5277209056734],\n     [-73.617235572865, 45.5277835686717],\n     [-73.6208637881685, 45.5236565043515],\n     [-73.6221372911404, 45.5240220054992],\n     [-73.6227576775837, 45.5233405909978],\n     [-73.6485543103777, 45.5308320827376],\n     [-73.6407295659042, 45.5429686686779],\n     [-73.618922206967, 45.5366652760056],\n     [-73.6168611349556, 45.5357046144078],\n     [-73.6182756954315, 45.5346855542873],\n     [-73.6215351178101, 45.5311083503899],\n     [-73.6190955864224, 45.5306548374863],\n     [-73.6174856875378, 45.5297416402976],\n     [-73.6170629786343, 45.5290836882431],\n     [-73.6170925681517, 45.5277209056734]]]},\n  'properties': {'district': '194-Parc-Extension'},\n  'id': '194'}]\n\n\n\n\n\ndf = px.data.election() ### 일반적인 데이터 프레임 \ngeojson = px.data.election_geojson() ### json파일 \n\nfig = px.choropleth_mapbox(df,  ### 데이터프레임 \n                           geojson=geojson, ### json파일 \n                           color=\"Bergeron\", ### df에서 코로플레스의 단계를 표시 \n                           locations=\"district\", ### df에 존재하는 연결변수 \n                           featureidkey=\"properties.district\", ### json에 존재하는 연결매개체\n                           center={\"lat\": 45.5517, \"lon\": -73.7073}, \n                           mapbox_style=\"carto-positron\", \n                           zoom=9)\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show(config=dict({'scrollZoom':False}))\n\n\n\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv')\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      행정구역(시군구)별\n      총인구수 (명)\n    \n  \n  \n    \n      0\n      서울특별시\n      9532428\n    \n    \n      1\n      부산광역시\n      3356311\n    \n    \n      2\n      대구광역시\n      2390721\n    \n    \n      3\n      인천광역시\n      2945009\n    \n    \n      4\n      광주광역시\n      1442454\n    \n    \n      5\n      대전광역시\n      1454228\n    \n    \n      6\n      울산광역시\n      1122566\n    \n    \n      7\n      세종특별자치시\n      368276\n    \n    \n      8\n      경기도\n      13549577\n    \n    \n      9\n      강원도\n      1537717\n    \n    \n      10\n      충청북도\n      1596948\n    \n    \n      11\n      충청남도\n      2118977\n    \n    \n      12\n      전라북도\n      1789770\n    \n    \n      13\n      전라남도\n      1834653\n    \n    \n      14\n      경상북도\n      2627925\n    \n    \n      15\n      경상남도\n      3318161\n    \n    \n      16\n      제주특별자치도\n      676569\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nglobal_dict[\"features\"][0][\"properties\"]\n\n{'name': '서울특별시', 'base_year': '2018', 'name_eng': 'Seoul', 'code': '11'}\n\n\n\ngeojson = global_dict ### json파일 \n\nfig = px.choropleth_mapbox(df,  ### 데이터프레임 \n                           geojson=geojson, ### json파일 \n                           color=\"총인구수 (명)\", ### df에서 코로플레스의 단계를 표시 \n                           locations=\"행정구역(시군구)별\", ### df에 존재하는 연결변수 \n                           featureidkey=\"properties.name\", ### json에 존재하는 연결매개체\n                           center={\"lat\": 35.84195368311022, \"lon\": 127.1155556693179}, ## 전주를 center로 잡아줌\n                           mapbox_style=\"carto-positron\", \n                           zoom=6)\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show(config=dict({'scrollZoom':False}))\n\nOutput hidden; open in https://colab.research.google.com to view."
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-16-12wk.html#matplotlib-2개의-y를-겹처그리기",
    "href": "post/Lecture/STDV/2023-05-16-12wk.html#matplotlib-2개의-y를-겹처그리기",
    "title": "10. Choropleth Map",
    "section": "matplotlib : 2개의 y를 겹처그리기",
    "text": "matplotlib : 2개의 y를 겹처그리기\n\ndf.plot.bar(x=\"Date\", y= [\"Samsung\",\"Apple\"],figsize=(10,5))\n\n<Axes: xlabel='Date'>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-16-12wk.html#막대-width-조정",
    "href": "post/Lecture/STDV/2023-05-16-12wk.html#막대-width-조정",
    "title": "10. Choropleth Map",
    "section": "막대 width 조정",
    "text": "막대 width 조정\n\ndf.plot.bar(x=\"Date\",y = [\"Samsung\",\"Apple\"],figsize=(10,5),width = 0.8)\n\n<Axes: xlabel='Date'>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-16-12wk.html#barh-그래프-가로로",
    "href": "post/Lecture/STDV/2023-05-16-12wk.html#barh-그래프-가로로",
    "title": "10. Choropleth Map",
    "section": "barh = 그래프 가로로",
    "text": "barh = 그래프 가로로\n\ndf.plot.barh(x='Date',y=['Samsung','Apple'],figsize=(5,10))\n\n<Axes: ylabel='Date'>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-16-12wk.html#subplot-적용",
    "href": "post/Lecture/STDV/2023-05-16-12wk.html#subplot-적용",
    "title": "10. Choropleth Map",
    "section": "subplot 적용",
    "text": "subplot 적용\n\ndf.plot.bar(x='Date',figsize=(15,10), subplots=True, layout=(4,4), legend=False)\n\narray([[<Axes: title={'center': 'Samsung'}, xlabel='Date'>,\n        <Axes: title={'center': 'Apple'}, xlabel='Date'>,\n        <Axes: title={'center': 'Huawei'}, xlabel='Date'>,\n        <Axes: title={'center': 'Xiaomi'}, xlabel='Date'>],\n       [<Axes: title={'center': 'Oppo'}, xlabel='Date'>,\n        <Axes: title={'center': 'Mobicel'}, xlabel='Date'>,\n        <Axes: title={'center': 'Motorola'}, xlabel='Date'>,\n        <Axes: title={'center': 'LG'}, xlabel='Date'>],\n       [<Axes: title={'center': 'Others'}, xlabel='Date'>,\n        <Axes: title={'center': 'Realme'}, xlabel='Date'>,\n        <Axes: title={'center': 'Google'}, xlabel='Date'>,\n        <Axes: title={'center': 'Nokia'}, xlabel='Date'>],\n       [<Axes: title={'center': 'Lenovo'}, xlabel='Date'>,\n        <Axes: title={'center': 'OnePlus'}, xlabel='Date'>,\n        <Axes: title={'center': 'Sony'}, xlabel='Date'>,\n        <Axes: title={'center': 'Asus'}, xlabel='Date'>]], dtype=object)"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-16-12wk.html#평균시각화",
    "href": "post/Lecture/STDV/2023-05-16-12wk.html#평균시각화",
    "title": "10. Choropleth Map",
    "section": "평균시각화",
    "text": "평균시각화\n\ndf.melt(id_vars = \"Date\")\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Date\n      variable\n      value\n    \n  \n  \n    \n      0\n      2019-10\n      Samsung\n      31.49\n    \n    \n      1\n      2019-11\n      Samsung\n      31.36\n    \n    \n      2\n      2019-12\n      Samsung\n      31.37\n    \n    \n      3\n      2020-01\n      Samsung\n      31.29\n    \n    \n      4\n      2020-02\n      Samsung\n      30.91\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      203\n      2020-06\n      Asus\n      0.75\n    \n    \n      204\n      2020-07\n      Asus\n      0.72\n    \n    \n      205\n      2020-08\n      Asus\n      0.70\n    \n    \n      206\n      2020-09\n      Asus\n      0.67\n    \n    \n      207\n      2020-10\n      Asus\n      0.64\n    \n  \n\n208 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.melt(id_vars = \"Date\").groupby(\"variable\")\\\n    .agg(np.mean).reset_index().\\\n    plot.bar(x=\"variable\",y=\"value\",legend=False)\n\nFutureWarning:\n\nThe operation <function mean at 0x7fbbd41484c0> failed on a column. If any error is raised, this will raise an exception in a future version of pandas. Drop these columns to avoid this warning.\n\n\n\n<Axes: xlabel='variable'>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-16-12wk.html#sorting-후-시각화",
    "href": "post/Lecture/STDV/2023-05-16-12wk.html#sorting-후-시각화",
    "title": "10. Choropleth Map",
    "section": "sorting 후 시각화",
    "text": "sorting 후 시각화\n\ndf.melt(id_vars = \"Date\").groupby(\"variable\")\\\n    .agg(np.mean).reset_index().\\\n    sort_values(\"value\",ascending=False).\\\n    plot.bar(x=\"variable\",y=\"value\",legend=False)\n\nFutureWarning:\n\nThe operation <function mean at 0x7fbbd41484c0> failed on a column. If any error is raised, this will raise an exception in a future version of pandas. Drop these columns to avoid this warning.\n\n\n\n<Axes: xlabel='variable'>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-16-12wk.html#plotly",
    "href": "post/Lecture/STDV/2023-05-16-12wk.html#plotly",
    "title": "10. Choropleth Map",
    "section": "plotly",
    "text": "plotly\n\ndf.melt(id_vars=\"Date\").groupby(\"variable\").\\\n        agg(np.mean).reset_index().\\\n        sort_values(\"value\",ascending=False).\\\n        plot.bar(backend=\"plotly\",x=\"variable\",y=\"value\")\n\nFutureWarning:\n\nThe operation <function mean at 0x7fbbd41484c0> failed on a column. If any error is raised, this will raise an exception in a future version of pandas. Drop these columns to avoid this warning.\n\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n회사별 색깔 입히기\n\ndf.melt(id_vars='Date').\\\nplot.bar(x='Date',y='value',color='variable',backend='plotly')\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n삼성, 애플, Huawel만 표시\n\ndf.melt(id_vars=\"Date\").\\\n            query(\"variable == 'Samsung' | variable == 'Apple' | variable == 'Huawei'\").\\\n             plot.bar(x=\"Date\",y=\"value\",color=\"variable\",backend=\"plotly\",barmode=\"group\")\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n+ text 수치\n\ndf.melt(id_vars='Date').query(\"variable=='Samsung' or variable=='Apple' or variable=='Huawei'\" ).\\\nplot.bar(x='Date',y='value',color='variable',\n            backend='plotly',barmode='group',text='value')\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n+ fact_col\n\ndf.melt(id_vars='Date').query(\"variable=='Samsung' or variable=='Apple' or variable=='Huawei'\" ).\\\nplot.bar(x='Date',y='value',color='variable',backend='plotly',facet_col='variable')\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n+ facet_row\n\ndf.melt(id_vars='Date').query(\"variable=='Samsung' or variable=='Apple' or variable=='Huawei'\" ).\\\nplot.bar(y='Date',x='value',color='variable',backend='plotly',facet_row='variable',height=700)"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-13wk.html",
    "href": "post/Lecture/STDV/2023-05-17-13wk.html",
    "title": "11. Choropleth Map & Plotly",
    "section": "",
    "text": "- 각 회사별 비율을 표시하여 그리기\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/kalilurrahman/datasets/main/mobilephonemktshare2020.csv')\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Date\n      Samsung\n      Apple\n      Huawei\n      Xiaomi\n      Oppo\n      Mobicel\n      Motorola\n      LG\n      Others\n      Realme\n      Google\n      Nokia\n      Lenovo\n      OnePlus\n      Sony\n      Asus\n    \n  \n  \n    \n      0\n      2019-10\n      31.49\n      22.09\n      10.02\n      7.79\n      4.10\n      3.15\n      2.41\n      2.40\n      9.51\n      0.54\n      2.35\n      0.95\n      0.96\n      0.70\n      0.84\n      0.74\n    \n    \n      1\n      2019-11\n      31.36\n      22.90\n      10.18\n      8.16\n      4.42\n      3.41\n      2.40\n      2.40\n      9.10\n      0.78\n      0.66\n      0.97\n      0.97\n      0.73\n      0.83\n      0.75\n    \n    \n      2\n      2019-12\n      31.37\n      24.79\n      9.95\n      7.73\n      4.23\n      3.19\n      2.50\n      2.54\n      8.13\n      0.84\n      0.75\n      0.90\n      0.87\n      0.74\n      0.77\n      0.70\n    \n    \n      3\n      2020-01\n      31.29\n      24.76\n      10.61\n      8.10\n      4.25\n      3.02\n      2.42\n      2.40\n      7.55\n      0.88\n      0.69\n      0.88\n      0.86\n      0.79\n      0.80\n      0.69\n    \n    \n      4\n      2020-02\n      30.91\n      25.89\n      10.98\n      7.80\n      4.31\n      2.89\n      2.36\n      2.34\n      7.06\n      0.89\n      0.70\n      0.81\n      0.77\n      0.78\n      0.80\n      0.69\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- 아래와 같은 형태는 우리가 원하는 형태의 시각화가 아님.\n\n\n\ndf.plot.pie(y=\"Samsung\")\n\n<Axes: ylabel='Samsung'>\n\n\n\n\n\n\n\n\n\ndf.set_index(\"Date\").plot.pie(y=\"Samsung\")\n\n<Axes: ylabel='Samsung'>\n\n\n\n\n\n\n\n\ndf.set_index(\"Date\").T.plot.pie(y=\"2019-10\",legend=False)\n\n<Axes: ylabel='2019-10'>\n\n\n\n\n\n\n\n\n\ndf.set_index(\"Date\").T\\\n    .plot.pie(legend=False,subplots=True,layout=(4,4),figsize=(20,20))\n\narray([[<Axes: ylabel='2019-10'>, <Axes: ylabel='2019-11'>,\n        <Axes: ylabel='2019-12'>, <Axes: ylabel='2020-01'>],\n       [<Axes: ylabel='2020-02'>, <Axes: ylabel='2020-03'>,\n        <Axes: ylabel='2020-04'>, <Axes: ylabel='2020-05'>],\n       [<Axes: ylabel='2020-06'>, <Axes: ylabel='2020-07'>,\n        <Axes: ylabel='2020-08'>, <Axes: ylabel='2020-09'>],\n       [<Axes: ylabel='2020-10'>, <Axes: >, <Axes: >, <Axes: >]],\n      dtype=object)"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-13wk.html#예제-1-matplotlib",
    "href": "post/Lecture/STDV/2023-05-17-13wk.html#예제-1-matplotlib",
    "title": "11. Choropleth Map & Plotly",
    "section": "예제 1 (matplotlib)",
    "text": "예제 1 (matplotlib)\n- 월별 점유율의 차이를 구한 후 boxplot을 그려보자\n\ndf.set_index(\"Date\").diff().dropna()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Samsung\n      Apple\n      Huawei\n      Xiaomi\n      Oppo\n      Mobicel\n      Motorola\n      LG\n      Others\n      Realme\n      Google\n      Nokia\n      Lenovo\n      OnePlus\n      Sony\n      Asus\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2019-11\n      -0.13\n      0.81\n      0.16\n      0.37\n      0.32\n      0.26\n      -0.01\n      0.00\n      -0.41\n      0.24\n      -1.69\n      0.02\n      0.01\n      0.03\n      -0.01\n      0.01\n    \n    \n      2019-12\n      0.01\n      1.89\n      -0.23\n      -0.43\n      -0.19\n      -0.22\n      0.10\n      0.14\n      -0.97\n      0.06\n      0.09\n      -0.07\n      -0.10\n      0.01\n      -0.06\n      -0.05\n    \n    \n      2020-01\n      -0.08\n      -0.03\n      0.66\n      0.37\n      0.02\n      -0.17\n      -0.08\n      -0.14\n      -0.58\n      0.04\n      -0.06\n      -0.02\n      -0.01\n      0.05\n      0.03\n      -0.01\n    \n    \n      2020-02\n      -0.38\n      1.13\n      0.37\n      -0.30\n      0.06\n      -0.13\n      -0.06\n      -0.06\n      -0.49\n      0.01\n      0.01\n      -0.07\n      -0.09\n      -0.01\n      0.00\n      0.00\n    \n    \n      2020-03\n      -0.11\n      1.14\n      -0.28\n      -0.10\n      -0.01\n      -0.02\n      -0.01\n      -0.06\n      -0.43\n      0.04\n      0.03\n      -0.09\n      -0.03\n      0.00\n      -0.04\n      -0.03\n    \n    \n      2020-04\n      -0.39\n      1.76\n      -0.42\n      -0.10\n      -0.10\n      -0.12\n      0.16\n      0.00\n      -0.79\n      -0.03\n      0.02\n      -0.03\n      -0.03\n      0.02\n      0.00\n      0.04\n    \n    \n      2020-05\n      -0.23\n      -2.07\n      0.11\n      0.76\n      0.50\n      0.37\n      -0.05\n      -0.09\n      0.47\n      0.14\n      -0.05\n      0.04\n      0.06\n      0.01\n      0.02\n      0.06\n    \n    \n      2020-06\n      0.88\n      -1.46\n      0.30\n      0.19\n      -0.05\n      0.06\n      0.11\n      -0.08\n      0.08\n      0.00\n      -0.02\n      0.01\n      -0.02\n      -0.04\n      0.00\n      -0.01\n    \n    \n      2020-07\n      -0.11\n      -0.44\n      0.06\n      0.39\n      0.04\n      0.28\n      -0.12\n      -0.08\n      0.02\n      0.09\n      -0.03\n      0.02\n      -0.01\n      -0.01\n      -0.03\n      -0.03\n    \n    \n      2020-08\n      0.09\n      0.33\n      -0.02\n      -0.04\n      0.00\n      -0.08\n      -0.06\n      -0.07\n      -0.10\n      0.05\n      -0.02\n      -0.02\n      -0.02\n      -0.01\n      -0.02\n      -0.02\n    \n    \n      2020-09\n      -0.47\n      -0.17\n      -0.15\n      0.59\n      0.25\n      0.12\n      -0.12\n      -0.08\n      -0.19\n      0.27\n      0.00\n      0.00\n      -0.05\n      0.06\n      -0.04\n      -0.03\n    \n    \n      2020-10\n      -0.32\n      1.55\n      -0.14\n      0.18\n      -0.11\n      -0.96\n      -0.06\n      -0.09\n      -0.08\n      0.10\n      0.00\n      -0.05\n      -0.02\n      0.04\n      -0.02\n      -0.03\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.set_index('Date').diff().dropna()\\\n    .boxplot(figsize=(20,5))\n\n<Axes: >\n\n\n\n\n\n- 삼성의 경우는 점유율 변동이 거의 없다.\n- 파이플랏으로 변동폭을 잡기 어려울 때는 아래처럼 변동의 차이를 boxplot로 그려 확인할 수 있다.\n- 음수값에 값이 몰려있으먄 점유율이 전체적으로 떨어지고 있다는 판단을 내릴 수 있다.\n- 또한, 중앙값을 보고 전체적인 추세를 짐작할 수 있다."
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-13wk.html#예제-2-plotly",
    "href": "post/Lecture/STDV/2023-05-17-13wk.html#예제-2-plotly",
    "title": "11. Choropleth Map & Plotly",
    "section": "예제 2 (plotly)",
    "text": "예제 2 (plotly)\n- 위와 동일한 그림을 plotly backend로 그릴 수 있음\n\nimport plotly.express as px\n\n\ndf.set_index(\"Date\").diff().\\\n  dropna().boxplot(backend = \"plotly\")\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n회사별 색깔 추가\n\ndf.set_index(\"Date\").diff().dropna().\\\n        melt().boxplot(backend=\"plotly\",color=\"variable\")"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-13wk.html#예제-3-plotly",
    "href": "post/Lecture/STDV/2023-05-17-13wk.html#예제-3-plotly",
    "title": "11. Choropleth Map & Plotly",
    "section": "예제 3 (plotly)",
    "text": "예제 3 (plotly)\n\ndf = px.data.tips()\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      total_bill\n      tip\n      sex\n      smoker\n      day\n      time\n      size\n    \n  \n  \n    \n      0\n      16.99\n      1.01\n      Female\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      1\n      10.34\n      1.66\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      2\n      21.01\n      3.50\n      Male\n      No\n      Sun\n      Dinner\n      3\n    \n    \n      3\n      23.68\n      3.31\n      Male\n      No\n      Sun\n      Dinner\n      2\n    \n    \n      4\n      24.59\n      3.61\n      Female\n      No\n      Sun\n      Dinner\n      4\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n팁은 대체로 얼마나 받을까?\n\ndf.plot.box(backend = \"plotly\", y = \"tip\")\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n시간대, 성별 구분\n\ndf.plot.box(backend = \"plotly\",\n            x = \"time\",\n            y = \"tip\", color = \"sex\")\n\n\n\n\n\n                                \n                                            \n\n\n\n\n- 점심은 남자가 더 많이 받는 것 같구, 저녁은 비슷하다. (중앙값을 보고 판단)\n- boxplot의 다점은 히스토그램처럼 해당 구간에 점들의 밀집도를 보기 힘들다는 것이다. 즉, 전체적인 경향만 알 수 있다.\n\ndf.plot.box(backend='plotly',\n            y='tip',x='time',\n            color='sex',points='all')\n\n\n\n\n\n                                \n                                            \n\n\n\n\n- point를 그려본 결과 여자보다 남자가 팁을 받는 횟수 자체가 많다. (얼마를 주는지는 알 수 없지만….)\n- boxplot의 아웃라이어가 사라지고, 데이터를 더 리치하게 볼 수 있다!\n\n\n시간대 + 성별 + 요일\n\ndf.plot.box(backend = \"plotly\",\n            x = \"time\",\n            y = \"tip\",\n            color = \"sex\",\n            facet_col = \"day\",\n            points = \"all\")\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n시간대 + 성별 + 요일 + 흡연유무\n\ndf.plot.box(backend = \"plotly\",\n            x = \"time\",\n            y = \"tip\",\n            color = \"sex\",\n            facet_col = \"day\",\n            facet_row = \"smoker\",\n            points = \"all\")\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nsummary\n- tidydata를 이용하면 생각한데로 데이터를 만들기 쉽다.\n- matplotlib는 위와 같은 형태를 그리는 것 자체가 불가능"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-13wk.html#matplotlib",
    "href": "post/Lecture/STDV/2023-05-17-13wk.html#matplotlib",
    "title": "11. Choropleth Map & Plotly",
    "section": "matplotlib",
    "text": "matplotlib\n\ndf = pd.read_csv('https://raw.githubusercontent.com/kalilurrahman/datasets/main/mobilephonemktshare2020.csv')\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Date\n      Samsung\n      Apple\n      Huawei\n      Xiaomi\n      Oppo\n      Mobicel\n      Motorola\n      LG\n      Others\n      Realme\n      Google\n      Nokia\n      Lenovo\n      OnePlus\n      Sony\n      Asus\n    \n  \n  \n    \n      0\n      2019-10\n      31.49\n      22.09\n      10.02\n      7.79\n      4.10\n      3.15\n      2.41\n      2.40\n      9.51\n      0.54\n      2.35\n      0.95\n      0.96\n      0.70\n      0.84\n      0.74\n    \n    \n      1\n      2019-11\n      31.36\n      22.90\n      10.18\n      8.16\n      4.42\n      3.41\n      2.40\n      2.40\n      9.10\n      0.78\n      0.66\n      0.97\n      0.97\n      0.73\n      0.83\n      0.75\n    \n    \n      2\n      2019-12\n      31.37\n      24.79\n      9.95\n      7.73\n      4.23\n      3.19\n      2.50\n      2.54\n      8.13\n      0.84\n      0.75\n      0.90\n      0.87\n      0.74\n      0.77\n      0.70\n    \n    \n      3\n      2020-01\n      31.29\n      24.76\n      10.61\n      8.10\n      4.25\n      3.02\n      2.42\n      2.40\n      7.55\n      0.88\n      0.69\n      0.88\n      0.86\n      0.79\n      0.80\n      0.69\n    \n    \n      4\n      2020-02\n      30.91\n      25.89\n      10.98\n      7.80\n      4.31\n      2.89\n      2.36\n      2.34\n      7.06\n      0.89\n      0.70\n      0.81\n      0.77\n      0.78\n      0.80\n      0.69\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.set_index(\"Date\").plot.area(alpha=0.6,figsize=(15,5))\n\n<Axes: xlabel='Date'>\n\n\n\n\n\n- 시간에 따른 점유율의 변화를 보기에 적절한 것 같다? \\(\\to\\) 내 생각엔 그렇지 않음..\n- 각 회사의 y축 폭은 점유율을 뜻함."
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-13wk.html#plotly",
    "href": "post/Lecture/STDV/2023-05-17-13wk.html#plotly",
    "title": "11. Choropleth Map & Plotly",
    "section": "plotly",
    "text": "plotly\n\ndf.melt(id_vars=\"Date\").\\\n  plot.area(backend = \"plotly\",\n            x = \"Date\",\n            y = \"value\",\n            color = \"variable\")"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-13wk.html#시각화",
    "href": "post/Lecture/STDV/2023-05-17-13wk.html#시각화",
    "title": "11. Choropleth Map & Plotly",
    "section": "시각화",
    "text": "시각화\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\nmat_data.plot.area()\n\n<Axes: >\n\n\n\n\n\n- 한글이 깨저서 나오지만 신경쓰지말장"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-13wk.html#plotly-1",
    "href": "post/Lecture/STDV/2023-05-17-13wk.html#plotly-1",
    "title": "11. Choropleth Map & Plotly",
    "section": "plotly",
    "text": "plotly\n\nmat_data.head().reset_index()\n\n\n\n  \n    \n      \n\n\n  \n    \n      (행정구역별, 행정구역별)\n      index\n      서울특별시\n      부산광역시\n      대구광역시\n      인천광역시\n      광주광역시\n      대전광역시\n      울산광역시\n      세종특별자치시\n      경기도\n      강원도\n      충청북도\n      충청남도\n      전라북도\n      전라남도\n      경상북도\n      경상남도\n      제주특별자치도\n    \n  \n  \n    \n      0\n      2000\n      5.4\n      6.0\n      5.9\n      5.4\n      5.6\n      5.5\n      4.0\n      NaN\n      5.7\n      9.3\n      9.1\n      11.2\n      10.3\n      11.9\n      10.7\n      8.4\n      8.0\n    \n    \n      1\n      2005\n      7.2\n      8.3\n      7.8\n      6.9\n      7.1\n      6.9\n      5.3\n      NaN\n      7.1\n      12.1\n      11.3\n      13.3\n      12.9\n      15.6\n      13.4\n      10.2\n      10.0\n    \n    \n      2\n      2010\n      9.7\n      11.3\n      10.0\n      8.6\n      9.0\n      8.7\n      6.8\n      NaN\n      8.7\n      14.8\n      13.2\n      14.9\n      15.2\n      18.3\n      15.6\n      11.8\n      12.2\n    \n    \n      3\n      2015\n      12.6\n      14.6\n      12.7\n      10.7\n      11.3\n      10.9\n      8.8\n      10.6\n      10.5\n      16.9\n      14.8\n      16.4\n      17.8\n      20.5\n      17.7\n      13.8\n      13.8\n    \n    \n      4\n      2021. 06\n      16.4\n      19.8\n      17.0\n      14.4\n      14.5\n      14.8\n      13.1\n      10.0\n      13.5\n      21.2\n      18.4\n      19.5\n      21.8\n      23.9\n      22.2\n      17.9\n      16.0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nmat_data.reset_index().melt(id_vars = \"index\")\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      index\n      (행정구역별, 행정구역별)\n      value\n    \n  \n  \n    \n      0\n      2000\n      서울특별시\n      5.4\n    \n    \n      1\n      2005\n      서울특별시\n      7.2\n    \n    \n      2\n      2010\n      서울특별시\n      9.7\n    \n    \n      3\n      2015\n      서울특별시\n      12.6\n    \n    \n      4\n      2021. 06\n      서울특별시\n      16.4\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      80\n      2000\n      제주특별자치도\n      8.0\n    \n    \n      81\n      2005\n      제주특별자치도\n      10.0\n    \n    \n      82\n      2010\n      제주특별자치도\n      12.2\n    \n    \n      83\n      2015\n      제주특별자치도\n      13.8\n    \n    \n      84\n      2021. 06\n      제주특별자치도\n      16.0\n    \n  \n\n85 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nl_data = mat_data.reset_index().melt(id_vars = \"index\")\n\n\nl_data.columns = [\"year\", \"city\",\"value\"]\n\n\nl_data.plot.area(backend = \"plotly\", x = \"year\", y = \"value\", color = \"city\")"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-final term.html",
    "href": "post/Lecture/STDV/2023-05-17-final term.html",
    "title": "12. final term",
    "section": "",
    "text": "import requests\nfrom plotnine import *\nfrom IPython.display import HTML\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom IPython.display import HTML\nimport json \nimport folium\n%matplotlib inline"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-final term.html#covid19-시도별-접종률-시각화-30점",
    "href": "post/Lecture/STDV/2023-05-17-final term.html#covid19-시도별-접종률-시각화-30점",
    "title": "12. final term",
    "section": "#1. COVID19 시도별 접종률 시각화 (30점)",
    "text": "#1. COVID19 시도별 접종률 시각화 (30점)\n아래의 그림은 COVID19 예방접종의 시도별 현황을 캡쳐한 것이다.\n\n이 정보는 특정 주기로 업데이트 되며 아래의 웹페이지 2번째 테이블에서 확인할 수 있다.\nhttps://ncv.kdca.go.kr/mainStatus.es?mid=a11702000000\n판다스의 pd.read_html() 함수를 이용해 위의 페이지에서 그림1의 테이블을 읽어오라. 그리고 folium의 choroplethmap을 활용하여 시도별 2차접종의 접종률을 시각화 하라. 시각화 예시는 아래와 같다.\n\n힌트1. pd.read_html()을 이용할때 encoding='utf-8'옵션을 사용하라.\n힌트2. https://ncv.kdca.go.kr/mainStatus.es?mid=a11702000000의 2번째 테이블을 판다스로 읽어온 결과는 아래와 같아야 한다.\n\n#hide_input\ndfhtml='<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr>\\n      <th></th>\\n      <th>구분</th>\\n      <th colspan=\"2\" halign=\"left\">1차접종</th>\\n      <th colspan=\"2\" halign=\"left\">2차접종</th>\\n      <th colspan=\"2\" halign=\"left\">3차(부스터)접종</th>\\n    </tr>\\n    <tr>\\n      <th></th>\\n      <th>구분</th>\\n      <th>당일 실적</th>\\n      <th>당일 누계</th>\\n      <th>당일 실적</th>\\n      <th>당일 누계</th>\\n      <th>당일 실적</th>\\n      <th>당일 누계</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>합계</td>\\n      <td>22505</td>\\n      <td>42618296</td>\\n      <td>60388</td>\\n      <td>41192348</td>\\n      <td>179831</td>\\n      <td>3570414</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>서울</td>\\n      <td>3613</td>\\n      <td>7933770</td>\\n      <td>8315</td>\\n      <td>7698119</td>\\n      <td>34397</td>\\n      <td>649999</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>부산</td>\\n      <td>1355</td>\\n      <td>2724073</td>\\n      <td>2942</td>\\n      <td>2631501</td>\\n      <td>11161</td>\\n      <td>209295</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>대구</td>\\n      <td>925</td>\\n      <td>1895746</td>\\n      <td>2485</td>\\n      <td>1830009</td>\\n      <td>7258</td>\\n      <td>130330</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>인천</td>\\n      <td>1331</td>\\n      <td>2430994</td>\\n      <td>3781</td>\\n      <td>2346905</td>\\n      <td>9049</td>\\n      <td>183461</td>\\n    </tr>\\n    <tr>\\n      <th>5</th>\\n      <td>광주</td>\\n      <td>669</td>\\n      <td>1190287</td>\\n      <td>2190</td>\\n      <td>1143284</td>\\n      <td>4852</td>\\n      <td>108004</td>\\n    </tr>\\n    <tr>\\n      <th>6</th>\\n      <td>대전</td>\\n      <td>659</td>\\n      <td>1173714</td>\\n      <td>1692</td>\\n      <td>1132714</td>\\n      <td>5326</td>\\n      <td>92697</td>\\n    </tr>\\n    <tr>\\n      <th>7</th>\\n      <td>울산</td>\\n      <td>527</td>\\n      <td>908812</td>\\n      <td>1657</td>\\n      <td>879965</td>\\n      <td>3311</td>\\n      <td>59007</td>\\n    </tr>\\n    <tr>\\n      <th>8</th>\\n      <td>세종</td>\\n      <td>146</td>\\n      <td>272339</td>\\n      <td>357</td>\\n      <td>261860</td>\\n      <td>989</td>\\n      <td>22115</td>\\n    </tr>\\n    <tr>\\n      <th>9</th>\\n      <td>경기</td>\\n      <td>6180</td>\\n      <td>11154058</td>\\n      <td>16579</td>\\n      <td>10788551</td>\\n      <td>39809</td>\\n      <td>876808</td>\\n    </tr>\\n    <tr>\\n      <th>10</th>\\n      <td>강원</td>\\n      <td>718</td>\\n      <td>1279589</td>\\n      <td>2076</td>\\n      <td>1237178</td>\\n      <td>6373</td>\\n      <td>118575</td>\\n    </tr>\\n    <tr>\\n      <th>11</th>\\n      <td>충북</td>\\n      <td>668</td>\\n      <td>1352302</td>\\n      <td>2529</td>\\n      <td>1307212</td>\\n      <td>6536</td>\\n      <td>124993</td>\\n    </tr>\\n    <tr>\\n      <th>12</th>\\n      <td>충남</td>\\n      <td>952</td>\\n      <td>1789964</td>\\n      <td>2610</td>\\n      <td>1725422</td>\\n      <td>8595</td>\\n      <td>170692</td>\\n    </tr>\\n    <tr>\\n      <th>13</th>\\n      <td>전북</td>\\n      <td>755</td>\\n      <td>1514759</td>\\n      <td>2705</td>\\n      <td>1460057</td>\\n      <td>9037</td>\\n      <td>160277</td>\\n    </tr>\\n    <tr>\\n      <th>14</th>\\n      <td>전남</td>\\n      <td>792</td>\\n      <td>1575741</td>\\n      <td>2498</td>\\n      <td>1520694</td>\\n      <td>9319</td>\\n      <td>199860</td>\\n    </tr>\\n    <tr>\\n      <th>15</th>\\n      <td>경북</td>\\n      <td>1255</td>\\n      <td>2160015</td>\\n      <td>2942</td>\\n      <td>2085263</td>\\n      <td>9743</td>\\n      <td>193965</td>\\n    </tr>\\n    <tr>\\n      <th>16</th>\\n      <td>경남</td>\\n      <td>1510</td>\\n      <td>2712518</td>\\n      <td>3997</td>\\n      <td>2614451</td>\\n      <td>11699</td>\\n      <td>226671</td>\\n    </tr>\\n    <tr>\\n      <th>17</th>\\n      <td>제주</td>\\n      <td>450</td>\\n      <td>549615</td>\\n      <td>1033</td>\\n      <td>529163</td>\\n      <td>2377</td>\\n      <td>43665</td>\\n    </tr>\\n  </tbody>\\n</table>'\nHTML(dfhtml)\n\n\n\n  \n    \n      \n      구분\n      1차접종\n      2차접종\n      3차(부스터)접종\n    \n    \n      \n      구분\n      당일 실적\n      당일 누계\n      당일 실적\n      당일 누계\n      당일 실적\n      당일 누계\n    \n  \n  \n    \n      0\n      합계\n      22505\n      42618296\n      60388\n      41192348\n      179831\n      3570414\n    \n    \n      1\n      서울\n      3613\n      7933770\n      8315\n      7698119\n      34397\n      649999\n    \n    \n      2\n      부산\n      1355\n      2724073\n      2942\n      2631501\n      11161\n      209295\n    \n    \n      3\n      대구\n      925\n      1895746\n      2485\n      1830009\n      7258\n      130330\n    \n    \n      4\n      인천\n      1331\n      2430994\n      3781\n      2346905\n      9049\n      183461\n    \n    \n      5\n      광주\n      669\n      1190287\n      2190\n      1143284\n      4852\n      108004\n    \n    \n      6\n      대전\n      659\n      1173714\n      1692\n      1132714\n      5326\n      92697\n    \n    \n      7\n      울산\n      527\n      908812\n      1657\n      879965\n      3311\n      59007\n    \n    \n      8\n      세종\n      146\n      272339\n      357\n      261860\n      989\n      22115\n    \n    \n      9\n      경기\n      6180\n      11154058\n      16579\n      10788551\n      39809\n      876808\n    \n    \n      10\n      강원\n      718\n      1279589\n      2076\n      1237178\n      6373\n      118575\n    \n    \n      11\n      충북\n      668\n      1352302\n      2529\n      1307212\n      6536\n      124993\n    \n    \n      12\n      충남\n      952\n      1789964\n      2610\n      1725422\n      8595\n      170692\n    \n    \n      13\n      전북\n      755\n      1514759\n      2705\n      1460057\n      9037\n      160277\n    \n    \n      14\n      전남\n      792\n      1575741\n      2498\n      1520694\n      9319\n      199860\n    \n    \n      15\n      경북\n      1255\n      2160015\n      2942\n      2085263\n      9743\n      193965\n    \n    \n      16\n      경남\n      1510\n      2712518\n      3997\n      2614451\n      11699\n      226671\n    \n    \n      17\n      제주\n      450\n      549615\n      1033\n      529163\n      2377\n      43665\n    \n  \n\n\n\n힌트3. 아래의 주소에서 json을 파일을 읽어온뒤 choroplethmap을 그릴때 이용하라.\nhttps://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-provinces-2018-geo.json\n힌트4. json파일의 지역명과 https://ncv.kdca.go.kr/mainStatus.es?mid=a11702000000의 지역이름을 맞추어라. json파일의 지역명은 아래와 같은 코드로 확인할 수 있다.\n[global_dict['features'][i]['properties']['name'] for i in range(17)]\n['서울특별시',\n '부산광역시',\n '대구광역시',\n '인천광역시',\n '광주광역시',\n '대전광역시',\n '울산광역시',\n '세종특별자치시',\n '경기도',\n '강원도',\n '충청북도',\n '충청남도',\n '전라북도',\n '전라남도',\n '경상북도',\n '경상남도',\n '제주특별자치도']\n\n여기에서 global_dict는 힌트3의 json파일이 저장된 dictionary 이다\n\n힌트5. 지역별 접종률은 아래의 수식으로 구한다.\n\\[\n\\textbf{서울특별시 2차접종 접종률}=\\frac{\\textbf{서울특별시 2차접종 당일누계}}{\\textbf{서울특별시 인구수}}\n\\]\n이때 위의 수식의 분모에 사용할 시도별 인구수는 아래의 주소에서 얻을 수 있다.\nhttps://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv\n힌트6. eval()을 이용하면 데이터프레임에서 간단한 column별 연산을 수행할 수 있다. (필요하다면 참고하라)\n\n#hide_input\nHTML('<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>prov</th>\\n      <th>prop</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>서울특별시</td>\\n      <td>0.807572</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>부산광역시</td>\\n      <td>0.784046</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>대구광역시</td>\\n      <td>0.765463</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>인천광역시</td>\\n      <td>0.796909</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>광주광역시</td>\\n      <td>0.792597</td>\\n    </tr>\\n    <tr>\\n      <th>5</th>\\n      <td>대전광역시</td>\\n      <td>0.778911</td>\\n    </tr>\\n    <tr>\\n      <th>6</th>\\n      <td>울산광역시</td>\\n      <td>0.783887</td>\\n    </tr>\\n    <tr>\\n      <th>7</th>\\n      <td>세종특별자치시</td>\\n      <td>0.711043</td>\\n    </tr>\\n    <tr>\\n      <th>8</th>\\n      <td>경기도</td>\\n      <td>0.796228</td>\\n    </tr>\\n    <tr>\\n      <th>9</th>\\n      <td>강원도</td>\\n      <td>0.804555</td>\\n    </tr>\\n    <tr>\\n      <th>10</th>\\n      <td>충청북도</td>\\n      <td>0.818569</td>\\n    </tr>\\n    <tr>\\n      <th>11</th>\\n      <td>충청남도</td>\\n      <td>0.814271</td>\\n    </tr>\\n    <tr>\\n      <th>12</th>\\n      <td>전라북도</td>\\n      <td>0.815779</td>\\n    </tr>\\n    <tr>\\n      <th>13</th>\\n      <td>전라남도</td>\\n      <td>0.828873</td>\\n    </tr>\\n    <tr>\\n      <th>14</th>\\n      <td>경상북도</td>\\n      <td>0.793502</td>\\n    </tr>\\n    <tr>\\n      <th>15</th>\\n      <td>경상남도</td>\\n      <td>0.787922</td>\\n    </tr>\\n    <tr>\\n      <th>16</th>\\n      <td>제주특별자치도</td>\\n      <td>0.782127</td>\\n    </tr>\\n  </tbody>\\n</table>')\n\n\n\n  \n    \n      \n      prov\n      prop\n    \n  \n  \n    \n      0\n      서울특별시\n      0.807572\n    \n    \n      1\n      부산광역시\n      0.784046\n    \n    \n      2\n      대구광역시\n      0.765463\n    \n    \n      3\n      인천광역시\n      0.796909\n    \n    \n      4\n      광주광역시\n      0.792597\n    \n    \n      5\n      대전광역시\n      0.778911\n    \n    \n      6\n      울산광역시\n      0.783887\n    \n    \n      7\n      세종특별자치시\n      0.711043\n    \n    \n      8\n      경기도\n      0.796228\n    \n    \n      9\n      강원도\n      0.804555\n    \n    \n      10\n      충청북도\n      0.818569\n    \n    \n      11\n      충청남도\n      0.814271\n    \n    \n      12\n      전라북도\n      0.815779\n    \n    \n      13\n      전라남도\n      0.828873\n    \n    \n      14\n      경상북도\n      0.793502\n    \n    \n      15\n      경상남도\n      0.787922\n    \n    \n      16\n      제주특별자치도\n      0.782127\n    \n  \n\n\n\n\nSol\n\ndf1 = pd.read_html(\"https://ncv.kdca.go.kr/mainStatus.es?mid=a11702000000\",encoding=\"utf-8\")[1]\n\n\ndf1\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      구분\n      1차접종\n      2차접종\n    \n    \n      \n      구분\n      신규\n      누적\n      접종률\n      신규\n      누적\n      접종률\n    \n  \n  \n    \n      0\n      서울\n      8\n      8367868\n      88.6%\n      20\n      8292445\n      87.8%\n    \n    \n      1\n      부산\n      0\n      2861527\n      86.1%\n      3\n      2834159\n      85.3%\n    \n    \n      2\n      대구\n      1\n      2005626\n      84.6%\n      2\n      1983564\n      83.7%\n    \n    \n      3\n      인천\n      2\n      2587198\n      87.2%\n      0\n      2563464\n      86.4%\n    \n    \n      4\n      광주\n      1\n      1254836\n      87.4%\n      0\n      1243835\n      86.7%\n    \n    \n      5\n      대전\n      3\n      1242386\n      85.7%\n      3\n      1230584\n      84.9%\n    \n    \n      6\n      울산\n      0\n      963325\n      86.2%\n      1\n      954338\n      85.4%\n    \n    \n      7\n      세종\n      0\n      302201\n      79.5%\n      1\n      298534\n      78.5%\n    \n    \n      8\n      경기\n      7\n      11606125\n      87.3%\n      16\n      11498671\n      86.5%\n    \n    \n      9\n      강원\n      1\n      1348536\n      87.7%\n      1\n      1337787\n      87%\n    \n    \n      10\n      충북\n      0\n      1425563\n      89.3%\n      4\n      1413625\n      88.6%\n    \n    \n      11\n      충남\n      4\n      1900648\n      89.5%\n      0\n      1883852\n      88.7%\n    \n    \n      12\n      전북\n      2\n      1569019\n      89.1%\n      2\n      1556937\n      88.5%\n    \n    \n      13\n      전남\n      1\n      1639354\n      90.1%\n      2\n      1625911\n      89.4%\n    \n    \n      14\n      경북\n      0\n      2274366\n      87.5%\n      3\n      2251690\n      86.6%\n    \n    \n      15\n      경남\n      0\n      2847601\n      87%\n      6\n      2819918\n      86.1%\n    \n    \n      16\n      제주\n      0\n      590868\n      87.3%\n      0\n      585437\n      86.5%\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf1.columns\n\nMultiIndex([(  '구분',  '구분'),\n            ('1차접종',  '신규'),\n            ('1차접종',  '누적'),\n            ('1차접종', '접종률'),\n            ('2차접종',  '신규'),\n            ('2차접종',  '누적'),\n            ('2차접종', '접종률')],\n           )\n\n\n\ndf1 = df1.set_index((  '구분',  '구분')).stack().\\\n       reset_index().query(\"level_1 == '누적'\").iloc[:,[0,3]].\\\n        reset_index(drop=True)\n\n\ndf1\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      (구분, 구분)\n      2차접종\n    \n  \n  \n    \n      0\n      서울\n      8292445\n    \n    \n      1\n      부산\n      2834159\n    \n    \n      2\n      대구\n      1983564\n    \n    \n      3\n      인천\n      2563464\n    \n    \n      4\n      광주\n      1243835\n    \n    \n      5\n      대전\n      1230584\n    \n    \n      6\n      울산\n      954338\n    \n    \n      7\n      세종\n      298534\n    \n    \n      8\n      경기\n      11498671\n    \n    \n      9\n      강원\n      1337787\n    \n    \n      10\n      충북\n      1413625\n    \n    \n      11\n      충남\n      1883852\n    \n    \n      12\n      전북\n      1556937\n    \n    \n      13\n      전남\n      1625911\n    \n    \n      14\n      경북\n      2251690\n    \n    \n      15\n      경남\n      2819918\n    \n    \n      16\n      제주\n      585437\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npop = pd.read_csv(\"https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv\")\n\n\ndata1 = pd.concat([df1,pop],axis=1).iloc[:,[0,2,1,3]]\n\n\ndata1.columns = [\"city_s\",\"city_l\",\"second\",\"pop\"]\n\n\ndata1.dtypes\n\ncity_s    object\ncity_l    object\nsecond    object\npop        int64\ndtype: object\n\n\n\ndata1[\"city_s\"] = data1[\"city_s\"].astype(\"str\")\ndata1[\"city_l\"] = data1[\"city_l\"].astype(\"str\")\ndata1[\"second\"] = data1[\"second\"].astype(\"int\")\n\n\ndata1 = data1.eval(\"prop = second/pop\")\n\n\nglobal_distriction_jsonurl='https://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-provinces-2018-geo.json'\nglobal_dict = json.loads(requests.get(global_distriction_jsonurl).text)\n\n\nname=[global_dict['features'][i]['properties']['name'] for i in range(17)]\n\n\ndata1\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      city_s\n      city_l\n      second\n      pop\n      prop\n    \n  \n  \n    \n      0\n      서울\n      서울특별시\n      8292445\n      9532428\n      0.869920\n    \n    \n      1\n      부산\n      부산광역시\n      2834159\n      3356311\n      0.844427\n    \n    \n      2\n      대구\n      대구광역시\n      1983564\n      2390721\n      0.829693\n    \n    \n      3\n      인천\n      인천광역시\n      2563464\n      2945009\n      0.870444\n    \n    \n      4\n      광주\n      광주광역시\n      1243835\n      1442454\n      0.862305\n    \n    \n      5\n      대전\n      대전광역시\n      1230584\n      1454228\n      0.846211\n    \n    \n      6\n      울산\n      울산광역시\n      954338\n      1122566\n      0.850140\n    \n    \n      7\n      세종\n      세종특별자치시\n      298534\n      368276\n      0.810626\n    \n    \n      8\n      경기\n      경기도\n      11498671\n      13549577\n      0.848637\n    \n    \n      9\n      강원\n      강원도\n      1337787\n      1537717\n      0.869983\n    \n    \n      10\n      충북\n      충청북도\n      1413625\n      1596948\n      0.885204\n    \n    \n      11\n      충남\n      충청남도\n      1883852\n      2118977\n      0.889038\n    \n    \n      12\n      전북\n      전라북도\n      1556937\n      1789770\n      0.869909\n    \n    \n      13\n      전남\n      전라남도\n      1625911\n      1834653\n      0.886223\n    \n    \n      14\n      경북\n      경상북도\n      2251690\n      2627925\n      0.856832\n    \n    \n      15\n      경남\n      경상남도\n      2819918\n      3318161\n      0.849844\n    \n    \n      16\n      제주\n      제주특별자치도\n      585437\n      676569\n      0.865303\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nm = folium.Map([35.84195368311022, 127.1155556693179], zoom_start=7,scrollWheelZoom = False)\n\nchoro = folium.Choropleth(data = data1,\n                           geo_data = global_dict,\n                           columns = [\"city_l\",\"prop\"],  ## 우리가 표현할 변수\n                           key_on = 'feature.properties.name') ## 지리정보데이터의 key값\n\nchoro.add_to(m)\nm\n\nOutput hidden; open in https://colab.research.google.com to view."
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-final term.html#covid19-시도별월별-확산과정-시각화-40점",
    "href": "post/Lecture/STDV/2023-05-17-final term.html#covid19-시도별월별-확산과정-시각화-40점",
    "title": "12. final term",
    "section": "#2. COVID19 시도별/월별 확산과정 시각화 (40점)",
    "text": "#2. COVID19 시도별/월별 확산과정 시각화 (40점)\n아래는 COVID19 확진자수를 지역별로 매일 기록한 자료이다.\nhttps://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/covid19_20211202.csv\n자료를 판다스로 불러온 결과는 아래와 같다.\n\n#hide_input\nHTML('<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>일자</th>\\n      <th>계(명)</th>\\n      <th>서울</th>\\n      <th>부산</th>\\n      <th>대구</th>\\n      <th>인천</th>\\n      <th>광주</th>\\n      <th>대전</th>\\n      <th>울산</th>\\n      <th>세종</th>\\n      <th>경기</th>\\n      <th>강원</th>\\n      <th>충북</th>\\n      <th>충남</th>\\n      <th>전북</th>\\n      <th>전남</th>\\n      <th>경북</th>\\n      <th>경남</th>\\n      <th>제주</th>\\n      <th>검역</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>누적(명)</td>\\n      <td>457,612</td>\\n      <td>158,774</td>\\n      <td>16,555</td>\\n      <td>19,114</td>\\n      <td>25,299</td>\\n      <td>6,353</td>\\n      <td>8,809</td>\\n      <td>5,675</td>\\n      <td>1,588</td>\\n      <td>136,546</td>\\n      <td>8,889</td>\\n      <td>8,942</td>\\n      <td>13,174</td>\\n      <td>6,453</td>\\n      <td>4,498</td>\\n      <td>11,471</td>\\n      <td>15,236</td>\\n      <td>3,762</td>\\n      <td>6,474</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>2020-01-20</td>\\n      <td>1</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>1</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>2020-01-21</td>\\n      <td>0</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>2020-01-22</td>\\n      <td>0</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>2020-01-23</td>\\n      <td>0</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>...</th>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n    </tr>\\n    <tr>\\n      <th>679</th>\\n      <td>2021-11-28</td>\\n      <td>3,925</td>\\n      <td>1,673</td>\\n      <td>148</td>\\n      <td>106</td>\\n      <td>278</td>\\n      <td>52</td>\\n      <td>53</td>\\n      <td>4</td>\\n      <td>5</td>\\n      <td>1,090</td>\\n      <td>63</td>\\n      <td>25</td>\\n      <td>121</td>\\n      <td>45</td>\\n      <td>25</td>\\n      <td>103</td>\\n      <td>89</td>\\n      <td>35</td>\\n      <td>10</td>\\n    </tr>\\n    <tr>\\n      <th>680</th>\\n      <td>2021-11-29</td>\\n      <td>3,308</td>\\n      <td>1,393</td>\\n      <td>144</td>\\n      <td>88</td>\\n      <td>233</td>\\n      <td>61</td>\\n      <td>43</td>\\n      <td>2</td>\\n      <td>15</td>\\n      <td>910</td>\\n      <td>56</td>\\n      <td>33</td>\\n      <td>52</td>\\n      <td>49</td>\\n      <td>28</td>\\n      <td>68</td>\\n      <td>86</td>\\n      <td>44</td>\\n      <td>3</td>\\n    </tr>\\n    <tr>\\n      <th>681</th>\\n      <td>2021-11-30</td>\\n      <td>3,032</td>\\n      <td>1,186</td>\\n      <td>79</td>\\n      <td>78</td>\\n      <td>192</td>\\n      <td>52</td>\\n      <td>43</td>\\n      <td>3</td>\\n      <td>22</td>\\n      <td>909</td>\\n      <td>84</td>\\n      <td>59</td>\\n      <td>81</td>\\n      <td>50</td>\\n      <td>36</td>\\n      <td>68</td>\\n      <td>60</td>\\n      <td>22</td>\\n      <td>8</td>\\n    </tr>\\n    <tr>\\n      <th>682</th>\\n      <td>2021-12-01</td>\\n      <td>5,123</td>\\n      <td>2,222</td>\\n      <td>143</td>\\n      <td>86</td>\\n      <td>326</td>\\n      <td>29</td>\\n      <td>88</td>\\n      <td>17</td>\\n      <td>20</td>\\n      <td>1,582</td>\\n      <td>105</td>\\n      <td>48</td>\\n      <td>96</td>\\n      <td>50</td>\\n      <td>40</td>\\n      <td>97</td>\\n      <td>127</td>\\n      <td>27</td>\\n      <td>20</td>\\n    </tr>\\n    <tr>\\n      <th>683</th>\\n      <td>2021-12-02</td>\\n      <td>5,266</td>\\n      <td>2,268</td>\\n      <td>158</td>\\n      <td>70</td>\\n      <td>355</td>\\n      <td>39</td>\\n      <td>166</td>\\n      <td>18</td>\\n      <td>8</td>\\n      <td>1,495</td>\\n      <td>145</td>\\n      <td>49</td>\\n      <td>149</td>\\n      <td>71</td>\\n      <td>39</td>\\n      <td>106</td>\\n      <td>94</td>\\n      <td>31</td>\\n      <td>5</td>\\n    </tr>\\n  </tbody>\\n</table>')\n\n\n\n  \n    \n      \n      일자\n      계(명)\n      서울\n      부산\n      대구\n      인천\n      광주\n      대전\n      울산\n      세종\n      경기\n      강원\n      충북\n      충남\n      전북\n      전남\n      경북\n      경남\n      제주\n      검역\n    \n  \n  \n    \n      0\n      누적(명)\n      457,612\n      158,774\n      16,555\n      19,114\n      25,299\n      6,353\n      8,809\n      5,675\n      1,588\n      136,546\n      8,889\n      8,942\n      13,174\n      6,453\n      4,498\n      11,471\n      15,236\n      3,762\n      6,474\n    \n    \n      1\n      2020-01-20\n      1\n      -\n      -\n      -\n      1\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      2\n      2020-01-21\n      0\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      3\n      2020-01-22\n      0\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      4\n      2020-01-23\n      0\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      679\n      2021-11-28\n      3,925\n      1,673\n      148\n      106\n      278\n      52\n      53\n      4\n      5\n      1,090\n      63\n      25\n      121\n      45\n      25\n      103\n      89\n      35\n      10\n    \n    \n      680\n      2021-11-29\n      3,308\n      1,393\n      144\n      88\n      233\n      61\n      43\n      2\n      15\n      910\n      56\n      33\n      52\n      49\n      28\n      68\n      86\n      44\n      3\n    \n    \n      681\n      2021-11-30\n      3,032\n      1,186\n      79\n      78\n      192\n      52\n      43\n      3\n      22\n      909\n      84\n      59\n      81\n      50\n      36\n      68\n      60\n      22\n      8\n    \n    \n      682\n      2021-12-01\n      5,123\n      2,222\n      143\n      86\n      326\n      29\n      88\n      17\n      20\n      1,582\n      105\n      48\n      96\n      50\n      40\n      97\n      127\n      27\n      20\n    \n    \n      683\n      2021-12-02\n      5,266\n      2,268\n      158\n      70\n      355\n      39\n      166\n      18\n      8\n      1,495\n      145\n      49\n      149\n      71\n      39\n      106\n      94\n      31\n      5\n    \n  \n\n\n\n일별로 기록된 COVID19 확진자수를 월별로 통합한 뒤 2021-01 ~ 2021-10 기간의 발생률을 계산하여 시각화하라. 시각화는 plotly의 choropleth_mapbox를 이용하며 시간의 추이를 표현하기 위해 animation_frame 옵션을 사용한다. 시각화 예시는 아래와 같다.\n\n#hide_input\nHTML(requests.get('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/_html').text)\n\n힌트1. 아래의 주소에서 json을 파일을 읽어온뒤 choroplethmap을 그릴때 이용하라.\nhttps://raw.githubusercontent.com/southkorea/southkorea-maps/master/kostat/2018/json/skorea-provinces-2018-geo.json\n힌트2. json파일의 지역명과 https://ncv.kdca.go.kr/mainStatus.es?mid=a11702000000의 지역이름을 맞추어라. json파일의 지역명은 아래와 같은 코드로 확인할 수 있다.\n[global_dict['features'][i]['properties']['name'] for i in range(17)]\n['서울특별시',\n '부산광역시',\n '대구광역시',\n '인천광역시',\n '광주광역시',\n '대전광역시',\n '울산광역시',\n '세종특별자치시',\n '경기도',\n '강원도',\n '충청북도',\n '충청남도',\n '전라북도',\n '전라남도',\n '경상북도',\n '경상남도',\n '제주특별자치도']\n\n여기에서 global_dict는 힌트1의 json파일이 저장된 dictionary이다.\n\nb힌트3. 각 지역의 월별 발생률은 아래의 수식으로 구한다.\n\\[\n\\textbf{2021-01 서울특별시 코로나 발생률}=\\frac{\\textbf{2021-01 서울특별시 코로나 확진자수 총합}}{\\textbf{서울특별시 인구수}}\n\\]\n이때 위의 수식의 분모에 사용할 시도별 인구수는 아래의 주소에서 얻을 수 있다.\nhttps://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-11-22-prov.csv\n힌트4. 3,925와 같은 문자열을 처리하기 위해 아래를 참고하라.\n힌트5. 시각화에 사용할 데이터프레임의 형태 및 핵심정보는 아래와 같다.\n\n#hide_input\nHTML('<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>ym</th>\\n      <th>prov</th>\\n      <th>confirmed</th>\\n      <th>pop</th>\\n      <th>prop</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>2021-01</td>\\n      <td>강원도</td>\\n      <td>488.0</td>\\n      <td>1537717</td>\\n      <td>0.000317</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>2021-02</td>\\n      <td>강원도</td>\\n      <td>169.0</td>\\n      <td>1537717</td>\\n      <td>0.000110</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>2021-03</td>\\n      <td>강원도</td>\\n      <td>466.0</td>\\n      <td>1537717</td>\\n      <td>0.000303</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>2021-04</td>\\n      <td>강원도</td>\\n      <td>354.0</td>\\n      <td>1537717</td>\\n      <td>0.000230</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>2021-05</td>\\n      <td>강원도</td>\\n      <td>501.0</td>\\n      <td>1537717</td>\\n      <td>0.000326</td>\\n    </tr>\\n    <tr>\\n      <th>...</th>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n    </tr>\\n    <tr>\\n      <th>165</th>\\n      <td>2021-06</td>\\n      <td>충청북도</td>\\n      <td>363.0</td>\\n      <td>1596948</td>\\n      <td>0.000227</td>\\n    </tr>\\n    <tr>\\n      <th>166</th>\\n      <td>2021-07</td>\\n      <td>충청북도</td>\\n      <td>544.0</td>\\n      <td>1596948</td>\\n      <td>0.000341</td>\\n    </tr>\\n    <tr>\\n      <th>167</th>\\n      <td>2021-08</td>\\n      <td>충청북도</td>\\n      <td>1302.0</td>\\n      <td>1596948</td>\\n      <td>0.000815</td>\\n    </tr>\\n    <tr>\\n      <th>168</th>\\n      <td>2021-09</td>\\n      <td>충청북도</td>\\n      <td>1192.0</td>\\n      <td>1596948</td>\\n      <td>0.000746</td>\\n    </tr>\\n    <tr>\\n      <th>169</th>\\n      <td>2021-10</td>\\n      <td>충청북도</td>\\n      <td>1556.0</td>\\n      <td>1596948</td>\\n      <td>0.000974</td>\\n    </tr>\\n  </tbody>\\n</table>')\n\n힌트 6. 데이터프레임이 힌트5와 유사한 형태로 정리되었다는 가정하에 적절한 시각화 코드는 아래와 같다. (여기에서 df는 힌트5의 데이터프레임, global_dict는 힌트1~2의 json파일이 저장된 dictionary 이다)\nimport plotly.express as px\nfrom IPython.display import HTML\nfig=px.choropleth_mapbox(df, \n                     geojson=global_dict, \n                     color=??,\n                     locations=?? \n                     animation_frame='ym',\n                     featureidkey=??,\n                     center={\"lat\": 36, \"lon\": 128},\n                     mapbox_style=\"carto-positron\", \n                     range_color=(0, df.prop.max()),\n                     height=1200,\n                     zoom=6.5)\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nHTML(fig.to_html(include_mathjax=False, config=dict({'scrollZoom':False})))\n\n주의: 컴퓨터 사양에 따라 시각화이후 컴퓨터가 심하게 느려질 수 있는데 이 경우 반드시 주피터 노트북에 시각화 결과를 임베딩하여 제출할 필요가 없다. 즉 아래의 같이 시각화를 수행하는 코드를 따로 주석처리 하여 제출하여도 무방하다. (의미가 이해안되면 질문하세요.)\n\n힌트 7. 아래의 자료에서 -는 확진자가 없다는 의미이므로 0으로 생각해도 무방하다.\n\n#hide_input\nHTML('<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>일자</th>\\n      <th>계(명)</th>\\n      <th>서울</th>\\n      <th>부산</th>\\n      <th>대구</th>\\n      <th>인천</th>\\n      <th>광주</th>\\n      <th>대전</th>\\n      <th>울산</th>\\n      <th>세종</th>\\n      <th>경기</th>\\n      <th>강원</th>\\n      <th>충북</th>\\n      <th>충남</th>\\n      <th>전북</th>\\n      <th>전남</th>\\n      <th>경북</th>\\n      <th>경남</th>\\n      <th>제주</th>\\n      <th>검역</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>누적(명)</td>\\n      <td>457,612</td>\\n      <td>158,774</td>\\n      <td>16,555</td>\\n      <td>19,114</td>\\n      <td>25,299</td>\\n      <td>6,353</td>\\n      <td>8,809</td>\\n      <td>5,675</td>\\n      <td>1,588</td>\\n      <td>136,546</td>\\n      <td>8,889</td>\\n      <td>8,942</td>\\n      <td>13,174</td>\\n      <td>6,453</td>\\n      <td>4,498</td>\\n      <td>11,471</td>\\n      <td>15,236</td>\\n      <td>3,762</td>\\n      <td>6,474</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>2020-01-20</td>\\n      <td>1</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>1</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>2020-01-21</td>\\n      <td>0</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>2020-01-22</td>\\n      <td>0</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>2020-01-23</td>\\n      <td>0</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>...</th>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n    </tr>\\n    <tr>\\n      <th>679</th>\\n      <td>2021-11-28</td>\\n      <td>3,925</td>\\n      <td>1,673</td>\\n      <td>148</td>\\n      <td>106</td>\\n      <td>278</td>\\n      <td>52</td>\\n      <td>53</td>\\n      <td>4</td>\\n      <td>5</td>\\n      <td>1,090</td>\\n      <td>63</td>\\n      <td>25</td>\\n      <td>121</td>\\n      <td>45</td>\\n      <td>25</td>\\n      <td>103</td>\\n      <td>89</td>\\n      <td>35</td>\\n      <td>10</td>\\n    </tr>\\n    <tr>\\n      <th>680</th>\\n      <td>2021-11-29</td>\\n      <td>3,308</td>\\n      <td>1,393</td>\\n      <td>144</td>\\n      <td>88</td>\\n      <td>233</td>\\n      <td>61</td>\\n      <td>43</td>\\n      <td>2</td>\\n      <td>15</td>\\n      <td>910</td>\\n      <td>56</td>\\n      <td>33</td>\\n      <td>52</td>\\n      <td>49</td>\\n      <td>28</td>\\n      <td>68</td>\\n      <td>86</td>\\n      <td>44</td>\\n      <td>3</td>\\n    </tr>\\n    <tr>\\n      <th>681</th>\\n      <td>2021-11-30</td>\\n      <td>3,032</td>\\n      <td>1,186</td>\\n      <td>79</td>\\n      <td>78</td>\\n      <td>192</td>\\n      <td>52</td>\\n      <td>43</td>\\n      <td>3</td>\\n      <td>22</td>\\n      <td>909</td>\\n      <td>84</td>\\n      <td>59</td>\\n      <td>81</td>\\n      <td>50</td>\\n      <td>36</td>\\n      <td>68</td>\\n      <td>60</td>\\n      <td>22</td>\\n      <td>8</td>\\n    </tr>\\n    <tr>\\n      <th>682</th>\\n      <td>2021-12-01</td>\\n      <td>5,123</td>\\n      <td>2,222</td>\\n      <td>143</td>\\n      <td>86</td>\\n      <td>326</td>\\n      <td>29</td>\\n      <td>88</td>\\n      <td>17</td>\\n      <td>20</td>\\n      <td>1,582</td>\\n      <td>105</td>\\n      <td>48</td>\\n      <td>96</td>\\n      <td>50</td>\\n      <td>40</td>\\n      <td>97</td>\\n      <td>127</td>\\n      <td>27</td>\\n      <td>20</td>\\n    </tr>\\n    <tr>\\n      <th>683</th>\\n      <td>2021-12-02</td>\\n      <td>5,266</td>\\n      <td>2,268</td>\\n      <td>158</td>\\n      <td>70</td>\\n      <td>355</td>\\n      <td>39</td>\\n      <td>166</td>\\n      <td>18</td>\\n      <td>8</td>\\n      <td>1,495</td>\\n      <td>145</td>\\n      <td>49</td>\\n      <td>149</td>\\n      <td>71</td>\\n      <td>39</td>\\n      <td>106</td>\\n      <td>94</td>\\n      <td>31</td>\\n      <td>5</td>\\n    </tr>\\n  </tbody>\\n</table>')\n\n\nSol\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/covid19_20211202.csv\").iloc[1:,:]\ndf = df.set_index(\"일자\").iloc[:,1:18]\n\ndf.columns = pd.Index(name)\ndf = df.applymap(lambda x : x.replace(\",\",\"\"))\ndf = df.applymap(lambda x : float(x) if x!=\"-\" else None )\n\n\n_df = df.reset_index().melt(id_vars= \"일자\")\n_df = _df.assign(Year = list(map(lambda x : x.split(\"-\")[0],_df.일자)),\n           Month = list(map(lambda x : x.split(\"-\")[1],_df.일자))).\\\n            query(\"Year == '2021' and (Month !='11' and Month !='12')\").\\\n            groupby([\"variable\",\"Year\",\"Month\"]).agg({\"value\":sum}).reset_index()\n\ndata2 = pd.merge(_df,data1,left_on=\"variable\",right_on=\"city_l\",how=\"left\")\n\ndata2= data2.eval(\"prop2 = value/pop\")\ndata2[\"ym\"] = data2[\"Year\"]  + \"-\" + data2[\"Month\"]\n\n\nfig=px.choropleth_mapbox(data2, \n                         geojson=global_dict, color=\"prop2\", \n                         locations=\"variable\", ## data2 의 key값 \n                        animation_frame='ym', ## data2 의 animation 변수 \n                         featureidkey=\"properties.name\", ## 지리정보 데이터의 key값. \n                         center={\"lat\": 36, \"lon\": 128}, ## 중심위치 \n                         mapbox_style=\"carto-positron\", ## 테마 \n                         range_color=(0, data2.prop2.max()), ## value범위 \n                         height=1200, zoom=6.5) \nfig"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-final term.html#covid19-시도별일별-감염자수-시각화-20점",
    "href": "post/Lecture/STDV/2023-05-17-final term.html#covid19-시도별일별-감염자수-시각화-20점",
    "title": "12. final term",
    "section": "#3. COVID19 시도별/일별 감염자수 시각화 (20점)",
    "text": "#3. COVID19 시도별/일별 감염자수 시각화 (20점)\n아래는 COVID19 확진자수를 지역별로 매일 기록한 자료이다. (#2와 동일한 자료임)\nhttps://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/covid19_20211202.csv\n자료를 판다스로 불러온 결과는 아래와 같다.\n\n#hide_input\nHTML('<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>일자</th>\\n      <th>계(명)</th>\\n      <th>서울</th>\\n      <th>부산</th>\\n      <th>대구</th>\\n      <th>인천</th>\\n      <th>광주</th>\\n      <th>대전</th>\\n      <th>울산</th>\\n      <th>세종</th>\\n      <th>경기</th>\\n      <th>강원</th>\\n      <th>충북</th>\\n      <th>충남</th>\\n      <th>전북</th>\\n      <th>전남</th>\\n      <th>경북</th>\\n      <th>경남</th>\\n      <th>제주</th>\\n      <th>검역</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>0</th>\\n      <td>누적(명)</td>\\n      <td>457,612</td>\\n      <td>158,774</td>\\n      <td>16,555</td>\\n      <td>19,114</td>\\n      <td>25,299</td>\\n      <td>6,353</td>\\n      <td>8,809</td>\\n      <td>5,675</td>\\n      <td>1,588</td>\\n      <td>136,546</td>\\n      <td>8,889</td>\\n      <td>8,942</td>\\n      <td>13,174</td>\\n      <td>6,453</td>\\n      <td>4,498</td>\\n      <td>11,471</td>\\n      <td>15,236</td>\\n      <td>3,762</td>\\n      <td>6,474</td>\\n    </tr>\\n    <tr>\\n      <th>1</th>\\n      <td>2020-01-20</td>\\n      <td>1</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>1</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>2</th>\\n      <td>2020-01-21</td>\\n      <td>0</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>3</th>\\n      <td>2020-01-22</td>\\n      <td>0</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>4</th>\\n      <td>2020-01-23</td>\\n      <td>0</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n      <td>-</td>\\n    </tr>\\n    <tr>\\n      <th>...</th>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n      <td>...</td>\\n    </tr>\\n    <tr>\\n      <th>679</th>\\n      <td>2021-11-28</td>\\n      <td>3,925</td>\\n      <td>1,673</td>\\n      <td>148</td>\\n      <td>106</td>\\n      <td>278</td>\\n      <td>52</td>\\n      <td>53</td>\\n      <td>4</td>\\n      <td>5</td>\\n      <td>1,090</td>\\n      <td>63</td>\\n      <td>25</td>\\n      <td>121</td>\\n      <td>45</td>\\n      <td>25</td>\\n      <td>103</td>\\n      <td>89</td>\\n      <td>35</td>\\n      <td>10</td>\\n    </tr>\\n    <tr>\\n      <th>680</th>\\n      <td>2021-11-29</td>\\n      <td>3,308</td>\\n      <td>1,393</td>\\n      <td>144</td>\\n      <td>88</td>\\n      <td>233</td>\\n      <td>61</td>\\n      <td>43</td>\\n      <td>2</td>\\n      <td>15</td>\\n      <td>910</td>\\n      <td>56</td>\\n      <td>33</td>\\n      <td>52</td>\\n      <td>49</td>\\n      <td>28</td>\\n      <td>68</td>\\n      <td>86</td>\\n      <td>44</td>\\n      <td>3</td>\\n    </tr>\\n    <tr>\\n      <th>681</th>\\n      <td>2021-11-30</td>\\n      <td>3,032</td>\\n      <td>1,186</td>\\n      <td>79</td>\\n      <td>78</td>\\n      <td>192</td>\\n      <td>52</td>\\n      <td>43</td>\\n      <td>3</td>\\n      <td>22</td>\\n      <td>909</td>\\n      <td>84</td>\\n      <td>59</td>\\n      <td>81</td>\\n      <td>50</td>\\n      <td>36</td>\\n      <td>68</td>\\n      <td>60</td>\\n      <td>22</td>\\n      <td>8</td>\\n    </tr>\\n    <tr>\\n      <th>682</th>\\n      <td>2021-12-01</td>\\n      <td>5,123</td>\\n      <td>2,222</td>\\n      <td>143</td>\\n      <td>86</td>\\n      <td>326</td>\\n      <td>29</td>\\n      <td>88</td>\\n      <td>17</td>\\n      <td>20</td>\\n      <td>1,582</td>\\n      <td>105</td>\\n      <td>48</td>\\n      <td>96</td>\\n      <td>50</td>\\n      <td>40</td>\\n      <td>97</td>\\n      <td>127</td>\\n      <td>27</td>\\n      <td>20</td>\\n    </tr>\\n    <tr>\\n      <th>683</th>\\n      <td>2021-12-02</td>\\n      <td>5,266</td>\\n      <td>2,268</td>\\n      <td>158</td>\\n      <td>70</td>\\n      <td>355</td>\\n      <td>39</td>\\n      <td>166</td>\\n      <td>18</td>\\n      <td>8</td>\\n      <td>1,495</td>\\n      <td>145</td>\\n      <td>49</td>\\n      <td>149</td>\\n      <td>71</td>\\n      <td>39</td>\\n      <td>106</td>\\n      <td>94</td>\\n      <td>31</td>\\n      <td>5</td>\\n    </tr>\\n  </tbody>\\n</table>')\n\n다음을 잘 읽고 시각화 하라.\n\n(1) 코로나 확진자 현황을 line plot으로 시각화라. 도시별로 별도의 lineplot을 그리고 x축은 요일, y축은 확진자수로 설정하라. 시각화도구는 판다스데이터프레임의 .plot() 메소드를 사용하고 backend는 matplotlib을 이용하라. 시각화 예시는 아래와 같다.\n(주의) 판다스백엔드를 이용하지 않을 경우 정답으로 인정하지 않음.\n\n\nSol\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ne_name=[global_dict['features'][i]['properties']['name_eng'] for i in range(17)]\n\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/covid19_20211202.csv\").iloc[1:,:]\ndf = df.set_index(\"일자\").iloc[:,1:18]\n\ndf.columns = pd.Index(name)\ndf = df.applymap(lambda x : x.replace(\",\",\"\"))\ndf = df.applymap(lambda x : float(x) if x!=\"-\" else None )\ndf.columns = pd.Index(e_name)\n\n\ndf.plot.line(subplots=True,layout=(9,2),figsize=(15,15))\n\narray([[<Axes: xlabel='일자'>, <Axes: xlabel='일자'>],\n       [<Axes: xlabel='일자'>, <Axes: xlabel='일자'>],\n       [<Axes: xlabel='일자'>, <Axes: xlabel='일자'>],\n       [<Axes: xlabel='일자'>, <Axes: xlabel='일자'>],\n       [<Axes: xlabel='일자'>, <Axes: xlabel='일자'>],\n       [<Axes: xlabel='일자'>, <Axes: xlabel='일자'>],\n       [<Axes: xlabel='일자'>, <Axes: xlabel='일자'>],\n       [<Axes: xlabel='일자'>, <Axes: xlabel='일자'>],\n       [<Axes: xlabel='일자'>, <Axes: xlabel='일자'>]], dtype=object)\n\n\n\n\n\n\n\n\n(2) (1)과 같은 시각화를 plotly backend를 이용하여 시각화 하라. 시각화 예시는 아래와 같다.\n(주의) 판다스백엔드를 이용하지 않을 경우 정답으로 인정하지 않음.\n\n#hide_input\nHTML(requests.get('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/_html2').text)\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\nSol\n\n_df= df.reset_index().melt(\"일자\").\\\n        rename(columns={\"일자\":\"Date\",\"variable\":\"city\"})\n\n\n_df\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Date\n      city\n      value\n    \n  \n  \n    \n      0\n      2020-01-20\n      Seoul\n      NaN\n    \n    \n      1\n      2020-01-21\n      Seoul\n      NaN\n    \n    \n      2\n      2020-01-22\n      Seoul\n      NaN\n    \n    \n      3\n      2020-01-23\n      Seoul\n      NaN\n    \n    \n      4\n      2020-01-24\n      Seoul\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      11606\n      2021-11-28\n      Jeju-do\n      35.0\n    \n    \n      11607\n      2021-11-29\n      Jeju-do\n      44.0\n    \n    \n      11608\n      2021-11-30\n      Jeju-do\n      22.0\n    \n    \n      11609\n      2021-12-01\n      Jeju-do\n      27.0\n    \n    \n      11610\n      2021-12-02\n      Jeju-do\n      31.0\n    \n  \n\n11611 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nfig=_df.plot.line(backend='plotly',\n                 x='Date',\n                 y='value',\n                 color='city',\n                 facet_col='city',\n                 facet_col_wrap=3, \n                 facet_col_spacing=0.04,\n                 height=1000)\nfig.update_yaxes(matches=None,showticklabels=True)"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-final term.html#fifa22-시각화-20점",
    "href": "post/Lecture/STDV/2023-05-17-final term.html#fifa22-시각화-20점",
    "title": "12. final term",
    "section": "#4. FIFA22 시각화 (20점)",
    "text": "#4. FIFA22 시각화 (20점)\n아래는 FIFA22라는 온라인게임에서 선수들의 능력치가 저정된 url이다.\nhttps://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-10-25-FIFA22_official_data.csv\n판다스를 이용하여 위의 데이터를 불러온 이후 물음에 답하라.\n\n(1) 한국선수와 일본선수간 평균능력치를 barplot을 이용하여 비교하라. 비교하고 싶은 능력치의 목록은 아래와 같다.\nabilities=['Crossing', 'Finishing','HeadingAccuracy','ShortPassing',\n           'Volleys', 'Dribbling','Curve','FKAccuracy',\n           'LongPassing','BallControl', 'Acceleration','SprintSpeed',\n           'Agility', 'Reactions','Balance','ShotPower', \n           'Jumping', 'Stamina', 'Strength','LongShots',\n           'Aggression','Interceptions', 'Positioning','Vision',\n           'Penalties', 'Composure', 'StandingTackle','SlidingTackle']\n(주의) 판다스백엔드를 이용하지 않거나 아래의 시각화와 상이한 플랏이 그려질 경우 정답으로 인정하지 않음. - 정답으로 인정하지 않는 예시: 바플랏이 세로로 되어있는 경우, Text가 보이지 않는 경우, 소수점이 3자리 이상 출력 되는 경우, interactive plot이 아닌 경우 등 - 허용예시: 변수들의 정렬 순서가 다른 경우, height가 조금 다를 경우\n\n#hide_input\nHTML(requests.get('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/_html3').text)\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\nSol\n\nabilities=[\"Nationality\",'Crossing', 'Finishing','HeadingAccuracy','ShortPassing',\n           'Volleys', 'Dribbling','Curve','FKAccuracy',\n           'LongPassing','BallControl', 'Acceleration','SprintSpeed',\n           'Agility', 'Reactions','Balance','ShotPower', \n           'Jumping', 'Stamina', 'Strength','LongShots',\n           'Aggression','Interceptions', 'Positioning','Vision',\n           'Penalties', 'Composure', 'StandingTackle','SlidingTackle']\n\n\nfifa = pd.read_csv(\"https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-10-25-FIFA22_official_data.csv\")[abilities]\n\nkj = [\"Korea Republic\",\"Japan\"]\n\n_fifa=fifa.query(\"Nationality == @kj\").groupby(\"Nationality\")\\\n                .agg(np.mean).reset_index()\\\n                .melt(\"Nationality\")\n\n_fifa[\"value\"] = np.round(_fifa[\"value\"],2)\n\n\n_fifa[\"Nationality\"]=list(map(lambda x : x.split(\" \")[0],_fifa.Nationality))\n\n\n_fifa.plot.barh(backend=\"plotly\",y=\"variable\",\n                                            x=\"value\",color=\"Nationality\",text=\"value\",\n                                              barmode =\"group\",height=1500,width=1000)\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n\n(2) 선수들의 Overall의 평균값이 가장 높은 상위 20개의 나라와 선수수가 가장 많은 상위 20개의 나라의 교집합을 구하라. 이 나라들에 한정하여 선수들의 평균연령(Age)을 계산한 뒤 평균연령을 정렬하고 barplot으로 시각화 하라. (시각화 패키지는 본인이 선호하는 패키지 사용)\n\nSol\n\nfifa = pd.read_csv(\"https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/2021-10-25-FIFA22_official_data.csv\")\n\n\nmean_table1 = fifa.groupby(\"Nationality\").agg({\"Overall\":np.mean}).reset_index()\nmean_table1[\"overall_rank20\"]=mean_table1[\"Overall\"].rank(ascending=False)<=20\n\nmean_table2 =  fifa.groupby(\"Nationality\").agg({\"Nationality\": \"count\"}).\\\n                    rename(columns = {\"Nationality\":\"count\"}).reset_index()\n\nmean_table2[\"count_rank20\"]=mean_table2[\"count\"].rank(ascending=False)<=20\n\n\nl1 = set(mean_table1.query(\"overall_rank20 == True\")[\"Nationality\"].to_list())\nl2 = set(mean_table2.query(\"count_rank20 == True\")[\"Nationality\"].to_list())\nl3 = list(l1.intersection(l2))\nl3\n\n['Argentina', 'Portugal', 'Brazil']\n\n\n\n_fifa=fifa.query(\"Nationality == @l3\").\\\n                groupby(\"Nationality\").agg({\"Age\": np.mean}).reset_index().sort_values(\"Age\",ascending=False)\n_fifa\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Nationality\n      Age\n    \n  \n  \n    \n      0\n      Argentina\n      27.255319\n    \n    \n      1\n      Brazil\n      26.927961\n    \n    \n      2\n      Portugal\n      25.692090\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n(\n    \nggplot(_fifa,aes(x=\"Nationality\",y=\"Age\")) + \n    geom_bar(aes(fill=\"Nationality\"),stat=\"identity\",alpha=0.5,width=0.4)+\n    theme_bw()+coord_cartesian(ylim=(25,28))\n\n)\n\n\n\n\n<ggplot: (8731109409555)>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-final term.html#정권별-gdp소득불균형-시각화-20점",
    "href": "post/Lecture/STDV/2023-05-17-final term.html#정권별-gdp소득불균형-시각화-20점",
    "title": "12. final term",
    "section": "#5. 정권별 GDP/소득불균형 시각화 (20점)",
    "text": "#5. 정권별 GDP/소득불균형 시각화 (20점)\n아래는 한 가상국가의 최근 18년간 GDP와 불평등지수이다.\n\npd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/gdp_df1.csv')\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      GDP\n      Inequality\n      Government\n    \n  \n  \n    \n      0\n      500\n      56\n      A\n    \n    \n      1\n      550\n      58\n      A\n    \n    \n      2\n      530\n      59\n      A\n    \n    \n      3\n      480\n      61\n      A\n    \n    \n      4\n      550\n      64\n      A\n    \n    \n      5\n      550\n      64\n      B\n    \n    \n      6\n      750\n      66\n      B\n    \n    \n      7\n      560\n      68\n      B\n    \n    \n      8\n      800\n      70\n      B\n    \n    \n      9\n      900\n      65\n      B\n    \n    \n      10\n      900\n      65\n      C\n    \n    \n      11\n      910\n      62\n      C\n    \n    \n      12\n      1100\n      61\n      C\n    \n    \n      13\n      1250\n      58\n      C\n    \n    \n      14\n      1350\n      63\n      C\n    \n    \n      15\n      1350\n      63\n      D\n    \n    \n      16\n      1500\n      57\n      D\n    \n    \n      17\n      1660\n      55\n      D\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n그리고 아래는 각 정권별 평균 인구수를 기록한 것이다. (단위천)\n\npd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/gdp_df2.csv',index_col=0)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n    \n  \n  \n    \n      pop\n      54232.213\n      48823.432\n      46823.453\n      45232.119\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n(1). 정권의 GDP와 불평등지수를 시각화하고 정권은 색깔별로 구분하라. 선의 두깨는 인구수따라 다르게 나타내라. 시각화 예시는 아래와 같다.\n\n\nSol\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/gdp_df1.csv')\npop = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/gdp_df2.csv',index_col=0)\n\n\n_df = pd.merge(df,pop.melt(),left_on=\"Government\",right_on=\"variable\").\\\n                        iloc[:,[0,1,2,4]].rename(columns={\"value\":\"pop\"})\n\n\n(\nggplot(_df) + \n    geom_path(aes(x=\"GDP\",\n                  y=\"Inequality\",\n                  color=\"Government\",\n                  size=\"pop\"))\n\n)\n\n\n\n\n<ggplot: (8731123240132)>\n\n\n\n\n\n(2). 정권 A,C는 진보성향의 정권이며 정권 B,D는 보수셩향의 정권이다. 아래의 데이터프레임을 이용하여 적절한 tidyset을 새로 만들고 정권의 성향을 선의 타입으로 나타내라.\n\npd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/gdp_df3.csv',index_col=0)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      type\n    \n  \n  \n    \n      A\n      prog\n    \n    \n      B\n      cons\n    \n    \n      C\n      prog\n    \n    \n      D\n      cons\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n(주의) 주어진 데이터프레임을 변형 및 merge 하지 않고 직접입력등으로 tidy dataframe을 생성할 경우 답안으로 인정하지 않음. (즉 확장불가능한 코드는 정답으로 인정하지 않음)\n\nSol\n\ndf3 = pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/gdp_df3.csv',index_col=0)\n\n\n_df2= pd.merge(_df,df3.reset_index(),left_on=\"Government\",right_on=\"index\").iloc[:,[0,1,2,3,5]]\n_df2.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      GDP\n      Inequality\n      Government\n      pop\n      type\n    \n  \n  \n    \n      0\n      500\n      56\n      A\n      54232.213\n      prog\n    \n    \n      1\n      550\n      58\n      A\n      54232.213\n      prog\n    \n    \n      2\n      530\n      59\n      A\n      54232.213\n      prog\n    \n    \n      3\n      480\n      61\n      A\n      54232.213\n      prog\n    \n    \n      4\n      550\n      64\n      A\n      54232.213\n      prog\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n(\nggplot(_df2) + \n    geom_path(aes(x=\"GDP\",\n                  y=\"Inequality\",\n                  color=\"Government\",\n                  size=\"pop\",\n                  linetype=\"type\"))\n\n)\n\n\n\n\n<ggplot: (8731123086189)>"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-final term.html#핸드폰-판매량-시각화-50점",
    "href": "post/Lecture/STDV/2023-05-17-final term.html#핸드폰-판매량-시각화-50점",
    "title": "12. final term",
    "section": "#6. 핸드폰 판매량 시각화 (50점)",
    "text": "#6. 핸드폰 판매량 시각화 (50점)\n\n(1) 제조사별 판매량을 lineplot으로 시각화 하라.\n\nSol\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\n\n\ndf.plot.line(x=\"Date\",figsize=(10,7))\n\n<Axes: xlabel='Date'>\n\n\n\n\n\n\n\n\n(2) 제조사별 판매량을 lineplot으로 시각화 하라.\n\nSol\n\ndf.plot.line(x=\"Date\",figsize=(10,15),subplots=True,layout=(4,4))\n\narray([[<Axes: xlabel='Date'>, <Axes: xlabel='Date'>,\n        <Axes: xlabel='Date'>, <Axes: xlabel='Date'>],\n       [<Axes: xlabel='Date'>, <Axes: xlabel='Date'>,\n        <Axes: xlabel='Date'>, <Axes: xlabel='Date'>],\n       [<Axes: xlabel='Date'>, <Axes: xlabel='Date'>,\n        <Axes: xlabel='Date'>, <Axes: xlabel='Date'>],\n       [<Axes: xlabel='Date'>, <Axes: xlabel='Date'>,\n        <Axes: xlabel='Date'>, <Axes: xlabel='Date'>]], dtype=object)\n\n\n\n\n\n\n\n\n(3) 제조사별 판매량을 조사하고 barplot으로 시각화 하라.\n\nSol\n\ndf.melt(\"Date\").groupby(\"variable\").\\\n                agg(sum).reset_index().\\\n                rename(columns={\"variable\":\"manufacturer\", \"value\":\"cumsum\"}).\\\n                sort_values(\"cumsum\",ascending=False).plot.bar(x=\"manufacturer\",y=\"cumsum\",legend=True)\n\n<Axes: xlabel='manufacturer'>\n\n\n\n\n\n\n\n\n(4) 2019년 제조사별 점유율을 조사하고 pieplot으로 시각화 하라.\nNote\n\n제조사별 점유율은 아래와 같이 계산.\n\n\\[\\textbf{2019년 삼성의 점유율} = \\frac{\\textbf{2019년 삼성의 판매량}}{\\textbf{2019년 판매량의 총합}}\\]\n\\[\\textbf{2019년 Apple의 점유율} = \\frac{\\textbf{2019년 Apple의 판매량}}{\\textbf{2019년 판매량의 총합}}\\]\n\\[\\dots\\]\n\nSol\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\nm_list = df.set_index(\"Date\").columns.to_list()\n\n\n_df = df.assign(Year = list(map(lambda x : x.split(\"-\")[0],df.Date))).\\\n            query(\"Year == '2019'\").\\\n            set_index(\"Date\")[m_list]\n_df\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Samsung\n      Apple\n      Huawei\n      Xiaomi\n      Oppo\n      Mobicel\n      Motorola\n      LG\n      Others\n      Realme\n      Google\n      Nokia\n      Lenovo\n      OnePlus\n      Sony\n      Asus\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2019-10\n      461\n      324\n      136\n      109\n      76\n      81\n      43\n      37\n      135\n      28\n      39\n      14\n      22\n      17\n      20\n      17\n    \n    \n      2019-11\n      461\n      358\n      167\n      141\n      86\n      61\n      29\n      36\n      141\n      27\n      29\n      20\n      23\n      10\n      19\n      27\n    \n    \n      2019-12\n      426\n      383\n      143\n      105\n      53\n      45\n      51\n      48\n      129\n      30\n      20\n      26\n      28\n      18\n      18\n      19\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ns_2019= _df.values.sum()\ns_2019\n\n4736\n\n\n\nm_df = (_df.reset_index().melt(\"Date\").groupby(\"variable\").agg(sum)/s_2019).\\\n        sort_values(\"value\",ascending=False).reset_index()\nm_df\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      variable\n      value\n    \n  \n  \n    \n      0\n      Samsung\n      0.284628\n    \n    \n      1\n      Apple\n      0.224873\n    \n    \n      2\n      Huawei\n      0.094172\n    \n    \n      3\n      Others\n      0.085515\n    \n    \n      4\n      Xiaomi\n      0.074958\n    \n    \n      5\n      Oppo\n      0.045397\n    \n    \n      6\n      Mobicel\n      0.039485\n    \n    \n      7\n      Motorola\n      0.025971\n    \n    \n      8\n      LG\n      0.025549\n    \n    \n      9\n      Google\n      0.018581\n    \n    \n      10\n      Realme\n      0.017948\n    \n    \n      11\n      Lenovo\n      0.015414\n    \n    \n      12\n      Asus\n      0.013302\n    \n    \n      13\n      Nokia\n      0.012669\n    \n    \n      14\n      Sony\n      0.012035\n    \n    \n      15\n      OnePlus\n      0.009502\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nm_df.set_index(\"variable\").plot.pie(y=\"value\",legend=False).set_ylabel(\"\")\n\nText(0, 0.5, '')\n\n\n\n\n\n\n\n\n(5) 2019년과 2020년의 월별판매량의 평균을 구하고 바플랏으로 비교하라.\nNote\n\n2019년도의 월별판매량은 아래와 같이 구한다. (그리고 2020년도의 월별판매량도 유사한 수식으로 구한다.)\n\n2019년 10월 판매량 = 삼성의 2019년 10월 판매량 + … + Asus의 2019년 10월 판매량\n2019년 11월 판매량 = 삼성의 2019년 10월 판매량 + … + Asus의 2019년 10월 판매량\n2019년 12월 판매량 = 삼성의 2019년 10월 판매량 + … + Asus의 2019년 10월 판매량\n\n2019년도의 월별판매량의 평균은 아래와 같이 구한다. (그리고 2020년도의 월별판매량의 평균도 유사한 수식으로 구한다. )\n\n\\[\\textbf{2019년 월별 판매량의 평균} = \\frac{\\textbf{2019년 10월 판매량 + 2019년 11월 판매량 + 2019년 12월 판매량}}{3}\\]\n\nSol\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/2021DV/master/_notebooks/phone.csv')\nm_list = df.set_index(\"Date\").columns.to_list()\n\ndf[\"month_sales\"] = list(df.set_index(\"Date\").sum(axis=1))\n\ndf = df.assign(Year = list(map(lambda x : x.split(\"-\")[0],df.Date)),\n               Month = list(map(lambda x : x.split(\"-\")[1],df.Date)))\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Date\n      Samsung\n      Apple\n      Huawei\n      Xiaomi\n      Oppo\n      Mobicel\n      Motorola\n      LG\n      Others\n      Realme\n      Google\n      Nokia\n      Lenovo\n      OnePlus\n      Sony\n      Asus\n      month_sales\n      Year\n      Month\n    \n  \n  \n    \n      0\n      2019-10\n      461\n      324\n      136\n      109\n      76\n      81\n      43\n      37\n      135\n      28\n      39\n      14\n      22\n      17\n      20\n      17\n      1559\n      2019\n      10\n    \n    \n      1\n      2019-11\n      461\n      358\n      167\n      141\n      86\n      61\n      29\n      36\n      141\n      27\n      29\n      20\n      23\n      10\n      19\n      27\n      1635\n      2019\n      11\n    \n    \n      2\n      2019-12\n      426\n      383\n      143\n      105\n      53\n      45\n      51\n      48\n      129\n      30\n      20\n      26\n      28\n      18\n      18\n      19\n      1542\n      2019\n      12\n    \n    \n      3\n      2020-01\n      677\n      494\n      212\n      187\n      110\n      79\n      65\n      49\n      158\n      23\n      13\n      19\n      19\n      22\n      27\n      22\n      2176\n      2020\n      01\n    \n    \n      4\n      2020-02\n      593\n      520\n      217\n      195\n      112\n      67\n      62\n      71\n      157\n      25\n      18\n      16\n      24\n      18\n      23\n      20\n      2138\n      2020\n      02\n    \n    \n      5\n      2020-03\n      637\n      537\n      246\n      187\n      92\n      66\n      59\n      67\n      145\n      21\n      16\n      24\n      18\n      31\n      22\n      14\n      2182\n      2020\n      03\n    \n    \n      6\n      2020-04\n      647\n      583\n      222\n      154\n      98\n      59\n      48\n      64\n      113\n      20\n      23\n      25\n      19\n      19\n      23\n      21\n      2138\n      2020\n      04\n    \n    \n      7\n      2020-05\n      629\n      518\n      192\n      176\n      91\n      87\n      50\n      66\n      150\n      43\n      27\n      15\n      18\n      19\n      19\n      13\n      2113\n      2020\n      05\n    \n    \n      8\n      2020-06\n      663\n      552\n      209\n      185\n      93\n      69\n      54\n      60\n      140\n      39\n      16\n      16\n      17\n      29\n      25\n      16\n      2183\n      2020\n      06\n    \n    \n      9\n      2020-07\n      599\n      471\n      214\n      193\n      89\n      78\n      65\n      59\n      130\n      40\n      27\n      25\n      21\n      18\n      18\n      12\n      2059\n      2020\n      07\n    \n    \n      10\n      2020-08\n      615\n      567\n      204\n      182\n      105\n      82\n      62\n      42\n      129\n      47\n      16\n      23\n      21\n      27\n      23\n      20\n      2165\n      2020\n      08\n    \n    \n      11\n      2020-09\n      621\n      481\n      230\n      220\n      102\n      88\n      56\n      49\n      143\n      54\n      14\n      15\n      17\n      15\n      19\n      15\n      2139\n      2020\n      09\n    \n    \n      12\n      2020-10\n      637\n      555\n      232\n      203\n      90\n      52\n      63\n      49\n      140\n      33\n      17\n      20\n      22\n      9\n      22\n      21\n      2165\n      2020\n      10\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n_df = df.iloc[:,map(lambda x : x not in m_list, df.columns)]\n_df\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Date\n      month_sales\n      Year\n      Month\n    \n  \n  \n    \n      0\n      2019-10\n      1559\n      2019\n      10\n    \n    \n      1\n      2019-11\n      1635\n      2019\n      11\n    \n    \n      2\n      2019-12\n      1542\n      2019\n      12\n    \n    \n      3\n      2020-01\n      2176\n      2020\n      01\n    \n    \n      4\n      2020-02\n      2138\n      2020\n      02\n    \n    \n      5\n      2020-03\n      2182\n      2020\n      03\n    \n    \n      6\n      2020-04\n      2138\n      2020\n      04\n    \n    \n      7\n      2020-05\n      2113\n      2020\n      05\n    \n    \n      8\n      2020-06\n      2183\n      2020\n      06\n    \n    \n      9\n      2020-07\n      2059\n      2020\n      07\n    \n    \n      10\n      2020-08\n      2165\n      2020\n      08\n    \n    \n      11\n      2020-09\n      2139\n      2020\n      09\n    \n    \n      12\n      2020-10\n      2165\n      2020\n      10\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n_df.groupby(\"Year\").agg(np.mean).T\n\n\n\n  \n    \n      \n\n\n  \n    \n      Year\n      2019\n      2020\n    \n  \n  \n    \n      month_sales\n      1578.666667\n      2145.8\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n_df.groupby(\"Year\").agg(np.mean).T.plot.bar(figsize=(10,5))\n\n<Axes: >"
  },
  {
    "objectID": "post/Lecture/STDV/2023-05-17-final term.html#대한민국-출산율-20점",
    "href": "post/Lecture/STDV/2023-05-17-final term.html#대한민국-출산율-20점",
    "title": "12. final term",
    "section": "#7. 대한민국 출산율 (20점)",
    "text": "#7. 대한민국 출산율 (20점)\n다음을 잘 읽고 적절한 시각화 플랏을 제시하라. (판다스 backend를 이용하여 시각화 하며, backend 엔진은 matplotlib, plotly 중 택하여 시각화)\n\n(1) 아래는 “2020년 행정구역별 출생아 수, 출생률, 출산율 통계”를 나타낸 표이다. 행정구역별 출생자수를 pie chart로 시각화하라.\n\nSol\n\ndf = pd.read_html('https://ko.wikipedia.org/wiki/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%EC%9D%98_%EC%9D%B8%EA%B5%AC')[17]\n\n\ndf = df.iloc[:17,:]\n\n\npx.pie(df,names = \"지역\", values= \"출생아 수(천명)\")\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\n\n(2) 아래는 “2001-2019 지역별 합계출산율”을 정리한 데이터이다. (1)의 데이터프레임과 결합하여 “2001-2020 지역별 합계출산율” 데이터를 만들고 lineplot을 이용하여 시각화하라. (\\(x\\)축은 년도, \\(y\\)축은 합계출산율, 지역은색깔로 구분하여 시각화할 것)\n\nSol\n\ndf = pd.read_html('https://ko.wikipedia.org/wiki/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%EC%9D%98_%EC%9D%B8%EA%B5%AC')[17]\ndf2 = pd.read_html('https://ko.wikipedia.org/wiki/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%EC%9D%98_%EC%9D%B8%EA%B5%AC')[18]\n\n\n_df = df.iloc[:17,:]\n_df2 = df2.iloc[1:,:].reset_index(drop=True)\n\n\nsp_2020 = _df.iloc[:,[0,3]]\nsp_2020 = sp_2020.sort_values(\"지역\",ascending=True).reset_index(drop=True).rename(columns={\"합계출산율\" : \"2020\"})\n_df2 = _df2.sort_values(\"Unnamed: 0\",ascending=True).reset_index(drop=True)\n\n\nl = [str(i) for i in range (2001,2021)]\n\n\ntd = pd.concat([sp_2020,_df2],axis=1).set_index(\"지역\")\n\n\n_td = td[l].applymap(lambda x : float(x) if x!=\"-\" else None)\n_td\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      2001\n      2002\n      2003\n      2004\n      2005\n      2006\n      2007\n      2008\n      2009\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n      2020\n    \n    \n      지역\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      강원도\n      1.413\n      1.317\n      1.279\n      1.261\n      1.188\n      1.202\n      1.356\n      1.253\n      1.248\n      1.313\n      1.338\n      1.374\n      1.249\n      1.248\n      1.311\n      1.237\n      1.123\n      1.067\n      1.082\n      1.04\n    \n    \n      경기도\n      1.437\n      1.305\n      1.321\n      1.280\n      1.183\n      1.239\n      1.361\n      1.285\n      1.226\n      1.309\n      1.314\n      1.355\n      1.226\n      1.241\n      1.272\n      1.194\n      1.069\n      1.002\n      0.943\n      0.88\n    \n    \n      경상남도\n      1.417\n      1.272\n      1.290\n      1.266\n      1.189\n      1.254\n      1.434\n      1.368\n      1.323\n      1.413\n      1.446\n      1.503\n      1.367\n      1.409\n      1.437\n      1.358\n      1.227\n      1.122\n      1.046\n      0.95\n    \n    \n      경상북도\n      1.402\n      1.232\n      1.253\n      1.203\n      1.173\n      1.208\n      1.369\n      1.313\n      1.274\n      1.377\n      1.434\n      1.489\n      1.379\n      1.408\n      1.464\n      1.396\n      1.256\n      1.167\n      1.089\n      1.00\n    \n    \n      광주\n      1.421\n      1.264\n      1.278\n      1.203\n      1.105\n      1.152\n      1.262\n      1.198\n      1.137\n      1.223\n      1.234\n      1.295\n      1.170\n      1.199\n      1.207\n      1.168\n      1.053\n      0.972\n      0.913\n      0.81\n    \n    \n      대구\n      1.216\n      1.076\n      1.116\n      1.087\n      1.001\n      1.011\n      1.137\n      1.072\n      1.029\n      1.109\n      1.146\n      1.217\n      1.127\n      1.169\n      1.216\n      1.186\n      1.067\n      0.987\n      0.932\n      0.81\n    \n    \n      대전\n      1.330\n      1.207\n      1.221\n      1.181\n      1.107\n      1.158\n      1.274\n      1.215\n      1.156\n      1.205\n      1.261\n      1.315\n      1.234\n      1.250\n      1.277\n      1.192\n      1.075\n      0.952\n      0.883\n      0.81\n    \n    \n      부산\n      1.103\n      0.975\n      0.988\n      0.953\n      0.887\n      0.915\n      1.024\n      0.980\n      0.940\n      1.045\n      1.078\n      1.135\n      1.049\n      1.090\n      1.139\n      1.095\n      0.976\n      0.899\n      0.827\n      0.75\n    \n    \n      서울\n      1.111\n      1.006\n      1.014\n      1.015\n      0.932\n      0.980\n      1.068\n      1.010\n      0.962\n      1.015\n      1.014\n      1.059\n      0.968\n      0.983\n      1.001\n      0.940\n      0.836\n      0.761\n      0.717\n      0.64\n    \n    \n      세종\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      1.597\n      1.435\n      1.354\n      1.893\n      1.821\n      1.668\n      1.566\n      1.472\n      1.28\n    \n    \n      울산\n      1.423\n      1.242\n      1.280\n      1.241\n      1.186\n      1.242\n      1.403\n      1.338\n      1.308\n      1.369\n      1.393\n      1.481\n      1.391\n      1.437\n      1.486\n      1.418\n      1.261\n      1.131\n      1.084\n      0.99\n    \n    \n      인천\n      1.324\n      1.185\n      1.213\n      1.158\n      1.075\n      1.116\n      1.257\n      1.186\n      1.143\n      1.214\n      1.232\n      1.301\n      1.195\n      1.212\n      1.216\n      1.144\n      1.007\n      1.006\n      0.940\n      0.83\n    \n    \n      전라남도\n      1.566\n      1.391\n      1.389\n      1.360\n      1.290\n      1.337\n      1.542\n      1.449\n      1.445\n      1.537\n      1.568\n      1.642\n      1.518\n      1.497\n      1.549\n      1.466\n      1.325\n      1.240\n      1.234\n      1.15\n    \n    \n      전라북도\n      1.426\n      1.275\n      1.274\n      1.239\n      1.184\n      1.213\n      1.380\n      1.305\n      1.279\n      1.374\n      1.405\n      1.440\n      1.320\n      1.329\n      1.352\n      1.251\n      1.151\n      1.044\n      0.971\n      0.91\n    \n    \n      제주도\n      1.564\n      1.394\n      1.438\n      1.365\n      1.310\n      1.372\n      1.489\n      1.386\n      1.378\n      1.463\n      1.487\n      1.598\n      1.427\n      1.481\n      1.477\n      1.432\n      1.305\n      1.220\n      1.145\n      1.02\n    \n    \n      충청남도\n      1.532\n      1.361\n      1.358\n      1.357\n      1.267\n      1.356\n      1.506\n      1.444\n      1.408\n      1.479\n      1.496\n      1.571\n      1.442\n      1.421\n      1.480\n      1.395\n      1.276\n      1.186\n      1.112\n      1.03\n    \n    \n      충청북도\n      1.426\n      1.294\n      1.270\n      1.272\n      1.195\n      1.233\n      1.398\n      1.319\n      1.317\n      1.402\n      1.428\n      1.485\n      1.365\n      1.363\n      1.414\n      1.358\n      1.235\n      1.172\n      1.050\n      0.98\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n_td.reset_index().melt(\"지역\").rename(columns=({\"variable\": \"Year\"}))\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      지역\n      Year\n      value\n    \n  \n  \n    \n      0\n      강원도\n      2001\n      1.413\n    \n    \n      1\n      경기도\n      2001\n      1.437\n    \n    \n      2\n      경상남도\n      2001\n      1.417\n    \n    \n      3\n      경상북도\n      2001\n      1.402\n    \n    \n      4\n      광주\n      2001\n      1.421\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      335\n      전라남도\n      2020\n      1.150\n    \n    \n      336\n      전라북도\n      2020\n      0.910\n    \n    \n      337\n      제주도\n      2020\n      1.020\n    \n    \n      338\n      충청남도\n      2020\n      1.030\n    \n    \n      339\n      충청북도\n      2020\n      0.980\n    \n  \n\n340 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nfig = _td.reset_index().melt(\"지역\").rename(columns=({\"variable\": \"Year\"})).\\\n                plot.line(backend=\"plotly\",\n                          x=\"Year\",y=\"value\",color=\"지역\",\n                          facet_col=\"지역\",\n                          facet_col_wrap=3,height=1000)\nfig.update_yaxes(matches=None,showticklabels=True)"
  },
  {
    "objectID": "post/Lecture/STML/2022-01-27-Intro.html",
    "href": "post/Lecture/STML/2022-01-27-Intro.html",
    "title": "01. Intro",
    "section": "",
    "text": "- 해당 자료는 전북대학교 통계학과 기계학습특강 강의를 바탕으로 작성하였습니다.\n- 자료출처"
  },
  {
    "objectID": "post/Lecture/STML/2022-01-27-Intro.html#데이터-저장",
    "href": "post/Lecture/STML/2022-01-27-Intro.html#데이터-저장",
    "title": "01. Intro",
    "section": "데이터 저장",
    "text": "데이터 저장\n\n옆에 폴더에 보면 저장된 데이터들을 확인해볼 수 있다!\n안보이는 파일들은 리눅스처럼 숨겨진 파일들을 의미한다.\n\n\nuntar_data(URLs.PETS)/'images'\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:11<00:00]\n    \n    \n\n\nPath('/root/.fastai/data/oxford-iiit-pet/images')\n\n\n\n위의 결과를 보면 해당 경로에 이미지를 저장한 것이다!\n즉, 위의 명령은 URL에서 데이터를 불러오고 해당경로에 데이터를 저장\n\n\nURLs.PETS\n\n'https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz'\n\n\n\n해당 경로를 복사하고 붙여 넣으면 자동으로 다운되는 것이 있는데 untar_data은 압축파일을 해제 후 images 폴더에 저장된다.\n경로로 가보면 압축이 해제되고 이미지 파일이 저장된 것을 볼 수 있으나 열어볼 수는 없음…..\n보고싶다면 아래와 같은 method를 이용하자\n\n\nPILImage.create('/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg')\n\n\n\n\n\nPILImage.create('/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_100.jpg')\n\n\n\n\n\n위 같은 과정이 그렇게 편하진 않은 것 같다.\n\n- 조금 더 쉬운 방법 * 파일들 이름의 리스트로 저장한다면?\n\npath = Path('/root/.fastai/data/oxford-iiit-pet/images')\n\n\nfiles = get_image_files(path)\n\n\nfiles[0]\n\nPath('/root/.fastai/data/oxford-iiit-pet/images/pomeranian_9.jpg')\n\n\n\nPILImage.create('/root/.fastai/data/oxford-iiit-pet/images/saint_bernard_134.jpg')\n\n\n\n\n\nPILImage.create(files[0])\n\n\n\n\n\nprint(files[2])\nPILImage.create(files[2])\n\n/root/.fastai/data/oxford-iiit-pet/images/shiba_inu_187.jpg\n\n\n\n\n\n- 해당 데이터는 앞글자가 대문자면 고양이, 소문자면 강아지이다.\n- 특1 : 앞글자가 대문자면 고양이, 소문자면 강아지 \\(\\to\\) 창의적인 방법임\n- 특2 : 이미지크기가 서로 다르다…\n- 특징 1을 이용해서 한번 강아지와 고양이를 분류해보자"
  },
  {
    "objectID": "post/Lecture/STML/2022-01-27-Intro.html#class-분류-함수-구현",
    "href": "post/Lecture/STML/2022-01-27-Intro.html#class-분류-함수-구현",
    "title": "01. Intro",
    "section": "Class 분류 함수 구현",
    "text": "Class 분류 함수 구현\n\ndef label_func(fname) :\n    if fname[0].isupper() :\n      return \"cat\"\n    else : \n      return \"dog\"\n\n\nlabel_func(\"Egyptian_Mau_120.jpg\")\n\n'cat'\n\n\n\nlabel_func(\"egyptian_Mau_120.jpg\")\n\n'dog'\n\n\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(224))\n\n\npath 경로에서 files에 해당하는 파일들을 불러와서 \\(X\\)를 만들고 정의한 함수 label_func를 적용해 item_tfms에 정의된 방식으로 \\(X\\)를 변환하여 저장한다.\n\n\ndls.show_batch(max_n=8) ## 8개의 그림만보자\n\n\n\n\n\ncnn_learner\n- 1차 목표 : 이미지 파일을 받으면 -> 개인지 고양이인지 판단하는 모형을 만들자\n- 2차 목표 : 그 모형에 새로운 이미지 파일을 전달하여 이미지를 분류할 것이다.\n- cnn_learner 라는 함수를 이용해서 1차 목표와 2차목표를 달성할 object를 만들것임.\n\nclsfr = cnn_learner(dls,resnet34, metrics = error_rate)\n\n/usr/local/lib/python3.8/dist-packages/fastai/vision/learner.py:288: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n- clsfr 필요한 정보 : 우리가 넣어줘야하는 것들이 대부분\n\n모델정보 : 어떠한 분류기를 사용할 것인가\n데이터\n평가기준표 : 채점을 할지표\n\n- clsfr에 필요한 동작 : 이미 구현이 되어있는 것들을 사용\n\n학습\n결과\n예측\n\n- cpu를 써서 학습을 하니 너무 오래 걸리니 런타임 유형을 gpu로 바꿔서 하자\n\nclsfr.fine_tune(1) # 한번 학습\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.149759\n      0.025045\n      0.006766\n      00:16\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.049579\n      0.015034\n      0.004060\n      00:10\n    \n  \n\n\n\n\ntrain error\n- 첫 번째 원소는 고양이일 확률, 두 번째 원소는 강아지일 확률 - TensorBase : 고양이면 1. 강아지면 0\n\nclsfr.predict(files[0])\n\n\n\n\n\n\n\n\n('dog', TensorBase(1), TensorBase([1.0673e-07, 1.0000e+00]))\n\n\n\nclsfr.predict(files[7])\n\n\n\n\n\n\n\n\n('cat', TensorBase(0), TensorBase([9.9998e-01, 1.7946e-05]))\n\n\n- 랜덤으로 예측결과를 살펴보자\n\nclsfr.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\n오답분석\n- 오답을 분석하는 오브젝트를 만들고 그 안에 학습한 clsfr을 집어넣자.\n\ninterpreter = Interpretation.from_learner(clsfr) \n\n\n\n\n\n\n\n\n- 오답중 loss가 큰 상위 9개만 살펴보자\n\ninterpreter.plot_top_losses(9)\n\n\n\n\n\n\n\n\n\n\n\n- 근데 모형이 과적합되어서 학습한거면?? \\(\\to\\) 약간 가슴깊이 의구심이 든다.\n\n\n\ntest error\n\ntest_file = get_image_files('/content')\n\n\ntest_file\n\n(#10) [Path('/content/pexels-fabian-köhler-14652203.jpg'),Path('/content/pexels-brett-sayles-15075137.jpg'),Path('/content/pexels-pixabay-57416.jpg'),Path('/content/pexels-vadim-b-127028.jpg'),Path('/content/pexels-peng-louis-1643457.jpg'),Path('/content/pexels-poodles-doodles-1458926.jpg'),Path('/content/pexels-pixabay-416160.jpg'),Path('/content/pexels-pixabay-45201.jpg'),Path('/content/pexels-poodles-doodles-1458914.jpg'),Path('/content/pexels-batitay-japheth-14308789.jpg')]\n\n\n\nPILImage.create(test_file[0])\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\ntest_file\n\n(#10) [Path('/content/pexels-fabian-köhler-14652203.jpg'),Path('/content/pexels-brett-sayles-15075137.jpg'),Path('/content/pexels-pixabay-57416.jpg'),Path('/content/pexels-vadim-b-127028.jpg'),Path('/content/pexels-peng-louis-1643457.jpg'),Path('/content/pexels-poodles-doodles-1458926.jpg'),Path('/content/pexels-pixabay-416160.jpg'),Path('/content/pexels-pixabay-45201.jpg'),Path('/content/pexels-poodles-doodles-1458914.jpg'),Path('/content/pexels-batitay-japheth-14308789.jpg')]\n\n\n\nPILImage.create(test_file[6])\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\nclsfr.predict(test_file[0])\n\n\n\n\n\n\n\n\n('dog', TensorBase(1), TensorBase([5.4642e-05, 9.9995e-01]))\n\n\n\nclsfr.predict(test_file[6])\n\n\n\n\n\n\n\n\n('cat', TensorBase(0), TensorBase([9.9993e-01, 6.6712e-05]))\n\n\n- 뭐 어느정도는? test 데이터에 대해서도 잘 분류를 하는것 같다!"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html",
    "href": "post/Lecture/STML/2023-02-01-Overview.html",
    "title": "02. Overview",
    "section": "",
    "text": "from fastai.vision.all import * ## 이미지분석\nfrom fastai.collab import * ## 추천시스템\nfrom fastai.text.all import * ## 텍스트 분석\nfrom fastai.vision.gan import * ## GAN (이미지 생성)"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-데이터-정리",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-데이터-정리",
    "title": "02. Overview",
    "section": "1단계 : 데이터 정리",
    "text": "1단계 : 데이터 정리\n- 데이터를 정리해서 ‘dls’ 객체에 저장\n\npath = untar_data(URLs.PETS)/\"images\"\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:08<00:00]\n    \n    \n\n\n\npath.ls() ## 파일 목록\n\n(#7393) [Path('/root/.fastai/data/oxford-iiit-pet/images/Bombay_58.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/British_Shorthair_270.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Birman_56.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/leonberger_150.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_143.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/german_shorthaired_130.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Russian_Blue_114.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/boxer_173.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Birman_121.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Siamese_225.jpg')...]\n\n\n\nfnames = get_image_files(path)\n\n\nfnames ## 이미지들의 리스트를 경로와 함께 저장\n\n(#7390) [Path('/root/.fastai/data/oxford-iiit-pet/images/Bombay_58.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/British_Shorthair_270.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Birman_56.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/leonberger_150.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_143.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/german_shorthaired_130.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Russian_Blue_114.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/boxer_173.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Birman_121.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Siamese_225.jpg')...]\n\n\n- path.ls 와 fnames의 차이 \\(\\to\\) path.ls는 모든 파일들을 리스트로 리턴, fnames는 이미지 파일만 리턴한 것임\n- 아래는 이미지 데이터 로드함수\n[python]\nImageDataLoaders.from_name_func(\n  path,\n  fnames,\n  #label_func,\n  item_tfms = Resize(224)    \n)\n- 지난 시간 함수\n\ndef f(fname) :\n    if fname[0].isupper() :\n        return \"cat\"\n    else :\n        return \"dog\"\n\n- 지난시간 함수 lambda 로 구현\n\nf = lambda fname : \"cat\" if fname[0].isupper() else \"dog\"\n\n\ndls =ImageDataLoaders.from_name_func(\n  path,\n  fnames,\n  f,\n  item_tfms = Resize(224)    \n)\n\n\ndls.show_batch()"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-irnr-오브젝트-생성",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-irnr-오브젝트-생성",
    "title": "02. Overview",
    "section": "2단계 : Irnr 오브젝트 생성",
    "text": "2단계 : Irnr 오브젝트 생성\n\n##cnn_learner??\n\n\n##vision_learner??\n\n\nlrnr = cnn_learner(dls,resnet34,metrics = error_rate)\n\n/usr/local/lib/python3.8/dist-packages/fastai/vision/learner.py:288: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n\nid(dls)\n\n140201663392160\n\n\n\nid(lrnr.dls)\n\n140201663392160\n\n\n- 둘의 메모리 공간 주소가 같다."
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-irnr.학습",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-irnr.학습",
    "title": "02. Overview",
    "section": "3단계 : Irnr.학습()",
    "text": "3단계 : Irnr.학습()\n\nfine_tune() 모든 가중치를 학습하는 것이 아니라 일부만 학습하는 것임.\nfine_tune을 직역하면 미세조정이다.\n즉, 주어진 환경에 맞게 이미 학습된 데이터를 마지막에 미세한 조정을 주어서 다시 학습한다. 이것을 transfer learning 이라고한다.\n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.141714\n      0.015523\n      0.007442\n      00:16\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.046919\n      0.023695\n      0.006766\n      00:10\n    \n  \n\n\n\n\nfine_tune()이외의 방법으로 학습할 수도 있음 \\(\\to\\) fit 함수로도 가능"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-lnnr.예측",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-lnnr.예측",
    "title": "02. Overview",
    "section": "4단계 : lnnr.예측()",
    "text": "4단계 : lnnr.예측()\n(방법 1) : lrnr.predict()함수를 이용\n\nlrnr.predict(fnames[0])\n\n\n\n\n\n\n\n\n('cat', TensorBase(0), TensorBase([9.9900e-01, 9.9530e-04]))\n\n\n(방법2) : lrrnr.model(X)를 이용 : X의 shape이 (?,3,224,224)의 형태의 텐서이어야 한다.\n\n#dir(lrnr.model) ##__call 이라고 되어있으면 함수처럼 사용가능\n\n\n이미지를 넘겨주면 에러출력 \\(\\to\\) 컴퓨터가 이해할 수 있는 숫자로 보내주어야 한다.\n\n\nlrnr.model(fnames[0])\n\nTypeError: ignored\n\n\n\n_rtn = dls.one_batch()\n\n\ntype(_rtn)\n#_rtn\n\ntuple\n\n\n\nlen(_rtn)\n\n2\n\n\n\nX,y = dls.one_batch()\n\n\nX.shape # 224는 이미지 사이즈를 우리가 잡았던 것, 3은 채널(빨강, 노랑, 초록), 64는 이미지의 수\n\ntorch.Size([64, 3, 224, 224])\n\n\n\ny.shape\n\ntorch.Size([64])\n\n\n\ny[:4]\n\nTensorCategory([1, 1, 1, 1], device='cuda:0')\n\n\n\nlrnr.model(X)[:4]\n\nTensorBase([[-4.0604,  6.3253],\n            [-6.1767,  7.2553],\n            [-2.5415,  3.9088],\n            [-5.6635,  6.0049]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\n즉 X를 모델 학습에 넘겨주고 y가 0인지 1인지 맞춘다. 왼쪽에 있는 숫자가 클수록 0, 오른쪽에 있는 숫자가 클수록 1이라고 예측한다.\nlrnr.model(X) 활성화함수 직전 상태 즉, softmax로 넘어가기전 y_hat이 되기 직전인 상태이다.\n\n\nX[0].shape ## 한장의 이미지 shape\n\ntorch.Size([3, 224, 224])\n\n\n\nlrnr.model(X[0]) ## 4차원 텐서를 넣어야 하는데 3차원 텐서를 넣어서 에러가 발생\n\nValueError: ignored\n\n\n\nlrnr.model(X[:1]) ## 이런식으로 넣어주면 가능!\n\nTensorBase([[-4.0608,  6.3263]], device='cuda:0', grad_fn=<AliasBackward0>)"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#프로그래밍-과정",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#프로그래밍-과정",
    "title": "02. Overview",
    "section": "프로그래밍 과정",
    "text": "프로그래밍 과정\n- overview\n\ndls 오브젝트 생성\nlrnr 오브젝트 생성\nlrnr.학습()\nlrnr.예측()"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-dls-생성",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-dls-생성",
    "title": "02. Overview",
    "section": "1단계 : dls 생성",
    "text": "1단계 : dls 생성\n\n임의의 커피, 홍차 맛 만족도 데이터\n\n\nhttps://raw.githubusercontent.com/guebin/STML2022/main/posts/I.%20Overview/2022-09-08-rcmd_view.csv\n\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/STML2022/main/posts/I.%20Overview/2022-09-08-rcmd_view.csv')\ndf_view.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      커피1\n      커피2\n      커피3\n      커피4\n      커피5\n      커피6\n      커피7\n      커피8\n      커피9\n      커피10\n      홍차1\n      홍차2\n      홍차3\n      홍차4\n      홍차5\n      홍차6\n      홍차7\n      홍차8\n      홍차9\n      홍차10\n    \n  \n  \n    \n      0\n      4.149209\n      NaN\n      NaN\n      4.078139\n      4.033415\n      4.071871\n      NaN\n      NaN\n      NaN\n      NaN\n      1.142659\n      1.109452\n      NaN\n      0.603118\n      1.084308\n      NaN\n      0.906524\n      NaN\n      NaN\n      0.903826\n    \n    \n      1\n      4.031811\n      NaN\n      NaN\n      3.822704\n      NaN\n      NaN\n      NaN\n      4.071410\n      3.996206\n      NaN\n      NaN\n      0.839565\n      1.011315\n      NaN\n      1.120552\n      0.911340\n      NaN\n      0.860954\n      0.871482\n      NaN\n    \n    \n      2\n      4.082178\n      4.196436\n      NaN\n      3.956876\n      NaN\n      NaN\n      NaN\n      4.450931\n      3.972090\n      NaN\n      NaN\n      NaN\n      NaN\n      0.983838\n      NaN\n      0.918576\n      1.206796\n      0.913116\n      NaN\n      0.956194\n    \n    \n      3\n      NaN\n      4.000621\n      3.89557\n      NaN\n      3.838781\n      3.967183\n      NaN\n      NaN\n      NaN\n      4.105741\n      1.147554\n      NaN\n      1.346860\n      NaN\n      0.614099\n      1.297301\n      NaN\n      NaN\n      NaN\n      1.147545\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      3.888208\n      NaN\n      3.97033\n      3.979490\n      NaN\n      4.010982\n      NaN\n      0.920995\n      1.081111\n      0.999345\n      NaN\n      1.195183\n      NaN\n      0.818332\n      1.236331\n      NaN\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf_view.shape\n\n(100, 20)\n\n\n- 위 데이터는 컴퓨터가 좋아하는 데이터 타입은 아님\n- 아래와 같은 데이터 타입이 컴퓨터가 좋아하는 데이터 형태!\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/STML2022/main/posts/I.%20Overview/2022-09-08-rcmd_anal.csv')\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      user\n      item\n      rating\n      item_name\n    \n  \n  \n    \n      0\n      1\n      15\n      1.084308\n      홍차5\n    \n    \n      1\n      1\n      1\n      4.149209\n      커피1\n    \n    \n      2\n      1\n      11\n      1.142659\n      홍차1\n    \n    \n      3\n      1\n      5\n      4.033415\n      커피5\n    \n    \n      4\n      1\n      4\n      4.078139\n      커피4\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndls = CollabDataLoaders.from_df(df)\n\n\ndls.show_batch() ## 데이터들의 뭉치를 보여준다고 생각\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n    \n  \n  \n    \n      0\n      38\n      10\n      4.050465\n    \n    \n      1\n      42\n      20\n      1.292435\n    \n    \n      2\n      99\n      5\n      0.927884\n    \n    \n      3\n      6\n      18\n      1.124469\n    \n    \n      4\n      81\n      18\n      4.178587\n    \n    \n      5\n      97\n      13\n      4.071385\n    \n    \n      6\n      97\n      14\n      3.935935\n    \n    \n      7\n      86\n      10\n      0.878860\n    \n    \n      8\n      10\n      10\n      3.947756\n    \n    \n      9\n      81\n      3\n      1.140266\n    \n  \n\n\n\n\ntype(dls.one_batch())\n\ntuple\n\n\n\nlen(dls.one_batch())\n\n2\n\n\n\nX, y = dls.one_batch()\n\n\nX[:5]\n\ntensor([[94, 18],\n        [48, 15],\n        [49, 12],\n        [96,  8],\n        [42, 20]])\n\n\n\ny[:5]\n\ntensor([[4.1571],\n        [1.3672],\n        [0.9212],\n        [0.6784],\n        [1.2924]])\n\n\n\n평점이 y, X의 2번째 열은 item의 인덱스 즉, 커피, 홍차의 인덱스이다.\n유저는 1-100 까지, 아이템은 1-20 까지 번호가 매겨짐\n\n\ndf.user.unique(), df.item.unique()\n\n(array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n         27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n         40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n         53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n         66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n         79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n         92,  93,  94,  95,  96,  97,  98,  99, 100]),\n array([15,  1, 11,  5,  4, 14,  6, 20, 12, 17,  8,  9, 13, 19, 18, 16,  2,\n         3, 10,  7]))"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-lrnr-생성",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-lrnr-생성",
    "title": "02. Overview",
    "section": "2단계 : lrnr 생성",
    "text": "2단계 : lrnr 생성\n\n?collab_learner\n\n\nlrnr = collab_learner(dls,y_range = (0,5))"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-학습",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-학습",
    "title": "02. Overview",
    "section": "3단계 : 학습",
    "text": "3단계 : 학습\n\nlrnr.fit(5)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.303518\n      2.387814\n      00:00\n    \n    \n      1\n      2.296435\n      2.382961\n      00:00\n    \n    \n      2\n      2.290963\n      2.361562\n      00:00\n    \n    \n      3\n      2.273144\n      2.310360\n      00:00\n    \n    \n      4\n      2.237350\n      2.212338\n      00:00"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-예측",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-예측",
    "title": "02. Overview",
    "section": "4단계 : 예측",
    "text": "4단계 : 예측\n\n!nvidia-smi ## gpu 메모리에 데이터를 올려야 gpu를 쓸 수 있다.\n            ## 현재 40960MIB 중에, 6508MIB를 쓴거\n\nMon Feb 13 03:02:41 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   31C    P0    52W / 400W |   6508MiB / 40960MiB |      0%      Default |\n|                               |                      |             Disabled |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A     38267      C                                    6505MiB |\n+-----------------------------------------------------------------------------+\n\n\n\n동작을 하지 않는 이유 \\(\\to\\) 데이터를 GPU메모리에 올리지 않았기 때문, 즉 X를 GPU에 올리지않았음\n\n\nlrnr.model(X.to(\"cuda:0\"))\n\ntensor([2.6203, 2.4078, 2.4296, 2.4147, 2.4379, 2.4201, 2.5469, 2.3894, 2.4491,\n        2.5716, 2.4053, 2.5628, 2.5315, 2.5821, 2.4122, 2.5509, 2.4293, 2.4100,\n        2.3865, 2.5788, 2.4476, 2.4217, 2.4088, 2.4145, 2.5559, 2.5627, 2.4015,\n        2.4415, 2.4449, 2.4041, 2.5609, 2.4372, 2.5280, 2.5970, 2.5629, 2.5665,\n        2.5634, 2.4260, 2.5410, 2.3713, 2.4034, 2.4298, 2.5526, 2.5474, 2.4270,\n        2.5978, 2.5144, 2.4287, 2.5997, 2.3872, 2.3990, 2.4277, 2.4715, 2.4507,\n        2.4164, 2.5287, 2.6307, 2.4279, 2.4225, 2.5513, 2.3779, 2.4348, 2.5664,\n        2.3808], device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\ny.reshape(-1)\n\ntensor([4.1571, 1.3672, 0.9212, 0.6784, 1.2924, 1.2927, 4.4312, 1.1095, 0.6699,\n        4.2545, 1.3018, 4.1020, 4.1378, 3.9670, 1.0989, 4.1546, 0.9216, 1.6479,\n        0.7114, 4.3233, 0.8254, 0.9476, 1.2973, 1.2095, 4.0181, 4.1052, 1.0210,\n        1.1272, 1.0811, 0.9954, 3.8129, 1.2068, 3.7339, 3.7450, 3.7918, 3.9419,\n        3.9602, 1.1130, 4.0269, 0.8396, 0.6148, 1.0967, 3.9091, 3.6707, 1.2705,\n        4.3294, 3.8896, 0.9221, 4.0591, 0.7438, 0.5774, 0.9987, 1.0781, 1.4077,\n        1.2158, 3.7970, 3.9063, 0.8789, 0.9647, 3.9110, 0.9113, 1.4332, 4.0734,\n        0.7531])\n\n\n\n가만보니까 예측을 한것이 아니라 걍 찍는 것같음.\n다시 10번학습, 총 15번 학습한것!\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      2.024679\n      2.062938\n      00:00\n    \n    \n      1\n      1.915701\n      1.863412\n      00:00\n    \n    \n      2\n      1.786727\n      1.625497\n      00:00\n    \n    \n      3\n      1.634479\n      1.367636\n      00:00\n    \n    \n      4\n      1.463962\n      1.107142\n      00:00\n    \n    \n      5\n      1.286827\n      0.865784\n      00:00\n    \n    \n      6\n      1.110320\n      0.655676\n      00:00\n    \n    \n      7\n      0.942005\n      0.483732\n      00:00\n    \n    \n      8\n      0.786892\n      0.351725\n      00:00\n    \n    \n      9\n      0.649683\n      0.254381\n      00:00\n    \n  \n\n\n\n\nlrnr.model(X.to(\"cuda:0\"))\n\ntensor([3.8132, 1.3016, 1.3479, 1.3472, 1.3713, 1.3401, 3.6193, 1.2546, 1.3294,\n        3.7289, 1.2876, 3.6382, 3.5721, 3.5878, 1.3477, 3.6536, 1.3710, 1.4517,\n        1.2269, 3.6255, 1.4529, 1.3778, 1.2844, 1.3243, 3.6570, 3.4017, 1.2812,\n        1.3968, 1.3998, 1.2739, 3.5619, 1.2896, 3.5557, 3.7600, 3.5782, 3.5810,\n        3.6740, 1.3642, 3.6071, 1.2202, 1.3395, 1.3461, 3.6561, 3.6573, 1.3087,\n        3.7091, 3.5337, 1.3310, 3.7013, 1.3189, 1.3117, 1.3816, 1.4482, 1.4022,\n        1.3442, 3.5761, 3.7976, 1.3848, 1.2933, 3.5884, 1.2519, 1.4102, 3.6621,\n        1.2266], device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\ny.reshape(-1)\n\ntensor([4.1571, 1.3672, 0.9212, 0.6784, 1.2924, 1.2927, 4.4312, 1.1095, 0.6699,\n        4.2545, 1.3018, 4.1020, 4.1378, 3.9670, 1.0989, 4.1546, 0.9216, 1.6479,\n        0.7114, 4.3233, 0.8254, 0.9476, 1.2973, 1.2095, 4.0181, 4.1052, 1.0210,\n        1.1272, 1.0811, 0.9954, 3.8129, 1.2068, 3.7339, 3.7450, 3.7918, 3.9419,\n        3.9602, 1.1130, 4.0269, 0.8396, 0.6148, 1.0967, 3.9091, 3.6707, 1.2705,\n        4.3294, 3.8896, 0.9221, 4.0591, 0.7438, 0.5774, 0.9987, 1.0781, 1.4077,\n        1.2158, 3.7970, 3.9063, 0.8789, 0.9647, 3.9110, 0.9113, 1.4332, 4.0734,\n        0.7531])\n\n\n\n전 보단 결과가 좋으나 아직까지도 자신감이 부족함\n\n\nlrnr.fit(20)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.140063\n      0.186549\n      00:00\n    \n    \n      1\n      0.115944\n      0.139581\n      00:00\n    \n    \n      2\n      0.097311\n      0.109824\n      00:00\n    \n    \n      3\n      0.082672\n      0.090060\n      00:00\n    \n    \n      4\n      0.071270\n      0.077270\n      00:00\n    \n    \n      5\n      0.062843\n      0.069694\n      00:00\n    \n    \n      6\n      0.056643\n      0.064374\n      00:00\n    \n    \n      7\n      0.051726\n      0.060618\n      00:00\n    \n    \n      8\n      0.047778\n      0.058101\n      00:00\n    \n    \n      9\n      0.044731\n      0.056307\n      00:00\n    \n    \n      10\n      0.042384\n      0.055452\n      00:00\n    \n    \n      11\n      0.040487\n      0.054989\n      00:00\n    \n    \n      12\n      0.038892\n      0.054412\n      00:00\n    \n    \n      13\n      0.037757\n      0.054033\n      00:00\n    \n    \n      14\n      0.036721\n      0.053958\n      00:00\n    \n    \n      15\n      0.035906\n      0.053978\n      00:00\n    \n    \n      16\n      0.035152\n      0.053730\n      00:00\n    \n    \n      17\n      0.034605\n      0.053580\n      00:00\n    \n    \n      18\n      0.034082\n      0.053545\n      00:00\n    \n    \n      19\n      0.033564\n      0.053754\n      00:00\n    \n  \n\n\n\n\nlrnr.model(X.to(\"cuda:0\"))\n\ntensor([4.1396, 1.0046, 0.9150, 0.9528, 1.0421, 1.0685, 3.9525, 0.8296, 0.9267,\n        4.1958, 1.0089, 3.9822, 3.9842, 3.9823, 1.0656, 4.0215, 1.0202, 1.1260,\n        0.9742, 4.1417, 1.0290, 1.0583, 1.0669, 1.0596, 3.9959, 3.9538, 1.0101,\n        1.0808, 1.0498, 1.0109, 4.0697, 1.0221, 3.8955, 3.9628, 4.0330, 4.0100,\n        4.1104, 1.0817, 3.9374, 0.8868, 0.9833, 1.0597, 3.9288, 3.9056, 0.9954,\n        4.1319, 3.9341, 1.0133, 4.0659, 0.8576, 0.9358, 0.9670, 1.0936, 1.0714,\n        1.1292, 3.8640, 3.9320, 0.9894, 0.9393, 3.8178, 0.9363, 1.1871, 3.9079,\n        0.9397], device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\ny.reshape(-1)\n\ntensor([4.1571, 1.3672, 0.9212, 0.6784, 1.2924, 1.2927, 4.4312, 1.1095, 0.6699,\n        4.2545, 1.3018, 4.1020, 4.1378, 3.9670, 1.0989, 4.1546, 0.9216, 1.6479,\n        0.7114, 4.3233, 0.8254, 0.9476, 1.2973, 1.2095, 4.0181, 4.1052, 1.0210,\n        1.1272, 1.0811, 0.9954, 3.8129, 1.2068, 3.7339, 3.7450, 3.7918, 3.9419,\n        3.9602, 1.1130, 4.0269, 0.8396, 0.6148, 1.0967, 3.9091, 3.6707, 1.2705,\n        4.3294, 3.8896, 0.9221, 4.0591, 0.7438, 0.5774, 0.9987, 1.0781, 1.4077,\n        1.2158, 3.7970, 3.9063, 0.8789, 0.9647, 3.9110, 0.9113, 1.4332, 4.0734,\n        0.7531])\n\n\n\n이제 거의 비슷해짐\n\n- 이제 없는 데이터를 예측해보자.\n- 첫번째 유저가 커피 2를 먹었을 때? \\(\\to\\) 예상 4점 근처\n\nX[0:1]\n\ntensor([[94, 18]])\n\n\n\nlrnr.model(tensor([[1,2]])) ## 첫 번째 유저가 커피2를 먹었을 때\n\nRuntimeError: ignored\n\n\n- 에러가 나는 이유는 데이터가 하나는 gpu, 하나는 cpu에 있기 때문이다.\n\nXnew = tensor([[1,2]])\n\n\nlrnr.model(Xnew.to(\"cuda:0\"))\n\ntensor([4.0007], device='cuda:0', grad_fn=<AddBackward0>)\n\n\n- 우리가 예상한 결과가 나온다!"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-dls-생성-1",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-dls-생성-1",
    "title": "02. Overview",
    "section": "1단계 : dls 생성",
    "text": "1단계 : dls 생성\n\ndf = pd.DataFrame({'text':['h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??']*20000})\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      text\n    \n  \n  \n    \n      0\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      1\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      2\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      3\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n    \n      4\n      h e l l o . h e l l o ! h e l l o ? h e l l o !! h e l l o ??\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndls = TextDataLoaders.from_df(df,text_col=\"text\",is_lm=True)\n\n\n\n\n\n\n\n\n- is_lm = True는 단어에 대한 예측, is_lm = False는 긍정, 부정과 같이 이 텍스트가 최종적으로 어떤 클래스에 속하는지 예측한다.\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o\n      h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o .\n    \n    \n      1\n      ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l\n      xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o\n    \n    \n      2\n      ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l\n      ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l\n    \n    \n      3\n      o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e\n      ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l\n    \n    \n      4\n      l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h\n      o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e\n    \n    \n      5\n      l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos\n      l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h\n    \n    \n      6\n      e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ?\n      l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos\n    \n    \n      7\n      h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ?\n      e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ?\n    \n    \n      8\n      ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o\n      h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ? ? xxbos h e l l o . h e l l o ! h e l l o ? h e l l o ! ! h e l l o ?\n    \n  \n\n\n\n- xxbos는 하나의 내용이 끝나고 다른 내용이 시작된다는 의미\n\ntext와, text_의 차이는 text는 예측전, text_는 예측 후라고 생각하자. \\(\\to\\) 즉, X와 y"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-lrnr-생성-1",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-lrnr-생성-1",
    "title": "02. Overview",
    "section": "2단계 : lrnr 생성",
    "text": "2단계 : lrnr 생성\n\nlnnr = language_model_learner(dls,arch=AWD_LSTM)\n\n\n\n\n\n\n    \n      \n      100.00% [105070592/105067061 00:01<00:00]"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-학습-1",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-학습-1",
    "title": "02. Overview",
    "section": "3단계 : 학습",
    "text": "3단계 : 학습\n\nlnnr.fit(5)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.965570\n      0.897048\n      00:22\n    \n    \n      1\n      1.031884\n      0.373120\n      00:21\n    \n    \n      2\n      0.640392\n      0.251359\n      00:21\n    \n    \n      3\n      0.492017\n      0.212869\n      00:21\n    \n    \n      4\n      0.389751\n      0.186374\n      00:21"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-예측-1",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-예측-1",
    "title": "02. Overview",
    "section": "4단계 : 예측",
    "text": "4단계 : 예측\n\nlnnr.predict('h e',n_words=30)\n\n\n\n\n\n\n\n\n'h e l l o ? h e l l o ! h e l l o ? ? h e l l o ! h e l l o !'\n\n\n\n오 예측이 되게 잘됬당"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#생성모형",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#생성모형",
    "title": "02. Overview",
    "section": "생성모형",
    "text": "생성모형\n- 데이터의 생성확률 \\(p(\\bf{x}, y)\\) 을 알면 클래스의 사후확률 \\(p(y|\\bf{x})\\) 를 알 수 있음(일반적인 예측모형), 하지만 역은 불가능\n- 생성모형은 \\(p(x,y)\\)에 관심이 있다. 따라서, 이미지를 생성하는일은 분류문제보다 더 어려운 일이라 해석가능"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#gan의-원리",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#gan의-원리",
    "title": "02. Overview",
    "section": "GAN의 원리",
    "text": "GAN의 원리\n- 생성모형중 하나\n- GAN의 원리는 경찰과 위조지폐범이 서로 선의의(?) 경쟁을 통하여 서로 발전하는 모형으로 설명할 수 있다.\n\nThe generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.\n\n- 서로 적대적인(adversarial) 네트워크(network)를 동시에 학습시켜 가짜이미지를 만든다(generate)\n- 상황극 : 경찰이 CNN, 위조범이 GAN\n\n위조범: 가짜돈을 만들어서 부자가 되어야지! (가짜돈을 그림)\n경찰: (위조범이 만든 돈을 보고) 이건 가짜다!\n위조범: 걸렸군.. 더 정교하게 만들어야지..\n경찰: 이건 진짠가?… –> 상사에게 혼남. 그것도 구분못하냐고\n위조범: 더 정교하게 만들자..\n경찰: 더 판별능력을 업그레이드 하자!\n반복..\n\n- 굉장히 우수한 경찰조차도 진짜와 가짜를 구분하지 못할때(=진짜 이미지를 0.5의 확률로만 진짜라고 말할때 = 가짜 이미지를 0.5의 확률로만 가짜라고 말할때) 학습을 멈춘다."
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-dls-생성-2",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-dls-생성-2",
    "title": "02. Overview",
    "section": "1단계 : dls 생성",
    "text": "1단계 : dls 생성\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\n\n\n\n\n    \n      \n      100.14% [3219456/3214948 00:00<00:00]\n    \n    \n\n\n\ndblock = DataBlock(blocks=(TransformBlock,ImageBlock),\n          get_x = generate_noise,\n          get_items=get_image_files,\n          item_tfms=Resize(32))\ndls = dblock.dataloaders(path) \n\n\ndls.show_batch()"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-lrnr-생성-2",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-lrnr-생성-2",
    "title": "02. Overview",
    "section": "2단계 : lrnr 생성",
    "text": "2단계 : lrnr 생성\n- 재료가 되는 경찰과 위조지폐범을 만듬\n- 위조지폐범의 32 : 입력데이터를 32 x 32로 생성해라\n- police의 32 : 32 x 32로 입력을 받아 0또는 1로 출력을해라\n\ncounterfeiter = basic_generator(32,n_channels=3,n_extra_layers=1) \npolice = basic_critic(32,n_channels=3,n_extra_layers=1)\n\n- 위조지폐범과 경찰을 집어넣음\n\nlrnr = GANLearner.wgan(dls,counterfeiter,police)"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-학습-2",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-학습-2",
    "title": "02. Overview",
    "section": "3단계 : 학습",
    "text": "3단계 : 학습\n\nlrnr.fit(60)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      gen_loss\n      crit_loss\n      time\n    \n  \n  \n    \n      0\n      -0.501905\n      0.272452\n      0.272452\n      -0.732317\n      00:05\n    \n    \n      1\n      -0.560972\n      0.261339\n      0.261339\n      -0.738020\n      00:05\n    \n    \n      2\n      -0.426593\n      0.238372\n      0.238372\n      -0.638903\n      00:05\n    \n    \n      3\n      -0.549378\n      0.277994\n      0.277994\n      -0.735531\n      00:05\n    \n    \n      4\n      -0.530167\n      0.310435\n      0.310435\n      -0.731865\n      00:05\n    \n    \n      5\n      -0.546979\n      0.303683\n      0.303683\n      -0.734299\n      00:06\n    \n    \n      6\n      -0.439625\n      0.263317\n      0.263317\n      -0.623146\n      00:06\n    \n    \n      7\n      -0.501293\n      0.257658\n      0.257658\n      -0.528969\n      00:06\n    \n    \n      8\n      -0.301703\n      0.209601\n      0.209601\n      -0.312943\n      00:06\n    \n    \n      9\n      -0.424916\n      0.251544\n      0.251544\n      -0.586661\n      00:05\n    \n    \n      10\n      -0.461934\n      0.139101\n      0.139101\n      -0.633617\n      00:06\n    \n    \n      11\n      -0.410035\n      0.199125\n      0.199125\n      -0.536341\n      00:06\n    \n    \n      12\n      -0.493449\n      0.256345\n      0.256345\n      -0.658428\n      00:05\n    \n    \n      13\n      -0.312562\n      0.312208\n      0.312208\n      -0.493477\n      00:05\n    \n    \n      14\n      -0.382400\n      0.347141\n      0.347141\n      -0.643486\n      00:06\n    \n    \n      15\n      -0.331892\n      0.385739\n      0.385739\n      -0.476501\n      00:05\n    \n    \n      16\n      -0.170304\n      0.169395\n      0.169395\n      -0.212899\n      00:06\n    \n    \n      17\n      -0.310682\n      0.243360\n      0.243360\n      -0.433320\n      00:06\n    \n    \n      18\n      -0.220975\n      0.251129\n      0.251129\n      -0.409254\n      00:06\n    \n    \n      19\n      -0.349662\n      0.202146\n      0.202146\n      -0.615371\n      00:06\n    \n    \n      20\n      -0.233808\n      0.050910\n      0.050910\n      -0.315928\n      00:06\n    \n    \n      21\n      -0.307061\n      0.267279\n      0.267279\n      -0.485837\n      00:06\n    \n    \n      22\n      -0.311691\n      0.241707\n      0.241707\n      -0.362771\n      00:05\n    \n    \n      23\n      -0.268940\n      -0.232714\n      -0.232714\n      -0.096314\n      00:06\n    \n    \n      24\n      -0.216904\n      0.140780\n      0.140780\n      -0.359679\n      00:06\n    \n    \n      25\n      -0.168833\n      0.148477\n      0.148477\n      -0.130964\n      00:06\n    \n    \n      26\n      -0.177647\n      0.264623\n      0.264623\n      -0.359113\n      00:05\n    \n    \n      27\n      -0.082595\n      0.003874\n      0.003874\n      -0.091398\n      00:06\n    \n    \n      28\n      -0.063921\n      0.037485\n      0.037485\n      -0.131270\n      00:05\n    \n    \n      29\n      -0.109081\n      0.241590\n      0.241590\n      -0.193368\n      00:05\n    \n    \n      30\n      -0.070794\n      -0.298603\n      -0.298603\n      -0.082873\n      00:05\n    \n    \n      31\n      -0.058322\n      0.064523\n      0.064523\n      -0.107654\n      00:05\n    \n    \n      32\n      -0.054619\n      -0.114057\n      -0.114057\n      -0.096975\n      00:05\n    \n    \n      33\n      -0.063974\n      -0.228303\n      -0.228303\n      0.002836\n      00:05\n    \n    \n      34\n      -0.038850\n      0.239098\n      0.239098\n      -0.018863\n      00:05\n    \n    \n      35\n      -0.049034\n      0.035363\n      0.035363\n      -0.068695\n      00:05\n    \n    \n      36\n      -0.096864\n      -0.048236\n      -0.048236\n      -0.320945\n      00:05\n    \n    \n      37\n      -0.076138\n      -0.369186\n      -0.369186\n      -0.017232\n      00:06\n    \n    \n      38\n      -0.054198\n      0.369465\n      0.369465\n      0.003171\n      00:06\n    \n    \n      39\n      -0.039503\n      0.065342\n      0.065342\n      -0.055770\n      00:05\n    \n    \n      40\n      -0.039892\n      -0.491991\n      -0.491991\n      0.008136\n      00:05\n    \n    \n      41\n      -0.029983\n      0.247375\n      0.247375\n      0.003377\n      00:05\n    \n    \n      42\n      -0.083779\n      0.239754\n      0.239754\n      -0.122858\n      00:05\n    \n    \n      43\n      -0.051302\n      -0.126592\n      -0.126592\n      -0.027422\n      00:05\n    \n    \n      44\n      -0.035274\n      -0.013859\n      -0.013859\n      -0.084820\n      00:06\n    \n    \n      45\n      -0.046283\n      -0.002334\n      -0.002334\n      -0.093167\n      00:05\n    \n    \n      46\n      -0.045804\n      -0.077428\n      -0.077428\n      -0.180404\n      00:05\n    \n    \n      47\n      -0.018050\n      0.009938\n      0.009938\n      -0.031895\n      00:05\n    \n    \n      48\n      -0.073091\n      0.134628\n      0.134628\n      -0.073974\n      00:06\n    \n    \n      49\n      -0.059265\n      0.106770\n      0.106770\n      -0.112714\n      00:05\n    \n    \n      50\n      -0.068173\n      -0.260318\n      -0.260318\n      -0.042849\n      00:05\n    \n    \n      51\n      -0.057045\n      0.302212\n      0.302212\n      -0.068016\n      00:06\n    \n    \n      52\n      -0.104484\n      0.112291\n      0.112291\n      -0.152773\n      00:06\n    \n    \n      53\n      -0.089554\n      0.157628\n      0.157628\n      -0.191660\n      00:05\n    \n    \n      54\n      -0.111737\n      -0.104993\n      -0.104993\n      -0.192626\n      00:06\n    \n    \n      55\n      -0.094386\n      0.111732\n      0.111732\n      -0.142037\n      00:05\n    \n    \n      56\n      -0.071494\n      0.141211\n      0.141211\n      -0.037046\n      00:05\n    \n    \n      57\n      -0.053501\n      0.174928\n      0.174928\n      -0.102489\n      00:05\n    \n    \n      58\n      -0.068883\n      -0.012831\n      -0.012831\n      -0.137998\n      00:06\n    \n    \n      59\n      -0.082970\n      -0.072856\n      -0.072856\n      -0.257704\n      00:05\n    \n  \n\n\n\n- 가짜 숫자의 결과\n\ndls.show_batch()\n\n\n\n\n\nlrnr.show_results()"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-01-Overview.html#단계-없음",
    "href": "post/Lecture/STML/2023-02-01-Overview.html#단계-없음",
    "title": "02. Overview",
    "section": "4단계 : 없음",
    "text": "4단계 : 없음"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-04-DNN 딥러닝의 기초 (1).html",
    "href": "post/Lecture/STML/2023-02-04-DNN 딥러닝의 기초 (1).html",
    "title": "03. 딥러닝의 기초 (1)",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-04-DNN 딥러닝의 기초 (1).html#회귀모형에서-데이터-생성",
    "href": "post/Lecture/STML/2023-02-04-DNN 딥러닝의 기초 (1).html#회귀모형에서-데이터-생성",
    "title": "03. 딥러닝의 기초 (1)",
    "section": "회귀모형에서 데이터 생성",
    "text": "회귀모형에서 데이터 생성\n\n_rtn = torch.randn(100).sort()\n\n\n_rtn\n\ntorch.return_types.sort(\nvalues=tensor([-3.1367e+00, -2.8208e+00, -2.2292e+00, -2.1231e+00, -2.0937e+00,\n        -1.9074e+00, -1.5389e+00, -1.4720e+00, -1.3928e+00, -1.3714e+00,\n        -1.3338e+00, -1.2976e+00, -1.2710e+00, -1.0745e+00, -9.2089e-01,\n        -9.0436e-01, -8.6659e-01, -8.5472e-01, -8.1530e-01, -8.0666e-01,\n        -7.9050e-01, -7.2011e-01, -6.5759e-01, -6.4015e-01, -6.3165e-01,\n        -5.8250e-01, -5.6724e-01, -5.6494e-01, -5.5067e-01, -5.3365e-01,\n        -5.3033e-01, -5.2203e-01, -5.0332e-01, -3.8757e-01, -3.8734e-01,\n        -3.6441e-01, -3.5983e-01, -3.4966e-01, -3.4702e-01, -3.2707e-01,\n        -3.2038e-01, -3.1154e-01, -3.0962e-01, -3.0318e-01, -2.9651e-01,\n        -2.1545e-01, -2.0184e-01, -1.4594e-01, -1.1142e-01, -9.6427e-02,\n        -9.5620e-02, -5.7341e-02, -4.1936e-02, -2.8048e-02, -6.9670e-04,\n         9.0079e-03,  2.5418e-02,  7.3700e-02,  1.1160e-01,  1.4730e-01,\n         2.4064e-01,  2.5896e-01,  2.8077e-01,  2.8109e-01,  2.8217e-01,\n         2.8941e-01,  3.2667e-01,  3.3933e-01,  4.0200e-01,  4.3625e-01,\n         4.5079e-01,  4.5483e-01,  4.6048e-01,  4.8760e-01,  5.5705e-01,\n         5.7382e-01,  6.5001e-01,  6.6529e-01,  6.7418e-01,  7.2758e-01,\n         7.9666e-01,  8.4189e-01,  8.4731e-01,  9.7818e-01,  9.8744e-01,\n         9.9391e-01,  1.0039e+00,  1.0543e+00,  1.0674e+00,  1.0744e+00,\n         1.1518e+00,  1.2864e+00,  1.3573e+00,  1.4386e+00,  1.5388e+00,\n         1.9962e+00,  2.0737e+00,  2.0793e+00,  2.1142e+00,  2.6126e+00]),\nindices=tensor([69, 44, 94, 10, 39, 57, 28, 77, 21, 40,  4, 73, 47, 17, 31, 13, 53, 52,\n        45, 67,  9, 36, 18, 58, 88, 97, 71,  1, 72, 30, 51, 37, 83, 49, 50, 59,\n        74, 33, 42,  0, 86, 85, 19, 63,  8, 16, 54, 22, 41, 46, 89, 23, 70,  7,\n        38, 76, 29, 43, 93, 75, 66, 99, 48, 84, 79,  5, 64, 82, 62, 20, 12, 60,\n        80, 92, 98,  3, 34, 14, 26, 78, 91, 56, 68, 25, 35, 81, 32,  6, 55, 11,\n        27,  2, 95, 87, 96, 61, 15, 24, 90, 65]))\n\n\n\n_rtn[0] ## 사실상 x, _rtn[0]은 인덱스 값이다.\n\ntensor([-3.1367e+00, -2.8208e+00, -2.2292e+00, -2.1231e+00, -2.0937e+00,\n        -1.9074e+00, -1.5389e+00, -1.4720e+00, -1.3928e+00, -1.3714e+00,\n        -1.3338e+00, -1.2976e+00, -1.2710e+00, -1.0745e+00, -9.2089e-01,\n        -9.0436e-01, -8.6659e-01, -8.5472e-01, -8.1530e-01, -8.0666e-01,\n        -7.9050e-01, -7.2011e-01, -6.5759e-01, -6.4015e-01, -6.3165e-01,\n        -5.8250e-01, -5.6724e-01, -5.6494e-01, -5.5067e-01, -5.3365e-01,\n        -5.3033e-01, -5.2203e-01, -5.0332e-01, -3.8757e-01, -3.8734e-01,\n        -3.6441e-01, -3.5983e-01, -3.4966e-01, -3.4702e-01, -3.2707e-01,\n        -3.2038e-01, -3.1154e-01, -3.0962e-01, -3.0318e-01, -2.9651e-01,\n        -2.1545e-01, -2.0184e-01, -1.4594e-01, -1.1142e-01, -9.6427e-02,\n        -9.5620e-02, -5.7341e-02, -4.1936e-02, -2.8048e-02, -6.9670e-04,\n         9.0079e-03,  2.5418e-02,  7.3700e-02,  1.1160e-01,  1.4730e-01,\n         2.4064e-01,  2.5896e-01,  2.8077e-01,  2.8109e-01,  2.8217e-01,\n         2.8941e-01,  3.2667e-01,  3.3933e-01,  4.0200e-01,  4.3625e-01,\n         4.5079e-01,  4.5483e-01,  4.6048e-01,  4.8760e-01,  5.5705e-01,\n         5.7382e-01,  6.5001e-01,  6.6529e-01,  6.7418e-01,  7.2758e-01,\n         7.9666e-01,  8.4189e-01,  8.4731e-01,  9.7818e-01,  9.8744e-01,\n         9.9391e-01,  1.0039e+00,  1.0543e+00,  1.0674e+00,  1.0744e+00,\n         1.1518e+00,  1.2864e+00,  1.3573e+00,  1.4386e+00,  1.5388e+00,\n         1.9962e+00,  2.0737e+00,  2.0793e+00,  2.1142e+00,  2.6126e+00])\n\n\n\ntorch.manual_seed(202150256)\nones= torch.ones(100)\nx,_ = torch.randn(100).sort()\nX = torch.stack([ones,x]).T # = torch.stack([ones,x],axis=1)\nW = torch.tensor([2.5,4])\nϵ = torch.randn(100)*0.5\ny = X@W + ϵ\n\n\nX[:5],y[:5]\n\n(tensor([[ 1.0000, -2.8720],\n         [ 1.0000, -2.6928],\n         [ 1.0000, -2.6554],\n         [ 1.0000, -2.1535],\n         [ 1.0000, -1.6357]]),\n tensor([-9.9523, -7.7394, -7.3931, -6.6678, -4.0281]))\n\n\n\nX.shape, y.shape\n\n(torch.Size([100, 2]), torch.Size([100]))\n\n\n\nplt.plot(x,y,\".\")\nplt.plot(x,2.5+4*x,\"--\")\nplt.legend([\"y\",\"y_hat\"])\n\n<matplotlib.legend.Legend at 0x7f96d388f910>"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-04-DNN 딥러닝의 기초 (1).html#회귀모형에서-학습이란",
    "href": "post/Lecture/STML/2023-02-04-DNN 딥러닝의 기초 (1).html#회귀모형에서-학습이란",
    "title": "03. 딥러닝의 기초 (1)",
    "section": "회귀모형에서 학습이란?",
    "text": "회귀모형에서 학습이란?\n- 파란점만 주어진 경우, 주황색 점선( \\(\\hat{y}\\) )를 추정하는 것 \\(\\to\\) 즉, 주어진 데이터로 \\(\\hat{\\bf{W}}=\\begin{bmatrix} \\hat{w_0} \\\\ \\hat{w_1} \\end{bmatrix}를 최대한 \\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}\\)와 비슷하게 찾는 것\n- 위 그림을 보고 적당한 추세선을 찾는것!\n- 시도 1 : \\((\\hat{w_0},\\hat{w_1}) = (-5,10)\\) 을 선택하여 선을 그려보고 적당한지 판단.\n\n\\(\\hat{y} = -5+10x_{i}\\)\n\n\nplt.plot(x,y,\".\")\nplt.plot(x,-5+10*x,\"--\")\nplt.legend([\"y\",\"y_hat\"])\n\n<matplotlib.legend.Legend at 0x7f96d38c3130>\n\n\n\n\n\n\n벡터표현으로 주황색 점선을 계산\n\n\nWhat = torch.tensor([-5.0, 10.0])\n\n\nplt.plot(x,y,\".\")\nplt.plot(x,X@What,\"--\")\nplt.legend([\"y\",\"y_hat\"])\n\n<matplotlib.legend.Legend at 0x7f96d378fa90>\n\n\n\n\n\n\n적합결과 적합한 회귀식은 아니나. loss값을 계산하여 갱신할 수 있는 토대를 만들었다.\n\n\n파라미터 학습\n\n이론적으로 추론 : 회귀분석시간에 배운것\n\n\\[\\bf{W} = (X^{\\top}X)^{-1}X^{\\top}y\\]\n\n컴퓨터의 반복계산을 이용하여 추론 (손실함수 + 경사하강법)\n\n- 전략\n\nstep 1 : 아무 점선이나 그어본다.\nstep 2 : step1에서 그은 점선보다 더 좋은 점선으로 바꾼다.\nstep 3 : step 1,2 를 반복한다.\n\n- step 1\n\n\\(\\hat {w_0} = -5, \\hat{w_1} = 10\\)으로 설정하고 임의의 선을 그어보자.\n\n\nWhat = torch.tensor([-5.0,10.0],requires_grad=True) ## 손실함수를 미분하기 위해\nWhat\n\ntensor([-5., 10.], requires_grad=True)\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What.detach(), '--') \nplt.legend([\"y\",\"y_hat\"])\n\n<matplotlib.legend.Legend at 0x7f96d36fd910>\n\n\n\n\n\n- step 2\n\nloss function 도입\n\n\\[loss = \\sum_{i=1}^{n} (y_i -\\hat{y_i})^2 = (\\bf{y-\\hat{y}})^{\\top}(\\bf{y-\\hat{y}}) \\]\n\nloss = torch.sum((y-X@What)**2)\nloss\n\ntensor(9210.4121, grad_fn=<SumBackward0>)\n\n\n\n갱신\n\n마이너스의 의미 : 기울기의 부호를 보고 반대방향으로 움직여라\n\\(\\frac{\\partial}{\\partial \\bf{W}} loss\\) : 기울기의 절대값 크기와 비례하여 움직이는 정도를 조정하라.\n\\(\\alpha\\) : \\(alpha\\)값이 클 수록 전체적으로 빠르게 움직, 즉 큼직큼직하게 \\(\\bf{W}\\) 를 갱신한다.\n\n\n\\[\\bf {W_{t+1}} = \\bf {W_t} - \\alpha \\times \\frac {\\partial}{\\partial \\bf{W}}  loss\\]\n\n\\(loss\\)를 줄이는 방법 \\(\\to\\) 경사하강법\n\n\nloss.backward()\n\n\nloss.backward() : loss를 requires_grad=True를 가진텐서로 미분하라.\n\n\nWhat.grad\n\ntensor([-1449.6279,  1244.9608])\n\n\n\n\\((w_0, w_1)=(-5,10)\\) 에서의 \\(loss(w_0,w_1)\\)의 순간기울기가 \\((-1449.6279, 1244.9608)\\) 이라는 의미.\n\n- (확인 1) : loss.backward()가 미분을 잘 계산해 주는 것이 맞는가 검산해보자. 손계산으로 검산\n\\[\\frac{\\partial}{\\partial \\bf{W}} loss = -2\\bf{X^{\\top}y} + 2 \\bf{X^{\\top}XW}\\]\n\n-2 * X.T @ y + 2 * X.T @ X @ What\n\ntensor([-1449.6281,  1244.9608], grad_fn=<AddBackward0>)\n\n\n- (확인 2) : 편미분을 구현하여 검증\n\n\\(\\frac{\\partial}{\\partial w_0} loss(w_0, w_1) \\approx \\frac{loss(w_0+h,w_1) - loss(w_0,w_1)}{h}\\)\n\\(\\frac{\\partial}{\\partial w_1} loss(w_0, w_1) \\approx \\frac{loss(w_0,w_1+h) - loss(w_0,w_1)}{h}\\)\n\n\n_loss_fn = lambda w0,w1 : torch.sum((y-w0-w1*x)**2)\n\n\n\\(h=0.001, w_0 =-5, w_1 *10\\)\n\n\nh = 0.001\n\n(_loss_fn(-5+h,10) - _loss_fn(-5,10))/h, (_loss_fn(-5,10+h) - _loss_fn(-5,10))/h\n\n(tensor(-1449.2186), tensor(1246.0938))\n\n\n\n약간의 오차가 있지만 얼추 비슷 \\(\\to\\) 잘 계산했다는 의미!\n\\(\\bf {W_{t+1}} = \\bf{W_{t}}\\)를 계산\n\n\nalpha = 0.001\n\nWbefore = What.data\nWafter = What.data - alpha * What.grad\nWbefore, Wafter\n\n(tensor([-5., 10.]), tensor([-3.5504,  8.7550]))\n\n\n- 1회 갱신 후 시각화\n\nplt.plot(x,y,\".\")\nplt.plot(x,X@Wbefore,\"--\")\nplt.plot(x,X@Wafter,\"--\")\nplt.legend([\"y\",\"before\",\"after\"])\n\n<matplotlib.legend.Legend at 0x7f96d35c9d90>\n\n\n\n\n\n- step 3\n\n학습\n\n\nWhat =  torch.tensor([-5.0, 10.0],requires_grad= True)\nWhat\n\nalpha  = 0.001\n\nfor epoch in range(30) : \n    yhat = X@What\n    loss  = torch.sum((y-yhat)**2)\n    loss.backward() # loss 미분\n    What.data = What.data - alpha * What.grad\n    What.grad = None ## 미분한 기울기 값을 다시 재설정\n\n\nWhat\n\ntensor([2.5137, 4.0065], requires_grad=True)\n\n\n\nWhat.data\n\ntensor([2.5137, 4.0065])\n\n\n\nplt.plot(x,y,\".\")\nplt.plot(x,X@What.data,\"--\")\nplt.legend([\"y\",\"hat y\"])\n\n<matplotlib.legend.Legend at 0x7f96d336c0a0>\n\n\n\n\n\n\n\n학습과정 모니터링\n\nloss_history = []\nyhat_history = []\nWhat_history = []\n\nWhat = torch.tensor([-5.0, 10.0],requires_grad=True)\nalpha = 0.001\n\nfor epoc in range(30) :\n    yhat = X@What\n    yhat_history.append(yhat.data.tolist())\n    loss = torch.sum((y-yhat)**2)\n    loss_history.append(loss.item())\n    loss.backward()\n    What.data = What.data - alpha * What.grad\n    What_history.append(What.data.tolist())\n    What.grad = None\n\n- \\(\\hat y,\\bf{W} , loss\\) (epoch=3, epoch=10, epoch=15) 관찰\n\nfig,axs = plt.subplots(1,2,figsize=(12,5))\nax1 = axs[0]\nax2 = axs[1]\n\nax1.plot(x,y,\".\")\nax1.plot(x,yhat_history[2],\"--\")\nax1.plot(x,yhat_history[9],\"--\")\nax1.plot(x,yhat_history[29],\"--\")\nax1.legend([\"y\",\"epoch=3\",\"epoch=10\",\"epcoh=30\"])\nax1.set_title(\"W, yhat\")\nax2.plot(loss_history)\nax2.set_title(\"loss\")\n\nText(0.5, 1.0, 'loss')\n\n\n\n\n\n\n\nanimation으로 시각화\n\nfrom matplotlib import animation\n\nplt.rcParams['figure.figsize'] = (12,5)\nplt.rcParams[\"animation.html\"] = \"jshtml\" \n\nfig = plt.figure()\nax1 = fig.add_subplot(1, 2, 1)\nax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\nax1.plot(x,y,'o')\nline, = ax1.plot(x,yhat_history[0]) # 나중에 애니메이션 할때 필요\nax1.legend([\"y\",\"yhat\"])\nax1.set_title([\"y vs yhat\"])\nax2.set_title(\"loss\")\n\nText(0.5, 0.92, 'loss')\n\n\n\n\n\n\n_w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) \n_w1 = np.arange(-6, 11, 0.5)\nw1,w0 = np.meshgrid(_w1,_w0)\nlss=w0*0\nfor i in range(len(_w0)):\n    for j in range(len(_w1)):\n        lss[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2)\nax2.plot_surface(w0, w1, lss, rstride=1, cstride=1, color='b',alpha=0.35) ## 파란색곡면을 그리는 코드(끝) \nax2.azim = 40  ## 3d plot의 view 조절 \nax2.dist = 8   ## 3d plot의 view 조절 \nax2.elev = 5   ## 3d plot의 view 조절 \n\n\nfig\n\n\n\n\n\nax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color='red',marker='*') ## 최소점을 표시하는 코드 (붉은색 별) \nax2.scatter(What_history[0][0],What_history[0][1],loss_history[0],color='blue') ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) \n\n<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f96d232ec70>\n\n\n\nfig\n\n\n\n\n\ndef animate(epoc):\n    line.set_ydata(yhat_history[epoc])\n    ax2.scatter(What_history[epoc][0],What_history[epoc][1],loss_history[epoc],color='blue')\n    return line\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n- 함수로 구현\n\ndef show_lrpr(data,history):\n    x,y = data \n    loss_history,yhat_history,What_history = history \n    \n    fig = plt.figure()\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n\n    ## ax1: 왼쪽그림 \n    ax1.plot(x,y,'o')\n    line, = ax1.plot(x,yhat_history[0]) # 나중에 애니메이션 할때 필요\n    ax1.legend([\"y\",\"yhat\"])\n    ax1.set_title(\"y vs yhat\")\n    ax2.set_title(\"loss\")\n    ## ax2: 오른쪽그림 \n    _w0 = np.arange(-6, 11, 0.5) ## 파란색곡면을 그리는 코드 (시작) \n    _w1 = np.arange(-6, 11, 0.5)\n    w1,w0 = np.meshgrid(_w1,_w0)\n    lss=w0*0\n    for i in range(len(_w0)):\n        for j in range(len(_w1)):\n            lss[i,j]=torch.sum((y-_w0[i]-_w1[j]*x)**2)\n    ax2.plot_surface(w0, w1, lss, rstride=1, cstride=1, color='b',alpha=0.35) ## 파란색곡면을 그리는 코드(끝) \n    ax2.scatter(2.5,4,torch.sum((y-2.5-4*x)**2),s=200,color='red',marker='*') ## 최소점을 표시하는 코드 (붉은색 별) \n    ax2.scatter(What_history[0][0],What_history[0][1],loss_history[0],color='b') ## 업데이트되는 What을 표시하는 점 (파란색 동그라미) \n    ax2.azim = 40  ## 3d plot의 view 조절 \n    ax2.dist = 8   ## 3d plot의 view 조절 \n    ax2.elev = 5   ## 3d plot의 view 조절 \n\n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        ax2.scatter(np.array(What_history)[epoc,0],np.array(What_history)[epoc,1],loss_history[epoc],color='grey')\n        return line\n\n    ani = animation.FuncAnimation(fig, animate, frames=30)\n    plt.close()\n    return ani\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\nOutput hidden; open in https://colab.research.google.com to view."
  },
  {
    "objectID": "post/Lecture/STML/2023-02-04-DNN 딥러닝의 기초 (1).html#학습률에-따른-효율",
    "href": "post/Lecture/STML/2023-02-04-DNN 딥러닝의 기초 (1).html#학습률에-따른-효율",
    "title": "03. 딥러닝의 기초 (1)",
    "section": "학습률에 따른 효율",
    "text": "학습률에 따른 효율\n\n(1) 0.001\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.0001 \nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n비효율적 최적값으로 너무 느리게 수렴, epoch가 더필요\n\n\n\n(2) 0.0083\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.0083 \nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\nepoch에 따른 변화율이 너무 크다 \\(\\to\\) 약간 작으면 좋을것 같음\n\n\n\n(3) 0.0085\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha=0.0085 \nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n최적값으로 수렴을 하는듯 하면서 약간 벗어난다. 만약 epoch이 늘어나면 수렴값에서 떨어질 가능성이 높다…\n\n\n\n(4) 0.01\n\nloss_history = [] # 기록하고 싶은것 1  \nyhat_history = [] # 기록하고 싶은것 2 \nWhat_history = [] # 기록하고 싶은것 3 \n\n\nWhat= torch.tensor([-5.0,10.0],requires_grad=True)\nalpha = 0.01 \nfor epoc in range(30): \n    yhat=X@What ; yhat_history.append(yhat.data.tolist())\n    loss=torch.sum((y-yhat)**2); loss_history.append(loss.item())\n    loss.backward() \n    What.data = What.data-alpha * What.grad; What_history.append(What.data.tolist())\n    What.grad=None\n\nshow_lrpr([x,y],[loss_history,yhat_history,What_history])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n기울기가 무한대로 터지는 경우가 발생\nsummary : 적절한 \\(\\alpha\\) 값을 구하는것이 중요하다!"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html",
    "href": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html",
    "title": "04. 딥러닝의 기초 (2)",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd\nimport torch"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#벡터-생성법",
    "href": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#벡터-생성법",
    "title": "04. 딥러닝의 기초 (2)",
    "section": "벡터 생성법",
    "text": "벡터 생성법\n- 길이가 3인 벡터 선언방법\n\na = torch.tensor([1,2,3])\na.shape\n\ntorch.Size([3])\n\n\n- 3 x 1 col vec 선언방법\n\na = torch.tensor([[1],[1],[1]])\na.shape\n\ntorch.Size([3, 1])\n\n\n- 1x 3 row vec 선언방법\n\na = torch.tensor([[1,2,3]])\na.shape\n\ntorch.Size([1, 3])"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#torch의-dtype",
    "href": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#torch의-dtype",
    "title": "04. 딥러닝의 기초 (2)",
    "section": "torch의 dtype",
    "text": "torch의 dtype\n- default \\(\\to\\) dtype = torch.float32\n\ntsr = torch.tensor([1.23,2.34])\ntsr.dtype\n\ntorch.float32\n\n\n- 정수로 선언하더라도 dtype을 torch.float32로 바꾸는게 유리하다.\n(안 좋은 선언예시)\n\ntsr = torch.tensor([1,2])\ntsr.dtype\n\ntorch.int64\n\n\n(좋은 선언예시1)\n\ntsr = torch.tensor([1,2],dtype = torch.float32)\ntsr.dtype\n\ntorch.float32\n\n\n(좋은 선언예시2)\n\ntsr = torch.tensor([1,2.0])\ntsr.dtype\n\ntorch.float32\n\n\n(사실 int로 선언해도 나중에 float로 바꿔주면 큰 문제가 없음)\n\ntsr = torch.tensor([1,2]).float()\ntsr.dtype\n\ntorch.float32\n\n\n- torch.float32로 바꾸는게 유리한 이유 \\(\\to\\) torch.tensor 끼리의 연산에서 문제가 될 수 있음.\n(별 문제가 없는 경우)\n\ntorch.tensor([1,2])-torch.tensor([1.0,2.0]) \n\ntensor([0., 0.])\n\n\n(에러 1)\n\ntorch.tensor([[1.0,0.0],[0.0,1.0]]) @ torch.tensor([[1],[2]])\n\nRuntimeError: ignored\n\n\n(에러 2)\n\ntorch.tensor([[1,0],[0,1]]) @ torch.tensor([[1.0],[2.0]])\n\nRuntimeError: ignored\n\n\n(해결 1) 둘다 정수로 통일\n\ntorch.tensor([[1,0],[0,1]]) @ torch.tensor([[1],[2]])\n\ntensor([[1],\n        [2]])\n\n\n(해결 2) 둘다 소수로 통일 \\(\\to\\) 더 좋은 방법\n\ntorch.tensor([[1.0,0.0],[0.0,1.0]]) @ torch.tensor([[1.0],[2.0]])\n\ntensor([[1.],\n        [2.]])"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#shape-of-vector",
    "href": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#shape-of-vector",
    "title": "04. 딥러닝의 기초 (2)",
    "section": "shape of vector",
    "text": "shape of vector\n\nA = torch.tensor([[2.00,0.00],[0.00,3.00]])\nb1 = torch.tensor([[-1.0, -5.0]])\nb2 = torch.tensor([[-1.0], [-5.0]])\nb3 = torch.tensor([-1.0, -5.0])\n\n\nA.shape\n\ntorch.Size([2, 2])\n\n\n\nb1.shape, b2.shape, b3.shape\n\n(torch.Size([1, 2]), torch.Size([2, 1]), torch.Size([2]))\n\n\n- 브로드캐스팅\n\na = torch.tensor([1,2,3])\na -1\n\ntensor([0, 1, 2])\n\n\n\nb = torch.tensor([[1],[2],[3]])\nb-1\n\ntensor([[0],\n        [1],\n        [2]])\n\n\n\nb.shape, a.shape\n\n(torch.Size([3, 1]), torch.Size([3]))\n\n\n(a는 그냥 길이가 3인 벡터이지만 연산시, a를 row-vec으로 해석)\n$ a= [1, 2, 3], b=\n\\[\\begin{bmatrix} 1 \\\\ 2 \\\\ 3  \\end{bmatrix}\\]\na =\n\\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 1 & 2 & 3 \\\\ 1 & 2 & 3  \\end{bmatrix}\\]\n, b =\n\\[\\begin{bmatrix} 1 & 1 & 1 \\\\ 2 & 2 & 2 \\\\ 3 & 3 & 3  \\end{bmatrix}\\]\n$\n\na-b\n\ntensor([[ 0,  1,  2],\n        [-1,  0,  1],\n        [-2, -1,  0]])\n\n\n\n(a-b).shape\n\ntorch.Size([3, 3])"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#loss-sse",
    "href": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#loss-sse",
    "title": "04. 딥러닝의 기초 (2)",
    "section": "loss = SSE",
    "text": "loss = SSE\n\nWhat= torch.tensor([[-5.0],[10.0]],requires_grad = True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nfor epoc in range(30) : \n    yhat = X@What\n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.001*What.grad ## 갱신한 가중치 저장\n    What.grad = None\n\n\nplt.plot(x,y,\".\")\nplt.plot(x,(X@What).data,\"--\")\nplt.legend([\"y\",\"yhat\"])\n\n<matplotlib.legend.Legend at 0x7f51db312d30>\n\n\n\n\n\n\nSSE의 다른버전 = MSE\n\nWhat= torch.tensor([[-5.0],[10.0]],requires_grad = True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nfor epoc in range(30) : \n    yhat = X@What\n    loss = torch.mean((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.1*What.grad ## 갱신한 가중치 저장\n    What.grad = None\n\n\nplt.plot(x,y,\".\")\nplt.plot(x,(X@What).data,\"--\")\nplt.legend([\"y\",\"yhat\"])\n\n<matplotlib.legend.Legend at 0x7f51e24f1310>"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#ver1-벡터형",
    "href": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#ver1-벡터형",
    "title": "04. 딥러닝의 기초 (2)",
    "section": "ver1 : 벡터형",
    "text": "ver1 : 벡터형\n- parameter\n\nin_features : 입력차원\nout_features : 출력차원\nbias : 절편유무\n\n\ntorch.manual_seed(202150256)\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n\n\nnet이 만들어질 때 최초 가중치 값이 설정된다.\n\n\nnet.bias, net.weight\n\n(Parameter containing:\n tensor([-0.0483], requires_grad=True), Parameter containing:\n tensor([[-0.8675]], requires_grad=True))\n\n\n\nplt.plot(x,y,\".\")\nplt.plot(x,net(x).data,\"--\")\nplt.plot(x,-0.483-0.8675*x,\"-\")\n\n\n\n\n- 데이터로드\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/main/posts/II.%20DNN/2022-09-22-regression.csv\") \n\n\nx = torch.tensor(df.x).float()\ny = torch.tensor(df.y).float()"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#ver2-매트릭스",
    "href": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#ver2-매트릭스",
    "title": "04. 딥러닝의 기초 (2)",
    "section": "ver2 : 매트릭스",
    "text": "ver2 : 매트릭스\n\nyhat = net(X)\n\n\ntorch.manual_seed(202150256)\nnet = torch.nn.Linear(in_features=2, out_features=1,bias=False)\n\n\nnet.weight\n\nParameter containing:\ntensor([[-0.6134, -0.0341]], requires_grad=True)\n\n\n\\(w_0 = -0.6134, w_1 =-0.0341\\)\n\nX.shape\n\ntorch.Size([100, 2])\n\n\n\nplt.plot(x,y,\"o\")\nplt.plot(x,net(X).data,\"--\")\nplt.plot(x,X@torch.tensor([[-0.6134], [-0.0341]]),\".\")\n\n\n\n\n- 수식표현 : \\(\\bf {\\hat y} = \\bf{X\\hat W} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots & \\dots \\\\ 1 & x_{100} \\end{bmatrix} \\times \\begin{bmatrix} \\hat{w_0} \\\\ \\hat {w_1}\\end{bmatrix}\\)\n\n위 표현을 하는게 net의 역할\n\n(잘못된 사용1)\n\n컬럼벡터로 명시하지 않은 경우, 즉 단순히 길이가 100인 벡터를 선언\n\n\nx = x.reshape(100,1)\n_x = x.reshape(-1)\ny = y.reshape(100,1)\n_y = y.reshape(-1)\n\n\nnet = torch.nn.Linear(in_features= 1, out_features=1, bias=True)\n\n\nnet(_x)\n\nRuntimeError: ignored\n\n\n(잘못된 사용2)\n\nbias = False 생략\n\n- 수식표현 : \\(\\bf {\\hat y} = \\bf{X\\hat W} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots & \\dots \\\\ 1 & x_{100} \\end{bmatrix} \\times \\begin{bmatrix} \\hat{w_0} \\\\ \\hat {w_1}\\end{bmatrix} + c\\)\n\ntorch.manual_seed(202150256)\nnet = torch.nn.Linear(in_features= 2, out_features=1) ##bias=False를 깜빡\n\n\nnet.weight\n\nParameter containing:\ntensor([[-0.6134, -0.0341]], requires_grad=True)\n\n\n\nnet.bias\n\nParameter containing:\ntensor([0.6102], requires_grad=True)\n\n\n\nplt.plot(x,y,\"o\")\nplt.plot(x,net(X).data,\"--\")\nplt.plot(x,X@torch.tensor([[-0.6134], [-0.0341]])+0.6102,\".\")"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#ver1",
    "href": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#ver1",
    "title": "04. 딥러닝의 기초 (2)",
    "section": "ver1",
    "text": "ver1\n- 네트워크 준비\n\nnet = torch.nn.Linear(1, 1)\nnet.bias.data = torch.tensor([-5.0])\nnet.weight.data = torch.tensor([[-10.0]])\nnet.bias,net.weight\n\n(Parameter containing:\n tensor([-5.], requires_grad=True), Parameter containing:\n tensor([[-10.]], requires_grad=True))\n\n\n\nplt.plot(x,y,\"o\")\nplt.plot(x,net(x).data,\"--\")\nplt.plot(x,-5-10*x,\".\")\nplt.legend([\"y\",\"net\",\"yhat\"])\n\n<matplotlib.legend.Legend at 0x7f51d9f3f070>\n\n\n\n\n\n- step1 : \\(\\hat {y}\\)를 구하는 과정\n\nyhat = net(x)\n\n- step2 : \\(loss\\) 계산\n\nloss = torch.mean((y-yhat)**2)\nloss\n\ntensor(305.5515, grad_fn=<MeanBackward0>)\n\n\n- step3\n(미분전)\n\nnet.weight,net.bias\n\n(Parameter containing:\n tensor([[-10.]], requires_grad=True), Parameter containing:\n tensor([-5.], requires_grad=True))\n\n\n\nnet.bias.grad, net.weight.grad\n\n(None, None)\n\n\n(미분후)\n\nloss.backward()\n\n\nnet.bias.grad, net.weight.grad\n\n(tensor([-18.3235]), tensor([[-33.8568]]))\n\n\n- step4\n(업데이트 전)\n\nnet.weight, net.bias\n\n(Parameter containing:\n tensor([[-10.]], requires_grad=True), Parameter containing:\n tensor([-5.], requires_grad=True))\n\n\n\nnet.bias.grad, net.weight.grad\n\n(tensor([-18.3235]), tensor([[-33.8568]]))\n\n\n(업데이트)\n\nnet.bias.data = net.bias.data - 0.1 * net.bias.grad\nnet.weight.data = net.weight.data - 0.1 * net.weight.grad\n\n\nnet.bias.grad = None\nnet.weight.grad = None\n\n(업데이트 후)\n\nnet.bias, net.weight\n\n(Parameter containing:\n tensor([-3.1677], requires_grad=True), Parameter containing:\n tensor([[-6.6143]], requires_grad=True))\n\n\n\nnet.bias.grad, net.weight.grad\n\n(None, None)\n\n\n(위 과정을 반복)\n\nfor epoc in range(30) :\n    ## step1\n    yhat = net(x)\n\n    ## step2\n\n    loss = torch.mean((y-yhat)**2)\n\n    ## step3\n\n    loss.backward()\n\n    ## step4\n\n    net.bias.data = net.bias.data - 0.1*net.bias.grad\n    net.weight.data = net.weight.data - 0.1*net.weight.grad\n    net.bias.grad = None\n    net.weight.grad = None\n\n\nplt.plot(x,y,\".\")\nplt.plot(x,net(x).data,\"--\")\nplt.legend([\"y\",\"yhat\"])\n\n<matplotlib.legend.Legend at 0x7f51dacf5790>"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#ver2",
    "href": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#ver2",
    "title": "04. 딥러닝의 기초 (2)",
    "section": "ver2",
    "text": "ver2\n\nnet = torch.nn.Linear(2,1,bias=False)\n\n\nnet.weight\n\nParameter containing:\ntensor([[-0.4208, -0.1402]], requires_grad=True)\n\n\n\nnet.weight.data = torch.tensor([[-5.0, -10.0]])\n\n\nfor epoc in range(30) : \n    ## step1\n    yhat = net(X)\n    ## step2\n    loss = torch.mean((y-yhat)**2)\n    ## step3\n    loss.backward()\n    ## step4\n    net.weight.data = net.weight.data - 0.1*net.weight.grad\n    net.weight.grad = None\n\n\nplt.plot(x,y,\".\")\nplt.plot(x,net(X).data,\"--\")\nplt.legend([\"y\",\"yhat\"])\n\n<matplotlib.legend.Legend at 0x7f51daf5a9d0>"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#ver1-1",
    "href": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#ver1-1",
    "title": "04. 딥러닝의 기초 (2)",
    "section": "ver1",
    "text": "ver1\n\nnet = torch.nn.Linear(1,1) \nnet.bias.data = torch.tensor([-5.0])\nnet.weight.data = torch.tensor([[10.0]])\nnet.bias,net.weight\n\n(Parameter containing:\n tensor([-5.], requires_grad=True), Parameter containing:\n tensor([[10.]], requires_grad=True))\n\n\n\noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\n\n\nfor epoc in range(30):\n    ## step1\n    yhat = net(x) \n    ## step2\n    loss = torch.mean((y-yhat)**2)\n    ## step3 \n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(x).data,'--')\nplt.legend([\"y\",\"yhat\"])\n\n<matplotlib.legend.Legend at 0x7f51dac9bca0>"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#ver2-가장-중요",
    "href": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#ver2-가장-중요",
    "title": "04. 딥러닝의 기초 (2)",
    "section": "ver2 (가장 중요)",
    "text": "ver2 (가장 중요)\n\nnet = torch.nn.Linear(2,1,bias=False)\nnet.weight.data = torch.tensor([[-5.0, -10.0]])\n\n\noptimizr = torch.optim.SGD(net.parameters(),lr=0.1)\n\n\nfor epoc in range(30) :\n    ## step1\n    yhat = net(X)\n    ## step2\n    loss = torch.mean((y-yhat)**2)\n    ## step3\n    loss.backward()\n    ## step4\n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'.')\nplt.plot(x,net(X).data,'--')\nplt.legend([\"y\",\"yhat\"])\n\n<matplotlib.legend.Legend at 0x7f51daf6fee0>"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#motive",
    "href": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#motive",
    "title": "04. 딥러닝의 기초 (2)",
    "section": "motive",
    "text": "motive\n- \\(x\\)가 커질수록(혹은 작아질수록) 성공확률이 증가함.\n- \\((\\bf{X},y)= (스펙,합격)\\)\n\n_df = pd.DataFrame({'x':range(-6,7),'y':[0,0,0,0,0,0,1,0,1,1,1,1,1]})\n_df \n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      -6\n      0\n    \n    \n      1\n      -5\n      0\n    \n    \n      2\n      -4\n      0\n    \n    \n      3\n      -3\n      0\n    \n    \n      4\n      -2\n      0\n    \n    \n      5\n      -1\n      0\n    \n    \n      6\n      0\n      1\n    \n    \n      7\n      1\n      0\n    \n    \n      8\n      2\n      1\n    \n    \n      9\n      3\n      1\n    \n    \n      10\n      4\n      1\n    \n    \n      11\n      5\n      1\n    \n    \n      12\n      6\n      1\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndef f(x) :\n    return torch.exp(x) / (1+torch.exp(x))\n\nxx = torch.linspace(-6,6,100)\nplt.plot(_df.x,_df.y,'o')\nplt.plot(xx,f(xx))"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#model",
    "href": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#model",
    "title": "04. 딥러닝의 기초 (2)",
    "section": "model",
    "text": "model\n\n\\(y_i \\sim Ber\\), \\(\\pi_i = \\frac{exp(w_0+w_1x_i)}{1+exp(w_0+w_1x_i)} = \\frac{1}{1+exp(-(w_0+w_1x_i))}\\)\n\\(\\hat{y_i} = \\hat {\\pi_i} = \\frac{1}{1+exp(-(\\hat{w_0}+\\hat{w_1}x_i))}\\)\n\\(loss = -\\sum_{i=1}^{n}{(y_i \\log(\\hat{y_i}) + (1-y_i) \\log(1-\\hat{y_i}))}\\)"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#toy-example",
    "href": "post/Lecture/STML/2023-02-11 DNN 딥러닝의 기초 (2).html#toy-example",
    "title": "04. 딥러닝의 기초 (2)",
    "section": "toy example",
    "text": "toy example\n\nx = torch.linspace(-1, 1, 2000).reshape(2000,1)\nw0  = -1\nw1 = 5\nu = w0+w1*x\nv = torch.exp(u) /(1+torch.exp(u))\ny = torch.bernoulli(v)\n\n\nplt.plot(x,y,'o',alpha=0.05,ms=4)\nplt.plot(x,v,'--')\n\n\n\n\n- 최초의 곡선\n\nw0hat= -1\nw1hat = 1\nyhat = f(w0hat+x*w1hat)\nplt.plot(x,y,'o',alpha=0.05,ms=4)\nplt.plot(x,v,'--')\nplt.plot(x,yhat,'--r')\nplt.legend([\"y\",\"v\",\"1st yhat\"])\n\n<matplotlib.legend.Legend at 0x7f51d9b71ee0>\n\n\n\n\n\n\nl1=torch.nn.Linear(1,1)\nl1.bias.data=torch.tensor([-1.0])\nl1.weight.data = torch.tensor([[1.0]])\n\n\na1=torch.nn.Sigmoid()\n\n\nfor epoc in range(6000):\n    ## step1 \n    _yhat = a1(l1(x))\n    ## step2 \n    loss = torch.mean((y-_yhat)**2) ## loss 를 원래 이렇게 하는건 아니에요.. \n    ## step3 \n    loss.backward()\n    ## step4 \n    l1.bias.data = l1.bias.data - 0.1 * l1.bias.grad \n    l1.weight.data = l1.weight.data - 0.1 * l1.weight.grad \n    l1.bias.grad = None \n    l1.weight.grad = None \n\n\nplt.plot(x,y,'o',alpha=0.05,ms=4)\nplt.plot(x,v,'--')\nplt.plot(x,yhat,'--r')\nplt.plot(x,_yhat.data,\"--b\")\nplt.legend([\"y\",\"true v\",\"1st yhat\",\"final yhat\"])\n\n<matplotlib.legend.Legend at 0x7f51d9d22b20>"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "",
    "text": "# imports"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#준비-1-loss_fn을-plot하는-함수",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#준비-1-loss_fn을-plot하는-함수",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "준비 1 : loss_fn을 plot하는 함수",
    "text": "준비 1 : loss_fn을 plot하는 함수\n\ndef plot_loss(loss_fn,ax=None):\n    if ax==None:\n        fig = plt.figure()\n        ax=fig.add_subplot(1,1,1,projection='3d')\n        ax.elev=15;ax.azim=75\n    w0hat,w1hat =torch.meshgrid(torch.arange(-10,3,0.15),torch.arange(-1,10,0.15),indexing='ij')\n    w0hat = w0hat.reshape(-1)\n    w1hat = w1hat.reshape(-1)\n    def l(w0hat,w1hat):\n        yhat = torch.exp(w0hat+w1hat*x)/(1+torch.exp(w0hat+w1hat*x))\n        return loss_fn(yhat,y) \n    loss = list(map(l,w0hat,w1hat))\n    ax.scatter(w0hat,w1hat,loss,s=0.1,alpha=0.2) \n    ax.scatter(-1,5,l(-1,5),s=200,marker='*') # 실제로 -1,5에서 최소값을 가지는건 아님.."
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#준비-2-for문-대신-돌려주고-epoch마다-필요한-정보를-기록하는함수",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#준비-2-for문-대신-돌려주고-epoch마다-필요한-정보를-기록하는함수",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "준비 2 : for문 대신 돌려주고 epoch마다 필요한 정보를 기록하는함수",
    "text": "준비 2 : for문 대신 돌려주고 epoch마다 필요한 정보를 기록하는함수\n\ndef learn_and_record(net, loss_fn, optimizr):\n    yhat_history = [] \n    loss_history = []\n    what_history = [] \n\n    for epoc in range(1000): \n        ## step1 \n        yhat = net(x)\n        ## step2 \n        loss = loss_fn(yhat,y)\n        ## step3\n        loss.backward() \n        ## step4 \n        optimizr.step()\n        optimizr.zero_grad() \n\n        ## record \n        if epoc % 20 ==0: \n            yhat_history.append(yhat.reshape(-1).data.tolist())\n            loss_history.append(loss.item())\n            what_history.append([net[0].bias.data.item(), net[0].weight.data.item()])\n    return yhat_history, loss_history, what_history"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#준비-3-애니매이션-함수",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#준비-3-애니매이션-함수",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "준비 3: 애니매이션 함수",
    "text": "준비 3: 애니매이션 함수\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\ndef show_lrpr2(net,loss_fn,optimizr,suptitle=''):\n    yhat_history,loss_history,what_history = learn_and_record(net,loss_fn,optimizr)\n    \n    fig = plt.figure(figsize=(7,2.5))\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n    ax1.set_xticks([]);ax1.set_yticks([])\n    ax2.set_xticks([]);ax2.set_yticks([]);ax2.set_zticks([])\n    ax2.elev = 15; ax2.azim = 75\n\n    ## ax1: 왼쪽그림 \n    ax1.plot(x,v,'--')\n    ax1.scatter(x,y,alpha=0.05)\n    line, = ax1.plot(x,yhat_history[0],'--') \n    plot_loss(loss_fn,ax2)\n    fig.suptitle(suptitle)\n    fig.tight_layout()\n\n    def animate(epoc):\n        line.set_ydata(yhat_history[epoc])\n        ax2.scatter(np.array(what_history)[epoc,0],np.array(what_history)[epoc,1],loss_history[epoc],color='grey')\n        return line\n\n    ani = animation.FuncAnimation(fig, animate, frames=30)\n    plt.close()\n    return ani"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#시각화-1-mse-좋은초기값",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#시각화-1-mse-좋은초기값",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "시각화 1: MSE, 좋은초기값",
    "text": "시각화 1: MSE, 좋은초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n\nl1,a1 = net \n\n(초기값 설정)\n\nl1.weight.data = torch.tensor([[-1.0]]) \nl1.bias.data = torch.tensor([-3.0])\n\n\nshow_lrpr2(net,loss_fn,optimizr,'MSELoss, good_init // SGD') \n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#시각화2-mse-나쁜-초기값",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#시각화2-mse-나쁜-초기값",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "시각화2 : MSE, 나쁜 초기값",
    "text": "시각화2 : MSE, 나쁜 초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n\nl1,a1 = net \nl1.weight.data = torch.tensor([[-1.0]])\nl1.bias.data = torch.tensor([-10.0])\n\nshow_lrpr2(net,loss_fn,optimizr,'MSELoss, bad_init // SGD') \n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#시각화3-bce-좋은초기값",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#시각화3-bce-좋은초기값",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "시각화3 : BCE, 좋은초기값",
    "text": "시각화3 : BCE, 좋은초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n\nl1,a1 = net \n\nl1.weight.data = torch.tensor([[-1.0]])\nl1.bias.data = torch.tensor([-3.0])\n\nshow_lrpr2(net,loss_fn,optimizr,'BCELoss, good_init // SGD') \n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#시각화4-bce-나쁜초기값",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#시각화4-bce-나쁜초기값",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "시각화4 : BCE, 나쁜초기값",
    "text": "시각화4 : BCE, 나쁜초기값\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss() \noptimizr = torch.optim.SGD(net.parameters(),lr=0.05) \n\nl1,a1 = net \n\nl1.weight.data = torch.tensor([[-1.0]])\nl1.bias.data = torch.tensor([-10.0])\n\nshow_lrpr2(net,loss_fn,optimizr,'BCELoss, bad_init // SGD') \n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n(loss function을 어떤걸 쓰내에 따라 최적해에 도달하는 정도가 다르다 \\(\\to\\) epoch 횟수를 줄일 수 있음)"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#신문기사-데이터의-모티브",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#신문기사-데이터의-모티브",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "신문기사 (데이터의 모티브)",
    "text": "신문기사 (데이터의 모티브)\n- 스펙이 높아도 취업이 안된다.\n- \\(x\\) 가 커지면 \\(y\\) 가 떨어진다."
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#가짜데이터",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#가짜데이터",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "가짜데이터",
    "text": "가짜데이터\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/main/posts/II.%20DNN/2022-10-04-dnnex0.csv')\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      x\n      underlying\n      y\n    \n  \n  \n    \n      0\n      -1.000000\n      0.000045\n      0.0\n    \n    \n      1\n      -0.998999\n      0.000046\n      0.0\n    \n    \n      2\n      -0.997999\n      0.000047\n      0.0\n    \n    \n      3\n      -0.996998\n      0.000047\n      0.0\n    \n    \n      4\n      -0.995998\n      0.000048\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      0.995998\n      0.505002\n      0.0\n    \n    \n      1996\n      0.996998\n      0.503752\n      0.0\n    \n    \n      1997\n      0.997999\n      0.502501\n      0.0\n    \n    \n      1998\n      0.998999\n      0.501251\n      1.0\n    \n    \n      1999\n      1.000000\n      0.500000\n      1.0\n    \n  \n\n2000 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n- 스펙이 굉장히 높을 경우 불합격하는 경우가 발생\n\nplt.plot(df.x,df.y,\"o\",alpha=0.01)\nplt.plot(df.x,df.underlying,\"--b\")"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#로지스틱-회귀로-적합",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#로지스틱-회귀로-적합",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "로지스틱 회귀로 적합",
    "text": "로지스틱 회귀로 적합\n\n x = torch.tensor(df.x).float().reshape(-1,1)\n y = torch.tensor(df.y).float().reshape(-1,1)\n\n\ntorch.manual_seed(202150256)\nnet = torch.nn.Sequential(\n    \n    torch.nn.Linear(in_features = 1, out_features=1, bias = True),\n    torch.nn.Sigmoid()\n)\n\nyhat = net(x)\nloss_fn = torch.nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters())\n\n- 모델 학습전\n\nplt.plot(x,y,\"o\",alpha = 0.01)\nplt.plot(df.x,df.underlying,\"--b\")\n\n\n\n\n\nfor epoc in range(6000) :\n  ## step1\n  yhat = net(x)\n  ## step2\n  loss = loss_fn(yhat,y)\n  ## step3\n  loss.backward()\n  ## step4\n  optimizr.step()\n  optimizr.zero_grad()\n\n\n학습이 잘안됨\n\n예상 1 : 에폭이 부족\n예상 2 : 옵티마이저가 잘못된 경우\n예상 3 : 학습률이 잘못된 경우\n예상 4 : 손실함수가 잘못된 경우\n\n그러나 아래의 경우 이중에 해당되는 것이 없다.\n\n이유 : 로지스틱의 겨우는 \\(x\\)가 커질수록 \\(y=1\\) 또는 \\(y=0\\) 이 잘 나와야함. 즉, 올라갔다가 내려가는 이 경우에는 설계한 네트워크의 표현력이 부족 \\(\\to\\) underfitting\n\n\n\nplt.plot(x,y,\"o\",alpha = 0.01)\nplt.plot(df.x,df.underlying,\"--b\")\nplt.plot(x,net(x).data,\"--\")\n\n\n\n\n\n해결책\n- sigmoid를 취하기 전의 상태가 꺽인 그래프여야 한다.\n\nsig = torch.nn.Sigmoid()"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#꺽인-그래프를-만드는-방법1",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#꺽인-그래프를-만드는-방법1",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "꺽인 그래프를 만드는 방법1",
    "text": "꺽인 그래프를 만드는 방법1\n- 중요한 코드는 아님\n\nu = [9*xi +4.5 if xi < 0 else -4.5*xi+4.5 for xi in x.reshape(-1).tolist()]\nplt.plot(u)\n\n\n\n\n- 위 코드를 안쓰는 이유는 \\(\\hat y\\)의 합리적 추정에 있어서 torch에 있는 linear transform을 거치는 등에 과정을 거치는 것이 좋다."
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#꺽인-그래프를-만드는-방법2",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#꺽인-그래프를-만드는-방법2",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "꺽인 그래프를 만드는 방법2",
    "text": "꺽인 그래프를 만드는 방법2\n- 전략 : 선형변환 \\(\\to\\) ReLU \\(\\to\\) 선형변환\n(예비학습) ReLU 함수란?\n\\(ReLU(x) = max(0,x)\\)\n\nrlu = torch.nn.ReLU()\n\n\nplt.title(\"ReLU\")\nplt.plot(x)\nplt.plot(rlu(x))\nplt.legend([\"x\",\"ReLU(x)\"])\n\n<matplotlib.legend.Legend at 0x7f71c2f23070>\n\n\n\n\n\n(선형변환1)\n\nplt.plot(x,color=\"C1\")\nplt.plot(-x,color=\"C0\")\nplt.legend([\"x\",\"- x\"])\n\n<matplotlib.legend.Legend at 0x7f71c286ca60>\n\n\n\n\n\n(렐루)\n\nplt.plot(x,color=\"C1\",alpha=0.3)\nplt.plot(-x,color=\"C0\",alpha=0.3)\nplt.plot(rlu(x),color=\"C1\",alpha=0.5)\nplt.plot(rlu(-x),color=\"C0\",alpha=0.5)\nplt.legend([\"x\",\"- x\", \"relu(x)\",\"relu(-x)\"])\n\n<matplotlib.legend.Legend at 0x7f71c261a880>\n\n\n\n\n\n(선형변환2)\n\noutput_features 를 2개 만든다.\n\n\\(\\bf{\\mu}= [\\mu_1,\\dots, \\mu_{2000}],\\quad \\mu_i = \\begin {cases} 9x_i +4.5, \\quad x_i < 0 \\\\ -4.5x_i + 4.5, \\quad x_i>0 \\end{cases}\\)\n\nplt.plot(x,color=\"C1\",alpha=0.3)\nplt.plot(-x,color=\"C0\",alpha=0.3)\nplt.plot(rlu(x),color=\"C1\",alpha=0.5)\nplt.plot(rlu(-x),color=\"C0\",alpha=0.5)\nplt.plot(-4.5*rlu(x) + -9*rlu(-x)+4.5,color=\"C2\") # = plt.plot(u)\n\n\n\n\n(시그모이드)\n\nplt.plot(sig(-4.5*rlu(x) + -9*rlu(-x)+4.5),color=\"C2\")\n\n\n\n\n정리!\n\\(\\bf{\\mu}= [\\mu_1,\\dots, \\mu_{2000}],\\quad \\mu_i = \\begin {cases} 9x_i +4.5, \\quad x_i < 0 \\\\ -4.5x_i + 4.5, \\quad x_i>0 \\end{cases}\\)\n\nfig = plt.figure(figsize=(8, 4))\nspec = fig.add_gridspec(4, 4)\nax1 = fig.add_subplot(spec[:2,0]); ax1.set_title('x'); ax1.plot(x,'--',color='C0')\nax2 = fig.add_subplot(spec[2:,0]); ax2.set_title('-x'); ax2.plot(-x,'--',color='C1')\nax3 = fig.add_subplot(spec[:2,1]); ax3.set_title('relu(x)'); ax3.plot(rlu(x),'--',color='C0')\nax4 = fig.add_subplot(spec[2:,1]); ax4.set_title('relu(-x)'); ax4.plot(rlu(-x),'--',color='C1')\nax5 = fig.add_subplot(spec[1:3,2]); ax5.set_title('u'); ax5.plot(-4.5*rlu(x)-9*rlu(-x)+4.5,'--',color='C2')\nax6 = fig.add_subplot(spec[1:3,3]); ax6.set_title('yhat'); ax6.plot(sig(-4.5*rlu(x)-9*rlu(-x)+4.5),'--',color='C2')\nfig.tight_layout()"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#torch.nn.linear를-이용한-꺽인-그래프-구현",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#torch.nn.linear를-이용한-꺽인-그래프-구현",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "torch.nn.Linear()를 이용한 꺽인 그래프 구현",
    "text": "torch.nn.Linear()를 이용한 꺽인 그래프 구현\n- 차원만 따져보자.\n\\(\\underset{(n,1)}{\\bf{X}} \\overset{l_1}{\\to} \\underset{(n,2)}{\\bf{\\mu^{(1)}}} \\overset{a_1}{\\to}\\underset{(n,2)}{\\bf{v^{(1)}}} \\overset{l_2}{\\to} \\underset{(0,2)}{\\mu^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{\\bf{v^{(2)}}} = \\underset{(n,1)} {\\hat {y}}\\)\n\nl1 = torch.nn.Linear(in_features=1,out_features=2,bias=True)\na1 = torch.nn.ReLU()\nl2 = torch.nn.Linear(in_features=2,out_features=1,bias=True)\na2 = torch.nn.Sigmoid()\n\n- 초기값 설정\n\nl1.weight.data = torch.tensor([[1.0],[-1.0]])\nl1.bias.data = torch.tensor([0.0, 0.0])\nl2.weight.data= torch.tensor([[-4.5,-9.0]])\nl2.bias.data = torch.tensor([4.5])\n\n\nnet = torch.nn.Sequential(l1,a1,l2,a2)\n\n\nplt.plot(y,'o',alpha=0.02)\nplt.plot(df.underlying,'--b')\nplt.plot(net(x).data) #plt.plot(a2(l2(a1(l1(x)))).data)"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#step1-step4",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#step1-step4",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "step1 ~ step4",
    "text": "step1 ~ step4\n\ntorch.manual_seed(1111111)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=2),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=2,out_features=1), \n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(3000):\n    ## 1 \n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y,'o',alpha=0.02)\nplt.plot(df.underlying,'--b')\nplt.plot(net(x).data)\n\n\n\n\n- 추가로 3000번 더\n\nfor epoc in range(3000):\n    ## 1 \n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y,'o',alpha=0.02)\nplt.plot(df.underlying,'--b')\nplt.plot(net(x).data)"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#예제-1",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#예제-1",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "예제 1",
    "text": "예제 1\n\nunderlying이 꺽은선으로 절대 표현할 수 없는 경우\npiece-wise linear로 해결가능\n\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/main/posts/II.%20DNN/2022-10-04-dnnex1.csv')\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      x\n      underlying\n      y\n    \n  \n  \n    \n      0\n      -1.000000\n      0.999877\n      1.0\n    \n    \n      1\n      -0.998999\n      0.999875\n      1.0\n    \n    \n      2\n      -0.997999\n      0.999873\n      1.0\n    \n    \n      3\n      -0.996998\n      0.999871\n      1.0\n    \n    \n      4\n      -0.995998\n      0.999869\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      0.995998\n      0.000123\n      0.0\n    \n    \n      1996\n      0.996998\n      0.000123\n      0.0\n    \n    \n      1997\n      0.997999\n      0.000123\n      0.0\n    \n    \n      1998\n      0.998999\n      0.000123\n      0.0\n    \n    \n      1999\n      1.000000\n      0.000123\n      0.0\n    \n  \n\n2000 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nplt.plot(df.x,df.y,'o',alpha=0.05)\nplt.plot(df.x,df.underlying,'--')\n\n\n\n\n- 데이터 정리\n\nx= torch.tensor(df.x).float().reshape(-1,1)\ny= torch.tensor(df.y).float().reshape(-1,1)\n\n\nout_features를 32로 설정\n\n\ntorch.manual_seed(11111)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=32,out_features=1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(20000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.01)\nplt.plot(df.x,df.underlying,'--b')\nplt.plot(x,net(x).data,'--')"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#예제-2",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#예제-2",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "예제 2",
    "text": "예제 2\n\n곡선분포의경우 \\(\\to\\) 역시 piece-wise linear로 어느정도 맞출 수 있음\n\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/main/posts/II.%20DNN/2022-10-04-dnnex2.csv')\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      x\n      underlying\n      y\n    \n  \n  \n    \n      0\n      -1.000000\n      14.791438\n      14.486265\n    \n    \n      1\n      -0.999000\n      14.756562\n      14.832600\n    \n    \n      2\n      -0.997999\n      14.721663\n      15.473211\n    \n    \n      3\n      -0.996999\n      14.686739\n      14.757734\n    \n    \n      4\n      -0.995998\n      14.651794\n      15.042901\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      0.995998\n      5.299511\n      5.511416\n    \n    \n      1996\n      0.996999\n      5.322140\n      6.022263\n    \n    \n      1997\n      0.997999\n      5.344736\n      4.989637\n    \n    \n      1998\n      0.999000\n      5.367299\n      5.575369\n    \n    \n      1999\n      1.000000\n      5.389829\n      5.466730\n    \n  \n\n2000 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nplt.plot(df.x,df.y,'o',alpha=0.05)\n\n\n\n\n\nx= torch.tensor(df.x).float().reshape(-1,1)\ny= torch.tensor(df.y).float().reshape(-1,1)\n\ntorch.manual_seed(5)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=32,out_features=1) \n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(20000):\n    ## 1\n    yhat = net(x) \n    ## 2\n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(df.x,df.y,'o',alpha=0.05)\nplt.plot(df.x,df.underlying,'--b')\nplt.plot(x,net(x).data,'--',lw=5)"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#예제-3",
    "href": "post/Lecture/STML/2023-02-18 DNN 딥러닝의 기초 (3).html#예제-3",
    "title": "05. 딥러닝의 기초 (3)",
    "section": "예제 3",
    "text": "예제 3\n\n중구난방 \\(\\to\\) piece-wise로 해결\n\n\nimport seaborn as sns\n\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/main/posts/II.%20DNN/2022-10-04-dnnex3.csv')\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      x1\n      x2\n      y\n    \n  \n  \n    \n      0\n      -0.874139\n      0.210035\n      0.0\n    \n    \n      1\n      -1.143622\n      -0.835728\n      1.0\n    \n    \n      2\n      -0.383906\n      -0.027954\n      0.0\n    \n    \n      3\n      2.131652\n      0.748879\n      1.0\n    \n    \n      4\n      2.411805\n      0.925588\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      -0.002797\n      -0.040410\n      0.0\n    \n    \n      1996\n      -1.003506\n      1.182736\n      0.0\n    \n    \n      1997\n      1.388121\n      0.079317\n      0.0\n    \n    \n      1998\n      0.080463\n      0.816024\n      1.0\n    \n    \n      1999\n      -0.416859\n      0.067907\n      0.0\n    \n  \n\n2000 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nsns.scatterplot(data=df,x='x1',y='x2',hue='y')\n\n<AxesSubplot:xlabel='x1', ylabel='x2'>\n\n\n\n\n\n\nx1= torch.tensor(df.x1).float().reshape(-1,1)\nx2= torch.tensor(df.x2).float().reshape(-1,1)\nX = torch.concat([x1,x2],axis=1)\ny=  torch.tensor(df.y).float().reshape(-1,1)\n\n\ntorch.manual_seed(202150256)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=2,out_features=64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=64,out_features=1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(20000):\n    ## 1\n    yhat = net(X) \n    ## 2\n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\ndf2 = df.assign(yhat=yhat.data.reshape(-1))\n\n\nsns.scatterplot(data=df2,x='x1',y='x2',hue='yhat')\n\n<AxesSubplot:xlabel='x1', ylabel='x2'>\n\n\n\n\n\n\n꽤 합리적으로 적합한 것 같다."
  },
  {
    "objectID": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html",
    "href": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html",
    "title": "06. 딥러닝의 기초 (4)",
    "section": "",
    "text": "강의출처 : 전북대학교 기계학습특강 6주차"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#지나시간-논리전개",
    "href": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#지나시간-논리전개",
    "title": "06. 딥러닝의 기초 (4)",
    "section": "지나시간 논리전개",
    "text": "지나시간 논리전개\n- 아이디어 : 아이디어: linear -> relu -> linear (-> sigmoid) 조합으로 꺽은선으로 표현되는 underlying 을 표현할 수 있었다.\n\n실제로 꺽은선이 아니라도 꺽은선의 조합인 곡선도 표현할 수 있다.\nX가 (n, 2)인 경우도 가능했다. (이 경우 꺽인선이 꺽인 평면이 된다.?? )\n아이디어에 해당하는 용어정리: 이 구조가 x->y 로 바로 가는 것이 아니라 x->(u1->v1)->(u2->v2)=y 의 구조인데 이러한 네트워크를 하나의 은닉층을 포함하는 네트워크라고 표현한다. (이 용어는 이따가..)"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#시벤코-정리",
    "href": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#시벤코-정리",
    "title": "06. 딥러닝의 기초 (4)",
    "section": "시벤코 정리",
    "text": "시벤코 정리\nuniversal approximation thm : (범용근사정리,보편근사정리,시벤코정리), 1989\n\n하나의 은닉층을 가지는 “linear -> sigmoid -> linear” 꼴의 네트워크를 이용하여 세상에 존재하는 모든 연속함수를 원하는 정확도로 근사시킬 수 있다.(계수를 잘 추정한다면)\n\n- 이해가 안되는 부분\n\n그렇게 잘 맞추면 1989년에 세상의 모든 문제를 다 풀어야 한거 아닌가?\n요즘은 “linear -> sigmoid -> linear”이 아닌 “linear -> relu -> linear” 조합으로 많이 쓰던데\n요즘은 하나의 은닉층만 포함하는 네트워크는 잘 안쓰지 않나?\n\n- 약간의 의구심이 있지만 해당 이론에 따르면 아래와 같다.\n\n\\(\\bf {X} : (n,p)\\) 꼴의 입력에서 \\(\\bf{y} : (n,1)\\) 꼴의 출력으로 향하는 맵핑을 “linear -> relu -> linear” 와 같은 네트워크를 이용해 “근사”시킬 수 있다."
  },
  {
    "objectID": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#proof",
    "href": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#proof",
    "title": "06. 딥러닝의 기초 (4)",
    "section": "Proof",
    "text": "Proof\n\n그림으로 보는 증명과정\n\nx = torch.linspace(-10,10,200).reshape(200,1)\n\n- 아래와 같은 네트워크를 고려\n\nl1 = torch.nn.Linear(in_features =1, out_features=2)\na1 = torch.nn.Sigmoid()\nl2 = torch.nn.Linear(in_features =2, out_features=1)\n\n- 직관 1: \\(l_1,l_2\\)의 가중치를 잘 겹합하다보면 우연히 아래와 같이 만들 수 있다.\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([10.00,10.00])\n\n\nl2.weight.data = torch.tensor([[1.00,1.00]])\nl2.bias.data = torch.tensor([-1.00])\n\n\nfig,ax = plt.subplots(1,3,figsize=(9,3))\nax[0].plot(x,l1(x).data); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,color='C2'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$')\n\nText(0.5, 1.0, '$(l_2 \\\\circ a_1 \\\\circ \\\\l_1)(x)$')\n\n\n\n\n\n- 직괸2 : 아래들도 가능할 것 같다.\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+0.00,+20.00])\nl2.weight.data = torch.tensor([[1.00,1.00]])\nl2.bias.data = torch.tensor([-1.00])\nfig,ax = plt.subplots(1,3,figsize=(9,3))\nax[0].plot(x,l1(x).data,'--',color='C0'); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data,'--',color='C0'); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,'--',color='C0'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$');\n\n\n\n\n\nl1.weight.data = torch.tensor([[-5.00],[5.00]])\nl1.bias.data = torch.tensor([+20.00,+0.00])\nl2.weight.data = torch.tensor([[2.50,2.50]])\nl2.bias.data = torch.tensor([-2.50])\nax[0].plot(x,l1(x).data,'--',color='C1'); ax[0].set_title('$l_1(x)$')\nax[1].plot(x,a1(l1(x)).data,'--',color='C1'); ax[1].set_title('$(a_1 \\circ l_1)(x)$')\nax[2].plot(x,l2(a1(l1(x))).data,'--',color='C1'); ax[2].set_title('$(l_2 \\circ a_1 \\circ \\l_1)(x)$');\nfig\n\n\n\n\n- 은닉층의노드수=4로 하고 적당한 가중치를 조정하면 ($l_2 a_1 l_1)(x) $ 의 결과로 주황색선 + 파란색선도 가능할 것 같다. 실제로 가능함\n\nl1 = torch.nn.Linear(in_features=1,out_features=4)\na1 = torch.nn.Sigmoid()\nl2 = torch.nn.Linear(in_features=4,out_features=1)\n\n\nl1.weight.data = torch.tensor([[-5.00],[5.00],[-5.00],[5.00]])\nl1.bias.data = torch.tensor([0.00, 20.00, 20.00, 0])\nl2.weight.data = torch.tensor([[1.00,  1.00, 2.50,  2.50]])\nl2.bias.data = torch.tensor([-1.0-2.5])\n\n\nplt.plot(l2(a1(l1(x))).data)\n\n\n\n\n- 2개의 시그모이드를 우현히 잘 겹합하면 아래와 같은 함수 \\(h\\) 를 만들 수 있다.\n\nh = lambda x : torch.sigmoid(200*(x+0.5)) + torch.sigmoid(-200*(x-0.5))-1.0\n\n\nplt.plot(x,h(x))\nplt.title(\"$h(x)$\")\n\nText(0.5, 1.0, '$h(x)$')\n\n\n\n\n\n-위와 같은 함수 \\(h\\) 를 활성화함수로 하고 \\(m\\) 개의 노드를 가지는 은닉층을 생각해보자. 이러한 은닉층을 사용한다면 전체 네트워크를 아래와 같이 표현할 수 있다.\n\\(\\underset{(n,1)} {\\bf{X}} \\overset{l_1}{\\to} \\underset{(n,m)}{u^{(1)}} \\overset{h}{\\to} \\underset{(n,m)}{v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\bf{\\hat{y}}}\\)\n그리고 위의 네트워크와 동일한 효과를 주는 아래의 네트워크가 항상 존재함\n\\(\\underset{(n,1)} {\\bf{X}} \\overset{l_1}{\\to} \\underset{(n,2m)}{u^{(1)}} \\overset{sig}{\\to} \\underset{(n,2m)}{v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{\\bf{\\hat{y}}}\\)\n- 첫 번쨰 루틴을 증명하면 두 번쨰 루틴을 증명하는 것과 같다.\n\nh = sig + sig 이므로\n\n- \\(h(x)\\)를 활성화함수로 가지는 네트워크를 설계\n\nclass MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법\n    def __init__(self):\n        super().__init__() \n    def forward(self, input): ## x에서 y로 가는 함수!\n        return h(input) # activation 의 출력 \n\n\na1=MyActivation()\n# a1 = torch.nn.Sigmoid(), a1 = torch.nn.ReLU() 대신에 a1 = MyActivation()\n\n\nplt.plot(x,a1(x)) \n\n\n\n\n- 히든레이어가 1개의 노드를 가지는 경우 \\(\\to\\) 총 16개의 경우를 그린거임, 16번의 새로운 계수를 이용한 거!\n\ntorch.manual_seed(202150256)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,1),\n            MyActivation(),\n            torch.nn.Linear(1,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n- 히든레이어가 2개의 노드를 가지는 경우\n\ntorch.manual_seed(202150256)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,2),\n            MyActivation(),\n            torch.nn.Linear(2,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n- 히든레이어가 3개의 노드를 가지는 경우\n\ntorch.manual_seed(202150256)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,3),\n            MyActivation(),\n            torch.nn.Linear(3,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n- 히든레이어가 1024개의 노드를 가지는 경우\n\ntorch.manual_seed(202150256)\nfig, ax = plt.subplots(4,4,figsize=(12,12))\nfor i in range(4):\n    for j in range(4):\n        net = torch.nn.Sequential(\n            torch.nn.Linear(1,1024),\n            MyActivation(),\n            torch.nn.Linear(1024,1)\n        )\n        ax[i,j].plot(x,net(x).data,'--')\n\n\n\n\n- 요약 : 하나의 은닉층에 노드의 개수를 늘리면 다양한 연속함수를 표현할 수 있다. \\(\\to\\) 단, 초기 가중치 계수가 우연히 잘 맞추어져야 한다."
  },
  {
    "objectID": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#예제-1-sin-exp",
    "href": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#예제-1-sin-exp",
    "title": "06. 딥러닝의 기초 (4)",
    "section": "예제 1 (sin, exp)",
    "text": "예제 1 (sin, exp)\n- 아래와 같은 underlying을 가지는 \\(\\hat {y}\\)를 만들어보자\n\ntorch.manual_seed(202150256)\nx = torch.linspace(-10,10,200).reshape(-1,1)\nunderlying = torch.sin(2*x) + torch.sin(0.5*x) + torch.exp(-0.2*x)\neps = torch.randn(200).reshape(-1,1)*0.1\ny = underlying + eps \nplt.plot(x,y,'o',alpha=0.5)\nplt.plot(x,underlying,lw=3)\nplt.legend([\"y\",\"underlying\"])\n\n<matplotlib.legend.Legend at 0x7f65d936f970>\n\n\n\n\n\n\nh = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0\nclass MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법\n    def __init__(self):\n        super().__init__() \n    def forward(self, input):\n        return h(input) \n\n\nnet= torch.nn.Sequential(\n    torch.nn.Linear(1,2048),\n    MyActivation(),\n    torch.nn.Linear(2048,1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters()) \n\n\nfor epoc in range(200):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.2)\nplt.plot(x,underlying,lw=3)\nplt.plot(x,net(x).data,'--')\nplt.legend([\"y\",\"underlying\",\"net(x)\"])\n\n<matplotlib.legend.Legend at 0x7f65f227cdc0>"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#예제2-스펙높아도-취업-x",
    "href": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#예제2-스펙높아도-취업-x",
    "title": "06. 딥러닝의 기초 (4)",
    "section": "예제2 (스펙높아도 취업 x)",
    "text": "예제2 (스펙높아도 취업 x)\n\ndf=pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/main/posts/II.%20DNN/2022-10-04-dnnex0.csv')\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      x\n      underlying\n      y\n    \n  \n  \n    \n      0\n      -1.000000\n      0.000045\n      0.0\n    \n    \n      1\n      -0.998999\n      0.000046\n      0.0\n    \n    \n      2\n      -0.997999\n      0.000047\n      0.0\n    \n    \n      3\n      -0.996998\n      0.000047\n      0.0\n    \n    \n      4\n      -0.995998\n      0.000048\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      1995\n      0.995998\n      0.505002\n      0.0\n    \n    \n      1996\n      0.996998\n      0.503752\n      0.0\n    \n    \n      1997\n      0.997999\n      0.502501\n      0.0\n    \n    \n      1998\n      0.998999\n      0.501251\n      1.0\n    \n    \n      1999\n      1.000000\n      0.500000\n      1.0\n    \n  \n\n2000 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nx = torch.tensor(df.x).reshape(-1,1).float()\ny = torch.tensor(df.y).reshape(-1,1).float()\nplt.plot(x,y,'o',alpha=0.1)\nplt.plot(df.x,df.underlying,lw=3)\n\n\n\n\n\nh = lambda x: torch.sigmoid(200*(x+0.5))+torch.sigmoid(-200*(x-0.5))-1.0\nclass MyActivation(torch.nn.Module): ## 사용자정의 활성화함수를 선언하는 방법\n    def __init__(self):\n        super().__init__() \n    def forward(self, input):\n        return h(input) \n\n\ntorch.manual_seed(2)\nnet= torch.nn.Sequential(\n    torch.nn.Linear(1,2048),\n    MyActivation(),\n    torch.nn.Linear(2048,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters()) \n\n\nfor epoc in range(100):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(x,y,'o',alpha=0.2)\nplt.plot(df.x,df.underlying,lw=3)\nplt.plot(x,net(x).data,'--')\nplt.legend([\"y\",\"underlying\",\"net(x)\"])\n\n<matplotlib.legend.Legend at 0x7f65d9af7280>"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#예제-3-mnist-data-with-dnn",
    "href": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#예제-3-mnist-data-with-dnn",
    "title": "06. 딥러닝의 기초 (4)",
    "section": "예제 3 (MNIST data with DNN)",
    "text": "예제 3 (MNIST data with DNN)\n\n데이터 읽기\n\npath = untar_data(URLs.MNIST) \npath\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:00<00:00]\n    \n    \n\n\nPath('/root/.fastai/data/mnist_png')\n\n\n\npath._str # 숨겨놓았네?\n\n'/root/.fastai/data/mnist_png'\n\n\n\npath.ls()\n\n(#2) [Path('/root/.fastai/data/mnist_png/training'),Path('/root/.fastai/data/mnist_png/testing')]\n\n\n\npath/'training'\n\nPath('/root/.fastai/data/mnist_png/training')\n\n\n\npath/'testing'\n\nPath('/root/.fastai/data/mnist_png/testing')\n\n\n\n이미지의 위치정보를이용해서 데이터를 읽음\n\n\n(path/'training/3').ls()\n\n(#6131) [Path('/root/.fastai/data/mnist_png/training/3/13740.png'),Path('/root/.fastai/data/mnist_png/training/3/45619.png'),Path('/root/.fastai/data/mnist_png/training/3/13383.png'),Path('/root/.fastai/data/mnist_png/training/3/26526.png'),Path('/root/.fastai/data/mnist_png/training/3/49494.png'),Path('/root/.fastai/data/mnist_png/training/3/3400.png'),Path('/root/.fastai/data/mnist_png/training/3/50141.png'),Path('/root/.fastai/data/mnist_png/training/3/32314.png'),Path('/root/.fastai/data/mnist_png/training/3/14883.png'),Path('/root/.fastai/data/mnist_png/training/3/47620.png')...]\n\n\n\nimgtsr = torchvision.io.read_image('/root/.fastai/data/mnist_png/training/3/13740.png')\n#imgtsr\n\n\nplt.imshow(imgtsr.reshape(28,28),cmap='gray')\n\n<matplotlib.image.AxesImage at 0x7f65d9253b50>\n\n\n\n\n\n\n\n데이터 정리\n\nthrees = (path/'training/3').ls() ## 숫자 3의 그림\nsevens = (path/'training/7').ls() ## 숫자 7의 그림\nlen(threes),len(sevens)\n\n(6131, 6265)\n\n\n\nX3 = torch.stack([torchvision.io.read_image(str(threes[i])) for i in range(6131)])\nX7 = torch.stack([torchvision.io.read_image(str(sevens[i])) for i in range(6265)])\n\n\nX3.shape,X7.shape\n\n(torch.Size([6131, 1, 28, 28]), torch.Size([6265, 1, 28, 28]))\n\n\n\nX = torch.concat([X3,X7])\n\n\nX.shape\n\ntorch.Size([12396, 1, 28, 28])\n\n\n\nXnp = X.reshape(-1,1*28*28).float()\nXnp.shape\n\ntorch.Size([12396, 784])\n\n\n- 이미지가 3이면 0, 7이면 1\n\ny = torch.tensor([0.0]*6131 + [1.0]*6265).reshape(-1,1) \ny.shape\n\ntorch.Size([12396, 1])\n\n\n\nplt.plot(y,'o')\n\n\n\n\n\n\n학습\n\\(\\underset{(n,784)} {\\bf{X}} \\overset{l_1}{\\to} \\underset{(n,30)}{u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,30)}{v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{v^{(2)}} = \\underset{(n,1)}{\\hat{y}}\\)\n\ntorch.manual_seed(202150256)\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1*28*28,out_features=30),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=30,out_features=1),\n    torch.nn.Sigmoid()\n)\n\n\nloss_fn = torch.nn.BCELoss()\n\n\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(200):\n    ## 1\n    yhat = net(Xnp) \n    ## 2\n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y,'o')\nplt.plot(net(Xnp).data,'.',alpha=0.2)\nplt.legend([\"y\",\"net(x)\"])\n\n<matplotlib.legend.Legend at 0x7f65d73b3820>"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#예제-1",
    "href": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#예제-1",
    "title": "06. 딥러닝의 기초 (4)",
    "section": "예제 1",
    "text": "예제 1\n- : linear -> sigmoid -> \\(\\hat y\\)\n- 모든 관측치와 가중치를 명시한 버전\n\ngv('''\n\"x\" -> \"x*ŵ,    bias=True\"[label=\"*ŵ\"] ;\n\"x*ŵ,    bias=True\" -> \"ŷ\"[label=\"sigmoid\"] ''')\n\n\n\n\n선형변환의 결과는 아래와 같이 로 표현하기도 한다.\n\ngv('''\n\"x\" -> \"u\";\n\"u\" -> \"y\"[label=\"sigmoid\"] ''')"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#예제-2",
    "href": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#예제-2",
    "title": "06. 딥러닝의 기초 (4)",
    "section": "예제 2",
    "text": "예제 2\n- linear -> relu -> linear -> sigmoid -> \\(\\hat y\\)\n\ntorch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=2),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=2,out_features=1),\n    torch.nn.Sigmoid()\n)\n\nSequential(\n  (0): Linear(in_features=1, out_features=2, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=2, out_features=1, bias=True)\n  (3): Sigmoid()\n)\n\n\n`- 일반화된 표현\n\ngv('''\n\"x\" -> \"u1[:,0]\"[label=\"*(-1)\"];\n\"x\" -> \"u1[:,1]\"[label=\"*1\"]\n\"u1[:,0]\" -> \"v1[:,0]\"[label=\"relu\"] \n\"u1[:,1]\" -> \"v1[:,1]\"[label=\"relu\"] \n\"v1[:,0]\" -> \"u2\"[label=\"*(-9.0)\"] \n\"v1[:,1]\" -> \"u2\"[label=\"*(-4.5)\"] \n\"u2\" -> \"v2=yhat\"[label=\"sig\"] \n'''\n)\n\n\n\n\n- 선형 + 비선형을 하나의 은닉층으로 묶은 표현\n\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"u1[:,0]\"\n    \"X\" -> \"u1[:,1]\"\n    \"u1[:,0]\" -> \"v1[:,0]\"[label=\"relu\"]\n    \"u1[:,1]\" -> \"v1[:,1]\"[label=\"relu\"]\n    label = \"Layer 1\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"v1[:,0]\" -> \"u2\"\n    \"v1[:,1]\" -> \"u2\"\n    \"u2\" -> \"v2=yhat\"[label=\"sigmoid\"]\n    label = \"Layer 2\"\n}\n''')\n\n\n\n\nLayer을 세는 방법\n\n정석 : 학습가능한 파라메터가 몇층으로 있는지\n일부 교재 설명 : 입력츠은 계산하지 않음. activation layer는 계산하지 않음\n위의 예제의 경우 1개의 은닉층과 1개의 출력층, 총 2개의 layer가 존재하는 것!\n\nHidden Layer 의 수를 세는 방법\n\nLayer의 수 = Hidden Layer의 수 + 출력층의 수 = Hidden Layer의 수 + 1\nnode의 개념 : \\(u \\to v\\) 로 가는 쌍을 간단히 노드라는 개념을 이용하여 나타낼 수 있음\n\n\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"node1\"\n    \"X\" -> \"node2\"\n    label = \"Layer 1:relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"yhat \"\n    \"node2\" -> \"yhat \"\n    label = \"Layer 2:sigmoid\"\n}\n''')\n\n\n\n\n(“number of nodes = number of features”로 이해한 그림)\n\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"feature1\"\n    \"X\" -> \"feature2\"\n    label = \"Layer 1:relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"feature1\" -> \"yhat \"\n    \"feature2\" -> \"yhat \"\n    label = \"Layer 2:sigmoid\"\n}\n''')\n\n\n\n\n여기에서 node의 숫자는 feature의 숫자와 같이 이해할 수 있음.\n\ngv('''\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"X\" \n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"X\" -> \"feature1\"\n    \"X\" -> \"feature2\"\n    label = \"Layer 1:relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"feature1\" -> \"yhat \"\n    \"feature2\" -> \"yhat \"\n    label = \"Layer 2:sigmoid\"\n}\n''')\n\n\n\n\n- 다이어그램 표현 방식은 교재마다 다르다. 다만 임의의 다이어그램을 보고 대응하는 네트워크를 구현하는 능력을 기르자"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#예제-3",
    "href": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#예제-3",
    "title": "06. 딥러닝의 기초 (4)",
    "section": "예제 3",
    "text": "예제 3\n- linear -> relu -> linear -> sigmoid -> \\(\\hat {y}\\)\n(다이어그램 표현)\n\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Input Layer\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \n    \"x784\" -> \"node1\"\n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node32\"\n    \"x2\" -> \"node32\"\n    \"..\" -> \"node32\"\n    \"x784\" -> \"node32\"\n\n\n    label = \"Hidden Layer: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n\n    \"node1\" -> \"yhat\"\n    \"node2\" -> \"yhat\"\n    \"...\" -> \"yhat\"\n    \"node32\" -> \"yhat\"\n    \n    label = \"Outplut Layer: sigmoid\"\n}\n''')\n\n\n\n\n- Layer 0,1,2 대신에 input, hidden, output layer로 표현\n- 위 다이어그램에 대응하는 코드\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=28*28*1,out_features=32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(in_features=32,out_features=1),\n    torch.nn.Sigmoid() \n)"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#시간측정-예비학습",
    "href": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#시간측정-예비학습",
    "title": "06. 딥러닝의 기초 (4)",
    "section": "시간측정 (예비학습)",
    "text": "시간측정 (예비학습)\n\nimport time\n\nt1 = time.time()\n\nt2 = time.time()\n\nt2 - t1\n\n2.47955322265625e-05"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#cpu-204800",
    "href": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#cpu-204800",
    "title": "06. 딥러닝의 기초 (4)",
    "section": "CPU (204800)",
    "text": "CPU (204800)\n- 데이터 준비\n\ntorch.manual_seed(5)\n\nx = torch.linspace(0,1,100).reshape(-1,1)\ny= torch.randn(100).reshape(-1,1)*0.01\n\n- net 준비\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,204800),\n    torch.nn.ReLU(),\n    torch.nn.Linear(204800,1)\n  )\n\nloss_fn =  torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- 학습시간 추정\n\nt1 = time.time()\n\nfor epoc in range(1000) :\n    ## 1\n    yhat = net(x)\n\n    ## 2\n     \n    loss = loss_fn(yhat,y)\n\n    ## 3\n\n    loss.backward()\n\n    ## 4\n\n    optimizr.step()\n    optimizr.zero_grad()\n\nt2 = time.time()\n\nt2 - t1\n\n105.55942583084106"
  },
  {
    "objectID": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#gpu-204800",
    "href": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#gpu-204800",
    "title": "06. 딥러닝의 기초 (4)",
    "section": "GPU (204800)",
    "text": "GPU (204800)\n- 데이터 준비\n\ntorch.manual_seed(202150256)\n\nx = torch.linspace(0,1,100).reshape(-1,1).to(\"cuda:0\")\ny = torch.randn(100).reshape(-1,1)*0.01\ny = y.to(\"cuda:0\")\n\n- net 설계\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,204800),\n    torch.nn.ReLU(),\n    torch.nn.Linear(204800,1)\n  ).to(\"cuda:0\")\n\nloss_fn =  torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- for문\n\nt1= time.time()\nfor epoc in range(1000):\n    ## 1 \n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2-t1\n\n1.0697400569915771\n\n\n- 오 약 100배 이상의 차이가 난다."
  },
  {
    "objectID": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#굳이-gpu에-넘겨야-하는가",
    "href": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#굳이-gpu에-넘겨야-하는가",
    "title": "06. 딥러닝의 기초 (4)",
    "section": "굳이 GPU에 넘겨야 하는가?",
    "text": "굳이 GPU에 넘겨야 하는가?\n- 데이터 셋을 짝홀로 나누어서 번갈아가며 GPU에 올렸다 내렸다하면 안되나?\n- 알고리즘\n\n데이터를 반으로 나눈다.\n짝수 관측치와 net의 파라미터를 GPU에 올린다.\n학습 수행\n짝수 관측치를 GPU 메모리에서 내린다. 그리고 홀수 관측치를 GPU메모리에 올린다.\n학습 수행\n홀수 관측치를 GPU메모리에서 내린다. 그리고 짝수 관측치를 GPU메모리에 올린다.\n반복…."
  },
  {
    "objectID": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#경사하강법",
    "href": "post/Lecture/STML/2023-02-25 DNN 딥러닝의 기초 (4).html#경사하강법",
    "title": "06. 딥러닝의 기초 (4)",
    "section": "경사하강법",
    "text": "경사하강법\n10개의 샘플이 있다고 가정\n- ver1 : 하나의 epoch의 모든 샘플을 이용한 방법 ( gradient descent : 경사하강법) \\(\\to\\) gpu 메모리를 너무 차지해 쓸 수가 없다.\n- ver2 : 하나의 epoch에서 10번의 역전파 과정을 거치는 방법 (stochastic gradient descent with batch size = 1)\n- ver3 : 하나의 epoch의 \\((m \\leq n)\\) 개의 샘플을 이용한 방법 (stochastic gradient descent) \\(\\to\\) 미니배치 경사하강법이라고도 하지만 요즘엔 그냥 확률적 경사하강법으로 불린다."
  },
  {
    "objectID": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html",
    "href": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html",
    "title": "07. 딥러닝의 기초 (5)",
    "section": "",
    "text": "강의출처 : 전북대학교 기계학습특강"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#데이터",
    "href": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#데이터",
    "title": "07. 딥러닝의 기초 (5)",
    "section": "데이터",
    "text": "데이터\n- model : \\(yi = (0 \\times x_i) +ɛ_i\\)\n\ntorch.manual_seed(202150256)\n\nx = torch.linspace(0,1,100).reshape(-1,1)\ny = torch.randn(100).reshape(100,1)*0.01\nplt.plot(x,y)"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#모든-데이터를-사용하여-적합512-relu-1000-epochs",
    "href": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#모든-데이터를-사용하여-적합512-relu-1000-epochs",
    "title": "07. 딥러닝의 기초 (5)",
    "section": "모든 데이터를 사용하여 적합(512, relu, 1000 epochs)",
    "text": "모든 데이터를 사용하여 적합(512, relu, 1000 epochs)\n\ntorch.manual_seed(5)\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1)\n  )\n\nloss_fn = torch.nn.MSELoss()\nopt = torch.optim.Adam(net.parameters())\n\nfor epoc in range(1000) :\n\n    yhat = net(x)\n\n    loss = loss_fn(yhat,y)\n\n    loss.backward()\n\n    opt.step()\n\n    opt.zero_grad()\n\n\nplt.plot(x,y)\nplt.plot(x,yhat.data,\"--\")\nplt.legend([\"y\", \"net(x)\"])\n\n<matplotlib.legend.Legend at 0x7fab403c7ee0>"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#데이터-셋-분할",
    "href": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#데이터-셋-분할",
    "title": "07. 딥러닝의 기초 (5)",
    "section": "데이터 셋 분할",
    "text": "데이터 셋 분할\n- 데이터를 8:2로 나눔\n\nxtr = x[:80]\nytr = y[:80]\n\nxtest = x[80:]\nytest = y[80:]\n\n\nplt.plot(xtr,ytr,\"o\")\nplt.plot(xtest,ytest,\"o\")\n\n\n\n\n- train data만 가지고 net을 학습\n\ntorch.manual_seed(5)\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(1,512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512,1)\n  )\n\nloss_fn = torch.nn.MSELoss()\nopt = torch.optim.Adam(net.parameters())\n\nfor epoc in range(1000) :\n\n    yhat = net(xtr)\n\n    loss = loss_fn(yhat,ytr)\n\n    loss.backward()\n\n    opt.step()\n\n    opt.zero_grad()\n\n\nplt.plot(xtr,ytr,\"o\")\nplt.plot(xtest,ytest,\"o\")\nplt.plot(x,net(x).data,\"--k\")\nplt.legend([\"train y\",\" test y\",\"net(x)\"])\n\n<matplotlib.legend.Legend at 0x7fab15ff75e0>\n\n\n\n\n\n- train 데이터에만 과적합된 오버피팅이라고 볼 수 있다.\n- 오버피팅\n\n오차항을 따라간다 \\(\\to\\) 학습하지말아야 할 것들을 학습하는 것임\n데이터의 수는 적은데 노드를 너무 과하게 잡은 경우 오버피팅이 일어날 수 있다."
  },
  {
    "objectID": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#오버피팅의-해결-드랍아웃",
    "href": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#오버피팅의-해결-드랍아웃",
    "title": "07. 딥러닝의 기초 (5)",
    "section": "오버피팅의 해결 : 드랍아웃",
    "text": "오버피팅의 해결 : 드랍아웃\n- 오버피팅을 무조건적으로 해결할 수 있는 것은 아님\n- 오버피팅을 해결하기 위한 방법들은 많다 \\(\\to\\) 완벽한 방법은 없다는 것임.\n\ntorch.manual_seed(1) \nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=512),\n    torch.nn.ReLU(),\n    torch.nn.Dropout(0.8),\n    torch.nn.Linear(in_features=512,out_features=1)\n)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\nfor epoc in range(1000):\n    ## 1 \n    #\n    ## 2 \n    loss = loss_fn(net(xtr),ytr) \n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(xtr,ytr,'o')\nplt.plot(xtest,ytest,'o')\nplt.plot(x,net(x).data,'--k') \nplt.title(\"network is in training mode\",fontsize=15)\n\nText(0.5, 1.0, 'network is in training mode')\n\n\n\n\n\n- 코드를 실행하면 그림이 계속 바뀐다? 먼가 틀린것임\n\nnet.eval()\n\nSequential(\n  (0): Linear(in_features=1, out_features=512, bias=True)\n  (1): ReLU()\n  (2): Dropout(p=0.8, inplace=False)\n  (3): Linear(in_features=512, out_features=1, bias=True)\n)\n\n\n\nnet.training\n\nFalse\n\n\n\nplt.plot(xtr,ytr,'o')\nplt.plot(xtest,ytest,'o')\nplt.plot(x,net(x).data,'--k') \nplt.title(\"network is in evaluation model\",fontsize=15)\n\nText(0.5, 1.0, 'network is in evaluation model')"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#드랍아웃-레이어",
    "href": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#드랍아웃-레이어",
    "title": "07. 딥러닝의 기초 (5)",
    "section": "드랍아웃 레이어",
    "text": "드랍아웃 레이어\n- 질문 1 : 왜 그림이 랜덤으로 나올까?\n\n_x = torch.linspace(0,1,101)\n_x\n\ntensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n        0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700,\n        0.1800, 0.1900, 0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600,\n        0.2700, 0.2800, 0.2900, 0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500,\n        0.3600, 0.3700, 0.3800, 0.3900, 0.4000, 0.4100, 0.4200, 0.4300, 0.4400,\n        0.4500, 0.4600, 0.4700, 0.4800, 0.4900, 0.5000, 0.5100, 0.5200, 0.5300,\n        0.5400, 0.5500, 0.5600, 0.5700, 0.5800, 0.5900, 0.6000, 0.6100, 0.6200,\n        0.6300, 0.6400, 0.6500, 0.6600, 0.6700, 0.6800, 0.6900, 0.7000, 0.7100,\n        0.7200, 0.7300, 0.7400, 0.7500, 0.7600, 0.7700, 0.7800, 0.7900, 0.8000,\n        0.8100, 0.8200, 0.8300, 0.8400, 0.8500, 0.8600, 0.8700, 0.8800, 0.8900,\n        0.9000, 0.9100, 0.9200, 0.9300, 0.9400, 0.9500, 0.9600, 0.9700, 0.9800,\n        0.9900, 1.0000])\n\n\n\ndout = torch.nn.Dropout(0.90)\ndout\n\nDropout(p=0.9, inplace=False)\n\n\n\ndout(_x)\n\ntensor([0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.7000,\n        0.0000, 1.9000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.5000,\n        0.0000, 0.0000, 0.0000, 3.9000, 0.0000, 4.1000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 4.7000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 5.6000, 0.0000, 0.0000, 0.0000, 6.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 7.1000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 7.9000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 9.7000, 0.0000,\n        9.9000, 0.0000])\n\n\n- 드랍아웃 레이어를 통과하니까 0으로 바뀐애들이 많고 10배이상 증가한 값들이 보인다.\n\ndout(_x)\n\ntensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  2.0000,  0.0000,  2.2000,  0.0000,\n         0.0000,  0.0000,  2.6000,  0.0000,  0.0000,  2.9000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  7.9000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  8.6000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000,  0.0000,  0.0000, 10.0000])\n\n\n- 심지어 결과도 랜덤 \\(\\to\\) 드랍아웃레이어를 통과한 애들은 90%는 0값이 되고 살아남은 애들은 값이 10배 증가한다.\n- 정리\n\n구조 : 입력 -> 드랍아웃레이어 -> 출력\n입력의 일부를 임의로 0으로 만듬\n0이 안된것들은 스칼라배하여 드랍아웃을 통과한 모든 숫자들의 총합이 일정하게 되도록 조정\n의미 : each iteration 마다 학습에 참여하는 노드가 랜덤으로 결정됨\n모든 노드가 골고루 학습 가능 + 한 두개의 특화된 능력치가 개발되기보다 평균적인 능력치가 전반적으로 개선됨 \\(\\to\\) 이상치의 영향도 줄일 수 있다!"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#기존의-mlp-모형",
    "href": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#기존의-mlp-모형",
    "title": "07. 딥러닝의 기초 (5)",
    "section": "기존의 MLP 모형",
    "text": "기존의 MLP 모형\n\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \n    \"x784\" -> \"node1\"\n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node30\"\n    \"x2\" -> \"node30\"\n    \"..\" -> \"node30\"\n    \"x784\" -> \"node30\"\n\n\n    label = \"Layer 1: ReLU\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"y\"\n    \"node2\" -> \"y\"\n    \"...\" -> \"y\"\n    \"node30\" -> \"y\"\n    label = \"Layer 2: Sigmoid\"\n}\n''')\n\n\n\n\n- 왜 28 x 28 이미지를 784개의 벡터로 만든 다음 모형을 돌려야 하는가?\n- 기존에 개발된 모형이 회귀분석 기반으로 되어있어서 결국 회귀분석 틀에 짜 맞추어 이미지 자료를 분석하는 느낌이다."
  },
  {
    "objectID": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#새로운-아키텍처의-제시",
    "href": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#새로운-아키텍처의-제시",
    "title": "07. 딥러닝의 기초 (5)",
    "section": "새로운 아키텍처의 제시",
    "text": "새로운 아키텍처의 제시\n- 예전\n\\(\\underset{(n,784)} {\\bf{X}} \\overset{l_1}{\\to} \\underset{(n,30)}{u^{(1)}} \\overset{a_1}{\\to} \\underset{(n,30)}{v^{(1)}} \\overset{l_2}{\\to} \\underset{(n,1)}{u^{(2)}} \\overset{a_2}{\\to} \\underset{(n,1)}{v^{(2)}} = \\underset{(n,1)}{\\hat{y}}\\)\n\n\\(l_1\\) : 선형변환, feature를 뻥튀기 하는 역할\n\\(relu\\) : 뻥튀기된 feature에 비선형을 추가하여 표현력 극대화\n\\(l_2\\) : 선형변환, 뻥튀기된 feature를 요약하는 역할\n\n- 새로운 아키텍처\n\n\\(conv\\) : feature를 뻥튀기하는 역할\n\\(relu\\)\n\\(pooling\\) : feature 요약\n\n\nCONV 레이어\n- 연산법\n(예시1)\n\ntorch.manual_seed(202150256)\n\n_conv = torch.nn.Conv2d(1,1,(2,2)) ## 입력, 출력, (2,2) window size\n\n_conv.weight.data, _conv.bias.data\n\n(tensor([[[[-0.4338, -0.0241],\n           [ 0.4315, -0.3468]]]]),\n tensor([-0.2340]))\n\n\n\n_X = torch.arange(0,4).reshape(1,2,2).float()\n_X\n\ntensor([[[0., 1.],\n         [2., 3.]]])\n\n\n\n(-0.4338)*0 + ( -0.0241)*1 + (0.4315)*2 + (-0.3468)*3 +(-0.2340)\n\n-0.4355\n\n\n\n_conv(_X)\n\ntensor([[[-0.4357]]], grad_fn=<SqueezeBackward1>)\n\n\n(예시2) : 평균도 계산 가능할 것 같다.\n\n_conv.weight.data = torch.tensor([[[[1/4, 1/4],[1/4,1/4]]]])\n_conv.bias.data = torch.tensor([0.0])\n\n\n_conv(_X) , (0+1+2+3)/4\n\n(tensor([[[1.5000]]], grad_fn=<SqueezeBackward1>), 1.5)\n\n\n(예시3) 이동평균?\n\n_X = torch.arange(0,25).float().reshape(1,5,5)\n_X\n\ntensor([[[ 0.,  1.,  2.,  3.,  4.],\n         [ 5.,  6.,  7.,  8.,  9.],\n         [10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19.],\n         [20., 21., 22., 23., 24.]]])\n\n\n\n_conv(_X)\n\ntensor([[[ 3.,  4.,  5.,  6.],\n         [ 8.,  9., 10., 11.],\n         [13., 14., 15., 16.],\n         [18., 19., 20., 21.]]], grad_fn=<SqueezeBackward1>)\n\n\n(예시4) windowsize가 증가한다면?\n\n_conv = torch.nn.Conv2d(1,1,(3,3)) # 입력1, 출력1, (3,3) window size\n_conv.bias.data = torch.tensor([0.0])\n_conv.weight.data = torch.tensor([[[[1/9,1/9,1/9],[1/9,1/9,1/9],[1/9,1/9,1/9]]]])\n\n\n_X,_conv(_X)\n\n(tensor([[[ 0.,  1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.,  9.],\n          [10., 11., 12., 13., 14.],\n          [15., 16., 17., 18., 19.],\n          [20., 21., 22., 23., 24.]]]),\n tensor([[[ 6.0000,  7.0000,  8.0000],\n          [11.0000, 12.0000, 13.0000],\n          [16.0000, 17.0000, 18.0000]]], grad_fn=<SqueezeBackward1>))\n\n\n(예시5) feature 뻥튀기 (8로 뻥튀기)\n\n_X = torch.tensor([1.0,1.0,1.0,1.0]).reshape(1,2,2)\n_X\n\ntensor([[[1., 1.],\n         [1., 1.]]])\n\n\n\n_conv = torch.nn.Conv2d(1,8,(2,2))\n_conv.weight.data.shape,_conv.bias.data.shape\n\n(torch.Size([8, 1, 2, 2]), torch.Size([8]))\n\n\n\n_conv(_X).shape\n\ntorch.Size([8, 1, 1])\n\n\n\n_conv(_X)[0,...]\n\ntensor([[0.1347]], grad_fn=<SelectBackward0>)\n\n\n? + _conv.bias.data[0]의 의미!\n\n_conv.weight.data[0,...]\n\ntensor([[[-0.2673,  0.4762],\n         [-0.2815,  0.2967]]])\n\n\n\ntorch.sum(_conv.weight.data[0,...])+_conv.bias.data[0],\\\ntorch.sum(_conv.weight.data[1,...])+_conv.bias.data[1]\n\n(tensor(0.1347), tensor(-0.7670))\n\n\n\ntorch.sum(_conv.weight.data,axis=(2,3)).reshape(-1)+ _conv.bias.data\n\ntensor([ 0.1347, -0.7670,  0.5132, -0.3334,  0.6376,  0.6994,  0.6399,  0.9393])\n\n\n결국 아래를 계산한 것임\n\n_conv(_X).reshape(-1)\n\ntensor([ 0.1347, -0.7670,  0.5132, -0.3334,  0.6376,  0.6994,  0.6399,  0.9393],\n       grad_fn=<ReshapeAliasBackward0>)\n\n\n\n\nReLU(2d)\n\n_X = torch.randn(25).reshape(1,5,5)\n_X\n\ntensor([[[ 0.3294, -2.6685, -0.7108,  1.3801,  3.1776],\n         [ 0.5248,  0.3491,  0.2382, -0.5133, -0.8363],\n         [-0.5835, -0.2984, -0.5396, -0.1182,  1.1060],\n         [-0.5037, -0.4344, -2.1023,  0.0490, -0.4926],\n         [ 0.8037,  0.2467,  1.1374, -0.4208, -1.0173]]])\n\n\n\na1=torch.nn.ReLU()\n\n\na1(_X)\n\ntensor([[[0.3294, 0.0000, 0.0000, 1.3801, 3.1776],\n         [0.5248, 0.3491, 0.2382, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000, 1.1060],\n         [0.0000, 0.0000, 0.0000, 0.0490, 0.0000],\n         [0.8037, 0.2467, 1.1374, 0.0000, 0.0000]]])\n\n\n\n\nMaxpooling 레이어\n- 설정한 윈도우 사이즈내에서 가장 큰 값을 산출함.\n\n_maxpooling = torch.nn.MaxPool2d((2,2))\n\n\n_X = torch.arange(16).float().reshape(1,4,4) \n\n\n_X, _maxpooling(_X) \n\n(tensor([[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]]]),\n tensor([[[ 5.,  7.],\n          [13., 15.]]]))\n\n\n- 아래처럼 윈도우 사이즈에 포함되지 않고 경계에 있는 값들은 버린다.\n\n_X = torch.arange(25).float().reshape(1,5,5) \n\n\n_X, _maxpooling(_X) \n\n(tensor([[[ 0.,  1.,  2.,  3.,  4.],\n          [ 5.,  6.,  7.,  8.,  9.],\n          [10., 11., 12., 13., 14.],\n          [15., 16., 17., 18., 19.],\n          [20., 21., 22., 23., 24.]]]),\n tensor([[[ 6.,  8.],\n          [16., 18.]]]))\n\n\n\n_X = torch.arange(36).float().reshape(1,6,6) \n\n\n_X, _maxpooling(_X)\n\n(tensor([[[ 0.,  1.,  2.,  3.,  4.,  5.],\n          [ 6.,  7.,  8.,  9., 10., 11.],\n          [12., 13., 14., 15., 16., 17.],\n          [18., 19., 20., 21., 22., 23.],\n          [24., 25., 26., 27., 28., 29.],\n          [30., 31., 32., 33., 34., 35.]]]),\n tensor([[[ 7.,  9., 11.],\n          [19., 21., 23.],\n          [31., 33., 35.]]]))\n\n\n\n\n이미지자료분석 - CNN 구현(CPU)\n\nX.shape\n\ntorch.Size([12665, 1, 28, 28])\n\n\n\n(1) Conv2d\n\nc1 = torch.nn.Conv2d(1,16,(5,5))\nprint(X.shape)\nprint(c1(X).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\n\n\n\n\n(2) ReLU\n\na1 = torch.nn.ReLU()\nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\n\n\n\n\n(3) MaxPool2D\n\nm1 =  torch.nn.MaxPool2d((2,2)) \nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\nprint(m1(a1(c1(X))).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 12, 12])\n\n\n\n\n(4) 시그모이드\n- 펼치자.\n(방법 1)\n\nm1(a1(c1(X))).reshape(-1,2304).shape\n\ntorch.Size([12665, 2304])\n\n\n\n16*12*12\n\n2304\n\n\n(방법 2)\n\nflttn = torch.nn.Flatten()\n\n\nprint(X.shape)\nprint(\"\\n\")\nprint(c1(X).shape)\nprint(\"\\n\")\nprint(a1(c1(X)).shape)\nprint(\"\\n\")\nprint(m1(a1(c1(X))).shape)\nprint(\"\\n\")\nprint(flttn(m1(a1(c1(X)))).shape)\n\ntorch.Size([12665, 1, 28, 28])\n\n\ntorch.Size([12665, 16, 24, 24])\n\n\ntorch.Size([12665, 16, 24, 24])\n\n\ntorch.Size([12665, 16, 12, 12])\n\n\ntorch.Size([12665, 2304])\n\n\n\n12*12*16\n\n2304\n\n\n- 16 x 12 x 12 = 2304를 1로 차원축소하는 선형레이어를 설계\n\nl1 = torch.nn.Linear(in_features=2304,out_features=1) \nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\nprint(m1(a1(c1(X))).shape)\nprint(flttn(m1(a1(c1(X)))).shape)\nprint(l1(flttn(m1(a1(c1(X))))).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 12, 12])\ntorch.Size([12665, 2304])\ntorch.Size([12665, 1])\n\n\n- 시그모이드\n\na2 = torch.nn.Sigmoid()\n\n\nl1 = torch.nn.Linear(in_features=2304,out_features=1) \nprint(X.shape)\nprint(c1(X).shape)\nprint(a1(c1(X)).shape)\nprint(m1(a1(c1(X))).shape)\nprint(flttn(m1(a1(c1(X)))).shape)\nprint(l1(flttn(m1(a1(c1(X))))).shape)\nprint(a1(l1(flttn(m1(a1(c1(X)))))).shape)\n\ntorch.Size([12665, 1, 28, 28])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 24, 24])\ntorch.Size([12665, 16, 12, 12])\ntorch.Size([12665, 2304])\ntorch.Size([12665, 1])\ntorch.Size([12665, 1])\n\n\n- 네트워크 설계\n\nnet = torch.nn.Sequential(\n    c1, # 2d: 컨볼루션(선형변환), 피처 뻥튀기 \n    a1, # 2d: 렐루(비선형변환)\n    m1, # 2d: 맥스풀링: 데이터요약\n    flttn, # 2d->1d \n    l1, # 1d: 선형변환\n    a2 # 1d: 시그모이드(비선형변환) \n)\n\n\nloss_fn = torch.nn.BCELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nt1= time.time()\nfor epoc in range(100): \n    ## 1\n    yhat = net(X) \n    ## 2\n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward()\n    ## 4\n    optimizr.step()\n    optimizr.zero_grad()\nt2= time.time()\nt2-t1\n\n63.58483362197876\n\n\n\nplt.plot(y)\nplt.plot(net(X).data,'.')\nplt.title('Traning Set',size=15)\n\nText(0.5, 1.0, 'Traning Set')\n\n\n\n\n\n\nplt.plot(yy)\nplt.plot(net(XX).data,'.')\nplt.title('Test Set',size=15)\n\nText(0.5, 1.0, 'Test Set')\n\n\n\n\n\n\n\n\n이미지자료분석 - CNN 구현(GPU)\n\n1. dls\n- train, test에 데이터로더를 만듬\n\nds1=torch.utils.data.TensorDataset(X,y)\nds2=torch.utils.data.TensorDataset(XX,yy)\n\n\nX.shape\n\ntorch.Size([12665, 1, 28, 28])\n\n\n\nlen(X)/10\n\n1266.5\n\n\n\nlen(XX)\n\n2115\n\n\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\n\ndls = DataLoaders(dl1,dl2) # 이거 fastai 지원함수\n\n\n\n2. lrnr 생성: 아키텍처, 손실함수, 옵티마이저\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\n\n\nlrnr = Learner(dls,net,loss_fn)\n\n\n\n3. 학습\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.982732\n      0.626775\n      00:07\n    \n    \n      1\n      0.691524\n      0.396832\n      00:00\n    \n    \n      2\n      0.541695\n      0.243815\n      00:00\n    \n    \n      3\n      0.427542\n      0.125900\n      00:00\n    \n    \n      4\n      0.332859\n      0.071563\n      00:00\n    \n    \n      5\n      0.260606\n      0.046392\n      00:00\n    \n    \n      6\n      0.206652\n      0.033661\n      00:00\n    \n    \n      7\n      0.166002\n      0.026269\n      00:00\n    \n    \n      8\n      0.134841\n      0.021473\n      00:00\n    \n    \n      9\n      0.110564\n      0.018103\n      00:00\n    \n  \n\n\n\n\n\n4. 예측 및 시각화\n\nnet.to(\"cpu\") \n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n\nfig, (ax1,ax2) = plt.subplots(1,2)\n\nax1.plot(net(X).data,'.')\nax1.set_title(\"Training Set\",size=15)\n\nax2.plot(net(XX).data,'.')\nax2.set_title(\"Test Set\",size=15)\n\nText(0.5, 1.0, 'Test Set')\n\n\n\n\n\n- 빠르고 적합 결과도 좋음\n\n\nLrnr 오브젝트\n\nlrnr.model\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n\nid(lrnr.model), id(net)\n\n(140372776204512, 140372776204512)\n\n\n\nlrnr.model(X)\n\ntensor([[3.7152e-03],\n        [4.3479e-04],\n        [3.2712e-03],\n        ...,\n        [9.8917e-01],\n        [9.9076e-01],\n        [9.9411e-01]], grad_fn=<SigmoidBackward0>)"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#dls-만들기",
    "href": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#dls-만들기",
    "title": "07. 딥러닝의 기초 (5)",
    "section": "dls 만들기",
    "text": "dls 만들기\n\nds1=torch.utils.data.TensorDataset(X,y)\nds2=torch.utils.data.TensorDataset(XX,yy)\n\n\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \n\n\ndls = DataLoaders(dl1,dl2)"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#lrnr-생성",
    "href": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#lrnr-생성",
    "title": "07. 딥러닝의 기초 (5)",
    "section": "lrnr 생성",
    "text": "lrnr 생성\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,1),\n    #torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCEWithLogitsLoss()\nlrnr = Learner(dls,net,loss_fn)"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#학습-1",
    "href": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#학습-1",
    "title": "07. 딥러닝의 기초 (5)",
    "section": "학습",
    "text": "학습\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.945972\n      0.627159\n      00:00\n    \n    \n      1\n      0.697180\n      0.423320\n      00:00\n    \n    \n      2\n      0.557252\n      0.285192\n      00:00\n    \n    \n      3\n      0.451076\n      0.165580\n      00:00\n    \n    \n      4\n      0.359250\n      0.096741\n      00:00\n    \n    \n      5\n      0.285009\n      0.060552\n      00:00\n    \n    \n      6\n      0.227394\n      0.041350\n      00:00\n    \n    \n      7\n      0.183023\n      0.030467\n      00:00\n    \n    \n      8\n      0.148572\n      0.023763\n      00:00\n    \n    \n      9\n      0.121533\n      0.019332\n      00:00"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#예측-및-시각화-1",
    "href": "post/Lecture/STML/2023-03-01-DNN 딥러닝의 기초(5).html#예측-및-시각화-1",
    "title": "07. 딥러닝의 기초 (5)",
    "section": "예측 및 시각화",
    "text": "예측 및 시각화\n\nnet.to(\"cpu\")\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n)\n\n\n\nnet(X)\n\ntensor([[-5.7707],\n        [-8.0542],\n        [-5.9666],\n        ...,\n        [ 4.1716],\n        [ 4.7260],\n        [ 4.9744]], grad_fn=<AddmmBackward0>)\n\n\n\nfig,ax = plt.subplots(1,2,figsize=(8,4))\nax[0].plot(net(X).data,',',color=\"C1\")\nax[1].plot(y)\nax[1].plot(a2(net(X)).data,',')\nfig.suptitle(\"Training Set\",size=15)\n\nText(0.5, 0.98, 'Training Set')\n\n\n\n\n\n\nfig,ax = plt.subplots(1,2,figsize=(8,4))\nax[0].plot(net(XX).data,',',color=\"C1\")\nax[1].plot(yy)\nax[1].plot(a2(net(XX)).data,',')\nfig.suptitle(\"Test Set\",size=15)\n\nText(0.5, 0.98, 'Test Set')"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-07-8wk1.html",
    "href": "post/Lecture/STML/2023-03-07-8wk1.html",
    "title": "09. 이미지분석 (2)",
    "section": "",
    "text": "import torch \nimport torchvision\nimport numpy as np\nfrom fastai.vision.all import * \n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }');"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-07-8wk1.html#실습-3개의-클래스-분류",
    "href": "post/Lecture/STML/2023-03-07-8wk1.html#실습-3개의-클래스-분류",
    "title": "09. 이미지분석 (2)",
    "section": "실습 : 3개의 클래스 분류",
    "text": "실습 : 3개의 클래스 분류\n\npath = untar_data(URLs.MNIST)\n\n\n\n\n\n\n    \n      \n      100.03% [15687680/15683414 00:02<00:00]\n    \n    \n\n\n- training set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/2').ls()])\nX = torch.concat([X0,X1,X2])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n- test set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nX2 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/2').ls()])\nXX = torch.concat([X0,X1,X2])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1)+ [2]*len(X2))#.reshape(-1,1)\n\n1 dls\n\nlen(X), len(XX)\n\n(18623, 3147)\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y) \nds2 = torch.utils.data.TensorDataset(XX,yy) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번 iter\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # \ndls = DataLoaders(dl1,dl2) \n\n2 lrnr\n\nnet1 = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten()\n)\n\n\nnet1(X).shape\n\ntorch.Size([18623, 2304])\n\n\n\nnet = torch.nn.Sequential(\n    net1,\n    torch.nn.Linear(2304,3) # 0,1,2 3개를 구분하는 문제이므로 out_features=3 \n)\nloss_fn = torch.nn.CrossEntropyLoss() \n\n\nlrnr = Learner(dls,net,loss_fn) \n\n3 학습\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.907315\n      1.137226\n      00:07\n    \n    \n      1\n      1.301616\n      0.839809\n      00:00\n    \n    \n      2\n      1.086795\n      0.712145\n      00:00\n    \n    \n      3\n      0.952067\n      0.489769\n      00:00\n    \n    \n      4\n      0.809088\n      0.334485\n      00:00\n    \n    \n      5\n      0.669083\n      0.227808\n      00:00\n    \n    \n      6\n      0.553239\n      0.164591\n      00:00\n    \n    \n      7\n      0.460001\n      0.127254\n      00:00\n    \n    \n      8\n      0.383911\n      0.105844\n      00:00\n    \n    \n      9\n      0.322828\n      0.092200\n      00:00\n    \n  \n\n\n\n4 예측\n\nlrnr.model.to(\"cpu\")\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n    (3): Flatten(start_dim=1, end_dim=-1)\n  )\n  (1): Linear(in_features=2304, out_features=3, bias=True)\n)\n\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy) \n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n      1\n      2\n      y\n    \n  \n  \n    \n      0\n      1.610155\n      -7.296218\n      -1.949740\n      0\n    \n    \n      1\n      2.721445\n      -11.693159\n      -2.574624\n      0\n    \n    \n      2\n      1.159257\n      -9.293894\n      -0.992080\n      0\n    \n    \n      3\n      2.946398\n      -10.934134\n      -1.913056\n      0\n    \n    \n      4\n      2.035376\n      -8.698581\n      -1.466725\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      3142\n      -1.260826\n      -5.236878\n      -0.114429\n      2\n    \n    \n      3143\n      -4.041571\n      -4.725406\n      1.132236\n      2\n    \n    \n      3144\n      -4.621160\n      -4.280747\n      1.188167\n      2\n    \n    \n      3145\n      -3.100066\n      -3.123898\n      0.582833\n      2\n    \n    \n      3146\n      -2.477711\n      -5.302809\n      1.269270\n      2\n    \n  \n\n3147 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n대체적으로 첫번째 칼럼의 숫자들이 다른 칼럼보다 크다.\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==0')\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n      1\n      2\n      y\n    \n  \n  \n    \n      0\n      1.610155\n      -7.296218\n      -1.949740\n      0\n    \n    \n      1\n      2.721445\n      -11.693159\n      -2.574624\n      0\n    \n    \n      2\n      1.159257\n      -9.293894\n      -0.992080\n      0\n    \n    \n      3\n      2.946398\n      -10.934134\n      -1.913056\n      0\n    \n    \n      4\n      2.035376\n      -8.698581\n      -1.466725\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      975\n      -0.421908\n      -4.137669\n      -1.921860\n      0\n    \n    \n      976\n      0.629439\n      -6.895987\n      -0.959484\n      0\n    \n    \n      977\n      1.539558\n      -7.074513\n      -2.124304\n      0\n    \n    \n      978\n      1.117325\n      -7.665346\n      -1.131635\n      0\n    \n    \n      979\n      2.633224\n      -6.995299\n      -3.818918\n      0\n    \n  \n\n980 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n대체적으로 두번쨰 칼럼의 숫자들이 다른 칼럼보다 크다.\n\n\npd.DataFrame(lrnr.model(XX)).assign(y=yy).query('y==1')\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n      1\n      2\n      y\n    \n  \n  \n    \n      980\n      -4.262898\n      1.944668\n      -0.949041\n      1\n    \n    \n      981\n      -3.462561\n      2.556862\n      -1.529038\n      1\n    \n    \n      982\n      -3.882701\n      1.947009\n      -1.357661\n      1\n    \n    \n      983\n      -3.104966\n      2.371521\n      -1.705071\n      1\n    \n    \n      984\n      -3.492548\n      2.551590\n      -1.425174\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2110\n      -3.436696\n      2.716884\n      -1.594630\n      1\n    \n    \n      2111\n      -3.732119\n      2.664111\n      -1.861893\n      1\n    \n    \n      2112\n      -2.843302\n      2.324082\n      -1.652022\n      1\n    \n    \n      2113\n      -4.504267\n      2.389974\n      -1.222201\n      1\n    \n    \n      2114\n      -3.525925\n      2.375131\n      -1.666616\n      1\n    \n  \n\n1135 rows × 4 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\\(i\\) 번째 칼럼의 숫자가 크다 \\(\\to\\) \\(y=i\\) 일 확률이 크다."
  },
  {
    "objectID": "post/Lecture/STML/2023-03-07-8wk1.html#공부-1-softmax",
    "href": "post/Lecture/STML/2023-03-07-8wk1.html#공부-1-softmax",
    "title": "09. 이미지분석 (2)",
    "section": "공부 1 : softmax",
    "text": "공부 1 : softmax\n- softmax를 쓰기 직전의 숫자들은 (n,k) 꼴로 되어 있음, 각 관측치 마다 \\(k\\)개의 숫자가 있는데, 그 중에서 유난히 큰 하나의 숫자가 있다.\n- torch.nn.Softmax() 손계산\n(예시1) 잘못된 계산\n\nsftmax = torch.nn.Softmax(dim=0)\nsftmax\n\nSoftmax(dim=0)\n\n\n\n_netout = torch.tensor([[-2.0,-2.0,0.0],\n                        [3.14,3.14,3.14],\n                        [0.0,0.0,2.0],\n                        [2.0,2.0,4.0],\n                        [0.0,0.0,0.0]])\n_netout\n\ntensor([[-2.0000, -2.0000,  0.0000],\n        [ 3.1400,  3.1400,  3.1400],\n        [ 0.0000,  0.0000,  2.0000],\n        [ 2.0000,  2.0000,  4.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n\n- 어라? 같은 값인데 확률이 다른 값들이 존재한다….\n\nsftmax(_netout)\n\ntensor([[0.0041, 0.0041, 0.0115],\n        [0.7081, 0.7081, 0.2653],\n        [0.0306, 0.0306, 0.0848],\n        [0.2265, 0.2265, 0.6269],\n        [0.0306, 0.0306, 0.0115]])\n\n\n- 심지어 다 더해서 1이 되지도 않는다…\n- 근데 세로로 더하면 1이 되네 핳\n(예시 2) 올바른 계산.\n\nsftmax = torch.nn.Softmax(dim=1)\n\n\nsftmax(_netout)\n\ntensor([[0.1065, 0.1065, 0.7870],\n        [0.3333, 0.3333, 0.3333],\n        [0.1065, 0.1065, 0.7870],\n        [0.1065, 0.1065, 0.7870],\n        [0.3333, 0.3333, 0.3333]])\n\n\n(예시 3) 차원을 명시안하면 맞게 계산해주고 경고 줌\n\nsftmax = torch.nn.Softmax()\n\n\nsftmax(_netout)\n\nUserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  sftmax(_netout)\n\n\ntensor([[0.1065, 0.1065, 0.7870],\n        [0.3333, 0.3333, 0.3333],\n        [0.1065, 0.1065, 0.7870],\n        [0.1065, 0.1065, 0.7870],\n        [0.3333, 0.3333, 0.3333]])\n\n\n(예시 4) 진짜 손계산\n\ntorch.exp(_netout)\n\ntensor([[ 0.1353,  0.1353,  1.0000],\n        [23.1039, 23.1039, 23.1039],\n        [ 1.0000,  1.0000,  7.3891],\n        [ 7.3891,  7.3891, 54.5981],\n        [ 1.0000,  1.0000,  1.0000]])\n\n\n- 첫번쨰 관측치\n\n0.1353/(0.1353 + 0.1353 + 1.0000), 0.1353/(0.1353 + 0.1353 + 1.0000), 1.0000/(0.1353 + 0.1353 + 1.0000) # 첫 obs\n\n(0.10648512513773022, 0.10648512513773022, 0.7870297497245397)\n\n\n- 두번쨰 관측치\n\nnp.exp(_netout[1])/np.exp(_netout[1]).sum() # 두번째 obs \n\ntensor([0.3333, 0.3333, 0.3333])\n\n\n\nnp.apply_along_axis(lambda x: np.exp(x) / np.exp(x).sum(),1,_netout)\n\narray([[0.10650698, 0.10650698, 0.78698605],\n       [0.33333334, 0.33333334, 0.33333334],\n       [0.10650699, 0.10650699, 0.78698605],\n       [0.10650698, 0.10650698, 0.78698605],\n       [0.33333334, 0.33333334, 0.33333334]], dtype=float32)\n\n\n참고링크"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-07-8wk1.html#공부2-crossentropyloss",
    "href": "post/Lecture/STML/2023-03-07-8wk1.html#공부2-crossentropyloss",
    "title": "09. 이미지분석 (2)",
    "section": "공부2 : CrossEntropyLoss",
    "text": "공부2 : CrossEntropyLoss\n\none-hot version 손계산\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n\n_netout\n\ntensor([[-2.0000, -2.0000,  0.0000],\n        [ 3.1400,  3.1400,  3.1400],\n        [ 0.0000,  0.0000,  2.0000],\n        [ 2.0000,  2.0000,  4.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n\n\n_y_onehot = torch.tensor([[0,0,1],\n                          [0,1,0],\n                          [0,0,1],\n                          [0,0,1],\n                          [1,0,0]])*1.0\n_y_onehot\n\ntensor([[0., 0., 1.],\n        [0., 1., 0.],\n        [0., 0., 1.],\n        [0., 0., 1.],\n        [1., 0., 0.]])\n\n\n\nsftmax = torch.nn.Softmax(dim=1) \nsftmax(_netout), _y_onehot\n\n(tensor([[0.1065, 0.1065, 0.7870],\n         [0.3333, 0.3333, 0.3333],\n         [0.1065, 0.1065, 0.7870],\n         [0.1065, 0.1065, 0.7870],\n         [0.3333, 0.3333, 0.3333]]),\n tensor([[0., 0., 1.],\n         [0., 1., 0.],\n         [0., 0., 1.],\n         [0., 0., 1.],\n         [1., 0., 0.]]))\n\n\n- 계산결과\n\nloss_fn(_netout,_y_onehot)\n\ntensor(0.5832)\n\n\n\\[\\hat {y_i} = \\text {softmax(net(X))}\\]\n\\[\\text {Cross entropy loss}=-\\sum_{i=1}^n  y_ilog (\\hat y_i)/n\\]\n\n- torch.sum(torch.log(sftmax(_netout)) * _y_onehot)/5 \n\ntensor(0.5832)\n\n\n- 계산하는 방법도 중요하나, 손실함수에 softmax 활성화함수가 이미 포함되어 있다는 것을 확인하는 것이 더 중요\n- 따라서 torch.nn.CrossEntropyLoss() 는 사실 torch.nn.CEWithSoftmaxLoss() 정도로 바꾸는 것이 더 말이 되는 것 같다.\n\n\n손계산 2 : y가 길이가 n인 벡터\n\n_netout \n\ntensor([[-2.0000, -2.0000,  0.0000],\n        [ 3.1400,  3.1400,  3.1400],\n        [ 0.0000,  0.0000,  2.0000],\n        [ 2.0000,  2.0000,  4.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n\n\n_y = torch.tensor([2,1,2,2,0])\n\n\nloss_fn(_netout,_y)\n\ntensor(0.5832)\n\n\n\n- torch.sum(torch.log(sftmax(_netout)) * _y_onehot)/5 \n\ntensor(0.5832)"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-07-8wk1.html#실습-k-2로-두면-이진분류도-가능",
    "href": "post/Lecture/STML/2023-03-07-8wk1.html#실습-k-2로-두면-이진분류도-가능",
    "title": "09. 이미지분석 (2)",
    "section": "실습 : k = 2로 두면 이진분류도 가능",
    "text": "실습 : k = 2로 두면 이진분류도 가능\n- download data\ntraining\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\ny = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\n\ny_onehot = torch.nn.functional.one_hot(y).float()\n#y_onehot = torch.tensor(list(map(lambda x: [1,0] if x==0 else [0,1],y))).float()\n\ntest\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\nyy = torch.tensor([0]*len(X0) + [1]*len(X1))#.reshape(-1,1)\n\n\nyy_onehot = torch.nn.functional.one_hot(yy).float()\n#yy_onehot = torch.tensor(list(map(lambda x: [1,0] if x==0 else [0,1],yy))).float()\n\n1 dls\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot) \nds2 = torch.utils.data.TensorDataset(XX,yy_onehot) \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1862) # 에폭당 11번 iter\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=3147) # \ndls = DataLoaders(dl1,dl2) \n\n2 lrnr\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2)\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss()\nlrnr = Learner(dls,net,loss_fn) \n\n3 학습\n\nlrnr.fit(10) \n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.856707\n      0.616278\n      00:00\n    \n    \n      1\n      0.680172\n      0.431423\n      00:00\n    \n    \n      2\n      0.553872\n      0.280581\n      00:00\n    \n    \n      3\n      0.452270\n      0.167101\n      00:00\n    \n    \n      4\n      0.369324\n      0.093444\n      00:00\n    \n    \n      5\n      0.302544\n      0.052857\n      00:00\n    \n    \n      6\n      0.249547\n      0.032362\n      00:00\n    \n    \n      7\n      0.207629\n      0.021897\n      00:00\n    \n    \n      8\n      0.174271\n      0.016139\n      00:00\n    \n    \n      9\n      0.147444\n      0.012657\n      00:00\n    \n  \n\n\n\n4 예측 및 시각화\n\nlrnr.model.to(\"cpu\")\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=2, bias=True)\n)\n\n\n\nsftmax = torch.nn.Softmax(dim=1) \nsig = torch.nn.Sigmoid()\nfig,ax = plt.subplots(1,2,figsize=(8,4))\nax[0].plot(net(X).diff(axis=1).data,',',color=\"C1\") ## \\mu_2 - \\mu_1\nax[1].plot(y,\"--\")\nax[1].plot(sftmax(net(X))[:,1].data,',')\n#ax[1].plot(sig(net(X).diff(axis=1)).data,',')\nfig.suptitle(\"Training Set\",size=15)\n\nText(0.5, 0.98, 'Training Set')\n\n\n\n\n\n\nfig,ax = plt.subplots(1,2,figsize=(8,4))\nax[0].plot(net(XX).diff(axis=1).data,',',color=\"C1\")\nax[1].plot(yy)\nax[1].plot(sftmax(net(XX))[:,1].data,',')\n#ax[1].plot(sig(net(XX).diff(axis=1)).data,',')\nfig.suptitle(\"Test Set\",size=15)\n\nText(0.5, 0.98, 'Test Set')"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-07-8wk1.html#공부-이진분류에서-소프트맥스-vs-시그모이드",
    "href": "post/Lecture/STML/2023-03-07-8wk1.html#공부-이진분류에서-소프트맥스-vs-시그모이드",
    "title": "09. 이미지분석 (2)",
    "section": "공부: 이진분류에서 소프트맥스 vs 시그모이드",
    "text": "공부: 이진분류에서 소프트맥스 vs 시그모이드\n- 이진분류문제 = “y=0 or y=1” 을 맞추는 문제 = 성공과 실패를 맞추는 문제 = 성공확률과 실패확률을 추정하는 문제\n\nsoftmax: (실패확률, 성공확률) 꼴로 결과가 나옴 // softmax는 실패확률과 성공확률을 둘다 추정한다.\nsigmoid: (성공확률) 꼴로 결과가 나옴 // sigmoid는 성공확률만 추정한다.\n\n\n\nCode\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"?\"\n    \"??\"\n    \"..\"\n    \"???\"\n    label = \"Layer ?\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"?\" -> \"node1\"\n    \"??\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \"???\" -> \"node1\"\n    \n    \"?\" -> \"node2\"\n    \"??\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"???\" -> \"node2\"\n    \n    \"?\" -> \"...\"\n    \"??\" -> \"...\"\n    \"..\" -> \"...\"\n    \"???\" -> \"...\"\n    \n    \"?\" -> \"node2304\"\n    \"??\" -> \"node2304\"\n    \"..\" -> \"node2304\"\n    \"???\" -> \"node2304\"\n\n    label = \"Layer: ReLU\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"y1\"\n    \"node2\" -> \"y1\"\n    \"...\" -> \"y1\"\n    \"node2304\" -> \"y1\"\n    \n    \"node1\" -> \"y2\"\n    \"node2\" -> \"y2\"\n    \"...\" -> \"y2\"\n    \"node2304\" -> \"y2\"    \n    label = \"Layer: Softmax\"\n}\n''')\n\n\n\n\n\n\n\nCode\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"?\"\n    \"??\"\n    \"..\"\n    \"???\"\n    label = \"Layer ?\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"?\" -> \"node1\"\n    \"??\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \"???\" -> \"node1\"\n    \n    \"?\" -> \"node2\"\n    \"??\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"???\" -> \"node2\"\n    \n    \"?\" -> \"...\"\n    \"??\" -> \"...\"\n    \"..\" -> \"...\"\n    \"???\" -> \"...\"\n    \n    \"?\" -> \"node2304\"\n    \"??\" -> \"node2304\"\n    \"..\" -> \"node2304\"\n    \"???\" -> \"node2304\"\n\n    label = \"Layer: ReLU\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"y\"\n    \"node2\" -> \"y\"\n    \"...\" -> \"y\"\n    \"node2304\" -> \"y\"\n    label = \"Layer: Sigmoid\"\n}\n''')\n\n\n\n\n\n-둘은 사실상 같은 효과를 주는 모형인데 학습할 파라메터는 sigmoid의 경우가 더 적다.\n\nsigmoid를 사용하는 모형이 비용은 싸고 효과는 동일하다는 말 이진분류 한정해서는 softmax를 쓰지말고 sigmoid를 써야함.\n\n- 결론\n\n소프트맥스는 시그모이드의 확장이다.\n클래스의 수가 2개일 경우에는 (Sigmoid, BCEloss) 조합을 사용해야 하고 클래스의 수가 2개보다 클 경우에는 (Softmax, CrossEntropyLoss) 를 사용해야 한다."
  },
  {
    "objectID": "post/Lecture/STML/2023-03-07-8wk1.html#데이터-준비",
    "href": "post/Lecture/STML/2023-03-07-8wk1.html#데이터-준비",
    "title": "09. 이미지분석 (2)",
    "section": "데이터 준비",
    "text": "데이터 준비\n- download data\n\npath = untar_data(URLs.MNIST)\n\n- training set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'training/1').ls()])\nX = torch.concat([X0,X1])/255\ny = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n- test set\n\nX0 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/0').ls()])\nX1 = torch.stack([torchvision.io.read_image(str(fname)) for fname in (path/'testing/1').ls()])\nXX = torch.concat([X0,X1])/255\nyy = torch.tensor([0.0]*len(X0) + [1.0]*len(X1)).reshape(-1,1)\n\n\nX.shape,XX.shape,y.shape,yy.shape\n\n(torch.Size([12665, 1, 28, 28]),\n torch.Size([2115, 1, 28, 28]),\n torch.Size([12665, 1]),\n torch.Size([2115, 1]))"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-07-8wk1.html#사용자정의-메트릭-이용",
    "href": "post/Lecture/STML/2023-03-07-8wk1.html#사용자정의-메트릭-이용",
    "title": "09. 이미지분석 (2)",
    "section": "사용자정의 메트릭 이용",
    "text": "사용자정의 메트릭 이용\n1 dls 만들기\n\nds1 = torch.utils.data.TensorDataset(X,y)\nds2 = torch.utils.data.TensorDataset(XX,yy)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n2 lrnr 생성\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss() \n\n\ndef acc(yhat,y) : \n    return ((yhat>0.5)==y).float().mean()\n\n\ndef err(yhat,y):\n    return 1-((yhat>0.5)==y).float().mean()\n\n\nlrnr = Learner(dls,net,loss_fn,metrics=[acc,err])\n\n3 학습\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      acc\n      err\n      time\n    \n  \n  \n    \n      0\n      0.981153\n      0.635162\n      0.463357\n      0.536643\n      00:00\n    \n    \n      1\n      0.695933\n      0.389219\n      0.987234\n      0.012766\n      00:00\n    \n    \n      2\n      0.537728\n      0.246010\n      0.987707\n      0.012293\n      00:00\n    \n    \n      3\n      0.424565\n      0.138897\n      0.992908\n      0.007092\n      00:00\n    \n    \n      4\n      0.333698\n      0.082636\n      0.993853\n      0.006147\n      00:00\n    \n    \n      5\n      0.263328\n      0.053516\n      0.994326\n      0.005674\n      00:00\n    \n    \n      6\n      0.209578\n      0.037179\n      0.995745\n      0.004255\n      00:00\n    \n    \n      7\n      0.168352\n      0.027381\n      0.996690\n      0.003310\n      00:00\n    \n    \n      8\n      0.136389\n      0.021127\n      0.997163\n      0.002837\n      00:00\n    \n    \n      9\n      0.111303\n      0.016876\n      0.997163\n      0.002837\n      00:00\n    \n  \n\n\n\n\n예측은 생략"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-07-8wk1.html#fastai지원-메트릭이용-잘못된사용",
    "href": "post/Lecture/STML/2023-03-07-8wk1.html#fastai지원-메트릭이용-잘못된사용",
    "title": "09. 이미지분석 (2)",
    "section": "fastai지원 메트릭이용– 잘못된사용",
    "text": "fastai지원 메트릭이용– 잘못된사용\n1 dls 생성\n\nds1 = torch.utils.data.TensorDataset(X,y)\nds2 = torch.utils.data.TensorDataset(XX,yy)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n2 lrnr 생성\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,1),\n    torch.nn.Sigmoid()\n)\nloss_fn = torch.nn.BCELoss()\nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n\naccuracy??\n\n\nerror_rate??\n\n3 학습\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.981847\n      0.638130\n      0.463357\n      0.536643\n      00:00\n    \n    \n      1\n      0.706334\n      0.414463\n      0.463357\n      0.536643\n      00:00\n    \n    \n      2\n      0.557118\n      0.275488\n      0.463357\n      0.536643\n      00:00\n    \n    \n      3\n      0.448049\n      0.156918\n      0.463357\n      0.536643\n      00:00\n    \n    \n      4\n      0.355552\n      0.091275\n      0.463357\n      0.536643\n      00:00\n    \n    \n      5\n      0.281673\n      0.058098\n      0.463357\n      0.536643\n      00:00\n    \n    \n      6\n      0.224770\n      0.040617\n      0.463357\n      0.536643\n      00:00\n    \n    \n      7\n      0.181157\n      0.030589\n      0.463357\n      0.536643\n      00:00\n    \n    \n      8\n      0.147395\n      0.024245\n      0.463357\n      0.536643\n      00:00\n    \n    \n      9\n      0.120925\n      0.019925\n      0.463357\n      0.536643\n      00:00\n    \n  \n\n\n\n\n먼가 이상함 \\(to\\) accuarcy와 error_rate이 좀 잘못됨`\n\n4 예측\n\nlrnr.model.to(\"cpu\")\n\nSequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n  (1): ReLU()\n  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (3): Flatten(start_dim=1, end_dim=-1)\n  (4): Linear(in_features=2304, out_features=1, bias=True)\n  (5): Sigmoid()\n)\n\n\n\nplt.plot(yy)\nplt.plot(lrnr.model(XX).data,'.')\n\n\n\n\n\n예측은 좀 잘한다."
  },
  {
    "objectID": "post/Lecture/STML/2023-03-07-8wk1.html#fastai지원-메트릭이용---올바른-사용1",
    "href": "post/Lecture/STML/2023-03-07-8wk1.html#fastai지원-메트릭이용---올바른-사용1",
    "title": "09. 이미지분석 (2)",
    "section": "fastai지원 메트릭이용 - 올바른 사용(1)",
    "text": "fastai지원 메트릭이용 - 올바른 사용(1)\n- 가정\n\nX의 형태는 (n,채널,픽셀,픽셀)로 가정한다.\ny의 형태는 (n,) 벡터이다. 즉 \\(n \\times 1\\)이 아니라 그냥 길이가 \\(n\\) 인 벡터로 가정한다.\ny의 각 원소는 0,1,2,3,… 와 같이 카테고리를 의미하는 숫자이어야 하며 이 숫자는 int형으로 저장되어야 한다.\nloss function은 CrossEntropyLoss()를 쓴다고 가정한다. (따라서 네트워크의 최종레이어는 torch.nn.Linear(?,클래스의수) 꼴이 되어야 한다.)\n\n1 dls 만들기\n\ny.to(torch.int64).reshape(-1),yy.to(torch.int64).reshape(-1)\n\n(tensor([0, 0, 0,  ..., 1, 1, 1]), tensor([0, 0, 0,  ..., 1, 1, 1]))\n\n\n\nds1 = torch.utils.data.TensorDataset(X,y.to(torch.int64).reshape(-1))\nds2 = torch.utils.data.TensorDataset(XX,yy.to(torch.int64).reshape(-1))\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n2 lrnr 생성\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n)\nloss_fn = torch.nn.CrossEntropyLoss()\nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy,error_rate])\n\n3 학습\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.237301\n      0.618571\n      0.463357\n      0.536643\n      00:00\n    \n    \n      1\n      0.717234\n      0.297716\n      0.965012\n      0.034988\n      00:00\n    \n    \n      2\n      0.524121\n      0.141834\n      0.987707\n      0.012293\n      00:00\n    \n    \n      3\n      0.388923\n      0.074604\n      0.994799\n      0.005201\n      00:00\n    \n    \n      4\n      0.293852\n      0.045211\n      0.996690\n      0.003310\n      00:00\n    \n    \n      5\n      0.226063\n      0.028293\n      0.996690\n      0.003310\n      00:00\n    \n    \n      6\n      0.176603\n      0.018968\n      0.997163\n      0.002837\n      00:00\n    \n    \n      7\n      0.139797\n      0.013722\n      0.997636\n      0.002364\n      00:00\n    \n    \n      8\n      0.111825\n      0.010514\n      0.998109\n      0.001891\n      00:00\n    \n    \n      9\n      0.090190\n      0.008434\n      0.998582\n      0.001418\n      00:00"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-07-8wk1.html#fastai-지원-메트릭-이용---올바른-사용2",
    "href": "post/Lecture/STML/2023-03-07-8wk1.html#fastai-지원-메트릭-이용---올바른-사용2",
    "title": "09. 이미지분석 (2)",
    "section": "fastai 지원 메트릭 이용 - 올바른 사용(2)",
    "text": "fastai 지원 메트릭 이용 - 올바른 사용(2)\n- 가정\n\nX의 형태는 (n,채널,픽셀,픽셀)로 가정한다.\ny의 형태는 (n,클래스의수)로 가정한다. 즉 y가 one_hot 인코딩된 형태로 가정한다.\ny의 각 원소는 0 혹은 1이다.\nloss function은 CrossEntropyLoss()를 쓴다고 가정한다. (따라서 네트워크의 최종레이어는 torch.nn.Linear(?,클래스의수) 꼴이 되어야 한다.)\n\n1 dls 만들기\n\ny_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], y)))\nyy_onehot = torch.tensor(list(map(lambda x: [1.0,0.0] if x==0 else [0.0,1.0], yy)))\n# y_onehot = torch.nn.functional.one_hot(y.reshape(-1).to(torch.int64)).to(torch.float32)\n# yy_onehot = torch.nn.functional.one_hot(yy.reshape(-1).to(torch.int64)).to(torch.float32)\n\n\nds1 = torch.utils.data.TensorDataset(X,y_onehot)\nds2 = torch.utils.data.TensorDataset(XX,yy_onehot)\ndl1 = torch.utils.data.DataLoader(ds1,batch_size=1266) \ndl2 = torch.utils.data.DataLoader(ds2,batch_size=2115) \ndls = DataLoaders(dl1,dl2) \n\n2 lrnr 생성\n\nnet = torch.nn.Sequential(\n    torch.nn.Conv2d(1,16,(5,5)),\n    torch.nn.ReLU(),\n    torch.nn.MaxPool2d((2,2)),\n    torch.nn.Flatten(),\n    torch.nn.Linear(2304,2),\n    #torch.nn.Softmax()\n)\nloss_fn = torch.nn.CrossEntropyLoss() \nlrnr = Learner(dls,net,loss_fn,metrics=[accuracy_multi])\n\n3 학습\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      1.103323\n      0.534813\n      0.465721\n      00:00\n    \n    \n      1\n      0.645414\n      0.314861\n      0.941371\n      00:00\n    \n    \n      2\n      0.489155\n      0.122688\n      0.990780\n      00:00\n    \n    \n      3\n      0.360367\n      0.063362\n      0.995981\n      00:00\n    \n    \n      4\n      0.270183\n      0.037917\n      0.996217\n      00:00\n    \n    \n      5\n      0.207195\n      0.026166\n      0.996217\n      00:00\n    \n    \n      6\n      0.162268\n      0.020020\n      0.996454\n      00:00\n    \n    \n      7\n      0.129200\n      0.016123\n      0.996690\n      00:00\n    \n    \n      8\n      0.104167\n      0.013462\n      0.997163\n      00:00\n    \n    \n      9\n      0.084834\n      0.011523\n      0.997400\n      00:00"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-08-8wk2.html",
    "href": "post/Lecture/STML/2023-03-08-8wk2.html",
    "title": "08. 이미지분석 (1)",
    "section": "",
    "text": "import torch \nimport torchvision\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-08-8wk2.html#수제네트워크",
    "href": "post/Lecture/STML/2023-03-08-8wk2.html#수제네트워크",
    "title": "08. 이미지분석 (1)",
    "section": "수제네트워크",
    "text": "수제네트워크\n1 dls\n\ndls = ImageDataLoaders.from_folder(path, train=\"train\",valid = \"test\")\n\n\n_X,_y = dls.one_batch()\n\n\n_X.shape,_y.shape\n\n(torch.Size([64, 3, 32, 32]), torch.Size([64]))\n\n\n- 10개의 클래스가 존재함을 확인..\n\n!ls /root/.fastai/data/cifar10/train\n\nairplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck\n\n\n- 10개의 클래스에 대응하는 label 확인\n\ndls.show_batch()\n\n\n\n\n(2) 학습 오브젝트 생성\n\nnet1 = torch.nn.Sequential(\n      torch.nn.Conv2d(3,128, kernel_size=(5,5)),\n      torch.nn.ReLU(),\n      torch.nn.MaxPool2d((2,2)),\n      torch.nn.Flatten()\n)\n\n- _X는 현재 cuda에 있어 아래와 같은 에러가 발생\n\nnet(_X)\n\nRuntimeError: ignored\n\n\n- _X를 GPU로 보내줌\n\nnet1(_X.to(\"cpu\")).shape\n\ntorch.Size([64, 25088])\n\n\n\nnet = torch.nn.Sequential(\n      net1,\n      torch.nn.Linear(25088,10)\n)\n\nloss_fn = torch.nn.CrossEntropyLoss()\nlrnr = Learner(dls,net,loss_fn, metrics = accuracy)\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.271368\n      1.271370\n      0.561100\n      00:18\n    \n    \n      1\n      1.073587\n      1.130481\n      0.605600\n      00:10\n    \n    \n      2\n      0.997678\n      1.071748\n      0.626900\n      00:09\n    \n    \n      3\n      0.917440\n      1.067222\n      0.632700\n      00:09\n    \n    \n      4\n      0.874401\n      1.058087\n      0.641100\n      00:09\n    \n    \n      5\n      0.766511\n      1.049834\n      0.648500\n      00:09\n    \n    \n      6\n      0.739402\n      1.022050\n      0.663900\n      00:10\n    \n    \n      7\n      0.689561\n      1.088572\n      0.649600\n      00:10\n    \n    \n      8\n      0.623682\n      1.130853\n      0.648500\n      00:10\n    \n    \n      9\n      0.604332\n      1.102263\n      0.650100\n      00:10\n    \n  \n\n\n\n- 모델 적합이 생각보다 힘들다…\n- 전이학습을 이용하자 \\(\\to\\) 남들이 만든 네트워크를 이용"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-08-8wk2.html#전이학습",
    "href": "post/Lecture/STML/2023-03-08-8wk2.html#전이학습",
    "title": "08. 이미지분석 (1)",
    "section": "전이학습",
    "text": "전이학습\n- 아래와 같이 다른 네트워크에 파라미터를 가져올 수 도 있음.\n\nnet = torchvision.models.resnet18(weights = torchvision.models.resnet.ResNet18_Weights.IMAGENET1K_V1)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 308MB/s]\n\n\n\nnet\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\n- Linear(in_features=512, out_features=1000, bias=True)\n\n1000개의 물체를 구분하는 분류네트워크임을 알 수 있다.\n우리는 10개의 클래스만 분류하면 되므로 바꾸어주자.\n\n\nnet.fc\n\nLinear(in_features=512, out_features=1000, bias=True)\n\n\n\nnet.fc = torch.nn.Linear(512,1000)\nloss_fn = torch.nn.CrossEntropyLoss()\nlrnr = Learner(dls,net,loss_fn, metrics = accuracy)\n\n3 학습\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.806613\n      0.827909\n      0.729600\n      00:19\n    \n    \n      1\n      0.656176\n      0.791772\n      0.734800\n      00:19\n    \n    \n      2\n      0.520055\n      0.641509\n      0.778400\n      00:19\n    \n    \n      3\n      0.438871\n      0.629200\n      0.791400\n      00:19\n    \n    \n      4\n      0.346179\n      0.654058\n      0.790200\n      00:19\n    \n    \n      5\n      0.277057\n      0.632388\n      0.810700\n      00:20\n    \n    \n      6\n      0.231102\n      0.742130\n      0.792100\n      00:20\n    \n    \n      7\n      0.201862\n      0.693124\n      0.805900\n      00:19\n    \n    \n      8\n      0.173905\n      0.751864\n      0.792700\n      00:18\n    \n    \n      9\n      0.143165\n      0.774660\n      0.798000\n      00:18\n    \n  \n\n\n\n\n다른 네트워크를 빌렸음에도 불구하고 상당히 잘맞음\n일반인이 거의 밑바닥에서 설계하는 것보다 전이학습을 이용하는 것이 효율적일 경우가 많다."
  },
  {
    "objectID": "post/Lecture/STML/2023-03-08-8wk2.html#전이학습-다른-구현-순수-fastai-이용",
    "href": "post/Lecture/STML/2023-03-08-8wk2.html#전이학습-다른-구현-순수-fastai-이용",
    "title": "08. 이미지분석 (1)",
    "section": "전이학습 다른 구현 : 순수 fastai 이용",
    "text": "전이학습 다른 구현 : 순수 fastai 이용\n- 예전코드 복습\n\npath = untar_data(URLs.PETS)/\"images\"\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 01:01<00:00]\n    \n    \n\n\n\nfiles = get_image_files(path)\n\n\ndef label_func(fname) :\n    if fname[0].isupper() :\n        return \"cat\"\n    else:\n        return \"dog\"\n\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) \n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|██████████| 83.3M/83.3M [00:00<00:00, 312MB/s]\n\n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.175729\n      0.027656\n      0.989851\n      00:22\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.051700\n      0.017342\n      0.993234\n      00:26\n    \n  \n\n\n\n- 사실 위의 코드도 transfer learning임\n\nlrnr.model\n\nSequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (3): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (4): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (5): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (2): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): fastai.layers.Flatten(full=False)\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1024, out_features=512, bias=False)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=2, bias=False)\n  )\n)"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-08-8wk2.html#잡담",
    "href": "post/Lecture/STML/2023-03-08-8wk2.html#잡담",
    "title": "08. 이미지분석 (1)",
    "section": "잡담",
    "text": "잡담\n딥러닝 연구의 네가치 축\nstep1 아키텍처 - 전통적으로 도메인 지식이 풍부한 각 영역의 교수님들… - 딥러닝으로 넘어오면서 한 영역의 전문적인 지식이 없는 일반인들도 끈기, 운, 직관, 좋은 컴퓨터 등등으로 아키텍처를 만드는 시대가 옴\nstep2 손실함수 - 통계학과 교수님들 : 예측값 \\(Y\\)에 따라 손실함수가 달라지므로 울과 교수님들이 담당 (어깨 슬쩍)\nstep3 미분계산 - 컴공교수님들 : 병렬처리 등에 대한 지식이 필요\nstep4 옵티마이저 - 산공교수님들 : 최적화에 대한 이론\n- step1의 최근연구 특징 : 비전문가 + 블랙박스 \\(\\to\\) 아키텍처에 대한 설명이 안됨\n- 점점 step1에 대한 연구가 설명이 부족하니 아키텍처, 즉 설명가능한 딥러닝에 대한 요구가 생김\n- 어떤 부분에서 강아지라고 생각했고를 이미지로 표현하는 방법이 탄생\n\npath = untar_data(URLs.PETS)/\"images\"\n\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512))"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-08-8wk2.html#구현-0단계-예비학습",
    "href": "post/Lecture/STML/2023-03-08-8wk2.html#구현-0단계-예비학습",
    "title": "08. 이미지분석 (1)",
    "section": "구현 0단계 – 예비학습",
    "text": "구현 0단계 – 예비학습\n\npath.ls()\n\n(#7393) [Path('/root/.fastai/data/oxford-iiit-pet/images/pug_182.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/shiba_inu_105.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Siamese_185.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/english_cocker_spaniel_64.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/basset_hound_5.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/american_bulldog_149.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/great_pyrenees_33.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/Bombay_30.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/boxer_200.jpg'),Path('/root/.fastai/data/oxford-iiit-pet/images/beagle_124.jpg')...]\n\n\n\nximg = PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/pug_182.jpg')\nximg\n\n\n\n\n\nx = first(dls.test_dl([ximg]))[0]\nx.shape\n\ntorch.Size([1, 3, 512, 512])\n\n\n\nAP layer\n\n``\n\n\nap = torch.nn.AdaptiveAvgPool2d(output_size=1)\n\n\nx = torch.arange(48).reshape(1,3,4,4)*1.0\n\n\nx\n\ntensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]],\n\n         [[16., 17., 18., 19.],\n          [20., 21., 22., 23.],\n          [24., 25., 26., 27.],\n          [28., 29., 30., 31.]],\n\n         [[32., 33., 34., 35.],\n          [36., 37., 38., 39.],\n          [40., 41., 42., 43.],\n          [44., 45., 46., 47.]]]])\n\n\n\nap(x)\n\ntensor([[[[ 7.5000]],\n\n         [[23.5000]],\n\n         [[39.5000]]]])\n\n\n\n각 채널별로 평균을 내줌\n\n\nap = torch.nn.AdaptiveAvgPool2d(output_size=2)\n\n\nx\n\ntensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]],\n\n         [[16., 17., 18., 19.],\n          [20., 21., 22., 23.],\n          [24., 25., 26., 27.],\n          [28., 29., 30., 31.]],\n\n         [[32., 33., 34., 35.],\n          [36., 37., 38., 39.],\n          [40., 41., 42., 43.],\n          [44., 45., 46., 47.]]]])\n\n\n\nap(x)\n\ntensor([[[[ 2.5000,  4.5000],\n          [10.5000, 12.5000]],\n\n         [[18.5000, 20.5000],\n          [26.5000, 28.5000]],\n\n         [[34.5000, 36.5000],\n          [42.5000, 44.5000]]]])\n\n\n\n\ntorch.einsum\n(예시1)\n\ntsr = torch.arange(12).reshape(4,3)\ntsr\n\ntensor([[ 0,  1,  2],\n        [ 3,  4,  5],\n        [ 6,  7,  8],\n        [ 9, 10, 11]])\n\n\n\n(4,3) -> (3,4)\n\n\ntorch.einsum('ij->ji',tsr)\n\ntensor([[ 0,  3,  6,  9],\n        [ 1,  4,  7, 10],\n        [ 2,  5,  8, 11]])\n\n\n(예시2)\n\ntsr1 = torch.arange(12).reshape(4,3).float()\ntsr2 = torch.arange(15).reshape(3,5).float()\n\n\ntsr1 @ tsr2\n\ntensor([[ 25.,  28.,  31.,  34.,  37.],\n        [ 70.,  82.,  94., 106., 118.],\n        [115., 136., 157., 178., 199.],\n        [160., 190., 220., 250., 280.]])\n\n\n결과는 4,5 행렬\n\ntorch.einsum(\"ij,jk->ik\",tsr1,tsr2)\n\ntensor([[ 25.,  28.,  31.,  34.,  37.],\n        [ 70.,  82.,  94., 106., 118.],\n        [115., 136., 157., 178., 199.],\n        [160., 190., 220., 250., 280.]])\n\n\n(예시3)\n처음에 읽어드린 이미지의 텐서가 아래 강아지가 맞는지 궁금하다.\n\nximg = PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/pug_182.jpg')\nximg\n\n\n\n\n\nx = first(dls.test_dl([ximg]))[0]\nx.shape\n\ntorch.Size([1, 3, 512, 512])\n\n\n\nx.to(\"cpu\").shape\n\ntorch.Size([1, 3, 512, 512])\n\n\n(1, 3, 512, 512) -> (512,512,3) 으로 변경해야함\n\n#torch.einsum(\"ocij -> ijc\",x.to(\"cpu\"))\n\n\nplt.imshow(torch.einsum(\"ocij -> ijc\",x.to(\"cpu\")))\n\n<matplotlib.image.AxesImage at 0x7fabae4fedf0>"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-08-8wk2.html#구현-1단계-이미지-분류-잘하는-네트워크-선택",
    "href": "post/Lecture/STML/2023-03-08-8wk2.html#구현-1단계-이미지-분류-잘하는-네트워크-선택",
    "title": "08. 이미지분석 (1)",
    "section": "구현 1단계 –> 이미지 분류 잘하는 네트워크 선택",
    "text": "구현 1단계 –> 이미지 분류 잘하는 네트워크 선택\n- 일단 예전 실습코드 그대로\n\nfiles= get_image_files(path)\ndef label_func(fname):\n    if fname[0].isupper():\n        return 'cat'\n    else:\n        return 'dog'\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(512)) \n\n\nlrnr = vision_learner(dls,resnet34,metrics=accuracy) \n\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\nlrnr.fine_tune(1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.137897\n      0.025098\n      0.991204\n      00:22\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.047219\n      0.007555\n      0.995940\n      00:26"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-08-8wk2.html#구현-2단계-네트워크-끝-부분-수정",
    "href": "post/Lecture/STML/2023-03-08-8wk2.html#구현-2단계-네트워크-끝-부분-수정",
    "title": "08. 이미지분석 (1)",
    "section": "구현 2단계 – 네트워크 끝 부분 수정",
    "text": "구현 2단계 – 네트워크 끝 부분 수정\n- 모형의 분해\n\n# lrnr.model\n\n\nnet1= lrnr.model[0]\nnet2= lrnr.model[1]\n\n- net2를 좀더 살펴보자.\n\nnet2\n\nSequential(\n  (0): AdaptiveConcatPool2d(\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (mp): AdaptiveMaxPool2d(output_size=1)\n  )\n  (1): fastai.layers.Flatten(full=False)\n  (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (3): Dropout(p=0.25, inplace=False)\n  (4): Linear(in_features=1024, out_features=512, bias=False)\n  (5): ReLU(inplace=True)\n  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (7): Dropout(p=0.5, inplace=False)\n  (8): Linear(in_features=512, out_features=2, bias=False)\n)\n\n\n\n_X, _y = dls.one_batch() \n\n\nnet1.to(\"cpu\")\nnet2.to(\"cpu\") \n_X = _X.to(\"cpu\")\n\n- net2를 아래와 같이 수정하고 싶다.\n\nnet2 = torch.nn.Sequential(\n      torch.nn.AdaptiveAvgPool2d(output_size=1), ## 64,512,16,16 -> 64,512,1,1\n      torch.nn.Flatten(),  ## 64,512,1,1 -> 64,512\n      torch.nn.Linear(512,2) ## 64,512 -> 64,2\n)\n\n\nnet = torch.nn.Sequential(\n      net1,\n      net2\n)\n\n\nlrnr2 = Learner(dls,net,metrics=accuracy)  ## loss_func?\n\n\nlrnr2.loss_func, lrnr.loss_func ## 알아서 기존의 loss function이 잘 들어가 있음음\n\n(FlattenedLoss of CrossEntropyLoss(), FlattenedLoss of CrossEntropyLoss())\n\n\n\nlrnr2.fine_tune(5)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.238512\n      1.397133\n      0.593369\n      00:26\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.143291\n      0.156326\n      0.943166\n      00:26\n    \n    \n      1\n      0.149757\n      0.110755\n      0.956698\n      00:26\n    \n    \n      2\n      0.105926\n      0.091007\n      0.965494\n      00:26\n    \n    \n      3\n      0.057871\n      0.061253\n      0.978349\n      00:26\n    \n    \n      4\n      0.028998\n      0.061297\n      0.981732\n      00:26"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-08-8wk2.html#구현-3단계",
    "href": "post/Lecture/STML/2023-03-08-8wk2.html#구현-3단계",
    "title": "08. 이미지분석 (1)",
    "section": "구현 3단계",
    "text": "구현 3단계\n- 수정된 net2에서 Linear와 AP의 순서를 바꿈\n\nnet2\n\nSequential(\n  (0): AdaptiveAvgPool2d(output_size=1)\n  (1): Flatten(start_dim=1, end_dim=-1)\n  (2): Linear(in_features=512, out_features=2, bias=True)\n)\n\n\n- 1개의 관측치를 고정하였을 경우 출력과정 상상\n\nximg = PILImage.create('/root/.fastai/data/oxford-iiit-pet/images/pug_182.jpg')\nximg\n\n\n\n\n\nx = first(dls.test_dl([ximg]))[0]\nx.shape\n\ntorch.Size([1, 3, 512, 512])\n\n\n\nnet2\n\nSequential(\n  (0): AdaptiveAvgPool2d(output_size=1)\n  (1): Flatten(start_dim=1, end_dim=-1)\n  (2): Linear(in_features=512, out_features=2, bias=True)\n)\n\n\n\nnet(x)\n\nTensorImage([[-5.9625,  6.4680]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\ndls.vocab\n\n['cat', 'dog']\n\n\n\nnet(x)에서 뒤쪽의 값이 클수록 “dog”를 의미한다.\n\n\nx.shape\n\ntorch.Size([1, 3, 512, 512])\n\n\n\n#net1\n\n\nnet2\n\nSequential(\n  (0): AdaptiveAvgPool2d(output_size=1)\n  (1): Flatten(start_dim=1, end_dim=-1)\n  (2): Linear(in_features=512, out_features=2, bias=True)\n)\n\n\n\nprint(net1(x).shape)\nprint(net2[0](net1(x)).shape)\nprint(net2[1](net2[0](net1(x))).shape)\nprint(net2[2](net2[1](net2[0](net1(x)))).shape)\n\ntorch.Size([1, 512, 16, 16])\ntorch.Size([1, 512, 1, 1])\ntorch.Size([1, 512])\ntorch.Size([1, 2])\n\n\n- 순서를 바꿔도 결과는 같을 것이다. (선형변환하고 평균내거나, 평균내거나 선형변환하는건 같으니까)\n\n_x = torch.tensor([1,2,3.14,5]).reshape(4,1)\n_x\n\ntensor([[1.0000],\n        [2.0000],\n        [3.1400],\n        [5.0000]])\n\n\n\n_l1 = torch.nn.Linear(1,1,bias=False)\n_l1(_x).mean() # _x -> 선형변환 -> 평균 \n\ntensor(-0.6190, grad_fn=<MeanBackward0>)\n\n\n\n_l1(_x.mean().reshape(1,1)) # _x -> 평균 -> 선형변환\n\ntensor([[-0.6190]], grad_fn=<MmBackward0>)\n\n\n- 구현해보자.\n(2.512), (1,512,16,16) -> (1,2,16,16)\n\nnet2[2].weight.shape,net1(x).shape\n\n(torch.Size([2, 512]), torch.Size([1, 512, 16, 16]))\n\n\n\nwhy = torch.einsum('ab,cbef-> caef',net2[2].weight,net1(x))\n\n\nnet2[0](why)\n\nTensorImage([[[[-5.9765]],\n\n              [[ 6.4657]]]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\nnet(x)\n\nTensorImage([[-5.9625,  6.4680]], device='cuda:0', grad_fn=<AliasBackward0>)"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-08-8wk2.html#잠깐-멈추고-생각",
    "href": "post/Lecture/STML/2023-03-08-8wk2.html#잠깐-멈추고-생각",
    "title": "08. 이미지분석 (1)",
    "section": "잠깐 멈추고 생각",
    "text": "잠깐 멈추고 생각\n- 이미지\n\nximg\n\n\n\n\n- 네트워크 결과\n\nnet2(net1(x))\n\nTensorImage([[-5.9625,  6.4680]], device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\nwhy.shape\n\ntorch.Size([1, 2, 16, 16])\n\n\n\n(why[0,0,:,:]).mean()\n\nTensorImage(-5.9765, device='cuda:0', grad_fn=<AliasBackward0>)\n\n\n\n이 값들의 평균은 -0.5975이다 \\(\\to\\) 이 값이 클루속 그림이 고양이라는 의미(작을수록 고양이가 아니라는 의미)\n그런데 살펴보니 대부분의 위치에서 0에 가까운 값을 가짐, 다만 특정 위치에서 엄청 작은 값이 있어 위와 같은 평균 값이 나옴\n즉, 특정 위치에서 존재하는 엄청 작은 값들은 ximg가 고양이가 아니라고 판단하는 근거가 된다.\n\n\n#why[0,0,:,:]\n\n- 시각화\n\nwhy_cat = why[0,0,:,:]\nwhy_dog = why[0,1,:,:]\n\n\nfig, ax = plt.subplots(1,3,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_cat.to(\"cpu\").detach(),cmap='magma')\nax[2].imshow(why_dog.to(\"cpu\").detach(),cmap='magma')\n\n<matplotlib.image.AxesImage at 0x7fabae362cd0>\n\n\n\n\n\n- 겹처그리기\n\nsftmax=torch.nn.Softmax(dim=1)\n\n\ncatprob, dogprob = sftmax(net(x))[0,0].item(), sftmax(net(x))[0,1].item()\n\n\nfig, ax = plt.subplots(1,2,figsize=(8,4))\nax[0].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[0].imshow(why_cat.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[0].set_title('catprob= %f' % catprob) \nax[1].imshow(torch.einsum('ocij -> ijc',dls.decode((x,))[0]).to(\"cpu\"))\nax[1].imshow(why_dog.to(\"cpu\").detach(),cmap='magma',extent=(0,511,511,0),interpolation='bilinear',alpha=0.5)\nax[1].set_title('dogprob=%f' % dogprob)\n\nText(0.5, 1.0, 'dogprob=0.999996')\n\n\n\n\n\n\nmagma = 검은색, 보라색, 빨간색, 노란색 순으로 큰 값을 가짐\n왼쪽 그림의 검은 부분(작은값은)은 고양이가 아니라는 근거, 오른쪽 그림의 노란부분(큰값)은 강아지라는 근거"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-08-8wk2.html#구현-4단계-cam-시각화",
    "href": "post/Lecture/STML/2023-03-08-8wk2.html#구현-4단계-cam-시각화",
    "title": "08. 이미지분석 (1)",
    "section": "구현 4단계 CAM 시각화",
    "text": "구현 4단계 CAM 시각화\n- 실행시간 조금 오래거림\n\nfig, ax = plt.subplots(5,5) \nk=25\nfor i in range(5):\n    for j in range(5): \n        x, = first(dls.test_dl([PILImage.create(get_image_files(path)[k])]))\n        why = torch.einsum('cb,abij -> acij', net2[2].weight, net1(x))\n        why_cat = why[0,0,:,:] \n        why_dog = why[0,1,:,:] \n        catprob, dogprob = sftmax(net(x))[0][0].item(), sftmax(net(x))[0][1].item()\n        if catprob>dogprob: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_cat.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"cat(%2f)\" % catprob)\n        else: \n            dls.train.decode((x,))[0].squeeze().show(ax=ax[i][j])\n            ax[i][j].imshow(why_dog.to(\"cpu\").detach(),alpha=0.5,extent=(0,511,511,0),interpolation='bilinear',cmap='magma')\n            ax[i][j].set_title(\"dog(%2f)\" % dogprob)\n        k=k+1 \nfig.set_figwidth(16)            \nfig.set_figheight(16)\nfig.tight_layout()"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-22-9wk.html",
    "href": "post/Lecture/STML/2023-03-22-9wk.html",
    "title": "10. 순환신경망 (1)",
    "section": "",
    "text": "import torch\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-22-9wk.html#data",
    "href": "post/Lecture/STML/2023-03-22-9wk.html#data",
    "title": "10. 순환신경망 (1)",
    "section": "data",
    "text": "data\n- 다음과 같이 문자열이 주어지면 다음 단어를 예측할 수 있도록 학습 시키고 싶음\n\n즉, a가들어오면 b를 뱉어내고, b가 들어가면 a를 뱉어내도록 학습시키고 싶음\n\n\ntxt = list('ab')*100\ntxt[:10]\n\n['a', 'b', 'a', 'b', 'a', 'b', 'a', 'b', 'a', 'b']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nlen(txt_x), len(txt_y)\n\n(199, 199)\n\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'a', 'b', 'a'], ['b', 'a', 'b', 'a', 'b'])"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-22-9wk.html#선형모형을-이용한-풀이",
    "href": "post/Lecture/STML/2023-03-22-9wk.html#선형모형을-이용한-풀이",
    "title": "10. 순환신경망 (1)",
    "section": "선형모형을 이용한 풀이",
    "text": "선형모형을 이용한 풀이\n\n(풀이 1) 1개의 파라미터 - 실패\n- 데이터 정리\n\nx = torch.tensor(f(txt_x,{'a':0,'b':1})).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,{'a':0,'b':1})).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 학습 및 결과 시각화\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=False)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:5],'o')\nplt.plot(net(x).data[:5])\nplt.legend([\"y\",\"net(x)\"])\n\n<matplotlib.legend.Legend at 0x7f874e067160>\n\n\n\n\n\n\n학습이 잘 되지 않음.\n\n- 잘 안된 이유\n\npd.DataFrame({'x':x[:5].reshape(-1),'y':y[:5].reshape(-1)})\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      0.0\n      1.0\n    \n    \n      1\n      1.0\n      0.0\n    \n    \n      2\n      0.0\n      1.0\n    \n    \n      3\n      1.0\n      0.0\n    \n    \n      4\n      0.0\n      1.0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n현재 \\(\\hat {y_i} = \\hat{w}x_i\\)꼴의 아키텍처이고 \\(y_i \\approx \\hat {w}x_i\\)가 되는 적당한 \\(\\hat {w_i}\\)를 찾아야 하는 상황\n\n\\((x_i,y_i)=(0,1)\\)이면 어떠한 \\(\\hat {w_i}\\)를 선택해도 \\(y_i \\approx \\hat {w}x_i\\)를 만드는 것이 불가능\n\\((x_i,y_i)=(1,0)\\)이면 \\(\\hat {w_i}=0\\)일 경우 \\(y_i \\approx \\hat {w}x_i\\)로 만드는 것이 가능\n\n상황을 종합해보니 \\(\\hat {w} = 0\\)으로 학습되는 것이 그나마 최선 \\(\\to\\) 와이? 그나마 절반은 맞출 수 있음.\n\n그리고 \\(x \\neq 0\\)으로 하면 되지않을까…??\n\n\n\n(풀이2) 1개의 파라메터 - 성공, but 확장성이 없는 풀이\n- 0이라는 값이 문제가 되므로 인코딩 방식의 변경\n\nx = torch.tensor(f(txt_x,{'a':-1,'b':1})).float().reshape(-1,1) \ny = torch.tensor(f(txt_y,{'a':-1,'b':1})).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[-1.],\n         [ 1.],\n         [-1.],\n         [ 1.],\n         [-1.]]),\n tensor([[ 1.],\n         [-1.],\n         [ 1.],\n         [-1.],\n         [ 1.]]))\n\n\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=False)\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(2000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과는 성공\n\nplt.plot(y[:5],'o')\nplt.plot(net(x).data[:5])\n\n\n\n\n- 일단 성공은 했다.\n\n근데…. 이거는 다른 문제에 적용이 불가능할 것 같음. \\(\\to\\) 그리고 인코딩도 납득이 안됨.\n\\(a =[1,0,0], b=[0,1,0], c=[0,0,1]\\) 이런식으로 바꾸고 횔성화함수를 소프트맥스를 이용해야 납득이 감\n또한, 당연히 loss도 크로스 엔트로피를 써야함.\n아니면 이진 분류에서 로지스틱을 쓰고, 손실함수를 binary cross entropy를 쓰면 되지 않을까…??"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-22-9wk.html#로지스틱-모형을-이용한-풀이",
    "href": "post/Lecture/STML/2023-03-22-9wk.html#로지스틱-모형을-이용한-풀이",
    "title": "10. 순환신경망 (1)",
    "section": "로지스틱 모형을 이용한 풀이",
    "text": "로지스틱 모형을 이용한 풀이\n\n(풀이 1) 1개의 파라미터 - 실패\n- 데이터를 다시 \\(a=0, b=1\\)로 정리\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n- 학습\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=False)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n- 결과해석 : 예상되었던 실패임\n\n아키텍처는 \\(\\hat {y_i} = \\text{sig}(\\hat {w}x_i)\\) 꼴이다.\n\\((x_i,y_i) = (0,1)\\) 이라면 어떠한 \\(\\hat {w}\\)를 선택해도 \\(\\hat {w}x_i=0\\)이다.\n이 경우 \\(\\hat {y_i}= \\text{sig}(0)=0.5\\)가 된다.\n\\((x_i,y_i) = (1,0)\\) 이라면 \\(\\hat {w}=-5\\)와 같은 큰 값으로 선택하면 \\(\\text {sig}(-5) \\approx 0 = y_i\\)와 같이 만들 수 있다.\n\n\nnet.weight\n\nParameter containing:\ntensor([[-3.8163]], requires_grad=True)\n\n\n\n\n(풀이 2) 2개의 파라미터 + 좋은 초기값 - 성공\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\n\nx[:5],y[:5]\n\n(tensor([[0.],\n         [1.],\n         [0.],\n         [1.],\n         [0.]]),\n tensor([[1.],\n         [0.],\n         [1.],\n         [0.],\n         [1.]]))\n\n\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=True)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nnet.weight.data = torch.tensor([[-5.00]])\nnet.bias.data = torch.tensor([+2.500])\n\n- 학습전 결과\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n- 학습후 결과\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n\n\n(풀이3) 2개의 파라메터 + 나쁜초기값 – 성공\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\n\nnet = torch.nn.Linear(in_features=1,out_features=1,bias=True)\nloss_fn = torch.nn.BCEWithLogitsLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nnet.weight.data = torch.tensor([[+5.00]])\nnet.bias.data = torch.tensor([-2.500])\n\n- 학습전 상태\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n- 학습\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x) \n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:10],'o')\nplt.plot(sig(net(x)).data[:10],'--o')\n\n\n\n\n- 결국 수렴하기 할듯? \\(\\to\\) 부호를 학습하는 무언가가 있었음 좋겠어.\n\n\n(풀이4) 3개의 파라메터를 쓴다면?\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,1)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,1)\n\ntorch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True), ## 2개 파라미터\n    torch.nn.ACTIVATION_FUNCTION(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False) ## 1개 파라미터\n)\n- 위와 같은 네트워크를 설정하면 3개의 파라메터를 사용할 수 있다.\n\n적절한 ACTIVATION_FUNCTION을 골라야 하는데 실험적으로 tanh가 적절하다고 알려져있다. (그래서 우리도 실험적으로 이해해보자)\n\n(예비학습1) net(x)와 사실 net.forward(x)는 같다.\n\nnet(x)[:5] # 풀이3에서 학습한 네트워크임\n\ntensor([[-0.1584],\n        [ 0.1797],\n        [-0.1584],\n        [ 0.1797],\n        [-0.1584]], grad_fn=<SliceBackward0>)\n\n\n\nnet.forward(x)[:5] # 풀이3에서 학습한 네트워크임\n\ntensor([[-0.1584],\n        [ 0.1797],\n        [-0.1584],\n        [ 0.1797],\n        [-0.1584]], grad_fn=<SliceBackward0>)\n\n\n그래서 net.forward를 재정의 하면 net(x)의 기능을 재정의 할 수 있다. (오버라이딩)\n\nnet.forward = lambda x : 1\n\n\n“lambda x: 1” 은 입력이 x 출력이 1인 함수를 의미 (즉 입력값에 상관없이 항상 1을 출력하는 함수)\n“net.forward = lambda x:1” 이라고 새롭게 선언하였므로 앞으론 net.forward(x), net(x) 도 입력값에 상관없이 항상 1을 출력하게 될 것임\n\n\nnet(x)\n\n1\n\n\n(예비학습 2)\n\ntorch.nn.Module을 상속받아서 네트뤄크를 만들면 (=“class XXX(torch.nn.Module)”)와 같은 방식으로 클래스를 선언하면?\n\n\\(\\to\\) 약속된 아키텍처를 가진 네트워크를 찍어내는 함수를 만들 수 있다.\n(예시 1) Sigmoid\n\nclass Mynet1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.a1 = torch.nn.Sigmoid()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):        \n        yhat = self.l2(self.a1(self.l1(x)))\n        return yhat\n\n이것은!\n\nnet = Mynet1()\n\n아래하구 같다!!\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=1,out_features=1,bias=True),\n    torch.nn.Sigmoid(),\n    torch.nn.Linear(in_features=1,out_features=1,bias=False)\n)\n\n(예시 2) Relu\n\nclass Mynet2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.a1 = torch.nn.ReLU()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        yhat = self.l2(self.a1(self.l1(x)))\n        return yhat\n\n(예시 3) Tanh\n\nclass Mynet3(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.a1 = torch.nn.Tanh()\n        self.l2 = torch.nn.Linear(in_features=1,out_features=1,bias=False)\n    def forward(self,x):\n        yhat = self.l2(self.a1(self.l1(x)))\n        return yhat\n\n\n\n클래스에 대한 이해\nstep1: 아래와 코드를 복사하여 틀을 만든다. (이건 무조건 고정임, XXXX 자리는 원하는 이름을 넣는다)\nclass XXXX(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## 우리가 사용할 레이어를 정의 \n        \n        ## 레이어 정의 끝\n    def forward(self,x):\n        ## yhat을 어떻게 구할것인지 정의 \n        \n        ## 정의 끝\n        return yhat\n\nnet(x)에 사용하는 x임, yhat은 net.forward(x) 함수의 리턴값임\n\nstep2 : def __init__(self):에 사용할 레이어를 정의하고 이름을 붙인다. 이름은 항상 self.xxx 와 같은 식으로 정의한다.\nclass XXXX(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## 우리가 사용할 레이어를 정의 \n        self.xxx1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.xxx2 = torch.nn.Tanh()\n        self.xxx3 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        ## 레이어 정의 끝\n    def forward(self,x):\n        ## yhat을 어떻게 구할것인지 정의 \n        \n        ## 정의 끝\n        return yhat\nstep3 : def forward 에 x –> yhat 으로 가는 과정을 묘사한 코드를 작성하고 yhat을 리턴하도록 한다.\nclass XXXX(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        ## 우리가 사용할 레이어를 정의 \n        self.xxx1 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        self.xxx2 = torch.nn.Tanh()\n        self.xxx3 = torch.nn.Linear(in_features=1,out_features=1,bias=True)\n        ## 레이어 정의 끝\n    def forward(self,x):\n        ## yhat을 어떻게 구할것인지 정의 \n        u = self.xxx1(x) \n        v = self.xxx2(u)\n        yhat = self.xxx3(v) \n        ## 정의 끝\n        return yhat\n예비학습 끝\n\n\nActivation : sigmoid\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        net = Mynet1()\n        loss_fn = torch.nn.BCEWithLogitsLoss()\n        optimizr = torch.optim.Adam(net.parameters())\n        for epoc in range(1000):\n            ## 1\n            yhat = net(x)\n            ## 2\n            loss = loss_fn(yhat,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        ax[i][j].plot(y[:5],'o')\n        ax[i][j].plot(sig(net(x[:5])).data,'--o')\nfig.suptitle(r\"$a_1(x):=Sigmoid(x)$\",size=20)\nfig.tight_layout()\n\n\n\n\n\n\nActivation : ReLU\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        net = Mynet2()\n        loss_fn = torch.nn.BCEWithLogitsLoss()\n        optimizr = torch.optim.Adam(net.parameters())\n        for epoc in range(1000):\n            ## 1\n            yhat = net(x)\n            ## 2\n            loss = loss_fn(yhat,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        ax[i][j].plot(y[:5],'o')\n        ax[i][j].plot(sig(net(x[:5])).data,'--o')\nfig.suptitle(r\"$a_2(x):=ReLU(x)$\",size=20)\nfig.tight_layout()\n\n\n\n\n\n\nActiovation : tanh\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        net = Mynet3()\n        loss_fn = torch.nn.BCEWithLogitsLoss()\n        optimizr = torch.optim.Adam(net.parameters())\n        for epoc in range(1000):\n            ## 1\n            yhat = net(x)\n            ## 2\n            loss = loss_fn(yhat,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        ax[i][j].plot(y[:5],'o')\n        ax[i][j].plot(sig(net(x[:5])).data,'--o')\nfig.suptitle(r\"$a_2(x):=Tanh(x)$\",size=20)        \nfig.tight_layout()\n\n\n\n\n\n\nSummary\n- 실험해석\n\nsigmoid : 주황색선의 변동폭이 작고 항상 0.5 근처로 머무는 적합값이 존재\nrelu : 주황색선의 변동폭이 크고, 항상 0.5 근처로 머무는 적합값이 존재\ntanh : 주황색선의 변동폭이 크고, 0.5 근처로 머무는 적합값이 없음\n\n- 실험해보니까 tanh가 우수한 것 같다 \\(\\to\\) 앞으로는 tanh를 쓰자."
  },
  {
    "objectID": "post/Lecture/STML/2023-03-22-9wk.html#풀이1-로지스틱모형에서-3개의-파라메터-버전을-그대로-확장",
    "href": "post/Lecture/STML/2023-03-22-9wk.html#풀이1-로지스틱모형에서-3개의-파라메터-버전을-그대로-확장",
    "title": "10. 순환신경망 (1)",
    "section": "(풀이1) 로지스틱모형에서 3개의 파라메터 버전을 그대로 확장",
    "text": "(풀이1) 로지스틱모형에서 3개의 파라메터 버전을 그대로 확장\n\nmapping = {'a':[1,0],'b':[0,1]}\nx = torch.tensor(f(txt_x,mapping)).float().reshape(-1,2)\ny = torch.tensor(f(txt_y,mapping)).float().reshape(-1,2)\nx[:5],y[:5]\n\n(tensor([[1., 0.],\n         [0., 1.],\n         [1., 0.],\n         [0., 1.],\n         [1., 0.]]),\n tensor([[0., 1.],\n         [1., 0.],\n         [0., 1.],\n         [1., 0.],\n         [0., 1.]]))\n\n\n\nnet = torch.nn.Sequential(\n    torch.nn.Linear(in_features=2,out_features=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=2,bias=False)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nfig,ax = plt.subplots(1,2)\nax[0].imshow(y[:5],cmap='bwr')\nax[1].imshow(soft(net(x[:5])).data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f874d34a160>"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-22-9wk.html#motive",
    "href": "post/Lecture/STML/2023-03-22-9wk.html#motive",
    "title": "10. 순환신경망 (1)",
    "section": "Motive",
    "text": "Motive\n- 결국 최종적으로는 아래와 같은 맵핑방식이 확장성이 있어보인다.\n\nmapping = {'a':[1,0,0],'b':[0,1,0],'c':[0,0,1]} # 원핫인코딩 방식 \n\n- 그런데 매번 \\(X\\)를 원핫 인코딩하고 Linear 변환하는 것이 번거로움\n\ntorch.nn.EmbeddingLayear가 이 역할을 대신해줌\n\n\nmapping = {'a':0,'b':1,'c':2}\nx = torch.tensor(f(list('abc')*100,mapping))\ny = torch.tensor(f(list('bca')*100,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 0, 1]), tensor([1, 2, 0, 1, 2]))\n\n\n\ntorch.manual_seed(202150256)\nebdd = torch.nn.Embedding(num_embeddings=3,embedding_dim=1)\n\n\nebdd(x)[:5]\n\ntensor([[-1.0770],\n        [-0.2042],\n        [ 1.3288],\n        [-1.0770],\n        [-0.2042]], grad_fn=<SliceBackward0>)\n\n\n- 그런데 사실 언뜻보면 아래의 linr 함수와 역할의 차이가 없어보인다.\n\ntorch.manual_seed(202150256)\nlinr = torch.nn.Linear(in_features=1,out_features=1)\n\n\nlinr(x.float().reshape(-1,1))[:5]\n\ntensor([[-0.0483],\n        [-0.9158],\n        [-1.7834],\n        [-0.0483],\n        [-0.9158]], grad_fn=<SliceBackward0>)\n\n\n- 차이점 : 파라미터수에 차이가 있다.\n\nebdd.weight\n\nParameter containing:\ntensor([[-1.0770],\n        [-0.2042],\n        [ 1.3288]], requires_grad=True)\n\n\n\nlinr.weight, linr.bias\n\n(Parameter containing:\n tensor([[-0.8675]], requires_grad=True),\n Parameter containing:\n tensor([-0.0483], requires_grad=True))"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-22-9wk.html#연습-ab문제-소프트맥스로-확장한-것-다시-풀이",
    "href": "post/Lecture/STML/2023-03-22-9wk.html#연습-ab문제-소프트맥스로-확장한-것-다시-풀이",
    "title": "10. 순환신경망 (1)",
    "section": "연습 (ab문제 소프트맥스로 확장한 것 다시 풀이)",
    "text": "연습 (ab문제 소프트맥스로 확장한 것 다시 풀이)\n- 맵핑\n\nmapping = {'a':0,'b':1}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 0, 1, 0]), tensor([1, 0, 1, 0, 1]))\n\n\n- torch.nn.Embedding 을 넣은 네트워크\n\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=2,embedding_dim=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=2)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n- 학습\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nplt.plot(y[:5],'o')\nplt.plot(soft(net(x[:5]))[:,1].data,'--r')\n\n\n\n\n\nplt.imshow(soft(net(x[:5])).data,cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f874e1a4280>\n\n\n\n\n\n\ny[:5]\n\ntensor([1, 0, 1, 0, 1])\n\n\n\nsoft(net(x[:5]))\n\ntensor([[0.0067, 0.9933],\n        [0.9933, 0.0067],\n        [0.0067, 0.9933],\n        [0.9933, 0.0067],\n        [0.0067, 0.9933]], grad_fn=<SoftmaxBackward0>)"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html",
    "href": "post/Lecture/STML/2023-03-29-10wk.html",
    "title": "11. 순환신경망 (2)",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html#스스로-학습-중간고사-문제",
    "href": "post/Lecture/STML/2023-03-29-10wk.html#스스로-학습-중간고사-문제",
    "title": "11. 순환신경망 (2)",
    "section": "스스로 학습 (중간고사 문제)",
    "text": "스스로 학습 (중간고사 문제)\n아래와 같은 자료가 있다고 가정하자.\n\nx = torch.rand([1000,1])*2-1\ny = 3.14 + 6.28*x + torch.randn([1000,1]) \n\n\nplt.plot(x,y,'o',alpha=0.1)\n\n\n\n\n아래의 모형을 가정하고 \\(\\alpha_0,\\alpha_1,\\beta_0,\\beta_1\\)을 파이토치를 이용하여 추정하고자한다.\n\n\\(y_i = \\alpha_0+\\beta_0+ \\beta_1x_i + \\alpha_1x_i + \\epsilon_i \\quad \\epsilon_i \\sim N(0,\\sigma^2)\\)\n\n아래는 이를 수행하기 위한 코드이다. ???를 적절히 채워서 코드를 완성하라.\n\nalpha0 = torch.tensor([0.5], requires_grad=True)\nalpha1 = torch.tensor([[0.5]], requires_grad=True)\nbeta0 = torch.tensor([0.7], requires_grad=True)\nbeta1 = torch.tensor([[0.7]], requires_grad=True)\n\n\nloss_fn = torch.nn.MSELoss()\noptimizr = torch.optim.SGD([alpha0,alpha1,beta0,beta1], lr=1/10)\n\n\nfor epoc in range(30):\n    ## 1\n    yhat = alpha0 + beta0 + alpha1*x + beta1*x \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nprint(alpha0+beta0)\n\ntensor([3.1592], grad_fn=<AddBackward0>)\n\n\n\n3.14 근처에 있음을 확인\n\n\nprint(alpha1+beta1)\n\ntensor([[6.2157]], grad_fn=<AddBackward0>)\n\n\n\n6.28 근처에 있음을 확인"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html#data",
    "href": "post/Lecture/STML/2023-03-29-10wk.html#data",
    "title": "11. 순환신경망 (2)",
    "section": "data",
    "text": "data\n\ntxt = list('abc')*100\ntxt[:10]\n\n['a', 'b', 'c', 'a', 'b', 'c', 'a', 'b', 'c', 'a']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'c', 'a', 'b'], ['b', 'c', 'a', 'b', 'c'])"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html#하나의-은닉노드를-이용한-풀이-억지로-성공",
    "href": "post/Lecture/STML/2023-03-29-10wk.html#하나의-은닉노드를-이용한-풀이-억지로-성공",
    "title": "11. 순환신경망 (2)",
    "section": "하나의 은닉노드를 이용한 풀이 – 억지로 성공",
    "text": "하나의 은닉노드를 이용한 풀이 – 억지로 성공\n- 데이터정리\n\nmapping = {'a':0,'b':1,'c':2}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 0, 1]), tensor([1, 2, 0, 1, 2]))\n\n\n- 학습\n\ntorch.manual_seed(202150256)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=3,embedding_dim=1),\n    torch.nn.Tanh(),\n    #===#\n    torch.nn.Linear(in_features=1,out_features=3)\n)\nloss_fn = torch.nn.CrossEntropyLoss() ## loss함수안에 소프트맥스가 포함됨\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    ## 2 \n    loss = loss_fn(net(x),y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과해석\n\nnet [:-1] (x) : 임베딩층과, Tanh를 지난 상태\n\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nplt.plot(hidden[:9],'--o')\n\n\n\n\n- 입력값이 a,b,c가 주어졌을 때 각각의 은닉층에서 출력된 값이다.\n\nplt.plot(net(x).data[:9],'--o')\n\n\n\n\n- 파란색은 net(x)에 첫번째 컬럼, 두번째 컬럼은 주황색, 세번째 컬럼은 녹색이다.\n\nplt.plot(yhat[:9],'--o')\n\n\n\n\n- 소프트맥스를 취했을 때…\n\n억지로 맞추고있긴한데 파라메터가 부족해보인다.(주황색은 확실하게 1에 가깝지 않다.)\nnet(x)에서 출력값을 봤을 때 확실하게 맞추는게 아닌 소거법으로 맞추고 있는 것과 같다.\n\n- 결과시각화1\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n\nhidden[:9], (net[-1].weight.data).T, net[-1].bias.data\n\n(tensor([[-0.0147],\n         [ 0.9653],\n         [-0.9896],\n         [-0.0147],\n         [ 0.9653],\n         [-0.9896],\n         [-0.0147],\n         [ 0.9653],\n         [-0.9896]]),\n tensor([[-4.6804,  0.3071,  5.2894]]),\n tensor([-1.5440,  0.9143, -1.3970]))\n\n\n\nhidden[:9]@(net[-1].weight.data).T + net[-1].bias.data\n\ntensor([[-1.4755,  0.9098, -1.4745],\n        [-6.0618,  1.2108,  3.7086],\n        [ 3.0875,  0.6104, -6.6312],\n        [-1.4755,  0.9098, -1.4745],\n        [-6.0618,  1.2108,  3.7086],\n        [ 3.0875,  0.6104, -6.6312],\n        [-1.4755,  0.9098, -1.4745],\n        [-6.0618,  1.2108,  3.7086],\n        [ 3.0875,  0.6104, -6.6312]])\n\n\n\n(파랑,주황,초록) 순서로 그려짐\n파랑 = hidden * (-4.6804) + (-1.5440)\n주황 = hidden * (0.3071) + (0.9143)\n초록 = hidden * (5.2894) + (-1.3970)\n\n- 내부동작을 잘 뜯어보니까 사실 엉성해. 엄청 위태위태하게 맞추고 있었음. - weight: 파랑과 초록을 구분하는 역할을 함 - weight + bias: 뭔가 교모하게 애매한 주황값을 만들어서 애매하게 ’b’라고 나올 확률을 학습시킨다. \\(\\to\\) 사실 학습하는 것 같지 않고 때려 맞추는 느낌, 쓸수있는 weight가 한정적이라서 생기는 현상 (양수,음수,0)\n\n참고: torch.nn.Linear()의 비밀? - 사실 \\({\\boldsymbol y}={\\boldsymbol x}{\\bf W} + {\\boldsymbol b}\\) 꼴에서의 \\({\\bf W}\\)와 \\({\\boldsymbol b}\\)가 저장되는게 아니다. - \\({\\boldsymbol y}={\\boldsymbol x}{\\bf A}^T + {\\boldsymbol b}\\) 꼴에서의 \\({\\bf A}\\)와 \\({\\boldsymbol b}\\)가 저장된다. - \\({\\bf W} = {\\bf A}^T\\) 인 관계에 있으므로 l1.weight 가 우리가 생각하는 \\({\\bf W}\\) 로 해석하려면 사실 transpose를 취해줘야 한다.\n왜 이렇게..? - 계산의 효율성 때문 (numpy의 구조를 알아야함) - \\({\\boldsymbol x}\\), \\({\\boldsymbol y}\\) 는 수학적으로는 col-vec 이지만 메모리에 저장할시에는 row-vec 로 해석하는 것이 자연스럽다. (사실 메모리는 격자모양으로 되어있지 않음)\n잠깐 딴소리!!\n(예시1)\n\n_arr = np.array(range(4)).reshape(2,2)\n\n\n_arr.strides\n\n(16, 8)\n\n\n\n아래로 한칸 = 16 byte jump\n오른쪽으로 한칸 = 8 byte jump\n\n(예시2)\n\n_arr = np.array(range(6)).reshape(3,2)\n\n\n_arr.strides\n\n(16, 8)\n\n\n\n아래로 한칸 = 16byte jump (2칸)\n오른쪽으로 한칸 = 8byte jump (1칸)\n\n(예시3)\n\n_arr = np.array(range(6)).reshape(2,3)\n\n\n_arr.strides\n\n(24, 8)\n\n\n\n아래로 한칸 = 24칸 jump\n오른쪽으로 한칸 = 8칸 jump\n\n(예시4)\n\n_arr = np.array(range(4),dtype=np.int8).reshape(2,2)\n\n\n_arr\n\narray([[0, 1],\n       [2, 3]], dtype=int8)\n\n\n\n_arr.strides\n\n(2, 1)\n\n\n\n아래로한칸 = 2칸 (= 2바이트 jump = 16비트 jump)\n오른쪽으로 한칸 = 1칸 jump (= 1바이트 jump = 8비트 jump)\n\n진짜 참고..\n\n1바이트 = 8비트\n1바이트는 2^8=256 의 정보 표현\nnp.int8은 8비트로 정수를 저장한다는 의미\n\n\n2**8\n\n256\n\n\n\nprint(np.array(55,dtype=np.int8))\nprint(np.array(127,dtype=np.int8))\nprint(np.array(300,dtype=np.int8)) # overflow \n\n55\n127\n44\n\n\n딴소리 끝!!\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([299, 7])\n\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n\n그래프 해석 중요!\n\n\nplt.matshow(combined[:15],vmin=-7,vmax=7,cmap='bwr')\nplt.xticks(range(7), labels=[r'$h$',r'$y=a?$',r'$y=b?$',r'$y=c?$',r'$P(y=a)$',r'$P(y=b)$',r'$P(y=c)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html#data-1",
    "href": "post/Lecture/STML/2023-03-29-10wk.html#data-1",
    "title": "11. 순환신경망 (2)",
    "section": "data",
    "text": "data\n\ntxt = list('abcd')*100\ntxt[:10]\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'd', 'a', 'b']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'c', 'd', 'a'], ['b', 'c', 'd', 'a', 'b'])"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html#하나의-은닉노드를-이용한-풀이-억지로-성공-1",
    "href": "post/Lecture/STML/2023-03-29-10wk.html#하나의-은닉노드를-이용한-풀이-억지로-성공-1",
    "title": "11. 순환신경망 (2)",
    "section": "하나의 은닉노드를 이용한 풀이 – 억지로 성공",
    "text": "하나의 은닉노드를 이용한 풀이 – 억지로 성공\n- 데이터정리\n\nmapping = {'a':0,'b':1,'c':2,'d':3}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 3, 0]), tensor([1, 2, 3, 0, 1]))\n\n\n- 학습\n\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=1),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=1,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nnet[0].weight.data = torch.tensor([[-0.3333],[-2.5000],[5.0000],[0.3333]])\n\nnet[-1].weight.data = torch.tensor([[1.5000],[-6.0000],[-2.0000],[6.0000]])\nnet[-1].bias.data = torch.tensor([0.1500, -2.0000,  0.1500, -2.000])\n\n\nfor epoc in range(5000):\n    ## 1\n    ## 2 \n    loss = loss_fn(net(x),y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([399, 9])\n\n\n\nplt.matshow(combined[:15],vmin=-15,vmax=15,cmap='bwr')\nplt.xticks(range(9), labels=[r'$h$',r'$y=a?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$P(y=a)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html#두개의-은닉노드를-이용한-풀이-깔끔한-성공",
    "href": "post/Lecture/STML/2023-03-29-10wk.html#두개의-은닉노드를-이용한-풀이-깔끔한-성공",
    "title": "11. 순환신경망 (2)",
    "section": "두개의 은닉노드를 이용한 풀이 – 깔끔한 성공",
    "text": "두개의 은닉노드를 이용한 풀이 – 깔끔한 성공\n- 데이터정리\n\nmapping = {'a':0,'b':1,'c':2,'d':3}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 3, 0]), tensor([1, 2, 3, 0, 1]))\n\n\n- 학습\n\ntorch.manual_seed(202150256)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=2),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([399, 10])\n\n\n\nplt.matshow(combined[:15],vmin=-7,vmax=7,cmap='bwr')\nplt.xticks(range(10), labels=[r'$h$',r'$h$',r'$y=a?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$P(y=a)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html#data-2",
    "href": "post/Lecture/STML/2023-03-29-10wk.html#data-2",
    "title": "11. 순환신경망 (2)",
    "section": "data",
    "text": "data\n주어진 자료가 다음과 같다고 하자.\n\ntxt = list('abcde')*100\ntxt[:10]\n\n['a', 'b', 'c', 'd', 'e', 'a', 'b', 'c', 'd', 'e']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['a', 'b', 'c', 'd', 'e'], ['b', 'c', 'd', 'e', 'a'])\n\n\n아래 코드를 변형하여 적절한 네트워크를 설계하고 위의 자료를 학습하라. (깔끔한 성공을 위한 최소한의 은닉노드를 설정할 것)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=??,embedding_dim=??),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=??,out_features=??)\n)"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html#개의-은닉노드를-이용한-풀이",
    "href": "post/Lecture/STML/2023-03-29-10wk.html#개의-은닉노드를-이용한-풀이",
    "title": "11. 순환신경망 (2)",
    "section": "3개의 은닉노드를 이용한 풀이",
    "text": "3개의 은닉노드를 이용한 풀이\na,b,c,d,e 를 표현함에 있어서 3개의 은닉노드면 충분하다. - 1개의 은닉노드 -> 2개의 문자를 표현할 수 있음. - 2개의 은닉노드 -> 4개의 문자를 표현할 수 있음. - 3개의 은닉노드 -> 8개의 문자를 표현할 수 있음.\n\nmapping = {'a':0,'b':1,'c':2,'d':3,'e':4}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 2, 3, 4]), tensor([1, 2, 3, 4, 0]))\n\n\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=5,embedding_dim=3),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=3,out_features=5)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([499, 13])\n\n\n\nplt.matshow(combined[:15],vmin=-5,vmax=5,cmap='bwr')\nplt.xticks(range(13), labels=[r'$h$',r'$h$',r'$h$',\n                              r'$y=A?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$y=e?$',\n                              r'$P(y=A)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$',r'$P(y=e)$'],size=13)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html#data-3",
    "href": "post/Lecture/STML/2023-03-29-10wk.html#data-3",
    "title": "11. 순환신경망 (2)",
    "section": "data",
    "text": "data\n\ntxt = list('AbAcAd')*100\ntxt[:10]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['A', 'b', 'A', 'c', 'A'], ['b', 'A', 'c', 'A', 'd'])"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html#두개의-은닉노드를-이용한-풀이-실패",
    "href": "post/Lecture/STML/2023-03-29-10wk.html#두개의-은닉노드를-이용한-풀이-실패",
    "title": "11. 순환신경망 (2)",
    "section": "두개의 은닉노드를 이용한 풀이 – 실패",
    "text": "두개의 은닉노드를 이용한 풀이 – 실패\n- 데이터정리\n\nmapping = {'A':0,'b':1,'c':2,'d':3}\nx = torch.tensor(f(txt_x,mapping))\ny = torch.tensor(f(txt_y,mapping))\nx[:5],y[:5]\n\n(tensor([0, 1, 0, 2, 0]), tensor([1, 0, 2, 0, 3]))\n\n\n- 학습\n\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=2),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n\n\nfor epoc in range(5000):\n    ## 1\n    yhat = net(x)\n    ## 2 \n    loss = loss_fn(yhat,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- 결과시각화1\n\nhidden = net[:-1](x).data\nyhat = soft(net(x)).data\n\n\nfig,ax = plt.subplots(1,3,figsize=(15,5))\nax[0].plot(hidden[:9],'--o'); ax[0].set_title('$h:=(tanh \\circ linr_1)(x)$',size=15)\nax[1].plot(net(x).data[:9],'--o'); ax[1].set_title('$net(x):=(linr_2 \\circ tanh \\circ linr_1)(x)$',size=15)\nax[2].plot(yhat[:9],'--o'); ax[2].set_title('$\\hat{y}$ = softmax$(net(x))$',size=15);\nfig.suptitle(r\"Vis1: $h,net(x),\\hat{y}$\",size=20)\nplt.tight_layout()\n\n\n\n\n- 결과시각화2\n\ncombined  = torch.concat([hidden,net(x).data,yhat],axis=1)\ncombined.shape\n\ntorch.Size([599, 10])\n\n\n\nplt.matshow(combined[:15],vmin=-5,vmax=5,cmap='bwr')\nplt.xticks(range(10), labels=[r'$h$',r'$h$',r'$y=A?$',r'$y=b?$',r'$y=c?$',r'$y=d?$',r'$P(y=A)$',r'$P(y=b)$',r'$P(y=c)$',r'$P(y=d)$'],size=14)\nplt.colorbar()\nplt.gcf().set_figwidth(15)\nplt.gcf().set_figheight(15)\nplt.title(r\"Vis2: $[h | net(x) | \\hat{y}]$\",size=25)\n\nText(0.5, 1.0, 'Vis2: $[h | net(x) | \\\\hat{y}]$')\n\n\n\n\n\n\n실패\n\n- 실패를 해결하는 순진한 접근방식: 위 문제를 해결하기 위해서는 아래와 같은 구조로 데이터를 다시 정리하면 될 것이다.\n\n\n\nX\ny\n\n\n\n\nA,b\nA\n\n\nb,A\nc\n\n\nA,c\nA\n\n\nc,A\nd\n\n\nA,d\nA\n\n\nd,A\nb\n\n\nA,b\nA\n\n\nb,A\nc\n\n\n…\n…\n\n\n\n- 순진한 접근방식의 비판: - 결국 정확하게 직전 2개의 문자를 보고 다음 문제를 예측하는 구조 - 만약에 직전 3개의 문자를 봐야하는 상황이 된다면 또 다시 코드를 수정해야함. - 그리고 실전에서는 직전 몇개의 문자를 봐야하는지 모름.\n이것에 대한 해결책은 순환신경망이다."
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html#순환망을-위하여-data-다시정리",
    "href": "post/Lecture/STML/2023-03-29-10wk.html#순환망을-위하여-data-다시정리",
    "title": "11. 순환신경망 (2)",
    "section": "순환망을 위하여 data 다시정리",
    "text": "순환망을 위하여 data 다시정리\n- 기존의 정리방식\n\ntxt = list('AbAcAd')*100\ntxt[:10]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['A', 'b', 'A', 'c', 'A'], ['b', 'A', 'c', 'A', 'd'])\n\n\n\nx = torch.tensor(f(txt_x,{'A':0,'b':1,'c':2,'d':3}))\ny = torch.tensor(f(txt_y,{'A':0,'b':1,'c':2,'d':3}))\n\n\nx[:8],y[:8]\n\n(tensor([0, 1, 0, 2, 0, 3, 0, 1]), tensor([1, 0, 2, 0, 3, 0, 1, 0]))\n\n\n- 이번엔 원핫인코딩형태까지 미리 정리하자. (임베딩 레이어 안쓸예정)\n\nx= torch.nn.functional.one_hot(x).float()\ny= torch.nn.functional.one_hot(y).float()\n\n\nx,y\n\n(tensor([[1., 0., 0., 0.],\n         [0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         ...,\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.]]),\n tensor([[0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         ...,\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 0., 1.]]))"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html#실패했던-풀이의-재구현1",
    "href": "post/Lecture/STML/2023-03-29-10wk.html#실패했던-풀이의-재구현1",
    "title": "11. 순환신경망 (2)",
    "section": "실패했던 풀이의 재구현1",
    "text": "실패했던 풀이의 재구현1\n- 방금 실패한 풀이\ntorch.manual_seed(43052)\nnet = torch.nn.Sequential(\n    torch.nn.Embedding(num_embeddings=4,embedding_dim=2),\n    torch.nn.Tanh(),\n    torch.nn.Linear(in_features=2,out_features=4)\n)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(net.parameters())\n- Tanh까지만 클래스로 바꾸어서 구현\n\nclass Hnet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(in_features=4,out_features=2)\n        self.tanh = torch.nn.Tanh()\n    def forward(self,x):\n        hidden = self.tanh(self.i2h(x))\n        return hidden\n\n- for문돌릴준비\n\ntorch.manual_seed(43052) \nhnet = Hnet()\nlinr = torch.nn.Linear(in_features=2,out_features=4)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(hnet.parameters())+list(linr.parameters()))\n\n- for문: 20회반복\n\nfor epoc in range(20): \n    ## 1 \n    ## 2 \n    hidden = hnet(x) \n    output = linr(hidden)\n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- linr(hnet(x)) 적합결과 <– 숫자체크\n\nlinr(hnet(x))\n\ntensor([[-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.2912,  0.8140, -0.2032,  0.0178],\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        ...,\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.1065,  0.6307, -0.0874,  0.1821],\n        [-0.3589,  0.7921, -0.1970, -0.0302]], grad_fn=<AddmmBackward0>)"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html#실패했던-풀이의-재구현2",
    "href": "post/Lecture/STML/2023-03-29-10wk.html#실패했던-풀이의-재구현2",
    "title": "11. 순환신경망 (2)",
    "section": "실패했던 풀이의 재구현2",
    "text": "실패했던 풀이의 재구현2\n- 위에 했던 코드를 걍 펼친거\n- Tanh까지 구현한 클래스\n\n#\n# class Hnet(torch.nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.i2h = torch.nn.Linear(in_features=4,out_features=2)\n#         self.tanh = torch.nn.Tanh()\n#     def forward(self,x):\n#         hidden = self.tanh(self.i2h(x))\n#         return hidden\n\n- for문돌릴준비\n\ntorch.manual_seed(43052) \nhnet = Hnet()\nlinr = torch.nn.Linear(in_features=2,out_features=4)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(hnet.parameters())+list(linr.parameters()))\n\n- for문: 20회 반복\n\nT = len(x) ## 559 개의 데이터\nfor epoc in range(20): \n    ## 1~2\n    loss = 0 \n    for t in range(T): ## 각각의 관측치에 대한 loss 계산\n        xt,yt = x[[t]], y[[t]]\n        ht = hnet(xt) \n        ot = linr(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n- linr(hnet(x)) 적합결과 <– 숫자체크\n\nlinr(hnet(x))\n\ntensor([[-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.2912,  0.8140, -0.2032,  0.0178],\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        ...,\n        [-0.3589,  0.7921, -0.1970, -0.0302],\n        [-0.1065,  0.6307, -0.0874,  0.1821],\n        [-0.3589,  0.7921, -0.1970, -0.0302]], grad_fn=<AddmmBackward0>)"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html#순환신경망의-아이디어",
    "href": "post/Lecture/STML/2023-03-29-10wk.html#순환신경망의-아이디어",
    "title": "11. 순환신경망 (2)",
    "section": "순환신경망의 아이디어",
    "text": "순환신경망의 아이디어\n\n모티브\n(예비생각1) \\({\\boldsymbol h}\\)에 대한 이해\n\\({\\boldsymbol h}\\)는 사실 문자열 ’abcd’들을 숫자로 바꾼 또 다른 형식의 숫자표현이라 해석할 수 있음. 즉 원핫인코딩과 다른 또 다른 형태의 숫자표현이라 해석할 수 있다. (사실 원핫인코딩보다 약간 더 (1) 액기스만 남은 느낌 + (2) 숙성된 느낌을 준다) - (why1) h는 “학습을 용이하게 하기 위해서 x를 적당히 선형적으로 전처리한 상태”라고 이해가능 - (why2) 실제로 예시를 살펴보면 그러했다.\n결론: 사실 \\({\\boldsymbol h}\\)는 잘 숙성되어있는 입력정보 \\({\\bf X}\\) 그 자체로 해석 할 수 있다.\n(예비생각2) 수백년전통을 이어가는 방법\n“1리터에 500만원에 낙찰된 적 있습니다.”\n“2kg에 1억원 정도 추산됩니다.”\n“20여 종 종자장을 블렌딩해 100ml에 5000만원씩 분양 예정입니다.”\n\n모두 씨간장(종자장) 가격에 관한 실제 일화다.\n\n(중략...)\n\n위스키나 와인처럼 블렌딩을 하기도 한다. \n새로 담근 간장에 씨간장을 넣거나, 씨간장독에 햇간장을 넣어 맛을 유지하기도 한다. \n이를 겹장(또는 덧장)이라 한다. \n몇몇 종갓집에선 씨간장 잇기를 몇백 년째 해오고 있다. \n매년 새로 간장을 담가야 이어갈 수 있으니 불씨 꺼트리지 않는 것처럼 굉장히 어려운 일이다.\n이렇게 하는 이유는 집집마다 내려오는 고유 장맛을 잃지 않기 위함이다. \n씨간장이란 그만큼 소중한 주방의 자산이며 정체성이다.\n덧장: 새로운간장을 만들때, 옛날간장을 섞어서 만듬\n* 기존방식 - \\(\\text{콩물} \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}\\)\n* 수백년 전통의 간장맛을 유지하는 방식\n\n\\(\\text{콩물}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1\\)\n\\(\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2\\)\n\\(\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3\\)\n\n* 수백년 전통의 간장맛을 유지하면서 조리를 한다면?\n\n\\(\\text{콩물}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_1\\)\n\\(\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_2\\)\n\\(\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_3\\)\n\n점점 맛있는 간장계란밥이 탄생함\n* 알고리즘의 편의상 아래와 같이 생각해도 무방\n\n\\(\\text{콩물}_1, \\text{간장}_0 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_1 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_1\\), \\(\\text{간장}_0=\\text{맹물}\\)\n\\(\\text{콩물}_2, \\text{간장}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_2 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_2\\)\n\\(\\text{콩물}_3, \\text{간장}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\text{간장}_3 \\overset{\\text{조리}}{\\longrightarrow} \\text{간장계란밥}_3\\)\n\n아이디어\n* 수백년 전통의 간장맛을 유지하면서 조리하는 과정을 수식으로?\n\n\\(\\boldsymbol{x}_1, \\boldsymbol{h}_0 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_1 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_1\\)\n\\(\\boldsymbol{x}_2, \\boldsymbol{h}_1 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_2 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_2\\)\n\\(\\boldsymbol{x}_3, \\boldsymbol{h}_2 \\overset{\\text{숙성}}{\\longrightarrow} \\boldsymbol{h}_3 \\overset{\\text{조리}}{\\longrightarrow} \\hat{\\boldsymbol y}_3\\)\n\n이제 우리가 배울것은 (1) “\\(\\text{콩물}_{t}\\)”와 “\\(\\text{간장}_{t-1}\\)”로 “\\(\\text{간장}_t\\)”를 숙성하는 방법 (2) “\\(\\text{간장}_t\\)”로 “\\(\\text{간장계란밥}_t\\)를 조리하는 방법이다\n즉 숙성담당 네트워크와 조리담당 네트워크를 각각 만들어 학습하면 된다.\n\n\n알고리즘\n세부적인 알고리즘 (\\(t=0,1,2,\\dots\\)에 대하여 한줄 한줄 쓴 알고리즘)\n\n\\(t=0\\)\n\n\\({\\boldsymbol h}_0=[[0,0]]\\) <– \\(\\text{간장}_0\\)은 맹물로 초기화\n\n\\(t=1\\)\n\n\\({\\boldsymbol h}_1= \\tanh({\\boldsymbol x}_1{\\bf W}_{ih}+{\\boldsymbol h}_0{\\bf W}_{hh}+{\\boldsymbol b}_{ih}+{\\boldsymbol b}_{hh})\\) - \\({\\boldsymbol x}_1\\): (1,4) - \\({\\bf W}_{ih}\\): (4,2) - \\({\\boldsymbol h}_0\\): (1,2) - \\({\\bf W}_{hh}\\): (2,2) - \\({\\boldsymbol b}_{ih}\\): (1,2) - \\({\\boldsymbol b}_{hh}\\): (1,2)\n\\({\\boldsymbol o}_1= {\\bf W}_{ho}{\\boldsymbol h}_1+{\\boldsymbol b}_{ho}\\)\n\\(\\hat{\\boldsymbol y}_1 = \\text{soft}({\\boldsymbol o}_1)\\)\n\n\\(t=2\\) <– 여기서부터는 \\(t=2\\)와 비슷\n\n\n좀 더 일반화된 알고리즘\n(ver1)\ninit \\(\\boldsymbol{h}_0\\)\nfor \\(t\\) in \\(1:T\\)\n\n\\({\\boldsymbol h}_t= \\tanh({\\boldsymbol x}_t{\\bf W}_{ih}+{\\boldsymbol h}_{t-1}{\\bf W}_{hh}+{\\boldsymbol b}_{ih}+{\\boldsymbol b}_{hh})\\)\n\\({\\boldsymbol o}_t= {\\bf W}_{ho}{\\boldsymbol h}_1+{\\boldsymbol b}_{ho}\\)\n\\(\\hat{\\boldsymbol y}_t = \\text{soft}({\\boldsymbol o}_t)\\)\n\n(ver2)\ninit hidden\n\nfor t in 1:T \n    hidden = tanh(linr(x)+linr(hidden))\n    output = linr(hidden)\n    yt_hat = soft(output)\n\n코드상으로는 \\(h_t\\)와 \\(h_{t-1}\\)의 구분이 교모하게 사라진다. (그래서 오히려 좋아)\n\n\n전체알고리즘은 대충 아래와 같은 형식으로 구현될 수 있음\n### \nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        linr1 = torch.nn.Linear(?,?) \n        linr2 = torch.nn.Linear(?,?) \n        tanh = torch.nn.Tanh()\n    def forward(self,x,hidden):\n        hidden = tanh(lrnr1(x)+lrnr2(hidden))\n        return hidden\n\ninit ht\nrnncell = rNNCell()\n\nfor t in 1:T \n    xt, yt = x[[t]], y[[t]] \n    ht = rnncell(xt, ht)\n    ot = linr(ht) \n    loss = loss + loss_fn(ot, yt)"
  },
  {
    "objectID": "post/Lecture/STML/2023-03-29-10wk.html#순환신경망-구현1-성공",
    "href": "post/Lecture/STML/2023-03-29-10wk.html#순환신경망-구현1-성공",
    "title": "11. 순환신경망 (2)",
    "section": "순환신경망 구현1 – 성공",
    "text": "순환신경망 구현1 – 성공\n(1) 숙성담당 네트워크\n\nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(4,2) \n        self.h2h = torch.nn.Linear(2,2) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,x,hidden):\n        hidden = self.tanh(self.i2h(x)+self.h2h(hidden))\n        return hidden\n\n\ntorch.manual_seed(43052)\nrnncell = rNNCell() # 숙성담당 네트워크 \n\n(2) 조리담당 네트워크\n\ntorch.manual_seed(43052)\ncook = torch.nn.Linear(2,4) \n\n(3) 손실함수, 옵티마이저 설계\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습 (6분정도 걸림)\n\nT = len(x) \nfor epoc in range(5000): \n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]] ## x[[t]]의 shape은 (1,4)\n        ht = rnncell(xt,ht) \n        ot = cook(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nT = len(x) \nhidden = torch.zeros(T,2) # 599년치 h를 담을 변수 \n_water = torch.zeros(1,2) # 맹물 \nhidden[[0]] = rnncell(x[[0]],_water) \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]]) \n\n\nyhat = soft(cook(hidden))\nyhat\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [7.4634e-05, 1.9501e-01, 8.0407e-01, 8.4751e-04],\n        [9.4785e-01, 7.4711e-03, 6.1182e-04, 4.4064e-02],\n        [3.6306e-02, 1.2466e-01, 2.8862e-03, 8.3615e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat.data[-15:])\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7fd584579720>\n\n\n\n\n\n\nyhat[:15]\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        [9.4671e-01, 7.6233e-03, 6.2076e-04, 4.5044e-02],\n        [3.6321e-02, 1.2369e-01, 2.8456e-03, 8.3715e-01],\n        [9.9549e-01, 7.4116e-04, 1.1089e-04, 3.6589e-03],\n        [7.8381e-03, 6.7206e-01, 1.9159e-01, 1.2850e-01],\n        [9.9966e-01, 6.3009e-05, 1.7874e-05, 2.5558e-04],\n        [7.4672e-05, 1.9505e-01, 8.0403e-01, 8.4796e-04],\n        [9.4783e-01, 7.4739e-03, 6.1198e-04, 4.4082e-02],\n        [3.6306e-02, 1.2464e-01, 2.8855e-03, 8.3617e-01],\n        [9.9553e-01, 7.3554e-04, 1.1027e-04, 3.6289e-03],\n        [7.6975e-03, 6.7204e-01, 1.9431e-01, 1.2595e-01],\n        [9.9966e-01, 6.2958e-05, 1.7864e-05, 2.5536e-04],\n        [7.4635e-05, 1.9501e-01, 8.0407e-01, 8.4752e-04]],\n       grad_fn=<SliceBackward0>)\n\n\n\nyhat[:-15]\n\ntensor([[1.6522e-02, 6.2036e-01, 1.0433e-01, 2.5879e-01],\n        [9.9965e-01, 6.5788e-05, 1.8450e-05, 2.6785e-04],\n        [7.6673e-05, 1.9704e-01, 8.0201e-01, 8.7218e-04],\n        ...,\n        [9.9553e-01, 7.3543e-04, 1.1026e-04, 3.6284e-03],\n        [7.6949e-03, 6.7203e-01, 1.9436e-01, 1.2591e-01],\n        [9.9966e-01, 6.2957e-05, 1.7863e-05, 2.5535e-04]],\n       grad_fn=<SliceBackward0>)\n\n\n\n_test =list(range(4))\n\n\n_test\n\n[0, 1, 2, 3]\n\n\n\n_test[:-3]\n\n[0]\n\n\n\n_test[:3]\n\n[0, 1, 2]\n\n\n\n아주 특이한 특징: yhat[:15], yhat[:-15] 의 적합결과가 다르다\n왜? 간장계란밥은 간장이 중요한데, 간장은 시간이 갈수록 맛있어지니까.."
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html",
    "href": "post/Lecture/STML/2023-04-05-11wk.html",
    "title": "12. 순환신경망 (3)",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#data",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#data",
    "title": "12. 순환신경망 (3)",
    "section": "data",
    "text": "data\n- 기존의 정리방식\n\ntxt = list('AbAcAd')*100\ntxt[:10]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:5],txt_y[:5]\n\n(['A', 'b', 'A', 'c', 'A'], ['b', 'A', 'c', 'A', 'd'])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,{'A':0,'b':1,'c':2,'d':3}))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,{'A':0,'b':1,'c':2,'d':3}))).float()\n\n\nx,y\n\n(tensor([[1., 0., 0., 0.],\n         [0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         ...,\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.]]),\n tensor([[0., 1., 0., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         ...,\n         [0., 0., 1., 0.],\n         [1., 0., 0., 0.],\n         [0., 0., 0., 1.]]))"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#순환신경망-구현1-손으로-직접구현-리뷰",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#순환신경망-구현1-손으로-직접구현-리뷰",
    "title": "12. 순환신경망 (3)",
    "section": "순환신경망 구현1 (손으로 직접구현) – 리뷰",
    "text": "순환신경망 구현1 (손으로 직접구현) – 리뷰\n(1) 숙성담당 네트워크\n\nclass rNNCell(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.i2h = torch.nn.Linear(4,2) \n        self.h2h = torch.nn.Linear(2,2) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,x,hidden):\n        hidden = self.tanh(self.i2h(x)+self.h2h(hidden))\n        return hidden\n\n\ntorch.manual_seed(202150256)\nrnncell = rNNCell() # 숙성담당 네트워크 \n\n(2) 조리담당 네트워크\n\ntorch.manual_seed(202150256)\ncook = torch.nn.Linear(2,4) \n\n(3) 손실함수, 옵티마이저 설계\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습 (15분정도 걸림)\n\nT = len(x) \nfor epoc in range(5000): \n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht) \n        ot = cook(ht) \n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nT = len(x) \nhidden = torch.zeros(T,2) # 599년치 h를 담을 변수 \n_water = torch.zeros(1,2) # 맹물 \nhidden[[0]] = rnncell(x[[0]],_water) \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]]) \n\n\nyhat = soft(cook(hidden))\nyhat\n\ntensor([[2.9189e-02, 8.0838e-01, 1.6199e-01, 4.4508e-04],\n        [9.9486e-01, 2.6187e-03, 2.4860e-03, 3.6038e-05],\n        [4.8180e-03, 1.0062e-02, 5.3523e-01, 4.4989e-01],\n        ...,\n        [1.0414e-02, 9.8797e-02, 7.9494e-01, 9.5846e-02],\n        [9.8301e-01, 1.5498e-02, 1.4859e-03, 2.0402e-06],\n        [5.9093e-04, 2.0405e-04, 1.0415e-01, 8.9506e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\ntxt[:15]\n\n['A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A']\n\n\n\n\nCode\nplt.matshow(yhat.data[:15],cmap='bwr')\nplt.colorbar()\n\n\n<matplotlib.colorbar.Colorbar at 0x7f19603f06d0>\n\n\n\n\n\n\ntxt[-15:]\n\n['c', 'A', 'd', 'A', 'b', 'A', 'c', 'A', 'd', 'A', 'b', 'A', 'c', 'A', 'd']\n\n\n\nplt.matshow(yhat.data[-15:],cmap='bwr')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7f1960458280>"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#순환신경망-구현2-with-rnncell-hidden-node-2",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#순환신경망-구현2-with-rnncell-hidden-node-2",
    "title": "12. 순환신경망 (3)",
    "section": "순환신경망 구현2 (with RNNCell, hidden node 2)",
    "text": "순환신경망 구현2 (with RNNCell, hidden node 2)\nref: https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html\n\n구현1과 같은 초기값 (확인용)\n(1) 숙성네트워크\n\ntorch.manual_seed(202150256)\n_rnncell = rNNCell() # 숙성담당 네트워크 \n\n\nrnncell = torch.nn.RNNCell(4,2)\n\n- rNNCell() 는 사실 torch.nn.RNNCell()와 같은 동작을 하도록 설계를 하였음.\n- 즉, rNNCell()의 초기값을 torch.nn.RNNCell()의 초기값으로 설정할 수 있다.\n\n _rnncell.i2h.weight.data, _rnncell.i2h.bias.data\n\n(tensor([[-0.4338, -0.0241,  0.4315, -0.3468],\n         [-0.2340,  0.1910, -0.2699,  0.0572]]),\n tensor([-0.0831,  0.3921]))\n\n\n\n _rnncell.i2h.weight.data.shape, _rnncell.i2h.bias.data.shape\n\n(torch.Size([2, 4]), torch.Size([2]))\n\n\n\nrnncell.weight_ih.data,rnncell.bias_ih.data\n\n(tensor([[ 0.6734, -0.3981,  0.4196, -0.2522],\n         [-0.4991, -0.0893,  0.0554, -0.1479]]),\n tensor([ 0.0601, -0.1892]))\n\n\n\nrnncell.weight_ih.data = _rnncell.i2h.weight.data\nrnncell.bias_ih.data = _rnncell.i2h.bias.data\nrnncell.weight_hh.data = _rnncell.h2h.weight.data\nrnncell.bias_hh.data = _rnncell.h2h.bias.data\n\n(2) 조리네트워크\n\ntorch.manual_seed(202150256)\ncook = torch.nn.Linear(2,4) # 숙성된 2차원의 단어를 다시 4차원으로 바꿔줘야지 나중에 softmax취할 수 있음\n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습\n\nT = len(x) \nfor epoc in range(5000):\n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht)\n        ot = cook(ht)\n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화\n\nhidden = torch.zeros(T,2) \n\n\n# t=0 \n_water = torch.zeros(1,2)\nhidden[[0]] = rnncell(x[[0]],_water)\n# t=1~T \nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]])\n\n\nyhat = soft(cook(hidden))\nyhat\n\ntensor([[2.9189e-02, 8.0838e-01, 1.6199e-01, 4.4508e-04],\n        [9.9486e-01, 2.6187e-03, 2.4860e-03, 3.6038e-05],\n        [4.8180e-03, 1.0062e-02, 5.3523e-01, 4.4989e-01],\n        ...,\n        [1.0414e-02, 9.8797e-02, 7.9494e-01, 9.5846e-02],\n        [9.8301e-01, 1.5498e-02, 1.4859e-03, 2.0402e-06],\n        [5.9093e-04, 2.0405e-04, 1.0415e-01, 8.9506e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n\nplt.matshow(yhat[:15].data,cmap='bwr')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7f196017ee60>\n\n\n\n\n\n\nplt.matshow(yhat[-15:].data,cmap='bwr')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7f196017cee0>\n\n\n\n\n\n- 뒷부분에 갈수록 더 확실하게 맞춤을 확인\n\n\n새로운 초기값\n(1) 숙성네트워크\n\ntorch.manual_seed(202150056)\ntorch.nn.RNNCell(4,2)\n\nRNNCell(4, 2)\n\n\n(2) 조리네트워크\n\ntorch.manual_seed(202150256)\ncook = torch.nn.Linear(2,4) # 숙성된 2차원의 단어를 다시 4차원으로 바꿔줘야지 나중에 softmax취할 수 있음\n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnncell.parameters())+list(cook.parameters()))\n\n(4) 학습\n\nT = len(x) \nfor epoc in range(5000):\n    ## 1~2\n    loss = 0 \n    ht = torch.zeros(1,2) \n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht = rnncell(xt,ht)\n        ot = cook(ht)\n        loss = loss + loss_fn(ot,yt) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#순환신경망-구현3-with-rnn-hidden-node-2-성공",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#순환신경망-구현3-with-rnn-hidden-node-2-성공",
    "title": "12. 순환신경망 (3)",
    "section": "순환신경망 구현3 (with RNN, hidden node 2) – 성공",
    "text": "순환신경망 구현3 (with RNN, hidden node 2) – 성공\n- 난 이거 쓸거얌\n(예비학습)\n- 네트워크학습이후 yhat을 구하려면 번거로웠음\nhidden = torch.zeros(T,2) \n_water = torch.zeros(1,2) \nhidden[[0]] = rnncell(x[[0]],_water)\nfor t in range(1,T):\n    hidden[[t]] = rnncell(x[[t]],hidden[[t-1]])\nyhat = soft(cook(hidden))\n- 이렇게 하면 쉽게(?) 구할 수 있음\n\nrnn = torch.nn.RNN(4,2)\n\n\nrnn.weight_hh_l0.data = rnncell.weight_hh.data \nrnn.weight_ih_l0.data = rnncell.weight_ih.data\nrnn.bias_hh_l0.data = rnncell.bias_hh.data\nrnn.bias_ih_l0.data = rnncell.bias_ih.data\n\n- rnn(x,_water)의 결과는 (1) 599년치 간장 (2) 599번째 간장 이다\n\nrnn(x,_water), hidden\n\n((tensor([[ 0.9765, -0.9970],\n          [ 0.4280,  0.9999],\n          [ 0.1560, -1.0000],\n          ...,\n          [ 0.0140, -1.0000],\n          [ 0.9965,  0.9998],\n          [-0.9783, -1.0000]], grad_fn=<SqueezeBackward1>),\n  tensor([[-0.9783, -1.0000]], grad_fn=<SqueezeBackward1>)),\n tensor([[ 0.7997, -0.6105],\n         [ 0.4600,  0.9822],\n         [-0.4392, -0.9890],\n         ...,\n         [-0.0204, -0.9879],\n         [ 0.9684,  0.9929],\n         [-0.9406, -0.9927]], grad_fn=<IndexPutBackward0>))\n\n\n\nsoft(cook(rnn(x,_water)[0]))\n\ntensor([[7.8210e-03, 9.3877e-01, 5.2606e-02, 8.0528e-04],\n        [9.8516e-01, 2.9026e-03, 1.0667e-02, 1.2718e-03],\n        [2.0010e-02, 1.8112e-01, 6.8655e-01, 1.1232e-01],\n        ...,\n        [1.5781e-02, 9.1106e-02, 7.1647e-01, 1.7664e-01],\n        [9.7901e-01, 1.7454e-02, 3.4545e-03, 7.9740e-05],\n        [5.8298e-04, 1.4535e-04, 1.8742e-01, 8.1185e-01]],\n       grad_fn=<SoftmaxBackward0>)\n\n\n(예비학습결론) torch.nn.RNN(4,2)는 torch.nn.RNNCell(4,2)의 batch 버전이다. (for문이 포함된 버전이다)\n\n?torch.nn.RNN\n\n\ntorch.nn.RNN(4,2)를 이용하여 구현하자.\n(1) 숙성네트워크\n선언\n\nrnn = torch.nn.RNN(4,2)\n\n가중치초기화\n\ntorch.manual_seed(202150256)\n_rnncell = torch.nn.RNNCell(4,2)\n\n\nrnn.weight_hh_l0.data = _rnncell.weight_hh.data \nrnn.weight_ih_l0.data = _rnncell.weight_ih.data\nrnn.bias_hh_l0.data = _rnncell.bias_hh.data\nrnn.bias_ih_l0.data = _rnncell.bias_ih.data\n\n(2) 조리네트워크\n\ntorch.manual_seed(202150256)\ncook = torch.nn.Linear(2,4) \n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(cook.parameters()))\n\n(4) 학습\n\n_water = torch.zeros(1,2) \nfor epoc in range(5000):\n    ## 1 \n    hidden,hT = rnn(x,_water)\n    output = cook(hidden) \n    ## 2 \n    loss = loss_fn(output,y)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화1: yhat\n\nyhat = soft(output)\n\n\nplt.matshow(yhat.data[:15],cmap='bwr')\nplt.colorbar()\n\n<matplotlib.image.AxesImage at 0x7f194de42230>\n\n\n\n\n\n\n처음은 좀 틀렸음 ㅎㅎ\n\n\nplt.matshow(yhat.data[-15:],cmap='bwr')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7f194dfbec20>\n\n\n\n\n\n\n뒤에는 잘맞음\n\n실전팁: _water 대신에 hT를 대입 (사실 큰 차이는 없음)\n\nhT\n\ntensor([[ 0.9221, -0.9094,  0.9954]], grad_fn=<SqueezeBackward1>)\n\n\n\nrnn(x[:6],_water),rnn(x[:6],hT)\n\n((tensor([[-0.9644, -0.9514],\n          [-0.9991,  0.9685],\n          [ 0.6879, -0.9995],\n          [-0.9972,  0.2970],\n          [-0.8253, -0.9948],\n          [-0.9998,  0.9781]], grad_fn=<SqueezeBackward1>),\n  tensor([[-0.9998,  0.9781]], grad_fn=<SqueezeBackward1>)),\n (tensor([[-0.9999, -0.5664],\n          [-0.9913,  0.8827],\n          [ 0.5273, -0.9993],\n          [-0.9972,  0.2034],\n          [-0.8965, -0.9928],\n          [-0.9998,  0.9760]], grad_fn=<SqueezeBackward1>),\n  tensor([[-0.9998,  0.9760]], grad_fn=<SqueezeBackward1>)))\n\n\n(6) 시각화2: hidden, yhat\n\ncombinded = torch.concat([hidden,yhat],axis=1)\n\n\nplt.matshow(combinded[-15:].data,cmap='bwr')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7f194d930b20>\n\n\n\n\n\n\n히든노드의 해석이 어려움."
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#순환신경망-구현4-with-rnn-hidden-node-3-성공",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#순환신경망-구현4-with-rnn-hidden-node-3-성공",
    "title": "12. 순환신경망 (3)",
    "section": "순환신경망 구현4 (with RNN, hidden node 3) – 성공",
    "text": "순환신경망 구현4 (with RNN, hidden node 3) – 성공\n(1) 숙성네트워크~ (2) 조리네트워크\n\ntorch.manual_seed(2) #1 \nrnn = torch.nn.RNN(4,3) \ncook = torch.nn.Linear(3,4) \n\n(3) 손실함수와 옵티마이저\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(cook.parameters()))\n\n(4) 학습\n\n_water = torch.zeros(1,3) \nfor epoc in range(5000):\n    ## 1\n    hidden,hT = rnn(x,_water) \n    output = cook(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n(5) 시각화1: yhat\n\nyhat = soft(output)\n\n\nplt.matshow(yhat[-15:].data,cmap='bwr')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7f194da24a90>\n\n\n\n\n\n(6) 시각화2: hidden, yhat\n\ncombinded = torch.concat([hidden,yhat],axis=1)\n\n\nplt.matshow(combinded[-15:].data,cmap='bwr')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x7f194dc35fc0>\n\n\n\n\n\n\n세번째 히든노드 = 대소문자를 구분\n1,2 히든노드 = bcd를 구분"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#len-20-hidden-nodes",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#len-20-hidden-nodes",
    "title": "12. 순환신경망 (3)",
    "section": "20000 len + 20 hidden nodes",
    "text": "20000 len + 20 hidden nodes\ncpu\n\nx = torch.randn([20000,4]) \ny = torch.randn([20000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nimport time\n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\ngpu\n\nx = torch.randn([20000,4]).to(\"cuda:0\")\ny = torch.randn([20000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\n\n왜 빠른지?"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#len-20-hidden-nodes-역전파주석처리",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#len-20-hidden-nodes-역전파주석처리",
    "title": "12. 순환신경망 (3)",
    "section": "20000 len + 20 hidden nodes + 역전파주석처리",
    "text": "20000 len + 20 hidden nodes + 역전파주석처리\ncpu\n\nx = torch.randn([20000,4]) \ny = torch.randn([20000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\ngpu\n\nx = torch.randn([20000,4]).to(\"cuda:0\")\ny = torch.randn([20000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#len-20-hidden-nodes-1",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#len-20-hidden-nodes-1",
    "title": "12. 순환신경망 (3)",
    "section": "2000 len + 20 hidden nodes",
    "text": "2000 len + 20 hidden nodes\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#len-20-hidden-nodes-역전파주석처리-1",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#len-20-hidden-nodes-역전파주석처리-1",
    "title": "12. 순환신경망 (3)",
    "section": "2000 len + 20 hidden nodes + 역전파주석처리",
    "text": "2000 len + 20 hidden nodes + 역전파주석처리\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,20) \nlinr = torch.nn.Linear(20,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,20).to(\"cuda:0\")\nlinr = torch.nn.Linear(20,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,20).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#len-5000-hidden-nodes",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#len-5000-hidden-nodes",
    "title": "12. 순환신경망 (3)",
    "section": "2000 len + 5000 hidden nodes",
    "text": "2000 len + 5000 hidden nodes\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,1000) \nlinr = torch.nn.Linear(1000,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,1000).to(\"cuda:0\")\nlinr = torch.nn.Linear(1000,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#len-5000-hidden-nodes-역전파주석처리",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#len-5000-hidden-nodes-역전파주석처리",
    "title": "12. 순환신경망 (3)",
    "section": "2000 len + 5000 hidden nodes + 역전파주석처리",
    "text": "2000 len + 5000 hidden nodes + 역전파주석처리\ncpu\n\nx = torch.randn([2000,4]) \ny = torch.randn([2000,4]) \n\n\nrnn = torch.nn.RNN(4,1000) \nlinr = torch.nn.Linear(1000,4) \noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000)\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1 \n\ngpu\n\nx = torch.randn([2000,4]).to(\"cuda:0\")\ny = torch.randn([2000,4]).to(\"cuda:0\")\n\n\nrnn = torch.nn.RNN(4,1000).to(\"cuda:0\")\nlinr = torch.nn.Linear(1000,4).to(\"cuda:0\")\noptimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()))\nloss_fn = torch.nn.MSELoss() \n\n\nt1 = time.time()\nfor epoc in range(100):\n    ## 1 \n    _water = torch.zeros(1,1000).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water) \n    yhat = linr(hidden) \n    ## 2 \n    loss = loss_fn(yhat,y) \n    ## 3\n    #loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\nt2 = time.time()\nt2 - t1"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#실험결과-요약",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#실험결과-요약",
    "title": "12. 순환신경망 (3)",
    "section": "실험결과 요약",
    "text": "실험결과 요약\n\n\n\nlen\n# of hidden nodes\nbackward\ncpu\ngpu\nratio\n\n\n\n\n20000\n20\nO\n93.02\n3.26\n28.53\n\n\n20000\n20\nX\n18.85\n1.29\n14.61\n\n\n2000\n20\nO\n6.53\n0.75\n8.70\n\n\n2000\n20\nX\n1.25\n0.14\n8.93\n\n\n2000\n1000\nO\n58.99\n4.75\n12.41\n\n\n2000\n1000\nX\n13.16\n2.29\n5.74"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#data-1",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#data-1",
    "title": "12. 순환신경망 (3)",
    "section": "data",
    "text": "data\n\ntxt = list('abcabC')*100\ntxt[:8]\n\n['a', 'b', 'c', 'a', 'b', 'C', 'a', 'b']\n\n\n\ntxt_x = txt[:-1] \ntxt_y = txt[1:]\n\n\nmapping = {'a':0,'b':1,'c':2,'C':3} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\nx = x.to(\"cuda:0\")\ny = y.to(\"cuda:0\") \n\n\nx.shape\n\ntorch.Size([599, 4])"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#rnn",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#rnn",
    "title": "12. 순환신경망 (3)",
    "section": "RNN",
    "text": "RNN\n\ntorch.manual_seed(202150256) \nrnn = torch.nn.RNN(4,3) \nlinr = torch.nn.Linear(3,4) \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(rnn.parameters())+ list(linr.parameters()))\n\n\nrnn.to(\"cuda:0\") \nlinr.to(\"cuda:0\")\n\nLinear(in_features=3, out_features=4, bias=True)\n\n\n- 3000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f47e032f890>\n\n\n\n\n\n- 6000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f47e1078b90>\n\n\n\n\n\n- 9000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f47e0358590>\n\n\n\n\n\n- 12000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f47e2de6f10>\n\n\n\n\n\n- 15000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, hT = rnn(x,_water)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr')\n\n<matplotlib.image.AxesImage at 0x7f47cc12ae50>"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#lstm",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#lstm",
    "title": "12. 순환신경망 (3)",
    "section": "LSTM",
    "text": "LSTM\n- LSTM\n\ntorch.manual_seed(202150256) \nlstm = torch.nn.LSTM(4,3) \nlinr = torch.nn.Linear(3,4) \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters())+ list(linr.parameters()))\n\n\nlstm.to(\"cuda:0\") \nlinr.to(\"cuda:0\")\n\nLinear(in_features=3, out_features=4, bias=True)\n\n\n- 3000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, (hT,cT) = lstm(x,(_water,_water))\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f47cc0608d0>\n\n\n\n\n\n- 6000 epochs\n\nfor epoc in range(3000):\n    ## 1 \n    _water = torch.zeros(1,3).to(\"cuda:0\")\n    hidden, (hT,cT) = lstm(x,(_water,_water))\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat = soft(output)\ncombinded  = torch.concat([hidden,yhat],axis=1).data.to(\"cpu\")\n\n\nplt.matshow(combinded[-6:],cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f47c61dd750>"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#rnn-vs-lstm-성능비교실험",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#rnn-vs-lstm-성능비교실험",
    "title": "12. 순환신경망 (3)",
    "section": "RNN vs LSTM 성능비교실험",
    "text": "RNN vs LSTM 성능비교실험\n- RNN\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(4,3).to(\"cuda:0\")\n        linr = torch.nn.Linear(3,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,3).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$RNN$\",size=20)\nfig.tight_layout()\n\n\n\n\n- LSTM\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        lstm = torch.nn.LSTM(4,3).to(\"cuda:0\")\n        linr = torch.nn.Linear(3,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,3).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, (hT,cT) = lstm(x,(_water,_water))\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$LSTM$\",size=20)\nfig.tight_layout()"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#data-2",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#data-2",
    "title": "12. 순환신경망 (3)",
    "section": "data",
    "text": "data\n\ntxt = list('abcdabcD')*100\ntxt[:8]\n\n['a', 'b', 'c', 'd', 'a', 'b', 'c', 'D']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {'a':0, 'b':1, 'c':2, 'd':3, 'D':4}\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\nx=x.to(\"cuda:0\")\ny=y.to(\"cuda:0\")"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#rnn-vs-lstm-성능비교실험-1",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#rnn-vs-lstm-성능비교실험-1",
    "title": "12. 순환신경망 (3)",
    "section": "RNN vs LSTM 성능비교실험",
    "text": "RNN vs LSTM 성능비교실험\n- RNN\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(5,4).to(\"cuda:0\")\n        linr = torch.nn.Linear(4,5).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,4).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-8:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$RNN$\",size=20)\nfig.tight_layout()\n\n\n\n\n- LSTM\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        lstm = torch.nn.LSTM(5,4).to(\"cuda:0\")\n        linr = torch.nn.Linear(4,5).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,4).to(\"cuda:0\")\n        for epoc in range(3000):\n            ## 1\n            hidden, (hT,cT) = lstm(x,(_water,_water))\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-8:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(r\"$LSTM$\",size=20)\nfig.tight_layout()\n\n\n\n\n- 관찰1: LSTM이 확실히 장기기억에 강하다.\n- 관찰2: LSTM은 hidden에 0이 잘 나온다.\n\n사실 확실히 구분되는 특징을 판별할때는 -1,1 로 히든레이어 값들이 설정되면 명확하다.\n히든레이어에 -1~1사이의 값이 나온다면 애매한 판단이 내려지게 된다.\n그런데 이 애매한 판단이 어떻게 보면 문맥의 뉘앙스를 이해하는데 더 잘 맞다.\n그런데 RNN은 -1,1로 셋팅된 상황에서 -1~1로의 변화가 더디다는 것이 문제임."
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#data-abab",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#data-abab",
    "title": "12. 순환신경망 (3)",
    "section": "data: abaB",
    "text": "data: abaB\n\ntxt = list('abaB')*100\ntxt[:5]\n\n['a', 'b', 'a', 'B', 'a']\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {'a':0, 'b':1, 'B':2}\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#epoch-ver1-with-torch.nn.lstmcell",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#epoch-ver1-with-torch.nn.lstmcell",
    "title": "12. 순환신경망 (3)",
    "section": "1 epoch ver1 (with torch.nn.LSTMCell)",
    "text": "1 epoch ver1 (with torch.nn.LSTMCell)\n\ntorch.manual_seed(43052) \nlstm_cell = torch.nn.LSTMCell(3,2) \nlinr = torch.nn.Linear(2,3)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm_cell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nT = len(x) \nfor epoc in range(1):\n    ht = torch.zeros(1,2)\n    ct = torch.zeros(1,2)\n    loss = 0 \n    ## 1~2\n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        ht,ct = lstm_cell(xt,(ht,ct))\n        ot = linr(ht) \n        loss = loss + loss_fn(ot,yt)\n    loss = loss / T\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nht,ct \n\n(tensor([[-0.0406,  0.2505]], grad_fn=<MulBackward0>),\n tensor([[-0.0975,  0.7134]], grad_fn=<AddBackward0>))"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#epoch-ver2-완전-손으로-구현",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#epoch-ver2-완전-손으로-구현",
    "title": "12. 순환신경망 (3)",
    "section": "1 epoch ver2 (완전 손으로 구현)",
    "text": "1 epoch ver2 (완전 손으로 구현)\n\nt=0 \\(\\to\\) t=1\n- lstm_cell 을 이용한 계산 (결과비교용)\n\ntorch.manual_seed(202150256) \nlstm_cell = torch.nn.LSTMCell(3,2) \nlinr = torch.nn.Linear(2,3)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm_cell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nT = len(x) \nfor epoc in range(1):\n    ht = torch.zeros(1,2)\n    ct = torch.zeros(1,2)\n    loss = 0 \n    ## 1~2\n    for t in range(1):\n        xt,yt = x[[t]], y[[t]]\n        ht,ct = lstm_cell(xt,(ht,ct))\n    #     ot = linr(ht) \n    #     loss = loss + loss_fn(ot,yt)\n    # loss = loss / T\n    # ## 3 \n    # loss.backward()\n    # ## 4 \n    # optimizr.step()\n    # optimizr.zero_grad()\n\n\nht,ct \n\n(tensor([[-0.0541,  0.0892]], grad_fn=<MulBackward0>),\n tensor([[-0.1347,  0.2339]], grad_fn=<AddBackward0>))\n\n\n\n이런결과를 어떻게 만드는걸까?\nhttps://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n\n- 직접계산\n\nht = torch.zeros(1,2)\nct = torch.zeros(1,2)\n\n\n_ifgo = xt @ lstm_cell.weight_ih.T + ht @ lstm_cell.weight_hh.T + lstm_cell.bias_ih + lstm_cell.bias_hh\n\n\ninput_gate = sig(_ifgo[:,0:2])\nforget_gate = sig(_ifgo[:,2:4])\ngt = tanh(_ifgo[:,4:6])\noutput_gate = sig(_ifgo[:,6:8])\n\n\nct = forget_gate * ct + input_gate * gt\nht = output_gate * tanh(ct)\n\n\nht,ct\n\n(tensor([[-0.0541,  0.0892]], grad_fn=<MulBackward0>),\n tensor([[-0.1347,  0.2339]], grad_fn=<AddBackward0>))\n\n\n\n\nt=0 \\(\\to\\) t=T\n\ntorch.manual_seed(202150256) \nlstm_cell = torch.nn.LSTMCell(3,2) \nlinr = torch.nn.Linear(2,3)\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm_cell.parameters())+list(linr.parameters()),lr=0.1)\n\n\nT = len(x) \nfor epoc in range(1):\n    ht = torch.zeros(1,2)\n    ct = torch.zeros(1,2)\n    loss = 0 \n    ## 1~2\n    for t in range(T):\n        xt,yt = x[[t]], y[[t]]\n        \n        ## lstm_cell step1: calculate _ifgo \n        _ifgo = xt @ lstm_cell.weight_ih.T + ht @ lstm_cell.weight_hh.T + lstm_cell.bias_ih + lstm_cell.bias_hh\n        ## lstm_cell step2: decompose _ifgo \n        input_gate = sig(_ifgo[:,0:2])\n        forget_gate = sig(_ifgo[:,2:4])\n        gt = tanh(_ifgo[:,4:6])\n        output_gate = sig(_ifgo[:,6:8])\n        ## lstm_cell step3: calculate ht,ct \n        ct = forget_gate * ct + input_gate * gt\n        ht = output_gate * tanh(ct)\n        \n    #     ot = linr(ht) \n    #     loss = loss + loss_fn(ot,yt)\n    # loss = loss / T\n    # ## 3 \n    # loss.backward()\n    # ## 4 \n    # optimizr.step()\n    # optimizr.zero_grad()\n\n\nht,ct\n\n(tensor([[-0.0406,  0.2505]], grad_fn=<MulBackward0>),\n tensor([[-0.0975,  0.7134]], grad_fn=<AddBackward0>))"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#epoch-ver3-with-torch.nn.lstm",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#epoch-ver3-with-torch.nn.lstm",
    "title": "12. 순환신경망 (3)",
    "section": "1 epoch ver3 (with torch.nn.LSTM)",
    "text": "1 epoch ver3 (with torch.nn.LSTM)\n\ntorch.manual_seed(202150256) \nlstm_cell = torch.nn.LSTMCell(3,2)\nlinr = torch.nn.Linear(2,3) \n\n\nlstm = torch.nn.LSTM(3,2) \n\n\nlstm.weight_hh_l0.data = lstm_cell.weight_hh.data \nlstm.bias_hh_l0.data = lstm_cell.bias_hh.data \nlstm.weight_ih_l0.data = lstm_cell.weight_ih.data \nlstm.bias_ih_l0.data = lstm_cell.bias_ih.data \n\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters()) + list(linr.parameters()), lr=0.1) \n\n\n_water = torch.zeros(1,2) \nfor epoc in range(1): \n    ## step1 \n    hidden, (ht,ct) = lstm(x,(_water,_water))\n    output = linr(hidden)\n    # ## step2\n    # loss = loss_fn(output,y) \n    # ## step3\n    # loss.backward()\n    # ## step4 \n    # optimizr.step()\n    # optimizr.zero_grad() \n\n\nht,ct\n\n(tensor([[-0.0406,  0.2505]], grad_fn=<SqueezeBackward1>),\n tensor([[-0.0975,  0.7134]], grad_fn=<SqueezeBackward1>))"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#data-abab-1",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#data-abab-1",
    "title": "12. 순환신경망 (3)",
    "section": "data: abaB",
    "text": "data: abaB\n\ntxt = list('abaB')*100\ntxt[:5]\n\n['a', 'b', 'a', 'B', 'a']\n\n\n\nn_words = 3\n\n\nmapping = {'a':0, 'b':1, 'B':2}\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[:10],txt_y[:10]\n\n(['a', 'b', 'a', 'B', 'a', 'b', 'a', 'B', 'a', 'b'],\n ['b', 'a', 'B', 'a', 'b', 'a', 'B', 'a', 'b', 'a'])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\nx,y\n\n(tensor([[1., 0., 0.],\n         [0., 1., 0.],\n         [1., 0., 0.],\n         ...,\n         [1., 0., 0.],\n         [0., 1., 0.],\n         [1., 0., 0.]]),\n tensor([[0., 1., 0.],\n         [1., 0., 0.],\n         [0., 0., 1.],\n         ...,\n         [0., 1., 0.],\n         [1., 0., 0.],\n         [0., 0., 1.]]))"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#epoch",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#epoch",
    "title": "12. 순환신경망 (3)",
    "section": "1000 epoch",
    "text": "1000 epoch\n\ntorch.manual_seed(202150256) \nlstm = torch.nn.LSTM(3,2) \nlinr = torch.nn.Linear(2,3) \n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+ list(linr.parameters()),lr=0.1)\n\n\n_water = torch.zeros(1,2) \nfor epoc in range(1000): \n    ## step1 \n    hidden, (ht,ct) = lstm(x,(_water,_water))\n    output = linr(hidden)\n    ## step2\n    loss = loss_fn(output,y) \n    ## step3\n    loss.backward()\n    ## step4 \n    optimizr.step()\n    optimizr.zero_grad()"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#시각화",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#시각화",
    "title": "12. 순환신경망 (3)",
    "section": "시각화",
    "text": "시각화\n\nT = len(x)\ninput_gate = torch.zeros(T,2)\nforget_gate = torch.zeros(T,2)\noutput_gate = torch.zeros(T,2)\ng = torch.zeros(T,2)\ncell = torch.zeros(T,2)\nh = torch.zeros(T,2) \n\n\nfor t in range(T): \n    ## 1: calculate _ifgo \n    _ifgo = x[[t]] @ lstm.weight_ih_l0.T + h[[t]] @ lstm.weight_hh_l0.T + lstm.bias_ih_l0 + lstm.bias_hh_l0 \n    ## 2: decompose _ifgo \n    input_gate[[t]] = sig(_ifgo[:,0:2])\n    forget_gate[[t]] = sig(_ifgo[:,2:4])\n    g[[t]] = tanh(_ifgo[:,4:6])\n    output_gate[[t]] = sig(_ifgo[:,6:8])\n    ## 3: calculate ht,ct \n    cell[[t]] = forget_gate[[t]] * cell[[t]] + input_gate[[t]] * g[[t]]\n    h[[t]] = output_gate[[t]] * tanh(cell[[t]])\n\n\ncombinded1 = torch.concat([input_gate,forget_gate,output_gate],axis=1)\ncombinded2 = torch.concat([g,cell,h,soft(output)],axis=1)\n\n\nplt.matshow(combinded1[-8:].data,cmap='bwr',vmin=-1,vmax=1);\nplt.xticks(range(combinded1.shape[-1]),labels=['i']*2 + ['f']*2 + ['o']*2);\nplt.matshow(combinded2[-8:].data,cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(combinded2.shape[-1]),labels=['g']*2 + ['c']*2 + ['h']*2 + ['yhat']*3);\n\n\n\n\n\n\n\n\n상단그림은 게이트의 값들만 시각화, 하단그림은 게이트 이외의 값들을 시각화"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#시각화의-해석i",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#시각화의-해석i",
    "title": "12. 순환신경망 (3)",
    "section": "시각화의 해석I",
    "text": "시각화의 해석I\n\nplt.matshow(combinded1[-8:].data,cmap='bwr',vmin=-1,vmax=1);\nplt.xticks(range(combinded1.shape[-1]),labels=['i']*2 + ['f']*2 + ['o']*2);\n\n\n\n\n- input_gate, forget_gate, output_gate는 모두 0~1 사이의 값을 가진다.\n- 이 값들은 각각 모두 \\({\\boldsymbol g}_t, {\\boldsymbol c}_{t-1}, \\tanh({\\boldsymbol c}_t)\\)에 곱해진다. 따라서 input_gate, forget_gate, output_gate 는 gate의 역할로 비유가능하다. (1이면 통과, 0이면 차단)\n\ninput_gate: \\({\\boldsymbol g}_t\\)의 값을 얼만큼 통과시킬지 0~1사이의 숫자로 결정\nforget_gate: \\({\\boldsymbol c}_{t-1}\\)의 값을 얼만큼 통과시킬지 0~1사이의 숫자로 결정\noutput_gate: \\(\\tanh({\\boldsymbol c}_t)\\)의 값을 얼만큼 통과시킬지 0~1사이의 숫자로 결정"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#시각화의-해석ii",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#시각화의-해석ii",
    "title": "12. 순환신경망 (3)",
    "section": "시각화의 해석II",
    "text": "시각화의 해석II\n\nplt.matshow(combinded2[-8:].data,cmap='bwr',vmin=-1,vmax=1)\nplt.xticks(range(combinded2.shape[-1]),labels=['g']*2 + ['c']*2 + ['h']*2 + ['yhat']*3);\n\n\n\n\n- 결국 \\({\\boldsymbol g}_t\\to {\\boldsymbol c}_t \\to {\\boldsymbol h}_t \\to \\hat{\\boldsymbol y}\\) 의 느낌이다. (\\({\\boldsymbol h}_t\\)를 계산하기 위해서는 \\({\\boldsymbol c}_t\\)가 필요했고 \\({\\boldsymbol c}_t\\)를 계산하기 위해서는 \\({\\boldsymbol c}_{t-1}\\)과 \\({\\boldsymbol g}_t\\)가 필요했음)\n\n\\({\\boldsymbol h}_t= \\tanh({\\boldsymbol c}_t) \\odot {\\boldsymbol o}_t\\)\n\\({\\boldsymbol c}_t ={\\boldsymbol c}_{t-1} \\odot {\\boldsymbol f}_t + {\\boldsymbol g}_{t} \\odot {\\boldsymbol i}_t\\)\n\n- \\({\\boldsymbol g}_t,{\\boldsymbol c}_t,{\\boldsymbol h}_t\\) 모두 \\({\\boldsymbol x}_t\\)의 정보를 숙성시켜 가지고 있는 느낌이 든다.\n- \\({\\boldsymbol g}_t\\) 특징: 보통 -1,1 중 하나의 값을 가지도록 학습되어 있다. (마치 RNN의 hidden node처럼!)\n\n\\(\\boldsymbol{g}_t = \\tanh({\\boldsymbol x}_t {\\bf W}_{ig} + {\\boldsymbol h}_{t-1} {\\bf W}_{hg}+ {\\boldsymbol b}_{ig}+{\\boldsymbol b}_{hg})\\)\n\n- \\({\\boldsymbol c}_t\\) 특징: \\({\\boldsymbol g}_t\\)와 매우 비슷하지만 약간 다른값을 가진다. 그래서 \\({\\boldsymbol g}_t\\)와는 달리 -1,1 이외의 값도 종종 등장.\n\nprint(\"first row: gt={}, ct={}\".format(g[-8].data, cell[-8].data))\nprint(\"second row: gt={}, ct={}\".format(g[-7].data, cell[-7].data))\n#g[-7], cell[-7]\n\nfirst row: gt=tensor([ 0.9999, -0.9999]), ct=tensor([ 0.9647, -0.9984])\nsecond row: gt=tensor([ 0.9970, -0.9554]), ct=tensor([ 0.3592, -0.9373])\n\n\n- \\({\\boldsymbol h}_t\\) 특징: (1) \\({\\boldsymbol c}_t\\)의 느낌이 있음 하지만 약간의 변형이 있음. (2) -1~1 사이에의 값을 훨씬 다양하게 가진다. (tanh때문)\n\nprint(\"first row: gt={}, ct={}, ht={}\".format(g[-8].data, cell[-8].data,h[-8].data))\nprint(\"second row: gt={}, ct={}, ht={}\".format(g[-7].data, cell[-7].data,h[-7].data))\n#g[-7], cell[-7]\n\nfirst row: gt=tensor([ 0.9999, -0.9999]), ct=tensor([ 0.9647, -0.9984]), ht=tensor([ 0.7370, -0.3323])\nsecond row: gt=tensor([ 0.9970, -0.9554]), ct=tensor([ 0.3592, -0.9373]), ht=tensor([ 0.0604, -0.6951])\n\n\n- 예전의문 해결\n\n실험적으로 살펴보니 LSTM이 RNN보다 장기기억에 유리했음.\n그 이유: RRN은 \\({\\boldsymbol h}_t\\)의 값이 -1 혹은 1로 결정되는 경우가 많았음. 그러나 경우에 따라서는 \\({\\boldsymbol h}_t\\)이 -1~1의 값을 가지는 것이 문맥적 뉘앙스를 포착하기에는 유리한데 LSTM이 이러한 방식으로 학습되는 경우가 많았음.\n왜 LSTM의 \\({\\boldsymbol h}_t\\)은 -1,1 이외의 값을 쉽게 가질 수 있는가? (1) gate들의 역할 (2) 마지막에 취해지는 tanh 때문"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#lstm의-알고리즘-리뷰-i-수식위주",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#lstm의-알고리즘-리뷰-i-수식위주",
    "title": "12. 순환신경망 (3)",
    "section": "LSTM의 알고리즘 리뷰 I (수식위주)",
    "text": "LSTM의 알고리즘 리뷰 I (수식위주)\n(step1) calculate \\({\\tt ifgo}\\)\n\\({\\tt ifgo} = {\\boldsymbol x}_t \\big[{\\bf W}_{ii} | {\\bf W}_{if}| {\\bf W}_{ig} |{\\bf W}_{io}\\big] + {\\boldsymbol h}_{t-1} \\big[ {\\bf W}_{hi}|{\\bf W}_{hf} |{\\bf W}_{hg} | {\\bf W}_{ho} \\big] + bias\\)\n\\(=\\big[{\\boldsymbol x}_t{\\bf W}_{ii} + {\\boldsymbol h}_{t-1}{\\bf W}_{hi} ~\\big|~ {\\boldsymbol x}_t{\\bf W}_{if}+ {\\boldsymbol h}_{t-1}{\\bf W}_{hf}~ \\big|~ {\\boldsymbol x}_t{\\bf W}_{ig} + {\\boldsymbol h}_{t-1}{\\bf W}_{hg} ~\\big|~ {\\boldsymbol x}_t{\\bf W}_{io} + {\\boldsymbol h}_{t-1}{\\bf W}_{ho} \\big] + bias\\)\n참고: 위의 수식은 아래코드에 해당하는 부분\nifgo = xt @ lstm_cell.weight_ih.T +\\\n       ht @ lstm_cell.weight_hh.T +\\\n       lstm_cell.bias_ih + lstm_cell.bias_hh\n(step2) decompose \\({\\tt ifgo}\\) and get \\({\\boldsymbol i}_t\\), \\({\\boldsymbol f}_t\\), \\({\\boldsymbol g}_t\\), \\({\\boldsymbol o}_t\\)\n\\({\\boldsymbol i}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{ii} + {\\boldsymbol h}_{t-1} {\\bf W}_{hi} +bias )\\)\n\\({\\boldsymbol f}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{if} + {\\boldsymbol h}_{t-1} {\\bf W}_{hf} +bias )\\)\n\\({\\boldsymbol g}_t = \\tanh({\\boldsymbol x}_t {\\bf W}_{ig} + {\\boldsymbol h}_{t-1} {\\bf W}_{hg} +bias )\\)\n\\({\\boldsymbol o}_t = \\sigma({\\boldsymbol x}_t {\\bf W}_{io} + {\\boldsymbol h}_{t-1} {\\bf W}_{ho} +bias )\\)\n(step3) calculate \\({\\boldsymbol c}_t\\) and \\({\\boldsymbol h}_t\\)\n\\({\\boldsymbol c}_t = {\\boldsymbol i}_t \\odot {\\boldsymbol g}_t+ {\\boldsymbol f}_t \\odot {\\boldsymbol c}_{t-1}\\)\n\\({\\boldsymbol h}_t = \\tanh({\\boldsymbol o}_t \\odot {\\boldsymbol c}_t)\\)"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#lstm의-알고리즘-리뷰-ii-느낌위주",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#lstm의-알고리즘-리뷰-ii-느낌위주",
    "title": "12. 순환신경망 (3)",
    "section": "LSTM의 알고리즘 리뷰 II (느낌위주)",
    "text": "LSTM의 알고리즘 리뷰 II (느낌위주)\n\n이해 및 암기를 돕기위해서 비유적으로 설명한 챕터입니다..\n\n- 느낌1: RNN이 콩물에서 간장을 한번에 숙성시키는 방법이라면 LSTM은 콩물에서 간장을 3차로 나누어 숙성하는 느낌이다.\n\n콩물: \\({\\boldsymbol x}_t\\)\n1차숙성: \\({\\boldsymbol g}_t\\)\n2차숙성: \\({\\boldsymbol c}_t\\)\n3차숙성: \\({\\boldsymbol h}_t\\)\n\n- 느낌2: \\({\\boldsymbol g}_t\\)에 대하여\n\n계산방법: \\({\\boldsymbol x}_t\\)와 \\({\\boldsymbol h}_{t-1}\\)를 \\({\\bf W}_{ig}, {\\bf W}_{hg}\\)를 이용해 선형결합하고 \\(\\tanh\\)를 취한 결과\nRNN에서 간장을 만들던 그 수식에서 \\(h_t\\)를 \\(g_t\\)로 바꾼것\n크게 2가지의 의미를 가진다 (1) 과거와 현재의 결합 (2) 활성화함수 \\(\\tanh\\)를 적용\n\n- 느낌3: \\({\\boldsymbol c}_t\\)에 대하여 (1)\n\n계산방법: \\({\\boldsymbol g}_{t}\\)와 \\({\\boldsymbol c}_{t-1}\\)를 요소별로 선택하고 더하는 과정\n\\(g_t\\)는 (1) 과거와 현재의 결합 (2) 활성화함수 tanh를 적용으로 나누어지는데 이중에서 (1) 과거와 현재의 정보를 결합하는 과정만 해당한다. 차이점은 요소별 선택 후 덧셈\n이러한 결합을 쓰는 이유? 게이트를 이용하여 과거와 현재의 정보를 제어 (일반적인 설명, 솔직히 내가 좋아하는 설명은 아님)\n\n- 느낌4: \\({\\boldsymbol c}_t\\)에 대하여 (2) // \\({\\boldsymbol c}_t\\)는 왜 과거와 현재의 정보를 제어한다고 볼 수 있는가?\n\\(t=1\\) 시점 계산과정관찰\n\ninput_gate[1],g[1],forget_gate[1],cell[0]\n\n(tensor([0.9065, 0.9999], grad_fn=<SelectBackward0>),\n tensor([0.9931, 0.9999], grad_fn=<SelectBackward0>),\n tensor([0.9931, 0.0014], grad_fn=<SelectBackward0>),\n tensor([ 0.3592, -0.9373], grad_fn=<SelectBackward0>))\n\n\n\\([0.9,1.0] \\odot {\\boldsymbol g}_t + [1.0,0.0] \\odot {\\boldsymbol c}_{t-1}\\)\n\nforget_gate는 \\(c_{t-1}\\)의 첫번째 원소는 기억하고, 두번째 원소는 잊으라고 말하고 있음 // forget_gate는 과거(\\(c_{t-1}\\))의 정보를 얼마나 잊을지 (= 얼마나 기억할지) 를 결정한다고 해석할 수 있다.\ninput_gate는 \\(g_{t}\\)의 첫번째 원소와 두번째 원소를 모두 기억하되 두번째 원소를 좀 더 중요하게 기억하라고 말하고 있음 // input_gate는 현재(\\(g_{t}\\))의 정보를 얼만큼 강하게 반영할지 결정한다.\n이 둘을 조합하면 \\({\\boldsymbol c}_t\\)가 현재와 과거의 정보중 어떠한 정보를 더 중시하면서 기억할지 결정한다고 볼 수 있다.\n\n\n이 설명은 제가 좀 싫어해요, 싫어하는 이유는 (1) “기억의 정도를 조절한다”와 “망각의 정도를 조절한다”는 사실 같은말임. 그래서 forget_gate의 용어가 모호함. (2) 기억과 망각을 조정하는 방식으로 꼭 gate의 개념을 사용해야 하는건 아님\n\n- 느낌5: \\({\\boldsymbol c}_t\\)에 대하여 (3)\n\n사실상 LSTM 알고리즘의 꽃이라 할 수 있음.\nLSTM은 long short term memory의 약자임. 기존의 RNN은 장기기억을 활용함에 약점이 있는데 LSTM은 단기기억/장기기억 모두 잘 활용함.\nLSTM이 장기기억을 잘 활용하는 비법은 바로 \\({\\boldsymbol c}_t\\)에 있다.\n\n- 느낌6: \\({\\boldsymbol h}_t\\)에 대하여 - 계산방법: \\(\\tanh({\\boldsymbol c}_t)\\)를 요소별로 선택\n- RNN, LSTM의 변수들 비교 테이블\n\n\n\n\n\n\n\n\n\n\n\n\n\n과거정보\n현재정보\n과거와 현재의 결합방식\n활성화\n느낌\n비고\n\n\n\n\nRNN-\\({\\boldsymbol h}_t\\)\n\\({\\boldsymbol h}_{t-1}\\)\n\\({\\boldsymbol x}_t\\)\n\\(\\times\\) \\(\\to\\) \\(+\\)\n\\(\\tanh\\)\n간장\n\n\n\n\n\n\n\n\n\n\n\n\nLSTM-\\({\\boldsymbol g}_t\\)\n\\({\\boldsymbol h}_{t-1}\\)\n\\({\\boldsymbol x}_t\\)\n\\(\\times\\) \\(\\to\\) \\(+\\)\n\\(\\tanh\\)\n1차간장\n\n\n\nLSTM-\\({\\boldsymbol c}_t\\)\n\\({\\boldsymbol c}_{t-1}\\)\n\\({\\boldsymbol g}_t\\)\n\\(\\odot\\) \\(\\to\\) \\(+\\)\nNone\n2차간장\ngate를 열림정도를 판단할때 \\({\\boldsymbol x}_t\\)와 \\({\\boldsymbol h}_{t-1}\\)을 이용\n\n\nLSTM-\\({\\boldsymbol h}_t\\)\nNone\n\\({\\boldsymbol c}_t\\)\nNone\n\\(\\tanh\\), \\(\\odot\\)\n3차간장\ngate를 열림정도를 판단할때 \\({\\boldsymbol x}_t\\)와 \\({\\boldsymbol h}_{t-1}\\)을 이용\n\n\n\n\nRNN은 기억할 과거정보가 \\({\\boldsymbol h}_{t-1}\\) 하나이지만 LSTM은 \\({\\boldsymbol c}_{t-1}\\), \\({\\boldsymbol h}_{t-1}\\) 2개이다.\n\n- 알고리즘리뷰 :\n\n콩물,과거3차간장 \\(\\overset{\\times,+,\\tanh}{\\longrightarrow}\\) 현재1차간장\n현재1차간장, 과거2차간장 \\(\\overset{\\odot,+,\\tanh}{\\longrightarrow}\\) 현재2차간장\n현재2차간장 \\(\\overset{\\tanh,\\odot}{\\longrightarrow}\\) 현재3차간장"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-05-11wk.html#lstm이-강한이유",
    "href": "post/Lecture/STML/2023-04-05-11wk.html#lstm이-강한이유",
    "title": "12. 순환신경망 (3)",
    "section": "LSTM이 강한이유",
    "text": "LSTM이 강한이유\n- LSTM이 장기기억에 유리함. 그 이유는 input, forget, output gate 들이 과거기억을 위한 역할을 하기 때문.\n\n비판: 아키텍처에 대한 이론적 근거는 없음. 장기기억을 위하여 꼭 LSTM같은 구조일 필요는 없음. (왜 3차간장을 만들때 tanh를 써야하는지? 게이트는 꼭3개이어야 하는지?)\n\n- 저는 사실 아까 살펴본 아래의 이유로 이해하고 있습니다.\n\n실험적으로 살펴보니 LSTM이 RNN보다 장기기억에 유리했음.\n그 이유: RRN은 \\({\\boldsymbol h}_t\\)의 값이 -1 혹은 1로 결정되는 경우가 많았음. 그러나 경우에 따라서는 \\({\\boldsymbol h}_t\\)이 -1~1의 값을 가지는 것이 문맥적 뉘앙스를 포착하기에는 유리한데 LSTM이 이러한 방식으로 학습되는 경우가 많았음.\n왜 LSTM의 \\({\\boldsymbol h}_t\\)은 -1,1 이외의 값을 쉽게 가질 수 있는가? (1) gate들의 역할 (2) 마지막에 취해지는 tanh 때문"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html",
    "href": "post/Lecture/STML/2023-04-12-12wk.html",
    "title": "13. 순환신경망 (4)",
    "section": "",
    "text": "import torch\nimport pandas as pd\nimport matplotlib.pyplot as plt \nfrom fastai.text.all import *\nimport pytorch_lightning as pl"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#data-abcabc",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#data-abcabc",
    "title": "13. 순환신경망 (4)",
    "section": "data: abcabC",
    "text": "data: abcabC\n\ntxt = list('abcabC')*100\ntxt[:8]\ntxt_x = txt[:-1] \ntxt_y = txt[1:]\n\n\nmapping = {'a':0,'b':1,'c':2,'C':3} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float()\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float()\n\n\nx = x.to(\"cuda:0\")\ny = y.to(\"cuda:0\") \n\n\nx.shape\n\ntorch.Size([599, 4])"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#실험",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#실험",
    "title": "13. 순환신경망 (4)",
    "section": "실험",
    "text": "실험\n- 실험1\n\nHIDDEN = 3\n\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(4,HIDDEN).to(\"cuda:0\")\n        linr = torch.nn.Linear(HIDDEN,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,HIDDEN).to(\"cuda:0\")\n        for epoc in range(500):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(\"experiment1: RNN with {} hidden nodes\".format(HIDDEN),size=20)\nfig.tight_layout()\n\n\n\n\n- 실험2\n\nHIDDEN = 4\n\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(4,HIDDEN).to(\"cuda:0\")\n        linr = torch.nn.Linear(HIDDEN,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,HIDDEN).to(\"cuda:0\")\n        for epoc in range(500):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(\"experiment2: RNN with {} hidden nodes\".format(HIDDEN),size=20)\nfig.tight_layout()\n\n\n\n\n- 실험3\n\nHIDDEN = 8\n\n\nfig, ax = plt.subplots(5,5,figsize=(10,8))\nfor i in range(5):\n    for j in range(5):\n        rnn = torch.nn.RNN(4,HIDDEN).to(\"cuda:0\")\n        linr = torch.nn.Linear(HIDDEN,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,HIDDEN).to(\"cuda:0\")\n        for epoc in range(500):\n            ## 1\n            hidden, hT = rnn(x,_water)\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combind = torch.concat([hidden,yhat],axis=1)\n        ax[i][j].matshow(combind.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(\"experiment3: RNN with {} hidden nodes\".format(HIDDEN),size=20)\nfig.tight_layout()"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#결론",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#결론",
    "title": "13. 순환신경망 (4)",
    "section": "결론",
    "text": "결론\n- 노드수가 많으면 학습에 유리함"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#data-abcc",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#data-abcc",
    "title": "13. 순환신경망 (4)",
    "section": "data: ab(c,C)",
    "text": "data: ab(c,C)\n\n# torch.manual_seed(43052)\n# txta = 'a'*50\n# txtb = 'b'*50\n# prob_upper = torch.bernoulli(torch.zeros(50)+0.5) \n# txtc = list(map(lambda x: 'c' if x==1 else 'C', prob_upper))\n# txt = ''.join([txta[i]+','+txtb[i]+','+txtc[i]+',' for i in range(50)]).split(',')[:-1]\n# txt_x = txt[:-1] \n# txt_y = txt[1:]\n# pd.DataFrame({'txt_x':txt_x,'txt_y':txt_y}).to_csv(\"2022-11-25-ab(c,C).csv\",index=False)\n\n\ndf= pd.read_csv(\"https://raw.githubusercontent.com/guebin/DL2022/main/posts/IV.%20RNN/2022-11-25-ab(c%2CC).csv\")\ndf\n\n\n\n\n\n  \n    \n      \n      txt_x\n      txt_y\n    \n  \n  \n    \n      0\n      a\n      b\n    \n    \n      1\n      b\n      c\n    \n    \n      2\n      c\n      a\n    \n    \n      3\n      a\n      b\n    \n    \n      4\n      b\n      c\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      144\n      a\n      b\n    \n    \n      145\n      b\n      C\n    \n    \n      146\n      C\n      a\n    \n    \n      147\n      a\n      b\n    \n    \n      148\n      b\n      c\n    \n  \n\n149 rows × 2 columns\n\n\n\n\nmapping = {'a':0,'b':1,'c':2,'C':3} \nx= torch.nn.functional.one_hot(torch.tensor(f(df.txt_x,mapping))).float()\ny= torch.nn.functional.one_hot(torch.tensor(f(df.txt_y,mapping))).float()\n\n\nx = x.to(\"cuda:0\")\ny = y.to(\"cuda:0\")"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#실험-1",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#실험-1",
    "title": "13. 순환신경망 (4)",
    "section": "실험",
    "text": "실험\n- 실험1\n\nHIDDEN = 3\n\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        lstm = torch.nn.LSTM(4,HIDDEN).to(\"cuda:0\")\n        linr = torch.nn.Linear(HIDDEN,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,HIDDEN).to(\"cuda:0\")\n        for epoc in range(500):\n            ## 1\n            hidden, (hT,cT) = lstm(x,(_water,_water))\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combinded = torch.concat([yhat,y],axis=1)\n        ax[i][j].matshow(combinded.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(\"experiment1: LSTM with {} hidden nodes\".format(HIDDEN),size=20)\nfig.tight_layout()\n\n\n\n\n- 실험2\n\nHIDDEN = 16\n\n\nfig, ax = plt.subplots(5,5,figsize=(10,10))\nfor i in range(5):\n    for j in range(5):\n        lstm = torch.nn.LSTM(4,HIDDEN).to(\"cuda:0\")\n        linr = torch.nn.Linear(HIDDEN,4).to(\"cuda:0\")\n        loss_fn = torch.nn.CrossEntropyLoss()\n        optimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n        _water = torch.zeros(1,HIDDEN).to(\"cuda:0\")\n        for epoc in range(500):\n            ## 1\n            hidden, (hT,cT) = lstm(x,(_water,_water))\n            output = linr(hidden)\n            ## 2\n            loss = loss_fn(output,y)\n            ## 3\n            loss.backward()\n            ## 4 \n            optimizr.step()\n            optimizr.zero_grad()\n        yhat=soft(output)    \n        combinded = torch.concat([yhat,y],axis=1)\n        ax[i][j].matshow(combinded.to(\"cpu\").data[-6:],cmap='bwr',vmin=-1,vmax=1)\nfig.suptitle(\"experiment2: LSTM with {} hidden nodes\".format(HIDDEN),size=20)\nfig.tight_layout()"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#결론-1",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#결론-1",
    "title": "13. 순환신경망 (4)",
    "section": "결론",
    "text": "결론\n- 노드수가 너무 많으면 오버피팅 경향도 있음"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#data-human-numbers-5",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#data-human-numbers-5",
    "title": "13. 순환신경망 (4)",
    "section": "data: human numbers 5",
    "text": "data: human numbers 5\n\ntxt = (['one',',','two',',','three',',','four',',','five',',']*100)[:-1]\n\n\nmapping = {',':0, 'one':1, 'two':2, 'three':3, 'four':4, 'five':5} \nmapping\n\n{',': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5}\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:] \n\n\ntxt_x[0:5], txt_y[0:5]\n\n(['one', ',', 'two', ',', 'three'], [',', 'two', ',', 'three', ','])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#torch를-이용한-learn",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#torch를-이용한-learn",
    "title": "13. 순환신경망 (4)",
    "section": "torch를 이용한 learn",
    "text": "torch를 이용한 learn\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(6,20).to(\"cuda:0\") \nlinr = torch.nn.Linear(20,6).to(\"cuda:0\") \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\n_water = torch.zeros(1,20).to(\"cuda:0\")\nfor epoc in range(50):\n    ## 1 \n    hidden, (hT,cT) =lstm(x,(_water,_water))\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()     \n\n\nplt.matshow(soft(output).data[-10:].to(\"cpu\"),cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6bbf69b890>"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#fastai-이용한-learn",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#fastai-이용한-learn",
    "title": "13. 순환신경망 (4)",
    "section": "fastai 이용한 learn",
    "text": "fastai 이용한 learn\n\nds1 = torch.utils.data.TensorDataset(x,y)\nds2 = torch.utils.data.TensorDataset(x,y) # dummy \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=998)\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=998) # dummy \ndls = DataLoaders(dl1,dl2) \n\n\nclass MyLSTM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = torch.nn.LSTM(6,20)\n        self.linr = torch.nn.Linear(20,6) \n    def forward(self,x):\n        _water = torch.zeros(1,20).to(\"cuda:0\")\n        hidden, (hT,cT) =self.lstm(x,(_water,_water))\n        output = self.linr(hidden)\n        return output         \n\n\nnet = MyLSTM().to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\n\n\nlrnr = Learner(dls,net,loss_fn,lr=0.1)\n\n\nlrnr.fit(50)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.918821\n      1.547683\n      00:00\n    \n    \n      1\n      1.731377\n      1.771274\n      00:00\n    \n    \n      2\n      1.744945\n      1.490624\n      00:00\n    \n    \n      3\n      1.679425\n      1.400951\n      00:00\n    \n    \n      4\n      1.621457\n      1.431488\n      00:00\n    \n    \n      5\n      1.588175\n      1.398044\n      00:00\n    \n    \n      6\n      1.559340\n      1.291965\n      00:00\n    \n    \n      7\n      1.523507\n      1.127941\n      00:00\n    \n    \n      8\n      1.475921\n      0.959611\n      00:00\n    \n    \n      9\n      1.419471\n      0.861778\n      00:00\n    \n    \n      10\n      1.363497\n      0.815888\n      00:00\n    \n    \n      11\n      1.312624\n      0.780459\n      00:00\n    \n    \n      12\n      1.266544\n      0.742232\n      00:00\n    \n    \n      13\n      1.223979\n      0.715809\n      00:00\n    \n    \n      14\n      1.185103\n      0.671282\n      00:00\n    \n    \n      15\n      1.147897\n      0.620188\n      00:00\n    \n    \n      16\n      1.111588\n      0.575581\n      00:00\n    \n    \n      17\n      1.076424\n      0.529901\n      00:00\n    \n    \n      18\n      1.042135\n      0.475089\n      00:00\n    \n    \n      19\n      1.008015\n      0.418487\n      00:00\n    \n    \n      20\n      0.973913\n      0.368120\n      00:00\n    \n    \n      21\n      0.940148\n      0.322788\n      00:00\n    \n    \n      22\n      0.906926\n      0.285818\n      00:00\n    \n    \n      23\n      0.874595\n      0.254371\n      00:00\n    \n    \n      24\n      0.843313\n      0.218208\n      00:00\n    \n    \n      25\n      0.812716\n      0.187723\n      00:00\n    \n    \n      26\n      0.782985\n      0.158780\n      00:00\n    \n    \n      27\n      0.754088\n      0.133884\n      00:00\n    \n    \n      28\n      0.726112\n      0.112403\n      00:00\n    \n    \n      29\n      0.699107\n      0.093460\n      00:00\n    \n    \n      30\n      0.673082\n      0.075678\n      00:00\n    \n    \n      31\n      0.647987\n      0.059713\n      00:00\n    \n    \n      32\n      0.623807\n      0.047068\n      00:00\n    \n    \n      33\n      0.600592\n      0.037162\n      00:00\n    \n    \n      34\n      0.578363\n      0.029585\n      00:00\n    \n    \n      35\n      0.557125\n      0.023816\n      00:00\n    \n    \n      36\n      0.536864\n      0.019337\n      00:00\n    \n    \n      37\n      0.517551\n      0.015811\n      00:00\n    \n    \n      38\n      0.499145\n      0.013043\n      00:00\n    \n    \n      39\n      0.481606\n      0.010892\n      00:00\n    \n    \n      40\n      0.464891\n      0.009220\n      00:00\n    \n    \n      41\n      0.448957\n      0.007893\n      00:00\n    \n    \n      42\n      0.433761\n      0.006812\n      00:00\n    \n    \n      43\n      0.419261\n      0.005925\n      00:00\n    \n    \n      44\n      0.405417\n      0.005203\n      00:00\n    \n    \n      45\n      0.392191\n      0.004621\n      00:00\n    \n    \n      46\n      0.379547\n      0.004154\n      00:00\n    \n    \n      47\n      0.367454\n      0.003775\n      00:00\n    \n    \n      48\n      0.355879\n      0.003463\n      00:00\n    \n    \n      49\n      0.344794\n      0.003202\n      00:00\n    \n  \n\n\n\n\nplt.matshow(soft(lrnr.model(x)[-10:]).data.to(\"cpu\"),cmap = 'bwr', vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6bbbb779d0>"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#data-hihello",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#data-hihello",
    "title": "13. 순환신경망 (4)",
    "section": "data: hi?hello!!",
    "text": "data: hi?hello!!\n\ntxt = list('hi?hello!!')*100 \ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {'!':0, '?':1,'h':2,'i':3,'e':4,'l':5,'o':6} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#세트1-_water의-생략",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#세트1-_water의-생략",
    "title": "13. 순환신경망 (4)",
    "section": "세트1: _water의 생략",
    "text": "세트1: _water의 생략\n- 코드1: 정석코드\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\n_water = torch.zeros(1,4).to(\"cuda:0\")\nlstm(x, (_water,_water))\n\n(tensor([[-0.1547,  0.0673,  0.0695,  0.1563],\n         [-0.0786, -0.1430, -0.0250,  0.1189],\n         [-0.0300, -0.2256, -0.1324,  0.1439],\n         ...,\n         [-0.0723,  0.0620,  0.1913,  0.2015],\n         [-0.1155,  0.0746,  0.1747,  0.2938],\n         [-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n (tensor([[-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>),\n  tensor([[-0.4451, -0.2456, -0.1900,  0.6232]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>)))\n\n\n- 코드2: _water 는 사실 없어도 괜찮았어..\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\nlstm(x)\n\n(tensor([[-0.1547,  0.0673,  0.0695,  0.1563],\n         [-0.0786, -0.1430, -0.0250,  0.1189],\n         [-0.0300, -0.2256, -0.1324,  0.1439],\n         ...,\n         [-0.0723,  0.0620,  0.1913,  0.2015],\n         [-0.1155,  0.0746,  0.1747,  0.2938],\n         [-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n (tensor([[-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>),\n  tensor([[-0.4451, -0.2456, -0.1900,  0.6232]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>)))"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#세트2-x.shape-l-h_in-or-lnh_in",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#세트2-x.shape-l-h_in-or-lnh_in",
    "title": "13. 순환신경망 (4)",
    "section": "세트2: x.shape = (\\(L\\), \\(H_{in}\\)) or (\\(L\\),\\(N\\),\\(H_{in}\\))",
    "text": "세트2: x.shape = (\\(L\\), \\(H_{in}\\)) or (\\(L\\),\\(N\\),\\(H_{in}\\))\n- 파라메터 설명\n\n\\(L\\) = sequece length = 시계열의 길이 = 간장을 몇 년 전통으로 이어갈지\n\\(N\\) = batch size = 전체데이터는 몇 개의 시계열이 있는지 = 전체 데이터를 몇개의 시계열로 쪼갤지 <– 왜 이걸 해야해?\n\\(H_{in}\\) = input_size = 시점을 고정하였을 경우 입력자료의 차원 = 입력시계열이 시점별로 몇개의 변수로 나타내어 지는지? = 만약에 원핫인코딩으로 단어를 정리하면 단어수를 의미함\n\n- 코드2: _water 는 사실 없어도 괜찮았어..\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\nlstm(x)\n\n(tensor([[-0.1547,  0.0673,  0.0695,  0.1563],\n         [-0.0786, -0.1430, -0.0250,  0.1189],\n         [-0.0300, -0.2256, -0.1324,  0.1439],\n         ...,\n         [-0.0723,  0.0620,  0.1913,  0.2015],\n         [-0.1155,  0.0746,  0.1747,  0.2938],\n         [-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n (tensor([[-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>),\n  tensor([[-0.4451, -0.2456, -0.1900,  0.6232]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>)))\n\n\n- 코드3: x의 차원은 사실 엄밀하게는 (\\(L\\),\\(N\\),\\(H_{in}\\)) 와 같다…\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\nlstm(x.reshape(999,1,7))\n\n(tensor([[[-0.1547,  0.0673,  0.0695,  0.1563]],\n \n         [[-0.0786, -0.1430, -0.0250,  0.1189]],\n \n         [[-0.0300, -0.2256, -0.1324,  0.1439]],\n \n         ...,\n \n         [[-0.0723,  0.0620,  0.1913,  0.2015]],\n \n         [[-0.1155,  0.0746,  0.1747,  0.2938]],\n \n         [[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n        grad_fn=<CudnnRnnBackward0>),\n (tensor([[[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>),\n  tensor([[[-0.4451, -0.2456, -0.1900,  0.6232]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>)))\n\n\n- 코드4: batch_first=True옵션을 사용하여 lstm을 만든경우\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4,batch_first=True).to(\"cuda:0\")\n\n\nlstm(x.reshape(1,999,7))\n\n(tensor([[[-0.1547,  0.0673,  0.0695,  0.1563],\n          [-0.0786, -0.1430, -0.0250,  0.1189],\n          [-0.0300, -0.2256, -0.1324,  0.1439],\n          ...,\n          [-0.0723,  0.0620,  0.1913,  0.2015],\n          [-0.1155,  0.0746,  0.1747,  0.2938],\n          [-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n        grad_fn=<CudnnRnnBackward0>),\n (tensor([[[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>),\n  tensor([[[-0.4451, -0.2456, -0.1900,  0.6232]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>)))"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#세트3-hidden.shape-dtimes-num_layers-h_out-or-dtimes-num_layers-n-h_out",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#세트3-hidden.shape-dtimes-num_layers-h_out-or-dtimes-num_layers-n-h_out",
    "title": "13. 순환신경망 (4)",
    "section": "세트3: hidden.shape = (\\(D\\times\\) num_layers, \\(H_{out}\\)) or (\\(D\\times\\) num_layers, \\(N\\), \\(H_{out}\\))",
    "text": "세트3: hidden.shape = (\\(D\\times\\) num_layers, \\(H_{out}\\)) or (\\(D\\times\\) num_layers, \\(N\\), \\(H_{out}\\))\n- 파라메터 설명\n\n\\(D\\) = 2 if bidirectional=True otherwise 1 = 양방향이면 2, 단방향이면 1 (우리는 단방향만 배움)\nnum_layres = 중첩된 RNN일 경우 (우리는 중첩을 안시켰음)\n\\(N\\) = batch size = 전체데이터는 몇 개의 시계열이 있는지 = 전체 데이터를 몇개의 시계열로 쪼갤지 <– 왜 이걸 해야해?\n\\(H_{out}\\) = 히든노드의 수\n\n- 코드5: x.shape = (\\(L\\),\\(1\\),\\(H_{in}\\)) \\(\\to\\) hidden.shape = (\\(1\\),\\(1\\),\\(H_{out}\\))\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\n_water = torch.zeros(1,1,4).to(\"cuda:0\") \nlstm(x.reshape(999,1,7),(_water,_water))\n\n(tensor([[[-0.1547,  0.0673,  0.0695,  0.1563]],\n \n         [[-0.0786, -0.1430, -0.0250,  0.1189]],\n \n         [[-0.0300, -0.2256, -0.1324,  0.1439]],\n \n         ...,\n \n         [[-0.0723,  0.0620,  0.1913,  0.2015]],\n \n         [[-0.1155,  0.0746,  0.1747,  0.2938]],\n \n         [[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n        grad_fn=<CudnnRnnBackward0>),\n (tensor([[[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>),\n  tensor([[[-0.4451, -0.2456, -0.1900,  0.6232]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>)))\n\n\n- 사실 _water.shape = (1,\\(H_{out}\\)) 에서 1은 observation의 차원을 의미하는게 아님 (그런데 대충 그렇게 생각해도 무방함)\n\n한 시점의 콩물에 대하여 양방향으로 간장을 만들면 _water.shape = (2,h)\n한 시점의 콩물에 대하여 3중첩으로 간장을 만들면 _water.shape = (3,h)\n한 시점의 콩물에 대하여 3중첩간장을 양방향으로 만들면 _water.shape = (6,h)"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#data-hihello-1",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#data-hihello-1",
    "title": "13. 순환신경망 (4)",
    "section": "data: hi?hello!!",
    "text": "data: hi?hello!!\n\ntxt = list('hi?hello!!')*100 \ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {'!':0, '?':1,'h':2,'i':3,'e':4,'l':5,'o':6} \nx= torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny= torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#세트1-_water의-생략-1",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#세트1-_water의-생략-1",
    "title": "13. 순환신경망 (4)",
    "section": "세트1: _water의 생략",
    "text": "세트1: _water의 생략\n- 코드1: 정석코드\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\") \n\n\nxt = x[[1]]\n_water = torch.zeros(1,4).to(\"cuda:0\")\nxt.shape, _water.shape\n\n(torch.Size([1, 7]), torch.Size([1, 4]))\n\n\n\nlstmcell(xt,(_water,_water))\n\n(tensor([[-0.0290, -0.1758, -0.0537,  0.0598]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>),\n tensor([[-0.0582, -0.4566, -0.1256,  0.1922]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>))\n\n\n- 코드2: _water의 생략\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\") \n\n\nxt = x[[1]]\nxt.shape\n\ntorch.Size([1, 7])\n\n\n\nlstmcell(xt)\n\n(tensor([[-0.0290, -0.1758, -0.0537,  0.0598]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>),\n tensor([[-0.0582, -0.4566, -0.1256,  0.1922]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>))"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#세트2-xt.shape-nh_in-or-h_in",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#세트2-xt.shape-nh_in-or-h_in",
    "title": "13. 순환신경망 (4)",
    "section": "세트2: xt.shape = (\\(N\\),\\(H_{in}\\)) or (\\(H_{in}\\))",
    "text": "세트2: xt.shape = (\\(N\\),\\(H_{in}\\)) or (\\(H_{in}\\))\n- 코드2: _water의 생략\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\") \n\n\nxt = x[[1]]\nxt.shape\n\ntorch.Size([1, 7])\n\n\n\nlstmcell(xt)\n\n(tensor([[-0.0290, -0.1758, -0.0537,  0.0598]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>),\n tensor([[-0.0582, -0.4566, -0.1256,  0.1922]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>))\n\n\n- 코드3:\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\") \n\n\nxt = x[1]\nxt.shape\n\ntorch.Size([7])\n\n\n\nlstmcell(xt)\n\n(tensor([-0.0290, -0.1758, -0.0537,  0.0598], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n tensor([-0.0582, -0.4566, -0.1256,  0.1922], device='cuda:0',\n        grad_fn=<SqueezeBackward1>))"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#세트3-hidden.shape-nh_out-or-h_out",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#세트3-hidden.shape-nh_out-or-h_out",
    "title": "13. 순환신경망 (4)",
    "section": "세트3: hidden.shape = (\\(N\\),\\(H_{out}\\)) or (\\(H_{out}\\))",
    "text": "세트3: hidden.shape = (\\(N\\),\\(H_{out}\\)) or (\\(H_{out}\\))\n- 코드4: xt.shape = (\\(H_{in}\\)) \\(\\to\\) _water.shape = \\((H_{out})\\)\n\ntorch.manual_seed(43052) \nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\") \n\n\nxt = x[1]\n_water = torch.zeros(4).to(\"cuda:0\")\nxt.shape,_water.shape\n\n(torch.Size([7]), torch.Size([4]))\n\n\n\nlstmcell(xt, (_water,_water))\n\n(tensor([-0.0290, -0.1758, -0.0537,  0.0598], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n tensor([-0.0582, -0.4566, -0.1256,  0.1922], device='cuda:0',\n        grad_fn=<SqueezeBackward1>))"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#똑같은-코드들-정리",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#똑같은-코드들-정리",
    "title": "13. 순환신경망 (4)",
    "section": "똑같은 코드들 정리",
    "text": "똑같은 코드들 정리\n- 원래 1은 단순히 observation의 차원이 아니다. 즉 \\({\\bf X}_{n \\times p}\\)에서 \\(n\\)에 대응하는 차원으로 생각할 수 없다.\n- 그런데 (1) 단방향 (2) 조각내지 않은 시계열 (3) 중첩하지 않은 순환망에 한정하여서는 observation 처럼 생각해도 무방하다. <– 엄밀하게는 이게 위험한 생각임. 하지만 정식으로 모두 따지려면 너무 헷갈림"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#실제구현시-기억할-것",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#실제구현시-기억할-것",
    "title": "13. 순환신경망 (4)",
    "section": "실제구현시 기억할 것",
    "text": "실제구현시 기억할 것\n- 현실적으로 (1)-(3)이 아닌 조건에서는 Cell 단위로 연산을 이용할 일이 없다. (느리거든요) // 그냥 이해용으로 구현\n- torch.nn.RNN 혹은 torch.nn.LSTM 으로 네트워크를 구성할시 _water의 dim을 명시할 일도 없다.\n- 오로지 고려해야 할 것은 입력시계열을 조각낼지 조각내지 않을지"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#data",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#data",
    "title": "13. 순환신경망 (4)",
    "section": "data",
    "text": "data\n\ntxt = list('hi!')*3 + list('hi?')*3"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#조각내지-않은-시계열",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#조각내지-않은-시계열",
    "title": "13. 순환신경망 (4)",
    "section": "조각내지 않은 시계열",
    "text": "조각내지 않은 시계열\n\ntxt_x = txt[:-1] \ntxt_y = txt[1:] \n\n\nmapping = {'!':0, '?':1, 'h':2, 'i':3} \nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")\n\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,10).to(\"cuda:0\")\nlinr = torch.nn.Linear(10,4).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1 \n    hidden, _ = lstm(x) \n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nhidden, _ = lstm(x)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6b994a6f50>"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#조각난-시계열",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#조각난-시계열",
    "title": "13. 순환신경망 (4)",
    "section": "조각난 시계열",
    "text": "조각난 시계열\n\ntxt1= txt[:9]\ntxt2= txt[9:]\n\n\ntxt1,txt2\n\n(['h', 'i', '!', 'h', 'i', '!', 'h', 'i', '!'],\n ['h', 'i', '?', 'h', 'i', '?', 'h', 'i', '?'])\n\n\n\ntxt1_x = txt1[:-1] \ntxt1_y = txt1[1:] \ntxt2_x = txt2[:-1] \ntxt2_y = txt2[1:] \n\n\nmapping = {'!':0, '?':1, 'h':2, 'i':3} \nx1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_x,mapping))).float().to(\"cuda:0\")\ny1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_y,mapping))).float().to(\"cuda:0\")\nx2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_x,mapping))).float().to(\"cuda:0\")\ny2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_y,mapping))).float().to(\"cuda:0\")\n\n\nx1.shape, y1.shape, x2.shape, y2.shape\n\n(torch.Size([8, 4]),\n torch.Size([8, 4]),\n torch.Size([8, 4]),\n torch.Size([8, 4]))\n\n\n\nxx = torch.stack([x1,x2],axis=1)\nyy = torch.stack([y1,y2],axis=1)\nxx.shape, yy.shape\n\n(torch.Size([8, 2, 4]), torch.Size([8, 2, 4]))\n\n\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,10).to(\"cuda:0\")\nlinr = torch.nn.Linear(10,4).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1 \n    hidden, _ = lstm(xx) \n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output[:,0,:],yy[:,0,:]) + loss_fn(output[:,1,:],yy[:,1,:])\n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nfig , ax = plt.subplots(1,2) \nax[0].matshow(soft(output[:,0,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\nax[1].matshow(soft(output[:,1,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6b70111650>\n\n\n\n\n\n\nhidden, _ = lstm(x)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6b70111350>\n\n\n\n\n\n- 조각난 시계열로 학습한 경우는 hi!에서 hi?로 바뀔 수 없다. 왜냐햐면 그러한 연결정보가 끊어져 있으니까"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#재미있는-실험",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#재미있는-실험",
    "title": "13. 순환신경망 (4)",
    "section": "재미있는 실험",
    "text": "재미있는 실험\n- x1만 배운다면?\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,10).to(\"cuda:0\")\nlinr = torch.nn.Linear(10,4).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1 \n    hidden, _ = lstm(x1) \n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y1)\n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nhidden, _ = lstm(x2)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6b701ba890>\n\n\n\n\n\n- x2만 배운다면?\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(4,10).to(\"cuda:0\")\nlinr = torch.nn.Linear(10,4).to(\"cuda:0\")\n\n\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(100):\n    ## 1 \n    hidden, _ = lstm(x2) \n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y2)\n    ## 3 \n    loss.backward() \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nhidden, _ = lstm(x1)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x7f6b9809ef50>"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#data-human-numbers-5-1",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#data-human-numbers-5-1",
    "title": "13. 순환신경망 (4)",
    "section": "data: human numbers 5",
    "text": "data: human numbers 5\n\ntxt = (['one',',','two',',','three',',','four',',','five',',']*100)[:-1]\n\n\nmapping = {',':0, 'one':1, 'two':2, 'three':3, 'four':4, 'five':5} \nmapping\n\n{',': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5}\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:] \n\n\ntxt_x[0:5], txt_y[0:5]\n\n(['one', ',', 'two', ',', 'three'], [',', 'two', ',', 'three', ','])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#fastai-이용한-learn-1",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#fastai-이용한-learn-1",
    "title": "13. 순환신경망 (4)",
    "section": "fastai 이용한 learn",
    "text": "fastai 이용한 learn\n\nds1 = torch.utils.data.TensorDataset(x,y)\nds2 = torch.utils.data.TensorDataset(x,y) # dummy \ndl1 = torch.utils.data.DataLoader(ds1,batch_size=998)\ndl2 = torch.utils.data.DataLoader(ds2,batch_size=998) # dummy \ndls = DataLoaders(dl1,dl2) \n\n\nclass MyLSTM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.manual_seed(43052)\n        self.lstm = torch.nn.LSTM(6,20)\n        self.linr = torch.nn.Linear(20,6) \n    def forward(self,x):\n        _water = torch.zeros(1,20).to(\"cuda:0\")\n        hidden, (hT,cT) =self.lstm(x,(_water,_water))\n        output = self.linr(hidden)\n        return output         \n\n\nnet = MyLSTM().to(\"cuda:0\")\nloss_fn = torch.nn.CrossEntropyLoss()\n\n\nlrnr = Learner(dls,net,loss_fn,lr=0.1)\n\n\nlrnr.fit(10)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.762846\n      1.502211\n      00:00\n    \n    \n      1\n      1.631212\n      1.620583\n      00:00\n    \n    \n      2\n      1.627597\n      1.443686\n      00:00\n    \n    \n      3\n      1.580216\n      1.368762\n      00:00\n    \n    \n      4\n      1.536200\n      1.307310\n      00:00\n    \n    \n      5\n      1.496099\n      1.216339\n      00:00\n    \n    \n      6\n      1.453670\n      1.113821\n      00:00\n    \n    \n      7\n      1.408125\n      1.019931\n      00:00\n    \n    \n      8\n      1.361426\n      0.941434\n      00:00\n    \n    \n      9\n      1.315507\n      0.884034\n      00:00\n    \n  \n\n\n\n\nsoft(lrnr.model(x)).data.to(\"cpu\").numpy().round(3)\n\narray([[0.935, 0.009, 0.015, 0.011, 0.016, 0.014],\n       [0.133, 0.164, 0.242, 0.172, 0.141, 0.147],\n       [0.982, 0.003, 0.004, 0.003, 0.004, 0.003],\n       ...,\n       [0.122, 0.171, 0.242, 0.174, 0.146, 0.144],\n       [0.984, 0.003, 0.004, 0.002, 0.004, 0.003],\n       [0.119, 0.172, 0.244, 0.175, 0.144, 0.145]], dtype=float32)"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-12-12wk.html#torch를-이용한-learn-1",
    "href": "post/Lecture/STML/2023-04-12-12wk.html#torch를-이용한-learn-1",
    "title": "13. 순환신경망 (4)",
    "section": "torch를 이용한 learn",
    "text": "torch를 이용한 learn\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(6,20).to(\"cuda:0\") \nlinr = torch.nn.Linear(20,6).to(\"cuda:0\") \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoc in range(10):\n    ## 1 \n    hidden, _ = lstm(x)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()     \n\n\nhidden, _ = lstm(x)\noutput = linr(hidden) \nsoft(output).data.to(\"cpu\").numpy().round(3)\n\narray([[0.935, 0.009, 0.015, 0.011, 0.016, 0.014],\n       [0.133, 0.164, 0.242, 0.172, 0.141, 0.147],\n       [0.982, 0.003, 0.004, 0.003, 0.004, 0.003],\n       ...,\n       [0.122, 0.171, 0.242, 0.174, 0.146, 0.144],\n       [0.984, 0.003, 0.004, 0.002, 0.004, 0.003],\n       [0.119, 0.172, 0.244, 0.175, 0.145, 0.145]], dtype=float32)"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-19-13wk.html",
    "href": "post/Lecture/STML/2023-04-19-13wk.html",
    "title": "14. 순환신경망 (5)",
    "section": "",
    "text": "import torch\nfrom fastai.text.all import *"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-19-13wk.html#잡담1-순환신경망-텍스트마이닝-시계열분석",
    "href": "post/Lecture/STML/2023-04-19-13wk.html#잡담1-순환신경망-텍스트마이닝-시계열분석",
    "title": "14. 순환신경망 (5)",
    "section": "잡담1: 순환신경망, 텍스트마이닝, 시계열분석",
    "text": "잡담1: 순환신경망, 텍스트마이닝, 시계열분석\n- 순환신경망은 순서가 있는 (말이 좀 애매하지만 아무튼 이렇게 많이 표현해요) 자료를 분석할때 사용할 수 있다. 순서가 있는 자료는 대표적으로 시계열자료과 텍스트자료가 있다.\n- 그래서 언뜻 생각하면 텍스트마이닝이나 시계열분석과 내용이 비슷할 것 같지만 사실 그렇지 않다.\n\n텍스트마이닝의 토픽: 단어를 어떻게 숫자로 잘 만들지, 토픽모델 // 자잘하고 실용적인 느낌? 공학적임..\n\n시계열분석의 토픽: 예측(forecasting)과 신뢰구간, 변화점과 관련한 연구 (detection/test), 정상/비정상시계열모형 (ARIMA, GARCH), Cointegration Test, // 느낌이 좀 거창해.. 경제와 관련 많음.\n순환신경망의 토픽(재작년까지): 텍스트생성, 텍스트분류 + 시계열 자료의 예측, 단어의 숫자화 … 텍스트마이닝과 시계열분석의 거의 모든 토픽에 관여함\n순환신경망의 토픽(작년부터?): 딥러닝의 거의 모든 영역에 관여하기 시작함 (심지어 요즘 이미지 분석도 순환망으로 합니다)\n\n\nhttps://youtu.be/thsXGOkcGGg"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-19-13wk.html#잡담2-순환신경망의-아키텍처를-얼마나-깊이-이해해야-할까",
    "href": "post/Lecture/STML/2023-04-19-13wk.html#잡담2-순환신경망의-아키텍처를-얼마나-깊이-이해해야-할까",
    "title": "14. 순환신경망 (5)",
    "section": "잡담2: 순환신경망의 아키텍처를 얼마나 깊이 이해해야 할까?",
    "text": "잡담2: 순환신경망의 아키텍처를 얼마나 깊이 이해해야 할까?\n- 과거기준(텍스트생성, 텍스트분류, 시계열자료예측 등에만 순환망이 이용되었을 때): 학부수준에서 순수 RNN만 알아도 충분했던 것 같음. LSTM이나 GRU는 석사수준?\n- 현재기준: 석사기준 LSTM 같은건 기본이고 어텐션, 트랜스포머등에 대한 개념도 잘 알고 있어야 함. (학부는 잘 모르겠네..)\n- 내 생각: 결국 아키텍처는 근데 유행이라 아키텍처는 한번 따라하면서 이해해보고 핵심 아이디어만 이해하면 된다고 생각함. 즉 LSTM 같은 특정모형의 아키텍처를 달달 외울필요는 없다, 수식써있는거 보고 이해하면 그만임. (수식정도를 이해할 능력은 필요한게.. 코드를 짤때 옵션을 이해할 수는 있어야하니까)\n- 망상: 나중에는 순환신경망이 거의 모든 딥러닝 방법의 base가 되지 않을까?"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-19-13wk.html#잡담3-fastai-pytorch-lightning",
    "href": "post/Lecture/STML/2023-04-19-13wk.html#잡담3-fastai-pytorch-lightning",
    "title": "14. 순환신경망 (5)",
    "section": "잡담3: fastai, pytorch lightning",
    "text": "잡담3: fastai, pytorch lightning\n- 비 컴퓨터공학 출신이 쓰기에는 fastai가 좀 더 쓰기 편한건 사실\n- pytorch lightning은 fastai보다 쓰기 어렵지만 (진짜 약간의 클래스관련 지식이 필요함, 솔직히 별로 어렵진 않아요) 좀 더 순수 파이토치에 가깝고 따라서 코드를 뜯어보기 편리하다.\n- 과거의 생각\n\n전문가: pytorch + fastai // pytorch + pytorch lightning (컴공출신)\n비 전문가: 순수 fastai\n\n- 요즘 생각\n\n모두: pytorch + pytorch lightning\n특정한경우: 순수 fastai <– 모형이 구현되어 있다면 fastai가 좋긴 좋아.. 그런데 모형의 구현속도가 못따라감"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-19-13wk.html#잡담4-우린-뭘-해야-할까-학석사-레벨에서..",
    "href": "post/Lecture/STML/2023-04-19-13wk.html#잡담4-우린-뭘-해야-할까-학석사-레벨에서..",
    "title": "14. 순환신경망 (5)",
    "section": "잡담4: 우린 뭘 해야 할까 (학석사 레벨에서..)",
    "text": "잡담4: 우린 뭘 해야 할까 (학석사 레벨에서..)\n- 능력1: 코드이해력 (= 구현능력 = 코드 베끼는 능력)\n\n이미지분석? 해봤음. 텍스트자료? 해봤음. 시계열? 해봤음. 등등등등? 다 해본적 있음. 어떤 원리인지 정확하게 몰라도 다 해본적 있고 그래서 일할 수 있음!!\n돌아가는 코드 최대한 많이 모아놓으세요. torch, fastai, pytorch lightning, tensorflow, keras 등등\n\n- 능력2: 최신트렌드를 파악할 수 있는 힘 (= 논문이해력)\n\n공부, 공부, 공부… A to Z 까지 수식 다 뜯어보고 코드 다 뜯어보면서 집요하게 공부해야함. (LSTM에서 했던것 처럼!) 물론 차근차근 알려주면 수업이 있다면 좋겠지 그런데 보통은 적당히 두리뭉실하게 설명하지 detail 하게 설명하는 수업은 잘 없음. (지루하거든요)\n수식이나 코드중 하나라도 볼 줄 모르면 능력2를 얻는것 자체가 불가능."
  },
  {
    "objectID": "post/Lecture/STML/2023-04-26-Extra-1.html",
    "href": "post/Lecture/STML/2023-04-26-Extra-1.html",
    "title": "15. Extra-1: 추천시스템",
    "section": "",
    "text": "import torch\nimport numpy as np \nimport pandas as pd\nfrom fastai.collab import *"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-26-Extra-1.html#주절주절-intro",
    "href": "post/Lecture/STML/2023-04-26-Extra-1.html#주절주절-intro",
    "title": "15. Extra-1: 추천시스템",
    "section": "주절주절 intro",
    "text": "주절주절 intro\n- Data\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/STML2022/main/posts/V.%20RecSys/2022-12-21-rcmdsolo.csv',index_col=0)\ndf_view \n\n\n\n\n\n  \n    \n      \n      영식\n      영철\n      영호\n      광수\n      상철\n      영수\n    \n  \n  \n    \n      옥순\n      3.9\n      4.1\n      NaN\n      0.5\n      0.3\n      NaN\n    \n    \n      영자\n      4.5\n      NaN\n      3.7\n      0.5\n      NaN\n      0.2\n    \n    \n      정숙\n      NaN\n      4.9\n      4.7\n      NaN\n      1.2\n      1.3\n    \n    \n      영숙\n      0.6\n      0.2\n      NaN\n      4.1\n      4.3\n      NaN\n    \n    \n      순자\n      0.7\n      0.9\n      NaN\n      4.2\n      NaN\n      3.9\n    \n    \n      현숙\n      NaN\n      0.2\n      0.3\n      NaN\n      3.5\n      3.4\n    \n  \n\n\n\n\n- 데이터를 이해할때 필요한 가정들 – 내맘대로 한 설정임.\n\n(옥순,영자,정숙)은 (영식,영철,영호)와 성격이 잘 맞고 (영숙,순자,현숙)은 (광수,상철,영수)와 성격이 잘맞음\n((옥순,영자,정숙),(영식,영철,영호))은 MBTI가 I로 시작하고 ((영숙,순자,현숙),(광수,상철,영수))는 MBTI가 E로 시작한다.\n\n- 목표: NaN 을 추론\n- 수동추론:\n\n(옥순,영호)이 만난다면? \\(\\to\\) 둘다 I성향이니까 잘 맞지 않을까? \\(\\to\\) 4.0 정도?\n(정숙,영식)조합은? \\(\\to\\) 둘다 I성향이니까 잘 맞지 않을까? + 정숙은 다 잘맞던데..? \\(\\to\\) 4.8 정도?\n(현숙,영식)조합은? \\(\\to\\) 현숙은 E성향인데 영식은 I성향이므로 잘 안맞을 것임 + 현숙은 원래 좀 눈이 높음 \\(\\to\\) 0.25 정도?\n\n- 좀 더 체계적인 추론\n사람들이 가지고 있는 성향들을 두 개의 숫자로 표현하자.\n\n옥순의 성향 = (I성향,E성향) = (1.9, 0.0)\n영식의 성향 = (I성향,E성향) = (2.0, 0.1)\n현숙의 성향 = (I성향,E성향) = (0.0, 1.5)\n\n\na1 = np.array([1.9,0.0]).reshape(2,1) \nb1= np.array([2.0,0.1]).reshape(2,1)\n\n\nsum(a1*b1)\n\narray([3.8])\n\n\n(1) 옥순과 영식의 궁합 \\(\\approx\\) 옥순의I성향\\(\\times\\)영식의I성향 \\(+\\) 옥순의E성향\\(\\times\\)영식의E성향 // 적합\n\na1= np.array([1.9,0.0]).reshape(2,1) # a1은 옥순의 성향, col-vec으로 선언하자. \nb1= np.array([2.0,0.1]).reshape(2,1) # b1은 영식의 성향, col-vec으로 선언하자.\n(a1*b1).sum()\n\n3.8\n\n\n(2) 현숙과 영식의 궁합 \\(\\approx\\) 현숙의I성향\\(\\times\\)영식의I성향 \\(+\\) 현숙의E성향\\(\\times\\)영식의E성향 // 예측\n\na6= np.array([0.0,1.5]).reshape(2,1)\n(a6*b1).sum()\n\n0.15000000000000002\n\n\n\n그럴듯함..\n\n- 모델링\n아래가 같음을 관찰하라. (차원만 다름)\n\n(a1*b1).sum(), a1.T@b1\n\n(3.8, array([[3.8]]))\n\n\n\n(a6*b1).sum(), a6.T@b1\n\n(0.15000000000000002, array([[0.15]]))\n\n\n만약에 여자의성향, 남자의성향을 적당한 매트릭스로 정리할 수 있다면 궁합매트릭스를 만들 수 있음\n\na1= np.array([1.9,0.0]).reshape(2,1)\na2= np.array([2.0,0.1]).reshape(2,1)\na3= np.array([2.5,1.0]).reshape(2,1)\na4= np.array([0.1,1.9]).reshape(2,1)\na5= np.array([0.2,2.1]).reshape(2,1)\na6= np.array([0.0,1.5]).reshape(2,1)\nA = np.concatenate([a1,a2,a3,a4,a5,a6],axis=1)\nA\n\narray([[1.9, 2. , 2.5, 0.1, 0.2, 0. ],\n       [0. , 0.1, 1. , 1.9, 2.1, 1.5]])\n\n\n\nb1= np.array([2.0,0.1]).reshape(2,1)\nb2= np.array([1.9,0.2]).reshape(2,1)\nb3= np.array([1.8,0.3]).reshape(2,1)\nb4= np.array([0.3,2.1]).reshape(2,1)\nb5= np.array([0.2,2.0]).reshape(2,1)\nb6= np.array([0.1,1.9]).reshape(2,1)\nB = np.concatenate([b1,b2,b3,b4,b5,b6],axis=1)\nB\n\narray([[2. , 1.9, 1.8, 0.3, 0.2, 0.1],\n       [0.1, 0.2, 0.3, 2.1, 2. , 1.9]])\n\n\n\nA.T@B\n\narray([[3.8 , 3.61, 3.42, 0.57, 0.38, 0.19],\n       [4.01, 3.82, 3.63, 0.81, 0.6 , 0.39],\n       [5.1 , 4.95, 4.8 , 2.85, 2.5 , 2.15],\n       [0.39, 0.57, 0.75, 4.02, 3.82, 3.62],\n       [0.61, 0.8 , 0.99, 4.47, 4.24, 4.01],\n       [0.15, 0.3 , 0.45, 3.15, 3.  , 2.85]])\n\n\n\na1.T@b1, a2.T@b2, a3.T@b1\n\n(array([[3.8]]), array([[3.82]]), array([[5.1]]))\n\n\n결국 모형은 아래와 같다.\n\\[\\text{궁합매트릭스} = {\\bf A}^\\top {\\bf B} + \\text{오차}\\]\n- 학습전략: 아래의 매트릭스중에서 어떤값은 관측하였고 어떤값은 관측하지 못함 \\(\\to\\) 관측한 값들만 대충 비슷하게 하면 되는거 아니야?\n\nA.T@B \n\narray([[3.8 , 3.61, 3.42, 0.57, 0.38, 0.19],\n       [4.01, 3.82, 3.63, 0.81, 0.6 , 0.39],\n       [5.1 , 4.95, 4.8 , 2.85, 2.5 , 2.15],\n       [0.39, 0.57, 0.75, 4.02, 3.82, 3.62],\n       [0.61, 0.8 , 0.99, 4.47, 4.24, 4.01],\n       [0.15, 0.3 , 0.45, 3.15, 3.  , 2.85]])\n\n\n\ndf_view\n\n\n\n\n\n  \n    \n      \n      영식\n      영철\n      영호\n      광수\n      상철\n      영수\n    \n  \n  \n    \n      옥순\n      3.9\n      4.1\n      NaN\n      0.5\n      0.3\n      NaN\n    \n    \n      영자\n      4.5\n      NaN\n      3.7\n      0.5\n      NaN\n      0.2\n    \n    \n      정숙\n      NaN\n      4.9\n      4.7\n      NaN\n      1.2\n      1.3\n    \n    \n      영숙\n      0.6\n      0.2\n      NaN\n      4.1\n      4.3\n      NaN\n    \n    \n      순자\n      0.7\n      0.9\n      NaN\n      4.2\n      NaN\n      3.9\n    \n    \n      현숙\n      NaN\n      0.2\n      0.3\n      NaN\n      3.5\n      3.4\n    \n  \n\n\n\n\n- 자료를 아래와 같이 정리한다면?\n\ndf = pd.DataFrame([(f,m,df_view.loc[f,m]) for f in df_view.index for m in df_view.columns if not np.isnan(df_view.loc[f,m])])\ndf.columns = ['X1','X2','y']\ndf\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n      y\n    \n  \n  \n    \n      0\n      옥순\n      영식\n      3.9\n    \n    \n      1\n      옥순\n      영철\n      4.1\n    \n    \n      2\n      옥순\n      광수\n      0.5\n    \n    \n      3\n      옥순\n      상철\n      0.3\n    \n    \n      4\n      영자\n      영식\n      4.5\n    \n    \n      5\n      영자\n      영호\n      3.7\n    \n    \n      6\n      영자\n      광수\n      0.5\n    \n    \n      7\n      영자\n      영수\n      0.2\n    \n    \n      8\n      정숙\n      영철\n      4.9\n    \n    \n      9\n      정숙\n      영호\n      4.7\n    \n    \n      10\n      정숙\n      상철\n      1.2\n    \n    \n      11\n      정숙\n      영수\n      1.3\n    \n    \n      12\n      영숙\n      영식\n      0.6\n    \n    \n      13\n      영숙\n      영철\n      0.2\n    \n    \n      14\n      영숙\n      광수\n      4.1\n    \n    \n      15\n      영숙\n      상철\n      4.3\n    \n    \n      16\n      순자\n      영식\n      0.7\n    \n    \n      17\n      순자\n      영철\n      0.9\n    \n    \n      18\n      순자\n      광수\n      4.2\n    \n    \n      19\n      순자\n      영수\n      3.9\n    \n    \n      20\n      현숙\n      영철\n      0.2\n    \n    \n      21\n      현숙\n      영호\n      0.3\n    \n    \n      22\n      현숙\n      상철\n      3.5\n    \n    \n      23\n      현숙\n      영수\n      3.4\n    \n  \n\n\n\n\n\nmapp1 = {k[1]:k[0] for k in enumerate(df.X1.unique())}\nmapp2 = {k[1]:k[0] for k in enumerate(df.X2.unique())}\nmapp1,mapp2\n\n({'옥순': 0, '영자': 1, '정숙': 2, '영숙': 3, '순자': 4, '현숙': 5},\n {'영식': 0, '영철': 1, '광수': 2, '상철': 3, '영호': 4, '영수': 5})\n\n\n\nX1 = torch.tensor(list(map(lambda name: mapp1[name], df.X1)))\nX2 = torch.tensor(list(map(lambda name: mapp2[name], df.X2)))\nX1 = torch.nn.functional.one_hot(X1).float()\nX2 = torch.nn.functional.one_hot(X2).float()\ny = torch.tensor(df.y).float()\n\n- yhat을 구하는 과정..\n\nl1 = torch.nn.Linear(in_features=6,out_features=2) \nl2 = torch.nn.Linear(in_features=6,out_features=2)\n\n\nl1(X1) # 옥순~현숙의 성향들 \n\ntensor([[-0.3442,  0.0280],\n        [-0.3442,  0.0280],\n        [-0.3442,  0.0280],\n        [-0.3442,  0.0280],\n        [ 0.2028,  0.1404],\n        [ 0.2028,  0.1404],\n        [ 0.2028,  0.1404],\n        [ 0.2028,  0.1404],\n        [ 0.1110,  0.0959],\n        [ 0.1110,  0.0959],\n        [ 0.1110,  0.0959],\n        [ 0.1110,  0.0959],\n        [-0.3608, -0.2406],\n        [-0.3608, -0.2406],\n        [-0.3608, -0.2406],\n        [-0.3608, -0.2406],\n        [-0.3450, -0.2079],\n        [-0.3450, -0.2079],\n        [-0.3450, -0.2079],\n        [-0.3450, -0.2079],\n        [ 0.3332, -0.2464],\n        [ 0.3332, -0.2464],\n        [ 0.3332, -0.2464],\n        [ 0.3332, -0.2464]], grad_fn=<AddmmBackward0>)\n\n\n\nl2(X2) # 영식~영수의 성향들 \n\ntensor([[-0.0777, -0.0163],\n        [-0.1901, -0.3110],\n        [-0.3925, -0.4103],\n        [-0.3446, -0.2070],\n        [-0.0777, -0.0163],\n        [-0.2570, -0.2648],\n        [-0.3925, -0.4103],\n        [-0.1879,  0.1743],\n        [-0.1901, -0.3110],\n        [-0.2570, -0.2648],\n        [-0.3446, -0.2070],\n        [-0.1879,  0.1743],\n        [-0.0777, -0.0163],\n        [-0.1901, -0.3110],\n        [-0.3925, -0.4103],\n        [-0.3446, -0.2070],\n        [-0.0777, -0.0163],\n        [-0.1901, -0.3110],\n        [-0.3925, -0.4103],\n        [-0.1879,  0.1743],\n        [-0.1901, -0.3110],\n        [-0.2570, -0.2648],\n        [-0.3446, -0.2070],\n        [-0.1879,  0.1743]], grad_fn=<AddmmBackward0>)\n\n\n- 몇개의 관측치만 생각해보자..\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n      y\n    \n  \n  \n    \n      0\n      옥순\n      영식\n      3.9\n    \n    \n      1\n      옥순\n      영철\n      4.1\n    \n    \n      2\n      옥순\n      광수\n      0.5\n    \n    \n      3\n      옥순\n      상철\n      0.3\n    \n    \n      4\n      영자\n      영식\n      4.5\n    \n  \n\n\n\n\n\n(l1(X1)[0]*l2(X2)[0]).sum() # (옥순의성향 * 영식의성향).sum()\n\ntensor(0.0263, grad_fn=<SumBackward0>)\n\n\n\n이 값이 실제로는 3.9 이어야 한다.\n\n\n(l1(X1)[1]*l2(X2)[1]).sum() # (옥순의성향 * 영철의성향).sum()\n\ntensor(0.0567, grad_fn=<SumBackward0>)\n\n\n\n이 값이 실제로는 4.1 이어야 한다.\n\n- yhat을 구하면!\n\nyhat = (l1(X1) * l2(X2)).sum(axis=1) # (l1(X1) * l2(X2)).sum(1)와 결과가 같음 \nyhat\n\ntensor([ 0.0263,  0.0567,  0.1236,  0.1128, -0.0180, -0.0893, -0.1372, -0.0136,\n        -0.0509, -0.0539, -0.0581, -0.0041,  0.0319,  0.1434,  0.2403,  0.1741,\n         0.0302,  0.1303,  0.2207,  0.0286,  0.0133, -0.0204, -0.0638, -0.1055],\n       grad_fn=<SumBackward1>)\n\n\n\nyhat[:2],y[:2] # 이 값들이 비슷해야 하는데..\n\n(tensor([0.0263, 0.0567], grad_fn=<SliceBackward0>), tensor([3.9000, 4.1000]))\n\n\n- 0~5 까지의 범위로 고정되어 있으니까 아래와 같이 해도 되겠음..\n\nsig = torch.nn.Sigmoid()\n\n\nyhat = sig((l1(X1) * l2(X2)).sum(axis=1))*5 # (l1(X1) * l2(X2)).sum(1)와 결과가 같음 \nyhat\n\ntensor([2.5329, 2.5709, 2.6543, 2.6409, 2.4774, 2.3884, 2.3288, 2.4829, 2.4363,\n        2.4326, 2.4274, 2.4948, 2.5399, 2.6790, 2.7990, 2.7171, 2.5377, 2.6626,\n        2.7748, 2.5357, 2.5166, 2.4745, 2.4203, 2.3682],\n       grad_fn=<MulBackward0>)\n\n\n\nloss = torch.mean((y-yhat)**2)\nloss\n\ntensor(3.3187, grad_fn=<MeanBackward0>)"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-26-Extra-1.html#torch를-이용한-학습",
    "href": "post/Lecture/STML/2023-04-26-Extra-1.html#torch를-이용한-학습",
    "title": "15. Extra-1: 추천시스템",
    "section": "torch를 이용한 학습",
    "text": "torch를 이용한 학습\n\ntorch.manual_seed(43052)\nl1 = torch.nn.Linear(6,2) \nl2 = torch.nn.Linear(6,2)\nsig = torch.nn.Sigmoid() \n\n\nloss_fn = torch.nn.MSELoss() \noptimizr = torch.optim.Adam(list(l1.parameters())+list(l2.parameters()))\n\n\nfor epoc in range(5000):\n    ## 1 \n    feature1 = l1(X1)\n    feature2 = l2(X2) \n    matching_score = (feature1*feature2).sum(axis=1) \n    yhat = sig(matching_score)*5 # 만약에 1~3점이라면 \"1+sig(matching_score)*2\" 와 같이 하면 되었을듯 \n    ## 2 \n    loss = loss_fn(yhat,y)    \n    ## 3 \n    loss.backward()    \n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\n\nyhat\n\ntensor([3.9382, 4.0624, 0.4665, 0.3353, 4.5038, 3.6975, 0.3562, 0.3558, 4.8614,\n        4.7208, 1.1813, 1.3158, 0.4606, 0.3573, 4.1288, 4.2734, 0.8611, 0.7347,\n        4.0493, 4.0464, 0.1810, 0.3124, 3.5031, 3.3948],\n       grad_fn=<MulBackward0>)\n\n\n\ny\n\ntensor([3.9000, 4.1000, 0.5000, 0.3000, 4.5000, 3.7000, 0.5000, 0.2000, 4.9000,\n        4.7000, 1.2000, 1.3000, 0.6000, 0.2000, 4.1000, 4.3000, 0.7000, 0.9000,\n        4.2000, 3.9000, 0.2000, 0.3000, 3.5000, 3.4000])\n\n\n\nl1(X1) # 두번째 칼럼이 I 성향 점수로 \"해석\"된다\n\ntensor([[-1.4663,  0.2938],\n        [-1.4663,  0.2938],\n        [-1.4663,  0.2938],\n        [-1.4663,  0.2938],\n        [-1.7086,  0.6597],\n        [-1.7086,  0.6597],\n        [-1.7086,  0.6597],\n        [-1.7086,  0.6597],\n        [-0.8705,  1.2945],\n        [-0.8705,  1.2945],\n        [-0.8705,  1.2945],\n        [-0.8705,  1.2945],\n        [ 1.1046, -0.8298],\n        [ 1.1046, -0.8298],\n        [ 1.1046, -0.8298],\n        [ 1.1046, -0.8298],\n        [ 0.9880, -0.5193],\n        [ 0.9880, -0.5193],\n        [ 0.9880, -0.5193],\n        [ 0.9880, -0.5193],\n        [ 0.6834, -1.2201],\n        [ 0.6834, -1.2201],\n        [ 0.6834, -1.2201],\n        [ 0.6834, -1.2201]], grad_fn=<AddmmBackward0>)\n\n\n\n포인트: 여성출연자중, 정숙은 대체로 잘 맞춰주고 현숙은 그렇지 않았음.. \\(\\to\\) 그러한 가중치가 잘 드러남!!"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-26-Extra-1.html#fastai를-이용한-학습",
    "href": "post/Lecture/STML/2023-04-26-Extra-1.html#fastai를-이용한-학습",
    "title": "15. Extra-1: 추천시스템",
    "section": "fastai를 이용한 학습",
    "text": "fastai를 이용한 학습\n(1) dls\n\ndf.head() # 앞단계 전처리의 산물\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n      y\n    \n  \n  \n    \n      0\n      옥순\n      영식\n      3.9\n    \n    \n      1\n      옥순\n      영철\n      4.1\n    \n    \n      2\n      옥순\n      광수\n      0.5\n    \n    \n      3\n      옥순\n      상철\n      0.3\n    \n    \n      4\n      영자\n      영식\n      4.5\n    \n  \n\n\n\n\n\ndls = CollabDataLoaders.from_df(df,bs=2,valid_pct=2/24)\n\n(2) lrnr 생성\n\nlrnr = collab_learner(dls,n_factors=2,y_range=(0,5))\n\n(3) 학습\n\nlrnr.fit(30,lr=0.05)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      3.889854\n      1.921828\n      00:00\n    \n    \n      1\n      3.429675\n      1.831365\n      00:00\n    \n    \n      2\n      2.782349\n      0.825877\n      00:00\n    \n    \n      3\n      2.016374\n      0.019480\n      00:00\n    \n    \n      4\n      1.467516\n      0.151512\n      00:00\n    \n    \n      5\n      1.098058\n      0.277732\n      00:00\n    \n    \n      6\n      0.836384\n      0.322864\n      00:00\n    \n    \n      7\n      0.646015\n      0.336643\n      00:00\n    \n    \n      8\n      0.504761\n      0.351580\n      00:00\n    \n    \n      9\n      0.398490\n      0.375448\n      00:00\n    \n    \n      10\n      0.316750\n      0.407963\n      00:00\n    \n    \n      11\n      0.253128\n      0.439455\n      00:00\n    \n    \n      12\n      0.203320\n      0.458523\n      00:00\n    \n    \n      13\n      0.163978\n      0.461103\n      00:00\n    \n    \n      14\n      0.133211\n      0.474495\n      00:00\n    \n    \n      15\n      0.108348\n      0.493935\n      00:00\n    \n    \n      16\n      0.088889\n      0.492454\n      00:00\n    \n    \n      17\n      0.073574\n      0.489081\n      00:00\n    \n    \n      18\n      0.061007\n      0.485161\n      00:00\n    \n    \n      19\n      0.050791\n      0.512837\n      00:00\n    \n    \n      20\n      0.042383\n      0.501657\n      00:00\n    \n    \n      21\n      0.035439\n      0.485179\n      00:00\n    \n    \n      22\n      0.029940\n      0.490993\n      00:00\n    \n    \n      23\n      0.025290\n      0.489756\n      00:00\n    \n    \n      24\n      0.021683\n      0.487266\n      00:00\n    \n    \n      25\n      0.019516\n      0.472549\n      00:00\n    \n    \n      26\n      0.018497\n      0.484601\n      00:00\n    \n    \n      27\n      0.017128\n      0.427455\n      00:00\n    \n    \n      28\n      0.015667\n      0.524356\n      00:00\n    \n    \n      29\n      0.013884\n      0.385270\n      00:00\n    \n  \n\n\n\n(4) 예측\n적합값 확인\n\nlrnr.show_results()\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n      y\n      y_pred\n    \n  \n  \n    \n      0\n      3.0\n      6.0\n      3.7\n      4.572644\n    \n    \n      1\n      6.0\n      2.0\n      3.5\n      3.595040\n    \n  \n\n\n\n(옥순의 궁합)\n\ndf_new = pd.DataFrame({'X1':['옥순']*6, 'X2':['영식','영철','영호','광수','상철','영수']})\ndf_new\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n    \n  \n  \n    \n      0\n      옥순\n      영식\n    \n    \n      1\n      옥순\n      영철\n    \n    \n      2\n      옥순\n      영호\n    \n    \n      3\n      옥순\n      광수\n    \n    \n      4\n      옥순\n      상철\n    \n    \n      5\n      옥순\n      영수\n    \n  \n\n\n\n\n\nlrnr.get_preds(dl=dls.test_dl(df_new))\n\n\n\n\n\n\n\n\n(tensor([3.8813, 4.0784, 3.9177, 0.5294, 0.3296, 0.4189]), None)\n\n\n비교를 위해서\n\ndf_view\n\n\n\n\n\n  \n    \n      \n      영식\n      영철\n      영호\n      광수\n      상철\n      영수\n    \n  \n  \n    \n      옥순\n      3.9\n      4.1\n      NaN\n      0.5\n      0.3\n      NaN\n    \n    \n      영자\n      4.5\n      NaN\n      3.7\n      0.5\n      NaN\n      0.2\n    \n    \n      정숙\n      NaN\n      4.9\n      4.7\n      NaN\n      1.2\n      1.3\n    \n    \n      영숙\n      0.6\n      0.2\n      NaN\n      4.1\n      4.3\n      NaN\n    \n    \n      순자\n      0.7\n      0.9\n      NaN\n      4.2\n      NaN\n      3.9\n    \n    \n      현숙\n      NaN\n      0.2\n      0.3\n      NaN\n      3.5\n      3.4\n    \n  \n\n\n\n\n(정숙의 궁합)\n\ndf_new = pd.DataFrame({'X1':['정숙']*6, 'X2':['영식','영철','영호','광수','상철','영수']})\ndf_new\n\n\n\n\n\n  \n    \n      \n      X1\n      X2\n    \n  \n  \n    \n      0\n      정숙\n      영식\n    \n    \n      1\n      정숙\n      영철\n    \n    \n      2\n      정숙\n      영호\n    \n    \n      3\n      정숙\n      광수\n    \n    \n      4\n      정숙\n      상철\n    \n    \n      5\n      정숙\n      영수\n    \n  \n\n\n\n\n\nlrnr.get_preds(dl=dls.test_dl(df_new))\n\n\n\n\n\n\n\n\n(tensor([4.6639, 4.8092, 4.7264, 1.7349, 1.1538, 1.3598]), None)\n\n\n비교를 위해서\n\ndf_view\n\n\n\n\n\n  \n    \n      \n      영식\n      영철\n      영호\n      광수\n      상철\n      영수\n    \n  \n  \n    \n      옥순\n      3.9\n      4.1\n      NaN\n      0.5\n      0.3\n      NaN\n    \n    \n      영자\n      4.5\n      NaN\n      3.7\n      0.5\n      NaN\n      0.2\n    \n    \n      정숙\n      NaN\n      4.9\n      4.7\n      NaN\n      1.2\n      1.3\n    \n    \n      영숙\n      0.6\n      0.2\n      NaN\n      4.1\n      4.3\n      NaN\n    \n    \n      순자\n      0.7\n      0.9\n      NaN\n      4.2\n      NaN\n      3.9\n    \n    \n      현숙\n      NaN\n      0.2\n      0.3\n      NaN\n      3.5\n      3.4\n    \n  \n\n\n\n\n- Appedix: fastai 구조공부..\n\nlrnr.model\n\nEmbeddingDotBias(\n  (u_weight): Embedding(7, 2)\n  (i_weight): Embedding(7, 2)\n  (u_bias): Embedding(7, 1)\n  (i_bias): Embedding(7, 1)\n)\n\n\n\nlrnr.model.forward??\n\n\nSignature: lrnr.model.forward(x)\nDocstring:\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them.\nSource:   \n    def forward(self, x):\n        users,items = x[:,0],x[:,1]\n        dot = self.u_weight(users)* self.i_weight(items)\n        res = dot.sum(1) + self.u_bias(users).squeeze() + self.i_bias(items).squeeze()\n        if self.y_range is None: return res\n        return torch.sigmoid(res) * (self.y_range[1]-self.y_range[0]) + self.y_range[0]\nFile:      ~/anaconda3/envs/py37/lib/python3.7/site-packages/fastai/collab.py\nType:      method\n\n\n\n\n\nbias를 제외하면 우리가 짠 모형과 같음!"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-26-Extra-1.html#data",
    "href": "post/Lecture/STML/2023-04-26-Extra-1.html#data",
    "title": "15. Extra-1: 추천시스템",
    "section": "data",
    "text": "data\n- 예전에 살펴본 예제\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/main/posts/I.%20Overview/2022-09-08-rcmd_anal.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n      item_name\n    \n  \n  \n    \n      0\n      1\n      15\n      1.084308\n      홍차5\n    \n    \n      1\n      1\n      1\n      4.149209\n      커피1\n    \n    \n      2\n      1\n      11\n      1.142659\n      홍차1\n    \n    \n      3\n      1\n      5\n      4.033415\n      커피5\n    \n    \n      4\n      1\n      4\n      4.078139\n      커피4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      100\n      18\n      4.104276\n      홍차8\n    \n    \n      996\n      100\n      17\n      4.164773\n      홍차7\n    \n    \n      997\n      100\n      14\n      4.026915\n      홍차4\n    \n    \n      998\n      100\n      4\n      0.838720\n      커피4\n    \n    \n      999\n      100\n      7\n      1.094826\n      커피7\n    \n  \n\n1000 rows × 4 columns\n\n\n\n- 기억을 살리기 위해서..\n\ndf_view = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/main/posts/I.%20Overview/2022-09-08-rcmd_view.csv')\ndf_view\n\n\n\n\n\n  \n    \n      \n      커피1\n      커피2\n      커피3\n      커피4\n      커피5\n      커피6\n      커피7\n      커피8\n      커피9\n      커피10\n      홍차1\n      홍차2\n      홍차3\n      홍차4\n      홍차5\n      홍차6\n      홍차7\n      홍차8\n      홍차9\n      홍차10\n    \n  \n  \n    \n      0\n      4.149209\n      NaN\n      NaN\n      4.078139\n      4.033415\n      4.071871\n      NaN\n      NaN\n      NaN\n      NaN\n      1.142659\n      1.109452\n      NaN\n      0.603118\n      1.084308\n      NaN\n      0.906524\n      NaN\n      NaN\n      0.903826\n    \n    \n      1\n      4.031811\n      NaN\n      NaN\n      3.822704\n      NaN\n      NaN\n      NaN\n      4.071410\n      3.996206\n      NaN\n      NaN\n      0.839565\n      1.011315\n      NaN\n      1.120552\n      0.911340\n      NaN\n      0.860954\n      0.871482\n      NaN\n    \n    \n      2\n      4.082178\n      4.196436\n      NaN\n      3.956876\n      NaN\n      NaN\n      NaN\n      4.450931\n      3.972090\n      NaN\n      NaN\n      NaN\n      NaN\n      0.983838\n      NaN\n      0.918576\n      1.206796\n      0.913116\n      NaN\n      0.956194\n    \n    \n      3\n      NaN\n      4.000621\n      3.895570\n      NaN\n      3.838781\n      3.967183\n      NaN\n      NaN\n      NaN\n      4.105741\n      1.147554\n      NaN\n      1.346860\n      NaN\n      0.614099\n      1.297301\n      NaN\n      NaN\n      NaN\n      1.147545\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      3.888208\n      NaN\n      3.970330\n      3.979490\n      NaN\n      4.010982\n      NaN\n      0.920995\n      1.081111\n      0.999345\n      NaN\n      1.195183\n      NaN\n      0.818332\n      1.236331\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      0.511905\n      1.066144\n      NaN\n      1.315430\n      NaN\n      1.285778\n      NaN\n      0.678400\n      1.023020\n      0.886803\n      NaN\n      4.055996\n      NaN\n      NaN\n      4.156489\n      4.127622\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      96\n      NaN\n      1.035022\n      NaN\n      1.085834\n      NaN\n      0.812558\n      NaN\n      1.074543\n      NaN\n      0.852806\n      3.894772\n      NaN\n      4.071385\n      3.935935\n      NaN\n      NaN\n      3.989815\n      NaN\n      NaN\n      4.267142\n    \n    \n      97\n      NaN\n      1.115511\n      NaN\n      1.101395\n      0.878614\n      NaN\n      NaN\n      NaN\n      1.329319\n      NaN\n      4.125190\n      NaN\n      4.354638\n      3.811209\n      4.144648\n      NaN\n      NaN\n      4.116915\n      3.887823\n      NaN\n    \n    \n      98\n      NaN\n      0.850794\n      NaN\n      NaN\n      0.927884\n      0.669895\n      NaN\n      NaN\n      0.665429\n      1.387329\n      NaN\n      NaN\n      4.329404\n      4.111706\n      3.960197\n      NaN\n      NaN\n      NaN\n      3.725288\n      4.122072\n    \n    \n      99\n      NaN\n      NaN\n      1.413968\n      0.838720\n      NaN\n      NaN\n      1.094826\n      0.987888\n      NaN\n      1.177387\n      3.957383\n      4.136731\n      NaN\n      4.026915\n      NaN\n      NaN\n      4.164773\n      4.104276\n      NaN\n      NaN\n    \n  \n\n100 rows × 20 columns"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-26-Extra-1.html#모형",
    "href": "post/Lecture/STML/2023-04-26-Extra-1.html#모형",
    "title": "15. Extra-1: 추천시스템",
    "section": "모형",
    "text": "모형\n(편의상 바이어스를 제외하면)\n- 특징벡터:\n\n유저1의 취향 = [커피를 좋아하는 정도, 홍차를 좋아하는 정도]\n아이템1의 특징 = [커피의 특징, 홍차인 특징]\n\n- 평점\n\n유저1이 아이템1을 먹었을경우 평점: 유저1의 취향과 아이템1의 특징의 내적 = (유저1의 취향 \\(\\odot\\) 아이템1의 특징).sum()"
  },
  {
    "objectID": "post/Lecture/STML/2023-04-26-Extra-1.html#학습",
    "href": "post/Lecture/STML/2023-04-26-Extra-1.html#학습",
    "title": "15. Extra-1: 추천시스템",
    "section": "학습",
    "text": "학습\n(1) dls\n\ndls = CollabDataLoaders.from_df(df)\n\n\ndls.items\n\n\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n      item_name\n    \n  \n  \n    \n      86\n      9\n      15\n      1.004437\n      홍차5\n    \n    \n      254\n      26\n      10\n      3.794259\n      커피10\n    \n    \n      960\n      97\n      20\n      4.267142\n      홍차10\n    \n    \n      825\n      83\n      6\n      0.771229\n      커피6\n    \n    \n      574\n      58\n      19\n      4.016329\n      홍차9\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      264\n      27\n      1\n      4.042835\n      커피1\n    \n    \n      759\n      76\n      19\n      3.715771\n      홍차9\n    \n    \n      46\n      5\n      13\n      1.081111\n      홍차3\n    \n    \n      72\n      8\n      5\n      4.022260\n      커피5\n    \n    \n      211\n      22\n      9\n      3.709681\n      커피9\n    \n  \n\n800 rows × 4 columns\n\n\n\n(2) lrnr\n\nlrnr = collab_learner(dls,n_factors=2) # 교재에는 y_range 를 설정하도록 되어있지만 설정 안해도 적합에는 크게 상관없음..\n\n(3) fit\n\nlrnr.fit(10,0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      5.639192\n      2.679791\n      00:00\n    \n    \n      1\n      3.614457\n      1.906409\n      00:00\n    \n    \n      2\n      2.405080\n      0.399353\n      00:00\n    \n    \n      3\n      1.641215\n      0.217753\n      00:00\n    \n    \n      4\n      1.173295\n      0.130624\n      00:00\n    \n    \n      5\n      0.861192\n      0.096714\n      00:00\n    \n    \n      6\n      0.647233\n      0.078799\n      00:00\n    \n    \n      7\n      0.495189\n      0.077169\n      00:00\n    \n    \n      8\n      0.384622\n      0.071355\n      00:00\n    \n    \n      9\n      0.303257\n      0.071730\n      00:00\n    \n  \n\n\n\n(4) predict\n(적합된 값 확인)\n\nlrnr.show_results() # 누를때마다 결과다름\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      user\n      item\n      rating\n      rating_pred\n    \n  \n  \n    \n      0\n      16.0\n      3.0\n      4.199087\n      3.899566\n    \n    \n      1\n      90.0\n      11.0\n      3.780528\n      4.062635\n    \n    \n      2\n      46.0\n      4.0\n      4.196852\n      4.435931\n    \n    \n      3\n      8.0\n      1.0\n      4.194341\n      3.967788\n    \n    \n      4\n      80.0\n      10.0\n      0.943593\n      1.080764\n    \n    \n      5\n      61.0\n      19.0\n      4.160296\n      3.965987\n    \n    \n      6\n      63.0\n      8.0\n      0.772492\n      1.177593\n    \n    \n      7\n      82.0\n      5.0\n      0.916553\n      1.179026\n    \n    \n      8\n      13.0\n      18.0\n      1.012880\n      0.861018\n    \n  \n\n\n\n(예측값)\n\ndf_new = pd.DataFrame({'user':[1,1,1,1], 'item':[9,10,11,12]})\ndf_new\n\n\n\n\n\n  \n    \n      \n      user\n      item\n    \n  \n  \n    \n      0\n      1\n      9\n    \n    \n      1\n      1\n      10\n    \n    \n      2\n      1\n      11\n    \n    \n      3\n      1\n      12\n    \n  \n\n\n\n\n\nlrnr.get_preds(dl=dls.test_dl(df_new))\n\n\n\n\n\n\n\n\n(tensor([3.9750, 4.0915, 1.0473, 0.9544]), None)"
  },
  {
    "objectID": "post/Lecture/STML/2023-05-03-Extra-2.html",
    "href": "post/Lecture/STML/2023-05-03-Extra-2.html",
    "title": "16. Extra-2: 생성모형 (GAN)",
    "section": "",
    "text": "- 저자: 이안굿펠로우 - 천재임 - 지도교수가 요수아 벤지오\n- 논문 NIPS,\n- https://arxiv.org/abs/1406.2661\n\n\n\n만들수 없다면 이해하지 못한 것이다, 리처드 파인만 (천재 물리학자)\n\n- 사진속에 들어있는 동물이 개인지 고양이인지 맞출수 있는 기계와 개와 고양이를 그릴수 있는 기계중 어떤것이 더 시각적보에 대한 이해가 깊다고 볼수 있는가?\n- 진정으로 인공지능이 이미지를 이해했다면, 이미지를 만들수도 있어야 한다. \\(\\to\\) 이미지를 생성하는 모형을 만들어보자 \\(\\to\\) 성공\n\n\n\n\n- 내가 찍은 사진이 피카소의 화풍으로 표현된다면?\n- 퀸의 라이브에이드가 4k로 나온다면?\n- 1920년대 서울의 모습이 칼라로 복원된다면?\n- 딥페이크: 유명인의 가짜 포르노, 가짜뉴스, 협박(거짓기소)\n- 게임영상 (파이널판타지)\n- 거북이의 커버..\n- 너무 많아요…..\n\n\n\n\n제한된 정보만으로 어떤 문제를 풀 때, 그 과정에서 원래의 문제보다 일반적인 문제를 풀지 말고, 가능한 원래의 문제를 직접 풀어야한다. 배프닉 (SVM 창시자)\n\n- 이미지 \\(\\boldsymbol{x}\\)가 주어졌을 경우 라벨을 \\(y\\)라고 하자.\n- 이미지를 보고 라벨을 맞추는 일은 \\(p(y| \\boldsymbol{x})\\)에 관심이 있다.\n- 이미지를 생성하는 일은 \\(p(\\boldsymbol{x},y)\\)에 관심이 있는것이다.\n- 데이터의 생성확률 \\(p(\\boldsymbol{x},y)\\)을 알면 클래스의 사후확률 \\(p(y|\\boldsymbol{x})\\)를 알 수 있음. (아래의 수식 참고) 하지만 역은 불가능\n\\[p(y|x) = \\frac{p(x,y)}{p(x)} = \\frac{p(x,y)}{\\sum_{y}p(x,y)} \\]\n\n즉 이미지를 생성하는일은 분류문제보다 더 어려운 일이라 해석가능\n\n- 따라서 배프닉의 원리에 의하면 식별적 분류가 생성적 분류보다 바람직한 접근법이라 할 수 있음.\n- 하지만 다양한 현실문제에서 생성모형이 유용할때가 많다.\n\n\n\n- GAN은 생성모형중 하나임\n- GAN의 원리는 경찰과 위조지폐범이 서로 선의의(?) 경쟁을 통하여 서로 발전하는 모형으로 설명할 수 있다.\n\nThe generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.\n\n- 서로 적대적인(adversarial) 네트워크(network)를 동시에 학습시켜 가짜이미지를 만든다(generate)\n- 무식한 상황극..\n\n위조범: 가짜돈을 만들어서 부자가 되어야지! (가짜돈을 그림)\n경찰: (위조범이 만든 돈을 보고) 이건 가짜다!\n위조범: 걸렸군.. 더 정교하게 만들어야지..\n경찰: 이건 진짠가?… –> 상사에게 혼남. 그것도 구분못하냐고\n위조범: 더 정교하게 만들자..\n경찰: 더 판별능력을 업그레이드 하자!\n반복..\n\n- 굉장히 우수한 경찰조차도 진짜와 가짜를 구분하지 못할때(=진짜 이미지를 0.5의 확률로만 진짜라고 말할때 = 가짜 이미지를 0.5의 확률로만 가짜라고 말할때) 학습을 멈춘다."
  },
  {
    "objectID": "post/Lecture/STML/2023-05-03-Extra-2.html#import",
    "href": "post/Lecture/STML/2023-05-03-Extra-2.html#import",
    "title": "16. Extra-2: 생성모형 (GAN)",
    "section": "import",
    "text": "import\n\nimport torch \nfrom fastai.vision.all import *"
  },
  {
    "objectID": "post/Lecture/STML/2023-05-03-Extra-2.html#data",
    "href": "post/Lecture/STML/2023-05-03-Extra-2.html#data",
    "title": "16. Extra-2: 생성모형 (GAN)",
    "section": "data",
    "text": "data\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\nthrees = (path/'train'/'3').ls()\n\n\nX = torch.stack([tensor(Image.open(i)) for i in threes]).float()/255\n\n\nX.shape\n\ntorch.Size([6131, 28, 28])\n\n\n\nplt.imshow(X[0])\n\n<matplotlib.image.AxesImage at 0x7f981c427ac0>\n\n\n\n\n\n- MLP를 이용해 학습하기 위해 X의 차원을 변경\n\nX=X.reshape(6131,28*28)\nX.shape\n\ntorch.Size([6131, 784])"
  },
  {
    "objectID": "post/Lecture/STML/2023-05-03-Extra-2.html#위조지폐범의-설계-noise-to-가짜이미지를-만들어-내는-네트워크를-만들자.",
    "href": "post/Lecture/STML/2023-05-03-Extra-2.html#위조지폐범의-설계-noise-to-가짜이미지를-만들어-내는-네트워크를-만들자.",
    "title": "16. Extra-2: 생성모형 (GAN)",
    "section": "위조지폐범의 설계: noise \\(\\to\\) 가짜이미지를 만들어 내는 네트워크를 만들자.",
    "text": "위조지폐범의 설계: noise \\(\\to\\) 가짜이미지를 만들어 내는 네트워크를 만들자.\n- 네트워크의 입력? 적당한 벡터, 혹은 매트릭스에 노이즈(랜덤으로 채운 어떠한 숫자들)를 채운것\n- 네트워크의 출력? (28,28)의 텐서, 784의 벡터\n\nnet1 = torch.nn.Sequential(torch.nn.Linear(in_features=28, out_features=64),\n                           torch.nn.ReLU(),\n                           torch.nn.Linear(in_features=64, out_features=64), \n                           torch.nn.ReLU(),\n                           torch.nn.Linear(in_features=64, out_features=784),\n                           torch.nn.Sigmoid()) ## 마지막의 시그모이드는 출력이 0~1사이로 나오게 하기 위함 \ncounterfeiter = net1"
  },
  {
    "objectID": "post/Lecture/STML/2023-05-03-Extra-2.html#경찰의-설계-진짜이미지는-1-가짜이미지는-0으로-판별하는-dnn을-만들자.",
    "href": "post/Lecture/STML/2023-05-03-Extra-2.html#경찰의-설계-진짜이미지는-1-가짜이미지는-0으로-판별하는-dnn을-만들자.",
    "title": "16. Extra-2: 생성모형 (GAN)",
    "section": "경찰의 설계: 진짜이미지는 1, 가짜이미지는 0으로 판별하는 DNN을 만들자.",
    "text": "경찰의 설계: 진짜이미지는 1, 가짜이미지는 0으로 판별하는 DNN을 만들자.\n- 네트워크의 입력? (28,28)의 텐서, 혹은 784의 벡터\n- 네트워크의 출력? yhat (y는 0 or 1)\n\nnet2 = torch.nn.Sequential(torch.nn.Linear(in_features=784,out_features=64),\n                           torch.nn.ReLU(),\n                           torch.nn.Linear(in_features=64,out_features=28),\n                           torch.nn.ReLU(),\n                           torch.nn.Linear(in_features=28,out_features=1),\n                           torch.nn.Sigmoid()\n                           )\npolice = net2"
  },
  {
    "objectID": "post/Lecture/STML/2023-05-03-Extra-2.html#스토리전개",
    "href": "post/Lecture/STML/2023-05-03-Extra-2.html#스토리전개",
    "title": "16. Extra-2: 생성모형 (GAN)",
    "section": "스토리전개",
    "text": "스토리전개\n- 아래는 진짜이미지\n\nrealimage=X[0].reshape(28,28)\nplt.imshow(realimage)\n\n<matplotlib.image.AxesImage at 0x7f981d1e6190>\n\n\n\n\n\n- 위와 같은 진짜 이미지를 경찰이 봤음 \\(\\to\\) yhat이 나오겠죠?\n\npolicehat_from_realimage = police(realimage.reshape(-1))\npolicehat_from_realimage\n\ntensor([0.5241], grad_fn=<SigmoidBackward0>)\n\n\n\n진짜 이미지일수록 policehat_from_realimage \\(\\approx\\) 1 이어야 함\n하지만 그렇지 못함 (배운것이 없는 무능한 경찰)\n\n- 이번에는 가짜이미지를 경찰이 봤다고 생각해보자.\n(step1) 랜덤으로 아무숫자나 28개를 생성한다.\n\nerr= torch.randn(28)\nerr\n\ntensor([-0.8568, -2.2982,  0.3489, -1.5801,  1.8611,  0.9158, -0.2985,  1.1087,\n         1.3798,  1.4870, -0.4455,  0.2140,  1.0784,  1.4805, -1.4886,  0.5036,\n         0.3742, -0.7351,  1.0031,  0.4538,  1.5372,  1.0739,  0.7793, -1.5347,\n        -0.5060,  0.0113,  0.2833, -0.8466])\n\n\n(step2) 위조범은 err를 입력으로 받고 가짜이미지를 만든다.\n\ncouterfeiter_output = counterfeiter(err)\nfakeimage=couterfeiter_output.reshape(28,28)\nplt.imshow(fakeimage.detach())\n\n<matplotlib.image.AxesImage at 0x7f981f4261c0>\n\n\n\n\n\n\n누가봐도 가짜자료임\n위조범의 실력이 형편없음\n\n(step3) 위조범이 생성한 이미지를 경찰한테 넘긴다.\n\npolicehat_from_fakeimage = police(couterfeiter_output)\n#policehat_from_fakeimage = police(fakeimage.detach().reshape(-1))\npolicehat_from_fakeimage\n\ntensor([0.5290], grad_fn=<SigmoidBackward0>)\n\n\n- 경찰의 실력도 형편없고 위조범의 실력도 형편없다."
  },
  {
    "objectID": "post/Lecture/STML/2023-05-03-Extra-2.html#경찰네트워크의-실력을-향상시키자.",
    "href": "post/Lecture/STML/2023-05-03-Extra-2.html#경찰네트워크의-실력을-향상시키자.",
    "title": "16. Extra-2: 생성모형 (GAN)",
    "section": "경찰네트워크의 실력을 향상시키자.",
    "text": "경찰네트워크의 실력을 향상시키자.\n- 데이터 정리 - 원래 \\(n=6131\\)개의 이미지 자료가 있음. 이를 \\({\\bf X}\\)라고 하자. 따라서 \\({\\bf X}\\)의 차원은 (6131,784). - 위조범이 만든 가짜자료를 원래 자료와 같은 숫자인 6131개 만듬. 이 가짜자료를 \\(\\tilde{\\bf X}\\)라고 하자. 따라서 \\(\\tilde{\\bf X}\\)의 차원은 (6131,784). - 진짜자료는 1, 가짜자료는 0으로 라벨링.\n\nX.shape\n\ntorch.Size([6131, 784])\n\n\n\nerr= torch.randn(6131,28)\ncounterfeiter_output = counterfeiter(err) # counterfeiter_output를 Xtilde로 생각하면 된다. \n\n\nreal_label=torch.tensor([[1.0]]*6131) ## y=1 \nfake_label=torch.tensor([[0.0]]*6131) ## y=0\n\n- step1: yhat, 경찰의 예측\n\npolicehat_from_realimage = police(X) \npolicehat_from_fakeimage = police(counterfeiter_output) \n\n- step2: 손실함수? 경찰의 미덕은 (1) 가짜를 가짜라고 하고 (2) 진짜를 진짜라 해야한다.\n\nloss_fn = torch.nn.BCELoss() \n\n\nloss_of_police =\\\nloss_fn(policehat_from_fakeimage,fake_label)+\\\nloss_fn(policehat_from_realimage,real_label)\n\nloss_of_police\n\ntensor(1.4023, grad_fn=<AddBackward0>)\n\n\n- step3~4는 미분이후 업데이트\n- 옵티마이저를 설계하자.\n\noptimizer_of_police = torch.optim.Adam(police.parameters())\n\n- for 문을 돌리자.\n\nfor i in range(50): \n    ## 1 yhat \n    policehat_from_realimage = police(X) \n    \n    #policehat_from_fakeimage = police(Xitlde)\n    err= torch.randn(6131,28)\n    counterfeiter_output = counterfeiter(err) # counterfeiter_output를 Xtilde로 생각하면 된다. \n    policehat_from_fakeimage= police(counterfeiter_output)\n    \n    ## 2 loss \n    loss_of_police =\\\n    loss_fn(policehat_from_fakeimage,fake_label)+\\\n    loss_fn(policehat_from_realimage,real_label)\n    \n    ## 3 back propagation \n    loss_of_police.backward()\n    \n    ## 4 update\n    optimizer_of_police.step()\n    optimizer_of_police.zero_grad()\n\n- 훈련된 경찰의 성능을 살펴보자.\n\npolice(counterfeiter_output)\n\ntensor([[0.0061],\n        [0.0061],\n        [0.0061],\n        ...,\n        [0.0062],\n        [0.0061],\n        [0.0061]], grad_fn=<SigmoidBackward0>)\n\n\n\npolice(X)\n\ntensor([[0.9999],\n        [0.9989],\n        [0.9995],\n        ...,\n        [0.9969],\n        [0.9995],\n        [0.9973]], grad_fn=<SigmoidBackward0>)\n\n\n- 우수한 경찰 (비록 위조범의 수준이 낮긴하지만)"
  },
  {
    "objectID": "post/Lecture/STML/2023-05-03-Extra-2.html#위조범네트워크의-성능을-향상시키자.",
    "href": "post/Lecture/STML/2023-05-03-Extra-2.html#위조범네트워크의-성능을-향상시키자.",
    "title": "16. Extra-2: 생성모형 (GAN)",
    "section": "위조범네트워크의 성능을 향상시키자.",
    "text": "위조범네트워크의 성능을 향상시키자.\n- 자료구조: X는 임의의 에러이미지, net(X)는 fakeimage\n\nerr=torch.randn(6131,28) \ncounterfeiter_output= counterfeiter(err) \n\n- 손실함수: 잘 훈련된 경찰조차도 잘못된 판단을 내릴만큼 가짜지폐를 잘 만들면 위조범의 실력이 우수하다 볼 수 있음\n\npolicehat_from_fakeimage = police(counterfeiter_output) \nloss_of_counterfeiter = loss_fn(policehat_from_fakeimage,real_label) ## 가짜이미지를 보고 경찰이 진짜라고 믿으면 위조범의 실력이 좋은것임  \n\n- 옵티마이저\n\noptimizer_of_counterfeiter = torch.optim.Adam(counterfeiter.parameters())\n\n- 학습\n\nfor i in range(50): \n    ## 1 \n    err=torch.randn(6131,28) \n    counterfeiter_output= counterfeiter(err)  \n    policehat_from_fakeimage = police(counterfeiter_output) \n    ## 2 \n    loss_of_counterfeiter = loss_fn(policehat_from_fakeimage,real_label)\n    ## 3 \n    loss_of_counterfeiter.backward()\n    ## 4 \n    optimizer_of_counterfeiter.step()\n    optimizer_of_counterfeiter.zero_grad()\n\n- 위조범의 실력향상을 감상해보자.\n\nplt.imshow(counterfeiter_output[0].reshape(28,28).data)\n\n<matplotlib.image.AxesImage at 0x7f981397ae20>"
  },
  {
    "objectID": "post/Lecture/STML/2023-05-03-Extra-2.html#두-적대적-네트워크를-경쟁시키자.",
    "href": "post/Lecture/STML/2023-05-03-Extra-2.html#두-적대적-네트워크를-경쟁시키자.",
    "title": "16. Extra-2: 생성모형 (GAN)",
    "section": "두 적대적 네트워크를 경쟁시키자.",
    "text": "두 적대적 네트워크를 경쟁시키자.\n\nfor k in range(100): \n    for i in range(50): \n        ## 1 yhat \n        policehat_from_realimage = police(X) \n    \n        #policehat_from_fakeimage = police(Xitlde)\n        err= torch.randn(6131,28)\n        counterfeiter_output = counterfeiter(err) # counterfeiter_output를 Xtilde로 생각하면 된다. \n        policehat_from_fakeimage= police(counterfeiter_output)\n    \n        ## 2 loss \n        loss_of_police =\\\n        loss_fn(policehat_from_fakeimage,fake_label)+\\\n        loss_fn(policehat_from_realimage,real_label)\n    \n        ## 3 back propagation \n        loss_of_police.backward()\n    \n        ## 4 update\n        optimizer_of_police.step()\n        optimizer_of_police.zero_grad()\n        \n    for i in range(50): \n        ## 1 \n        err=torch.randn(6131,28) \n        counterfeiter_output= counterfeiter(err)  \n        policehat_from_fakeimage = police(counterfeiter_output) \n        ## 2 \n        loss_of_counterfeiter = loss_fn(policehat_from_fakeimage,real_label)\n        ## 3 \n        loss_of_counterfeiter.backward()\n        ## 4 \n        optimizer_of_counterfeiter.step()\n        optimizer_of_counterfeiter.zero_grad()        \n\n- 위조범의 최종적 실력향상감상\n\nplt.imshow(counterfeiter_output[0].reshape(28,28).data)\n\n<matplotlib.image.AxesImage at 0x7f9813856970>"
  }
]