[
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n- 전북대학교 통계학과 학사(부전공: 컴퓨터공학) 졸업 | 2015. 03 ~ 2021. 02\n- 전북대학교 통계학과 석사 졸업예정 | 2021. 03 ~ 2023. 02"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n- 국민연금공단 빅데이터부 현장실습 | 2020. 03 ~ 2020. 06\n- 지역 문화산업 융복합 데이터 전문가 과정 | 과학기술정보통신부, 한국데이터산업진흥원 | 2021. 06 ~ 2021. 08\n- 에너지AI융합대학원 빅데이터 분석 특강 조교 | 전북대학교 AI에너지 융합대학원 |2021. 06 ~ 2021. 10\n- SPSS를 이용한 통계자료분석 특강 조교 | 전북대학교 통계학과 | 2022. 01 ~ 2022. 02\n- 데이터 준전문가 ADsP 특강 조교 | 전북대학교 통계학과 | 2022. 01 ~ 2022. 02"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About Me",
    "section": "Publications",
    "text": "Publications\n- 데이터 분석을 통한 지역별 고령친화도 시각화\n`-` 김영선, 강민구, 이강철 등  | 문화융복합아카이빙연구소 | 2021. 10 | 기록관리/보존 \n- 핵심어 추출 및 데이터 증강기법을 이용한 텍스트 분류 모델 성능 개선\n`-` 이강철, 안정용 | 한국자료분석학회 | 한국자료분석학회 | 2022. 10 | 통계학"
  },
  {
    "objectID": "about.html#certificate",
    "href": "about.html#certificate",
    "title": "About Me",
    "section": "Certificate",
    "text": "Certificate\n- 워드프로세서 | 대한상공회의소 | 19-19-017981 | 2019. 08. 30\n- 데이터분석준전문가(ADsP) | 한국데이터진흥원 | ADsP-0223898 | 2019. 10. 01\n- 사회조사분석사 2급 | 한국산업인력공단 | 19201142418N | 2019. 10. 01"
  },
  {
    "objectID": "about.html#post",
    "href": "about.html#post",
    "title": "About Me",
    "section": "Post",
    "text": "Post\n- Statistical Computing | 2021. 03 ~ 2021. 06\n- Theoritical Statistics | 2021. 03 ~ 2021. 06\n- R for Data Science | 2021. 06 ~\n- Data Visualization | 2021. 09 ~ 2021. 12\n- Time Series Analysis | 2021. 09 ~ 2021. 12\n- BigData Analysis | 2022. 08 ~\n- ML with Pytorch | 2023. 01 ~"
  },
  {
    "objectID": "about.html#conctact",
    "href": "about.html#conctact",
    "title": "About Me",
    "section": "Conctact",
    "text": "Conctact\n- rkdcjf8232@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gang Cheol Portfolio",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nFeb 10, 2023\n\n\npart 5\n\n\nGANGCHEOL LEE\n\n\n\n\nFeb 9, 2023\n\n\npart 4\n\n\nGANGCHEOL LEE\n\n\n\n\nJan 27, 2023\n\n\n0. Intro\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 13, 2022\n\n\n14. Final term\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 30, 2022\n\n\n13. model fitting\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 23, 2022\n\n\n13. CNN-2\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 11, 2022\n\n\n12. Max pooling, CNN\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 8, 2022\n\n\n11. softmax\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 2, 2022\n\n\n10. 확률적 경사하강법\n\n\nGANGCHEOL LEE\n\n\n\n\nMay 1, 2022\n\n\n09. MLE\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 27, 2022\n\n\n08. Mid term\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 18, 2022\n\n\n07. Adam\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 11, 2022\n\n\n06. layer 과제\n\n\nGANGCHEOL LEE\n\n\n\n\nApr 11, 2022\n\n\n06. layer\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 30, 2022\n\n\n05. 경사하강법-2\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 28, 2022\n\n\n04. 경사하강법-1\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 21, 2022\n\n\n03. Tensorflow-2\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 14, 2022\n\n\n02. Tensorflow-1\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 8, 2022\n\n\n01. 단순선형회귀\n\n\nGANGCHEOL LEE\n\n\n\n\nDec 5, 2021\n\n\n01. 결측치 처리\n\n\nGANGCHEOL LEE\n\n\n\n\nDec 5, 2021\n\n\n03. 날짜 데이터\n\n\nGANGCHEOL LEE\n\n\n\n\nDec 5, 2021\n\n\n02. 표준화와 정규화\n\n\nGANGCHEOL LEE\n\n\n\n\nJul 24, 2021\n\n\n01. text mining\n\n\nGANGCHEOL LEE\n\n\n\n\nJul 11, 2021\n\n\n02. unsupervised learning\n\n\nGANGCHEOL LEE\n\n\n\n\nJul 10, 2021\n\n\n01. Supervised Learning\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 28, 2021\n\n\n11. Time-seris analysis\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 24, 2021\n\n\n10. Multidimensional scaling\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 21, 2021\n\n\n09. Factor Analysis\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 17, 2021\n\n\n08. PCA\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 16, 2021\n\n\n07. regression analysis-2\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 15, 2021\n\n\n06. regression analysis\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 9, 2021\n\n\n05. cor-test\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 7, 2021\n\n\n04. chi-square test\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 5, 2021\n\n\n03. ANOVA\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 3, 2021\n\n\n02. T-test\n\n\nGANGCHEOL LEE\n\n\n\n\nJun 1, 2021\n\n\n01. sampling\n\n\nGANGCHEOL LEE\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/Bigdata Analysis/2020-04-12-(6주차) 과제.html",
    "href": "post/Bigdata Analysis/2020-04-12-(6주차) 과제.html",
    "title": "06. layer 과제",
    "section": "",
    "text": "- 케라스를 이용하여 아래를 만족하는 적절한 \\(\\beta_0\\)와 \\(\\beta_1\\)을 구하라. 적합결과를 시각화하라. (애니메이션 시각화 X)\npython\n\\[\\hat {y} \\approx 2.5+ 4e^{-x}  \\]"
  },
  {
    "objectID": "post/Bigdata Analysis/2020-04-12-(6주차) 과제.html#solution",
    "href": "post/Bigdata Analysis/2020-04-12-(6주차) 과제.html#solution",
    "title": "06. layer 과제",
    "section": "Solution",
    "text": "Solution\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nnp.random.seed(43052) \nN= 100\nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) +epsilon\n\n\nX= np.stack([np.ones(N),np.exp(-x)],axis=1)\ny= y.reshape(N,1)\n\n\nnet = tf.keras.Sequential()\n\n\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\nnet.compile(tf.keras.optimizers.SGD(0.1), loss='mse') \n\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7f77b5e4c250>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_8/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.4630797],\n        [3.996811 ]], dtype=float32)>]\n\n\n\nbeta_hat = net.weights\ny_hat = (X @ beta_hat).reshape(-1)\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt \n\n\nplt.plot(x,y,\".\")\nplt.plot(x,y_hat,\"--\")"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-08-(1주차).html#모형의-매트릭스화",
    "href": "post/Bigdata Analysis/2022-03-08-(1주차).html#모형의-매트릭스화",
    "title": "01. 단순선형회귀",
    "section": "모형의 매트릭스화",
    "text": "모형의 매트릭스화\n\n모형을 행렬로 표현하면 변수가 여러개인 multiple linear regression 에서도 단순형태로 표현이 가능하다.\n\n우리의 모형은 아래와 같다.\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i=1,2,\\dots,10\\)\n풀어서 쓰면\n\\(\\begin{cases} y_1 = \\beta_0 +\\beta_1 x_1 + \\epsilon_1 \\\\ y_2 = \\beta_0 +\\beta_1 x_2 + \\epsilon_2 \\\\ \\dots \\\\ y_{10} = \\beta_0 +\\beta_1 x_{10} + \\epsilon_{10} \\end{cases}\\)\n아래와 같이 쓸 수 있다.\n$\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\dots \\\\\ny_{10}\n\\end{bmatrix}\\]\n=\n\\[\\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\dots & \\dots \\\\\n1 & x_{10}\n\\end{bmatrix}\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\]\n\n\\[\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\dots \\\\\n\\epsilon_{10}\n\\end{bmatrix}\\]\n$\n\n벡터와 매트릭스 형태로 정리하면\n\\({\\bf y} = {\\bf X} {\\boldsymbol \\beta} + \\boldsymbol{\\epsilon}\\)\n- 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다.\n\\(loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2\\)\n이것을 벡터표현으로 하면 아래와 같다.\n\\(loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\)\n풀어보면\n\\(loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\)\n- 미분하는 과정의 매트릭스화\nloss를 최소화하는 \\({\\boldsymbol \\beta}\\)를 구해야하므로 loss를 \\({\\boldsymbol \\beta}\\)로 미분한식을 0이라고 놓고 풀면 된다.\n\\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} loss = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\)\n$= 0 - {}^- {}^ + 2{}^ $\n따라서 \\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}loss=0\\)을 풀면 아래와 같다.\n$= ({}){-1}{}^ $\n- 공식도 매트릭스로 표현하면 : \\(\\left(\\boldsymbol{\\hat\\beta}= ({\\bf X}^\\top {\\bf X})^{-1}{\\bf X}^\\top {\\bf y}\\right) \\leftarrow\\) 외우자 이건..\n- 적용을 해보자.\n(X를 만드는 방법1)\n\nX=tf.transpose(tf.concat([[[1.0]*10],[x]],0)) # \nX \n\n<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[ 1. , 20.1],\n       [ 1. , 22.2],\n       [ 1. , 22.7],\n       [ 1. , 23.3],\n       [ 1. , 24.4],\n       [ 1. , 25.1],\n       [ 1. , 26.2],\n       [ 1. , 27.3],\n       [ 1. , 28.4],\n       [ 1. , 30.4]], dtype=float32)>\n\n\n(X를 만드는 방법2)\n\ntf.concat([[[1.0]*10],[x]],0)\n\n<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\narray([[ 1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ],\n       [20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]],\n      dtype=float32)>\n\n\n\ntf.concat([[[1.0]*10],[x]],0).T\n\nAttributeError: \n        'EagerTensor' object has no attribute 'T'.\n        If you are looking for numpy-related methods, please run the following:\n        from tensorflow.python.ops.numpy_ops import np_config\n        np_config.enable_numpy_behavior()\n\n\n\n위 처럼 하면 error가 남\nerror를 읽어보면 numpy 스타일로 구성하고 싶을 경우 np_config.enable_numpy_behavior() 을 이용하라는 문구가 나옴\n\n\nfrom tensorflow.python.ops.numpy_ops import np_config\nnp_config.enable_numpy_behavior()\n\n\nX = tf.concat([[[1.0]*10],[x]],0).T\n\n오 이제된다.\n\nX\n\n<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[ 1. , 20.1],\n       [ 1. , 22.2],\n       [ 1. , 22.7],\n       [ 1. , 23.3],\n       [ 1. , 24.4],\n       [ 1. , 25.1],\n       [ 1. , 26.2],\n       [ 1. , 27.3],\n       [ 1. , 28.4],\n       [ 1. , 30.4]], dtype=float32)>\n\n\n\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([9.945015 , 2.2156935], dtype=float32)>\n\n\n결과를 보면 $ (_0,_1) = (9.94…,2.21…)$ 로 산출되었다.\n- 잘 구해진다.\n- 그런데..\n\nbeta0_estimated,beta1_estimated\n\n(<tf.Tensor: shape=(), dtype=float32, numpy=9.94458>,\n <tf.Tensor: shape=(), dtype=float32, numpy=2.2157042>)\n\n\n값이 좀 다르다..?\n- 같은 값입니다! 신경쓰지 마세요! 텐서플로우가 좀 대충계산합니다.\n- 실제로 조금 더 정확히 계산하기 위해서는 tensorflow 안에 내장된 numpy 를 사용한다.\n\nimport tensorflow.experimental.numpy as tnp \n\n\nx=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) \ny=10.2 + 2.2*x + epsilon \n\n\nbeta1_estimated = sum((x-sum(x)/10)*(y-sum(y)/10)) / sum((x-sum(x)/10)**2)\nbeta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 \n\n\nbeta0_estimated, beta1_estimated\n\n(<tf.Tensor: shape=(), dtype=float64, numpy=9.944573294798559>,\n <tf.Tensor: shape=(), dtype=float64, numpy=2.2157046054834106>)\n\n\n\nX=tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\n<tf.Tensor: shape=(2,), dtype=float64, numpy=array([9.94457329, 2.21570461])>"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-08-(1주차).html#회귀계수-추정",
    "href": "post/Bigdata Analysis/2022-03-08-(1주차).html#회귀계수-추정",
    "title": "01. 단순선형회귀",
    "section": "회귀계수 추정",
    "text": "회귀계수 추정\n\\[Loss = \\sum (y-\\beta_0-\\beta_1 x)\\]\n\n\\(\\beta_0\\) 추정\n\n\\[L=Loss = \\sum (y-\\beta_0-\\beta_1 x)\\]\n\\[\\frac {d L}{d \\beta_0} = -2\\sum(y-\\beta_0-\\beta_1x) = 0\\]\n\\[\\therefore\\,\\,\\hat \\beta_0 = \\bar y -\\beta_1 \\bar x\\]\n\n\\(\\beta_1\\) 추정\n\n\\[\\begin{align}\\frac {d L}{d \\beta_1} &= \\sum x \\left(y-\\beta_0-\\beta_1x\\right) \\\\ \\\\ \\nonumber\n                                        & = \\left (\\sum xy -\\beta_1 x^2\\right )- n\\bar x\\left (\\bar y -\\beta_1\\bar x \\right) \\\\ \\\\ \\nonumber\n                                         &= \\left (\\sum xy -\\bar x \\bar y\\right ) -\\beta_1 \\left (\\sum x^2 - (\\bar x)^2\\right )\\nonumber\\end{align}\\]\n\\[\\begin{align}\n\\therefore \\hat {\\beta_1} &=  \\frac{\\sum xy -\\bar x \\bar y}{\\sum x^2 - (\\bar x)^2}  \\nonumber \\\\ \\\\\n&= \\frac {S_{xy}}{S_{xx}}  \\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-08-(1주차).html#벡터-미분-매트릭스-미분",
    "href": "post/Bigdata Analysis/2022-03-08-(1주차).html#벡터-미분-매트릭스-미분",
    "title": "01. 단순선형회귀",
    "section": "벡터 미분 / 매트릭스 미분",
    "text": "벡터 미분 / 매트릭스 미분\n\\[L=loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\]\n\n벡터 미분\n\\[\\begin{align}  x^{\\top}y &= \\begin{bmatrix}x_1\\dots x_n\\end{bmatrix}\\begin{bmatrix} y_1   \\\\ \\dots \\\\ y_n\\end{bmatrix}\\nonumber  \\\\  \\\\\n      &=  x_1y_1 + x_2y_2+\\dots x_ny_n\\nonumber  \\end{align}\\]\n위를 미분하면\n\\[\\begin{align} \\frac {d x^{\\top}y}{d x} &= \\, \\begin{bmatrix} \\frac {d}{d x_1}  \\\\\n                                                                                       \\dots \\\\ \\frac {d}{d x_n}\n                                                                                        \\end{bmatrix} ( x_1y_1 + x_2y_2+\\dots x_ny_n) \\nonumber \\\\ \\\\  \n                                                         &=  \\, \\begin{bmatrix} y_1   \\\\ \\dots \\\\ y_n\\end{bmatrix}=y                         \\nonumber  \\end{align}\\]\n\n\n벡터 미분의 다른풀이\n\n(1)\n\\[ \\frac {d x^{\\top}y}{d x} = \\left (\\frac {d x^{\\top}}{d x}\\right) y=y\\]\n\\[\\begin{align}\\left( \\frac {d x^{\\top}}{d x} \\right)  = \\begin{bmatrix} \\frac {d}{d x_1} \\\\  \\dots \\\\ \\frac {d}{d x_{n}} \\end {bmatrix} \\begin{bmatrix} x_1 \\dots x_n \\end{bmatrix} =\\begin{bmatrix}\\frac {d x_1}{d x_1} & \\dots & \\frac {d x_n}{d x_1} \\\\\n                                                                         \\dots & \\dots  &\\dots  \\\\\n                                                                        \\frac {d x_1}{d x_n} & \\dots & \\frac {d x_n}{d x_n}     \\end{bmatrix} = \\mathbf{I} \\nonumber \\end{align}\\]\n\\[\\therefore \\quad \\frac {d x^{\\top}y}{d x} = \\left (\\frac {d x^{\\top}}{d x}\\right) y = \\mathbf{I} y = y\\]\n\n\n(2)\n\\[ \\frac {d y^{\\top}x}{d x} = \\left (\\frac {d y^{\\top}x}{d x}\\right) =y \\]\n$ y^{} x $는 \\(1 \\times 1\\) 차원이므로 인간이면 이해할 수 있을 듯?\n\n\n(3)\n\\[\\frac {d}{d \\boldsymbol{\\beta}} \\left ( \\mathbf{y^{\\top}X\\boldsymbol \\beta}\\right ) = \\mathbf{X^{\\top}y}\\]\n\\(\\mathbf{y^{\\top}X\\boldsymbol \\beta}\\) 는 \\(1 \\times 1\\) 인 스칼라 이므로\n\\[\\mathbf{y^{\\top}X\\boldsymbol \\beta} = \\left (\\mathbf{y^{\\top}X\\boldsymbol \\beta}\\right)^{\\top} = \\boldsymbol \\beta^{\\top}\\mathbf{X^{\\top} y}\\]\n따라서\n\\[\\begin{align} \\frac {d}{d \\boldsymbol{\\beta}} \\left ( \\mathbf{y^{\\top}X\\boldsymbol \\beta}\\right ) &= \\frac {d}{d \\boldsymbol{\\beta}} \\left (\\boldsymbol \\beta^{\\top}\\mathbf{X^{\\top} y}\\right )\\nonumber \\\\ \\\\\n                                                  &= \\left ( \\frac{d}{d \\boldsymbol{\\beta}}\\boldsymbol{\\beta}^{\\top}\\right)\\mathbf{X^{\\top y}} \\nonumber\\\\ \\\\\n                                                    &= \\mathbf{I\\,X^{\\top}y} \\nonumber \\\\ \\\\ &= \\mathbf{X^{\\top}y}    \\nonumber      \\end{align}\\]\n\n\n(4)\n\\[ \\frac {d }{d \\mathbf {y}} (\\mathbf{y^{\\top}y}) = 2\\mathbf{y}\\]\n\\[ d\\, \\mathbf{y} = \\left [\\frac {d}{y_1}, \\,\\frac {d}{y_2}\\dots\\dots \\frac {d}{y_n}\\right ]\\]\n\\[\\mathbf{y^{\\top}y} =  \\sum{y_i}^2\\]\n\\[\\therefore  \\quad \\frac {d }{d \\mathbf {y}} (\\mathbf{y^{\\top}y}) = 2\\mathbf{y}\\]\n\n\n참고할 틀린풀이\n\n아래와 같은 풀이는 1번의 벡터 미분의 다른 풀이처럼 풀면 안된다.\n\n\\[ \\frac {d }{d \\mathbf {y}} (\\mathbf{y^{\\top}y}) = \\mathbf{y}\\]\n\\[ \\frac {d\\,\\mathbf{y^{\\top}y}}{d \\,\\mathbf{y}} = \\left (\\frac {d\\, \\mathbf{y^{\\top}y}}{d\\, \\mathbf y}\\right) \\mathbf y= \\mathbf{I} y \\neq y\\]\nbecause 스칼라 경우를 생각해보자\n(틀린풀이 )\\(\\quad \\frac {d }{d\\,y} y^2 = \\left (\\frac {d}{d\\,y } yy \\right) = y\\) ?\n(올바른 풀이) \\(\\quad \\frac {d }{d\\,y} y^2 = \\left (\\frac {d}{d\\,y } y_1 \\right)+ \\left (\\frac {d}{d\\,y } y_2 \\right) = 2y\\)\n스칼라를 예제로 들었는데 벡터에서 이런 느낌이라고 생각하자 이게 표준적으로 사용되는 설명은 아니지만 이해적? 으로는 간편한 듯\n다른풀이 (3)번의 경우도 원래는 안되는데 값이 스칼라 이므로 틀린풀이 처럼 안되는 경우이나 1 x 1 행렬이므로 가능한 것이다.\n다시 벡터로 돌아오면\n(올바른 풀이) \\(\\quad \\left ( \\frac {d}{d\\,\\mathbf{y}} (\\mathbf{y^{\\top}y})\\right ) = A + B\\)\n\\(A = f(\\mathbf y)\\) , \\(B = g(\\mathbf y)\\) 라고 생각하자\n$ A =( ) = = $\n\\(B\\) 의 경우도 위와 동일하므로\n\\[\\frac {d\\,(A+B)}{d\\,\\mathbf{y}} = \\{f(\\mathbf {y})\\}^{\\prime} +\\{g(\\mathbf {y})\\}^{\\prime} = 2\\mathbf{y}\\]\n\n\n(5)\n\\[\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta} = 2\\mathbf{X^{\\top}X}\\boldsymbol \\beta\\]\n4번의 원리를 이용하면 이지이지"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-08-(1주차).html#loss를-미분",
    "href": "post/Bigdata Analysis/2022-03-08-(1주차).html#loss를-미분",
    "title": "01. 단순선형회귀",
    "section": "loss를 미분",
    "text": "loss를 미분\n\\[L=loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\]\n\\[L = {\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\]\n\\(L\\) 을 미분하면\n\\[\\begin{align} \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} L &= \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta} \\nonumber \\\\ \\\\\n            &=  0 - \\mathbf{X^{\\top}y} - \\mathbf{X^{\\top}y} + 2\\mathbf{X^{\\top}X\\boldsymbol \\beta}\\nonumber \\end{align}\\]\n따라서 아래와 같은식이 성립한다.\n\\[  \\mathbf{X^{\\top}y}= \\mathbf{X^{\\top} X} \\boldsymbol \\beta \\]\n\\[\\hat {\\boldsymbol \\beta} = \\mathbf{\\left (X^{\\top}X\\right)^{-1}Xy}  \\]"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-14-(2주차).html",
    "href": "post/Bigdata Analysis/2022-03-14-(2주차).html",
    "title": "02. Tensorflow-1",
    "section": "",
    "text": "단순선형회귀의 경우 일반적인 베타계수의 추정치\n\n\\(\\hat {\\beta_0} = \\bar y - \\beta _1\\bar x, \\quad \\hat {\\beta_1} = \\frac {S_{xy}} {S_{xx}}\\)\n\n다중 회귀의 경우\n\n\\[L=loss =({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\]\n\\[ L= {\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\]\n\n위를 미분하면\n\n\\[\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} L = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}  \\]\n\\[\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} L=- \\mathbf{X^{\\top}y} - \\mathbf{X^{\\top}y} + 2\\mathbf{X^{\\top}X\\boldsymbol \\beta}\\]\n따라서\n\\[\\quad \\bf{X^{\\top}X}\\beta\n= \\bf{X^{\\top}Y}\\]\n\\[\\therefore \\quad \\hat {\\beta} = \\left(\\bf{X^{\\top}X}^{-1}\\right)XY\\]"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-14-(2주차).html#예비학습-중첩리스트",
    "href": "post/Bigdata Analysis/2022-03-14-(2주차).html#예비학습-중첩리스트",
    "title": "02. Tensorflow-1",
    "section": "예비학습: 중첩리스트",
    "text": "예비학습: 중첩리스트\n- 리스트\n\nlst = list(range(6))\nlst\n\n[0, 1, 2, 3, 4, 5]\n\n\n\nlst[-1]\n\n5\n\n\n- 리스트 안에 리스트 생성\n\nlst =[[1,2,],[3,4]]\nlst\n\n[[1, 2], [3, 4]]\n\n\n\nprint(lst[1][0],lst[0][0])\n\n3 1\n\n\n- 위 같은 2차원의 리스트 구조를 행렬로 생각할 수 있다\n1 2 \\ 3 4\n또는\n1 \\ 2 \\ 3 \\ 4\n- (4,1) 행렬 느낌의 리스트\n\nlst = [[1],[2],[3],[4]]\nlst\n\n[[1], [2], [3], [4]]\n\n\n\nnp.array(lst)\n\narray([[1],\n       [2],\n       [3],\n       [4]])\n\n\n- (1,4) 행렬 느낌의 리스트\n\nlst = [1,2,3,4]\nlst\n\n[1, 2, 3, 4]\n\n\n\nnp.array(lst)\n\narray([1, 2, 3, 4])"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-14-(2주차).html#변수-선언",
    "href": "post/Bigdata Analysis/2022-03-14-(2주차).html#변수-선언",
    "title": "02. Tensorflow-1",
    "section": "변수 선언",
    "text": "변수 선언\n\n스칼라\n\ntf.constant(3.14)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=3.14>\n\n\n\ntf.constant(3.14) + tf.constant(3.14)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=6.28>\n\n\n\n\n벡터\n\n_vector = tf.constant([1,2,3 ])\n_vector\n\n<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>\n\n\n\n_vector[0]\n\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n\n\n\n\n매트릭스 생성\n\n_matrix = tf.constant([[1,0],[0,1]])\n_matrix\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 0],\n       [0, 1]], dtype=int32)>\n\n\n\n\n텐서 == 3차원 이상의 배열\n\nnp.array([[[0,1],[1,2]],[[0,1],[1,2]]])\n\narray([[[0, 1],\n        [1, 2]],\n\n       [[0, 1],\n        [1, 2]]])\n\n\n\ntf.constant([[[0,1],[1,2]],[[0,1],[1,2]]])\n\n<tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\narray([[[0, 1],\n        [1, 2]],\n\n       [[0, 1],\n        [1, 2]]], dtype=int32)>\n\n\n\n\n타입\n\ntype(tf.constant([[[0,1],[1,2]],[[0,1],[1,2]]]))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n- 끝에 EagerTensor 가 나오는 것을 기억하자\n\n\n인덱싱\n\n_matrix  = tf.constant([[1,2],[3,4]])\n_matrix\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)>\n\n\n\n_matrix[0]\n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>\n\n\n\n_matrix[0,:]\n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>\n\n\n\n_matrix[0,0]\n\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n\n\n\n_matrix[0][0]\n\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n\n\n\n\ntf.constant는 불편하다.\n- 각 컬럼의 데이터 타입이 전부 동일하여야 한다.\n- 원소 수정이 불가능함.\n\n a= tf.constant([1,22,33])\n a\n\n<tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 1, 22, 33], dtype=int32)>\n\n\n\na[0] =11\n\nTypeError: ignored\n\n\n- 묵시적(간접적) 형변환이 불가능하다.\n\n1+3.14\n\n4.140000000000001\n\n\n\ntf.constant(1) + tf.constant(3.14)\n\nInvalidArgumentError: ignored\n\n\n- 같은 float 도 안되는 경우가 있음\n\ntf.constant(1.0, dtype= tf.float64) + tf.constant(3.14)\n\nInvalidArgumentError: ignored\n\n\n\n\ntf.constant \\(\\to\\) 넘파이\n\nnp.array(tf.constant(1))\n\narray(1, dtype=int32)\n\n\n\na = tf.constant(3.14)\ntype(a)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\na.numpy()\n\n3.14"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-14-(2주차).html#연산",
    "href": "post/Bigdata Analysis/2022-03-14-(2주차).html#연산",
    "title": "02. Tensorflow-1",
    "section": "연산",
    "text": "연산\n\n더하기\n\na = tf.constant([1,2])\nb = tf.constant([3,4])\na+b\n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)>\n\n\n\ntf.add(a,b) ## 이건 예전버전 \n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)>\n\n\n\n\n곱하기\n- 결과가 조금 이상하다. 일반적인 행렬연사이 아니다\n\na = tf.constant([[1,2],[3,4]])\nb = tf.constant([[5,6],[7,8]])\na*b\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 5, 12],\n       [21, 32]], dtype=int32)>\n\n\n- but matrix의 곱은\n\na = tf.constant([[1,0],[0,1]])\nb = tf.constant([[5],[7]])\na@b\n\n<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\narray([[5],\n       [7]], dtype=int32)>\n\n\n\ntf.matmul(a,b) ## 위와 같은 표현\n\n\n\n역행렬\n\na = tf.constant([[1,0],[0,2]])\na\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 0],\n       [0, 2]], dtype=int32)>\n\n\n\ntf.linalg.inv(a)\n\nInvalidArgumentError: ignored\n\n\n\n위의 경우는 자료가 int 형이여서 안되는 거임\n\n\n?tf.constant\n\n\n아래오 같이 자료형을 선언해 주어야함\n\n\na = tf.constant([[1,0],[0,2]],dtype=float)\ntf.linalg.inv(a)\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1. , 0. ],\n       [0. , 0.5]], dtype=float32)>\n\n\n\n\ndeterminant\n\na = tf.constant([[1,2],[3,4]],dtype=float)\nprint(a)\ntf.linalg.det(a)\n\ntf.Tensor(\n[[1. 2.]\n [3. 4.]], shape=(2, 2), dtype=float32)\n\n\n<tf.Tensor: shape=(), dtype=float32, numpy=-2.0>\n\n\n\n\nTrace\n\ntf.linalg.trace(a)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=5.0>"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-14-(2주차).html#형태변환",
    "href": "post/Bigdata Analysis/2022-03-14-(2주차).html#형태변환",
    "title": "02. Tensorflow-1",
    "section": "형태변환",
    "text": "형태변환\n- 1 x 4 행렬을 \\(\\to\\) 4 x 1\n\na = tf.constant([1,2,3,4])\na\n\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n\n\n\ntf.reshape(a,(2,2))\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)>\n\n\n- 3차원으로도 변경이 가능\n\ntf.reshape(a,(2,2,1))\n\n<tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\narray([[[1],\n        [2]],\n\n       [[3],\n        [4]]], dtype=int32)>\n\n\n- 다차원의 경우 적용\n\na = tf.constant(list(range(1,13)))\na\n\n<tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)>\n\n\n\ntf.reshape(a,(2,2,3))\n\n<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)>\n\n\n\ntf.reshape(a,(4,3))\n\n<tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)>\n\n\n\n-1 을 기입하면 남은 차원 수를 알아서 기입해줌 -1 = ? 라고 생각\n\n\ntf.reshape(a,(4,-1))\n\n<tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)>\n\n\n\nb= tf.reshape(a,(2,2,-1))\nb\n\n<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)>\n\n\n- 다시 일차원으로 되돌림\n\ntf.reshape(b,-1)\n\n<tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)>"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-14-(2주차).html#선언고급",
    "href": "post/Bigdata Analysis/2022-03-14-(2주차).html#선언고급",
    "title": "02. Tensorflow-1",
    "section": "선언고급",
    "text": "선언고급\n- 리스트나, 넘파이로 만들고 output을 tensor로 변경하는 것도 좋은 방법이다.\n\nㅣ = [1,2,3,4]\ntf.constant(np.diag(ㅣ))\n\n<tf.Tensor: shape=(4, 4), dtype=int64, numpy=\narray([[1, 0, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 0],\n       [0, 0, 0, 4]])>\n\n\n- tf.ones, tf.zeros\n\ntf.zeros([3,3])\n\n<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]], dtype=float32)>\n\n\n- tf.linspace(0,1,10)\n\ntf.linspace(0,1,10)\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])>"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-14-(2주차).html#tf.concat",
    "href": "post/Bigdata Analysis/2022-03-14-(2주차).html#tf.concat",
    "title": "02. Tensorflow-1",
    "section": "tf.concat",
    "text": "tf.concat\n\na = tf.constant([1,2])\nb = tf.constant([3,4])\na,b\n\n(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>,\n <tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 4], dtype=int32)>)\n\n\n\na = tf.constant([[1],[2]])\nb = tf.constant([[3],[4]])\na,b\n\n(<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[1],\n        [2]], dtype=int32)>, <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[3],\n        [4]], dtype=int32)>)\n\n\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]], dtype=int32)>\n\n\n\na = tf.constant([[1],[2]])\nb = tf.constant([[3],[4]])\na,b\n\n(<tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[1],\n        [2]], dtype=int32)>, <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[3],\n        [4]], dtype=int32)>)\n\n\n\ntf.concat([a,b],axis=1)\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 3],\n       [2, 4]], dtype=int32)>\n\n\n\na = tf.constant([1,2])\nb = tf.constant([3,4])\na,b\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n\n\n\na = tf.constant([[1,2]]) \nb = tf.constant([[3,4]])\na,b\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)>\n\n\n\n차원 수 증가\n\n(2,3,4,5) concat (2,3,4,5) => (4,3,4,5)\n\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb= -a\n\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(4, 3, 4, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119]]],\n\n\n       [[[   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)>\n\n\n\n(2,3,4,5) concat (2,3,4,5) => (2,6,4,5)\n\n\ntf.concat([a,b],axis=1)\n\n<tf.Tensor: shape=(2, 6, 4, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59]],\n\n        [[   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119]],\n\n        [[ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)>\n\n\n\n(2,3,4,5) concat (2,3,4,5) => (2,3,8,5)\n\n\ntf.concat([a,b],axis=2)\n\n<tf.Tensor: shape=(2, 3, 8, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19],\n         [   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39],\n         [ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59],\n         [ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79],\n         [ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99],\n         [ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119],\n         [-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)>\n\n\n\n(2,3,4,5) concat (2,3,4,5) => (2,3,4,10)\n\n\ntf.concat([a,b],axis=3)\n\n<tf.Tensor: shape=(2, 3, 4, 10), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4,    0,   -1,   -2,   -3,   -4],\n         [   5,    6,    7,    8,    9,   -5,   -6,   -7,   -8,   -9],\n         [  10,   11,   12,   13,   14,  -10,  -11,  -12,  -13,  -14],\n         [  15,   16,   17,   18,   19,  -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24,  -20,  -21,  -22,  -23,  -24],\n         [  25,   26,   27,   28,   29,  -25,  -26,  -27,  -28,  -29],\n         [  30,   31,   32,   33,   34,  -30,  -31,  -32,  -33,  -34],\n         [  35,   36,   37,   38,   39,  -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44,  -40,  -41,  -42,  -43,  -44],\n         [  45,   46,   47,   48,   49,  -45,  -46,  -47,  -48,  -49],\n         [  50,   51,   52,   53,   54,  -50,  -51,  -52,  -53,  -54],\n         [  55,   56,   57,   58,   59,  -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64,  -60,  -61,  -62,  -63,  -64],\n         [  65,   66,   67,   68,   69,  -65,  -66,  -67,  -68,  -69],\n         [  70,   71,   72,   73,   74,  -70,  -71,  -72,  -73,  -74],\n         [  75,   76,   77,   78,   79,  -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84,  -80,  -81,  -82,  -83,  -84],\n         [  85,   86,   87,   88,   89,  -85,  -86,  -87,  -88,  -89],\n         [  90,   91,   92,   93,   94,  -90,  -91,  -92,  -93,  -94],\n         [  95,   96,   97,   98,   99,  -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104, -100, -101, -102, -103, -104],\n         [ 105,  106,  107,  108,  109, -105, -106, -107, -108, -109],\n         [ 110,  111,  112,  113,  114, -110, -111, -112, -113, -114],\n         [ 115,  116,  117,  118,  119, -115, -116, -117, -118, -119]]]],\n      dtype=int32)>\n\n\n\n아래와 같은 방법도 있긴하나 난 안할래\n\n\ntf.concat([a,b],axis=-1)\n\n<tf.Tensor: shape=(2, 3, 4, 10), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4,    0,   -1,   -2,   -3,   -4],\n         [   5,    6,    7,    8,    9,   -5,   -6,   -7,   -8,   -9],\n         [  10,   11,   12,   13,   14,  -10,  -11,  -12,  -13,  -14],\n         [  15,   16,   17,   18,   19,  -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24,  -20,  -21,  -22,  -23,  -24],\n         [  25,   26,   27,   28,   29,  -25,  -26,  -27,  -28,  -29],\n         [  30,   31,   32,   33,   34,  -30,  -31,  -32,  -33,  -34],\n         [  35,   36,   37,   38,   39,  -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44,  -40,  -41,  -42,  -43,  -44],\n         [  45,   46,   47,   48,   49,  -45,  -46,  -47,  -48,  -49],\n         [  50,   51,   52,   53,   54,  -50,  -51,  -52,  -53,  -54],\n         [  55,   56,   57,   58,   59,  -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64,  -60,  -61,  -62,  -63,  -64],\n         [  65,   66,   67,   68,   69,  -65,  -66,  -67,  -68,  -69],\n         [  70,   71,   72,   73,   74,  -70,  -71,  -72,  -73,  -74],\n         [  75,   76,   77,   78,   79,  -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84,  -80,  -81,  -82,  -83,  -84],\n         [  85,   86,   87,   88,   89,  -85,  -86,  -87,  -88,  -89],\n         [  90,   91,   92,   93,   94,  -90,  -91,  -92,  -93,  -94],\n         [  95,   96,   97,   98,   99,  -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104, -100, -101, -102, -103, -104],\n         [ 105,  106,  107,  108,  109, -105, -106, -107, -108, -109],\n         [ 110,  111,  112,  113,  114, -110, -111, -112, -113, -114],\n         [ 115,  116,  117,  118,  119, -115, -116, -117, -118, -119]]]],\n      dtype=int32)>\n\n\n\n\n차원을 한번 줄여보자\n\n(4,) -> (8,)\n\n\na=tf.constant([1,2,3,4])\nb=-a\na,b\n\n(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>,\n <tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)>)\n\n\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(8,), dtype=int32, numpy=array([ 1,  2,  3,  4, -1, -2, -3, -4], dtype=int32)>\n\n\n\n(4,) -> (4,2)\n\n- 에러가 뜬다\n\ntf.concat([a,b],axis=1)\n\nInvalidArgumentError: ignored"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-14-(2주차).html#tf.stack",
    "href": "post/Bigdata Analysis/2022-03-14-(2주차).html#tf.stack",
    "title": "02. Tensorflow-1",
    "section": "tf.stack",
    "text": "tf.stack\n\n(4,) -> (4,2)\n\n\na=tf.constant([1,2,3,4])\nb=-a\na,b\n\n(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>,\n <tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)>)\n\n\n\ntf.stack([a,b],axis=1)\n\n<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3],\n       [ 4, -4]], dtype=int32)>"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-14-(2주차).html#tf.einsum",
    "href": "post/Bigdata Analysis/2022-03-14-(2주차).html#tf.einsum",
    "title": "02. Tensorflow-1",
    "section": "tf.einsum",
    "text": "tf.einsum"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-14-(2주차).html#tnp-사용방법-불만해결방법",
    "href": "post/Bigdata Analysis/2022-03-14-(2주차).html#tnp-사용방법-불만해결방법",
    "title": "02. Tensorflow-1",
    "section": "tnp 사용방법 (불만해결방법)",
    "text": "tnp 사용방법 (불만해결방법)\n- int 와 float 을 더할 수 있음\n\ntnp.array([1,2,3]) + tnp.array([1.0,2.0,3.0]) \n\n<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])>\n\n\n\n심지어\n\n\ntnp.array(1) + tnp.array([1.0,2.0,3.0]) \n\n<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 3., 4.])>\n\n\n\ntnp.array([1,2,3]) + tf.constant([1.0,2.0,3.0]) \n\n<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])>\n\n\n\na = tnp.diag([1,2,3])\ntype(a)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\na.min(),a.max()\n\n(<tf.Tensor: shape=(), dtype=int64, numpy=0>,\n <tf.Tensor: shape=(), dtype=int64, numpy=3>)\n\n\n\na.reshape(9,1)\n\n<tf.Tensor: shape=(9, 1), dtype=int64, numpy=\narray([[1],\n       [0],\n       [0],\n       [0],\n       [2],\n       [0],\n       [0],\n       [0],\n       [3]])>\n\n\n\n선언, 선언고급\n\nnp.random.randn(5)\n\narray([-1.79271696, -0.17190837,  1.01536417,  0.10096996,  0.6384037 ])\n\n\n\ntnp.random.randn(5)\n\n<tf.Tensor: shape=(5,), dtype=float64, numpy=array([ 0.68371875, -0.77886642, -0.78283853, -1.91862598, -0.36602414])>\n\n\n\n\n타입\n\ntype(tnp.random.randn(5))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n\ntf.contant로 만들어도 마치 넘파이인듯 쓰는 기능들\n- 묵시적 형변환이 가능해짐\n- 메소드를 쓸 수 있음.\n\n\n그렇지만 np.array는 아님\n\n여전히 값을 바꾸는 것은 허용하지 않는다.\n\n\n a = tf.constant([1,2,3])\n\n\na[0]=11\n\nTypeError: ignored\n\n\n\n\ntf.Variable\n\n선언\n\n\n타입\n\n\n인덱싱\n\n\ntf.Variable \\(\\to\\) 넘파이\n\n\ntf.Variable 도 불편하다.\n\n\n연산\n\n\n형태변환\n\n\n선언고급\n\n\ntf.concat\n\n\ntf.stack\n\n\n심지어 tf.Variable()로 만들어진 오브젝트는 tnp의 효과(은총)도 받지 못함"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-21-(3주차).html",
    "href": "post/Bigdata Analysis/2022-03-21-(3주차).html",
    "title": "03. Tensorflow-2",
    "section": "",
    "text": "imports\n\nimport tensorflow as tf\nimport numpy as np\n\n\ntf.config.experimental.list_physical_devices('GPU')\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\n\n지난강의 보충\n- max, min, sum, mean\n\na= tf.constant([1.0,2.0,3.0,4.0])\na\n\n<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)>\n\n\n\ntf.reduce_mean(a)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=2.5>\n\n\n\nconcat, stack\n- 예제: (2,3,4,5) stack (2,3,4,5) -> (?,?,?,?,?)\n\na = tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5))\nb = -a \n\ncase1 (1,2,3,4,5) stack (1,2,3,4,5) –> (2,2,3,4,5) # axis=0\n\ntf.stack([a,b],axis=0)\n\n<tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]]],\n\n\n        [[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]]]],\n\n\n\n       [[[[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]],\n\n\n        [[[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)>\n\n\ncase2 (2,1,3,4,5) stack (2,1,3,4,5) –> (2,2,3,4,5) # axis=1\n\ntf.stack([a,b],axis=1)\n\n<tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]]],\n\n\n        [[[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]]],\n\n\n        [[[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)>\n\n\ncase3 (2,3,1,4,5) stack (2,3,1,4,5) –> (2,3,2,4,5) # axis=2\n\ntf.stack([a,b],axis=2)\n\n<tf.Tensor: shape=(2, 3, 2, 4, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   5,    6,    7,    8,    9],\n          [  10,   11,   12,   13,   14],\n          [  15,   16,   17,   18,   19]],\n\n         [[   0,   -1,   -2,   -3,   -4],\n          [  -5,   -6,   -7,   -8,   -9],\n          [ -10,  -11,  -12,  -13,  -14],\n          [ -15,  -16,  -17,  -18,  -19]]],\n\n\n        [[[  20,   21,   22,   23,   24],\n          [  25,   26,   27,   28,   29],\n          [  30,   31,   32,   33,   34],\n          [  35,   36,   37,   38,   39]],\n\n         [[ -20,  -21,  -22,  -23,  -24],\n          [ -25,  -26,  -27,  -28,  -29],\n          [ -30,  -31,  -32,  -33,  -34],\n          [ -35,  -36,  -37,  -38,  -39]]],\n\n\n        [[[  40,   41,   42,   43,   44],\n          [  45,   46,   47,   48,   49],\n          [  50,   51,   52,   53,   54],\n          [  55,   56,   57,   58,   59]],\n\n         [[ -40,  -41,  -42,  -43,  -44],\n          [ -45,  -46,  -47,  -48,  -49],\n          [ -50,  -51,  -52,  -53,  -54],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [  65,   66,   67,   68,   69],\n          [  70,   71,   72,   73,   74],\n          [  75,   76,   77,   78,   79]],\n\n         [[ -60,  -61,  -62,  -63,  -64],\n          [ -65,  -66,  -67,  -68,  -69],\n          [ -70,  -71,  -72,  -73,  -74],\n          [ -75,  -76,  -77,  -78,  -79]]],\n\n\n        [[[  80,   81,   82,   83,   84],\n          [  85,   86,   87,   88,   89],\n          [  90,   91,   92,   93,   94],\n          [  95,   96,   97,   98,   99]],\n\n         [[ -80,  -81,  -82,  -83,  -84],\n          [ -85,  -86,  -87,  -88,  -89],\n          [ -90,  -91,  -92,  -93,  -94],\n          [ -95,  -96,  -97,  -98,  -99]]],\n\n\n        [[[ 100,  101,  102,  103,  104],\n          [ 105,  106,  107,  108,  109],\n          [ 110,  111,  112,  113,  114],\n          [ 115,  116,  117,  118,  119]],\n\n         [[-100, -101, -102, -103, -104],\n          [-105, -106, -107, -108, -109],\n          [-110, -111, -112, -113, -114],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)>\n\n\ncase4 (2,3,4,1,5) stack (2,3,4,1,5) –> (2,3,4,2,5) # axis=3\n\ntf.stack([a,b],axis=-2)\n\n<tf.Tensor: shape=(2, 3, 4, 2, 5), dtype=int32, numpy=\narray([[[[[   0,    1,    2,    3,    4],\n          [   0,   -1,   -2,   -3,   -4]],\n\n         [[   5,    6,    7,    8,    9],\n          [  -5,   -6,   -7,   -8,   -9]],\n\n         [[  10,   11,   12,   13,   14],\n          [ -10,  -11,  -12,  -13,  -14]],\n\n         [[  15,   16,   17,   18,   19],\n          [ -15,  -16,  -17,  -18,  -19]]],\n\n\n        [[[  20,   21,   22,   23,   24],\n          [ -20,  -21,  -22,  -23,  -24]],\n\n         [[  25,   26,   27,   28,   29],\n          [ -25,  -26,  -27,  -28,  -29]],\n\n         [[  30,   31,   32,   33,   34],\n          [ -30,  -31,  -32,  -33,  -34]],\n\n         [[  35,   36,   37,   38,   39],\n          [ -35,  -36,  -37,  -38,  -39]]],\n\n\n        [[[  40,   41,   42,   43,   44],\n          [ -40,  -41,  -42,  -43,  -44]],\n\n         [[  45,   46,   47,   48,   49],\n          [ -45,  -46,  -47,  -48,  -49]],\n\n         [[  50,   51,   52,   53,   54],\n          [ -50,  -51,  -52,  -53,  -54]],\n\n         [[  55,   56,   57,   58,   59],\n          [ -55,  -56,  -57,  -58,  -59]]]],\n\n\n\n       [[[[  60,   61,   62,   63,   64],\n          [ -60,  -61,  -62,  -63,  -64]],\n\n         [[  65,   66,   67,   68,   69],\n          [ -65,  -66,  -67,  -68,  -69]],\n\n         [[  70,   71,   72,   73,   74],\n          [ -70,  -71,  -72,  -73,  -74]],\n\n         [[  75,   76,   77,   78,   79],\n          [ -75,  -76,  -77,  -78,  -79]]],\n\n\n        [[[  80,   81,   82,   83,   84],\n          [ -80,  -81,  -82,  -83,  -84]],\n\n         [[  85,   86,   87,   88,   89],\n          [ -85,  -86,  -87,  -88,  -89]],\n\n         [[  90,   91,   92,   93,   94],\n          [ -90,  -91,  -92,  -93,  -94]],\n\n         [[  95,   96,   97,   98,   99],\n          [ -95,  -96,  -97,  -98,  -99]]],\n\n\n        [[[ 100,  101,  102,  103,  104],\n          [-100, -101, -102, -103, -104]],\n\n         [[ 105,  106,  107,  108,  109],\n          [-105, -106, -107, -108, -109]],\n\n         [[ 110,  111,  112,  113,  114],\n          [-110, -111, -112, -113, -114]],\n\n         [[ 115,  116,  117,  118,  119],\n          [-115, -116, -117, -118, -119]]]]], dtype=int32)>\n\n\ncase5 (2,3,4,5,1) stack (2,3,4,5,1) –> (2,3,4,5,2) # axis=4\n\ntf.stack([a,b],axis=-1)\n\n<tf.Tensor: shape=(2, 3, 4, 5, 2), dtype=int32, numpy=\narray([[[[[   0,    0],\n          [   1,   -1],\n          [   2,   -2],\n          [   3,   -3],\n          [   4,   -4]],\n\n         [[   5,   -5],\n          [   6,   -6],\n          [   7,   -7],\n          [   8,   -8],\n          [   9,   -9]],\n\n         [[  10,  -10],\n          [  11,  -11],\n          [  12,  -12],\n          [  13,  -13],\n          [  14,  -14]],\n\n         [[  15,  -15],\n          [  16,  -16],\n          [  17,  -17],\n          [  18,  -18],\n          [  19,  -19]]],\n\n\n        [[[  20,  -20],\n          [  21,  -21],\n          [  22,  -22],\n          [  23,  -23],\n          [  24,  -24]],\n\n         [[  25,  -25],\n          [  26,  -26],\n          [  27,  -27],\n          [  28,  -28],\n          [  29,  -29]],\n\n         [[  30,  -30],\n          [  31,  -31],\n          [  32,  -32],\n          [  33,  -33],\n          [  34,  -34]],\n\n         [[  35,  -35],\n          [  36,  -36],\n          [  37,  -37],\n          [  38,  -38],\n          [  39,  -39]]],\n\n\n        [[[  40,  -40],\n          [  41,  -41],\n          [  42,  -42],\n          [  43,  -43],\n          [  44,  -44]],\n\n         [[  45,  -45],\n          [  46,  -46],\n          [  47,  -47],\n          [  48,  -48],\n          [  49,  -49]],\n\n         [[  50,  -50],\n          [  51,  -51],\n          [  52,  -52],\n          [  53,  -53],\n          [  54,  -54]],\n\n         [[  55,  -55],\n          [  56,  -56],\n          [  57,  -57],\n          [  58,  -58],\n          [  59,  -59]]]],\n\n\n\n       [[[[  60,  -60],\n          [  61,  -61],\n          [  62,  -62],\n          [  63,  -63],\n          [  64,  -64]],\n\n         [[  65,  -65],\n          [  66,  -66],\n          [  67,  -67],\n          [  68,  -68],\n          [  69,  -69]],\n\n         [[  70,  -70],\n          [  71,  -71],\n          [  72,  -72],\n          [  73,  -73],\n          [  74,  -74]],\n\n         [[  75,  -75],\n          [  76,  -76],\n          [  77,  -77],\n          [  78,  -78],\n          [  79,  -79]]],\n\n\n        [[[  80,  -80],\n          [  81,  -81],\n          [  82,  -82],\n          [  83,  -83],\n          [  84,  -84]],\n\n         [[  85,  -85],\n          [  86,  -86],\n          [  87,  -87],\n          [  88,  -88],\n          [  89,  -89]],\n\n         [[  90,  -90],\n          [  91,  -91],\n          [  92,  -92],\n          [  93,  -93],\n          [  94,  -94]],\n\n         [[  95,  -95],\n          [  96,  -96],\n          [  97,  -97],\n          [  98,  -98],\n          [  99,  -99]]],\n\n\n        [[[ 100, -100],\n          [ 101, -101],\n          [ 102, -102],\n          [ 103, -103],\n          [ 104, -104]],\n\n         [[ 105, -105],\n          [ 106, -106],\n          [ 107, -107],\n          [ 108, -108],\n          [ 109, -109]],\n\n         [[ 110, -110],\n          [ 111, -111],\n          [ 112, -112],\n          [ 113, -113],\n          [ 114, -114]],\n\n         [[ 115, -115],\n          [ 116, -116],\n          [ 117, -117],\n          [ 118, -118],\n          [ 119, -119]]]]], dtype=int32)>\n\n\n- 예제: (2,3,4), (2,3,4), (2,3,4)\n\na= tf.reshape(tf.constant(range(2*3*4)),(2,3,4))\nb= -a \nc= 2*a\n\n(예시1) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (6,3,4)\n\ntf.concat([a,b,c],axis=0)\n\n<tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]],\n\n       [[  0,   2,   4,   6],\n        [  8,  10,  12,  14],\n        [ 16,  18,  20,  22]],\n\n       [[ 24,  26,  28,  30],\n        [ 32,  34,  36,  38],\n        [ 40,  42,  44,  46]]], dtype=int32)>\n\n\n(예시2) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,9,4)\n\ntf.concat([a,b,c],axis=1)\n\n<tf.Tensor: shape=(2, 9, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11],\n        [  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11],\n        [  0,   2,   4,   6],\n        [  8,  10,  12,  14],\n        [ 16,  18,  20,  22]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23],\n        [-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23],\n        [ 24,  26,  28,  30],\n        [ 32,  34,  36,  38],\n        [ 40,  42,  44,  46]]], dtype=int32)>\n\n\n(예시3) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,12)\n\ntf.concat([a,b,c],axis=-1)\n\n<tf.Tensor: shape=(2, 3, 12), dtype=int32, numpy=\narray([[[  0,   1,   2,   3,   0,  -1,  -2,  -3,   0,   2,   4,   6],\n        [  4,   5,   6,   7,  -4,  -5,  -6,  -7,   8,  10,  12,  14],\n        [  8,   9,  10,  11,  -8,  -9, -10, -11,  16,  18,  20,  22]],\n\n       [[ 12,  13,  14,  15, -12, -13, -14, -15,  24,  26,  28,  30],\n        [ 16,  17,  18,  19, -16, -17, -18, -19,  32,  34,  36,  38],\n        [ 20,  21,  22,  23, -20, -21, -22, -23,  40,  42,  44,  46]]],\n      dtype=int32)>\n\n\n(예시4) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (3,2,3,4)\n\ntf.stack([a,b,c],axis=0)\n\n<tf.Tensor: shape=(3, 2, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  4,   5,   6,   7],\n         [  8,   9,  10,  11]],\n\n        [[ 12,  13,  14,  15],\n         [ 16,  17,  18,  19],\n         [ 20,  21,  22,  23]]],\n\n\n       [[[  0,  -1,  -2,  -3],\n         [ -4,  -5,  -6,  -7],\n         [ -8,  -9, -10, -11]],\n\n        [[-12, -13, -14, -15],\n         [-16, -17, -18, -19],\n         [-20, -21, -22, -23]]],\n\n\n       [[[  0,   2,   4,   6],\n         [  8,  10,  12,  14],\n         [ 16,  18,  20,  22]],\n\n        [[ 24,  26,  28,  30],\n         [ 32,  34,  36,  38],\n         [ 40,  42,  44,  46]]]], dtype=int32)>\n\n\n(예시5) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,3,4)\n\ntf.stack([a,b,c],axis=1)\n\n<tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  4,   5,   6,   7],\n         [  8,   9,  10,  11]],\n\n        [[  0,  -1,  -2,  -3],\n         [ -4,  -5,  -6,  -7],\n         [ -8,  -9, -10, -11]],\n\n        [[  0,   2,   4,   6],\n         [  8,  10,  12,  14],\n         [ 16,  18,  20,  22]]],\n\n\n       [[[ 12,  13,  14,  15],\n         [ 16,  17,  18,  19],\n         [ 20,  21,  22,  23]],\n\n        [[-12, -13, -14, -15],\n         [-16, -17, -18, -19],\n         [-20, -21, -22, -23]],\n\n        [[ 24,  26,  28,  30],\n         [ 32,  34,  36,  38],\n         [ 40,  42,  44,  46]]]], dtype=int32)>\n\n\n(예시6) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,3,4)\n\ntf.stack([a,b,c],axis=2)\n\n<tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy=\narray([[[[  0,   1,   2,   3],\n         [  0,  -1,  -2,  -3],\n         [  0,   2,   4,   6]],\n\n        [[  4,   5,   6,   7],\n         [ -4,  -5,  -6,  -7],\n         [  8,  10,  12,  14]],\n\n        [[  8,   9,  10,  11],\n         [ -8,  -9, -10, -11],\n         [ 16,  18,  20,  22]]],\n\n\n       [[[ 12,  13,  14,  15],\n         [-12, -13, -14, -15],\n         [ 24,  26,  28,  30]],\n\n        [[ 16,  17,  18,  19],\n         [-16, -17, -18, -19],\n         [ 32,  34,  36,  38]],\n\n        [[ 20,  21,  22,  23],\n         [-20, -21, -22, -23],\n         [ 40,  42,  44,  46]]]], dtype=int32)>\n\n\n(예시7) (2,3,4), (2,3,4), (2,3,4) \\(\\to\\) (2,3,4,3)\n\ntf.stack([a,b,c],axis=-1)\n\n<tf.Tensor: shape=(2, 3, 4, 3), dtype=int32, numpy=\narray([[[[  0,   0,   0],\n         [  1,  -1,   2],\n         [  2,  -2,   4],\n         [  3,  -3,   6]],\n\n        [[  4,  -4,   8],\n         [  5,  -5,  10],\n         [  6,  -6,  12],\n         [  7,  -7,  14]],\n\n        [[  8,  -8,  16],\n         [  9,  -9,  18],\n         [ 10, -10,  20],\n         [ 11, -11,  22]]],\n\n\n       [[[ 12, -12,  24],\n         [ 13, -13,  26],\n         [ 14, -14,  28],\n         [ 15, -15,  30]],\n\n        [[ 16, -16,  32],\n         [ 17, -17,  34],\n         [ 18, -18,  36],\n         [ 19, -19,  38]],\n\n        [[ 20, -20,  40],\n         [ 21, -21,  42],\n         [ 22, -22,  44],\n         [ 23, -23,  46]]]], dtype=int32)>\n\n\n- 예제: (2,3,4) (4,3,4) \\(\\to\\) (6,3,4)\n\na=tf.reshape(tf.constant(range(2*3*4)),(2,3,4))\nb=tf.reshape(-tf.constant(range(4*3*4)),(4,3,4))\n\n\ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy=\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [  8,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]],\n\n       [[  0,  -1,  -2,  -3],\n        [ -4,  -5,  -6,  -7],\n        [ -8,  -9, -10, -11]],\n\n       [[-12, -13, -14, -15],\n        [-16, -17, -18, -19],\n        [-20, -21, -22, -23]],\n\n       [[-24, -25, -26, -27],\n        [-28, -29, -30, -31],\n        [-32, -33, -34, -35]],\n\n       [[-36, -37, -38, -39],\n        [-40, -41, -42, -43],\n        [-44, -45, -46, -47]]], dtype=int32)>\n\n\n\ntf.concat([a,b],axis=1)\n\nInvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat\n\n\n\ntf.concat([a,b],axis=2)\n\nInvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat\n\n\n- (2,2) @ (2,) 의 연산?\nnumpy\n\nnp.array([[1,0],[0,1]]) @ np.array([77,-88])\n\narray([ 77, -88])\n\n\n\nnp.array([77,-88]) @ np.array([[1,0],[0,1]])\n\narray([ 77, -88])\n\n\n\nnp.array([[1,0],[0,1]]) @ np.array([77,-88]).reshape(2,1)\n\narray([[ 77],\n       [-88]])\n\n\n\nnp.array([77,-88]).reshape(2,1) @ np.array([[1,0],[0,1]]) \n\nValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 1)\n\n\n\nnp.array([77,-88]).reshape(1,2) @ np.array([[1,0],[0,1]]) \n\narray([[ 77, -88]])\n\n\ntensorflow\n\nI = tf.constant([[1.0,0.0],[0.0,1.0]]) \nx = tf.constant([77.0,-88.0]) \n\n\nI @ x \n\nInvalidArgumentError: In[0] and In[1] has different ndims: [2,2] vs. [2] [Op:MatMul]\n\n\n\nx @ I\n\nInvalidArgumentError: In[0] and In[1] has different ndims: [2] vs. [2,2] [Op:MatMul]\n\n\n\nI @ tf.reshape(x,(2,1))\n\n<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[ 77.],\n       [-88.]], dtype=float32)>\n\n\n\ntf.reshape(x,(1,2)) @ I \n\n<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 77., -88.]], dtype=float32)>\n\n\n\n\n\n\ntf.Variable\n\n선언\n- tf.Variable()로 선언\n\ntf.Variable([1,2,3,4])\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n\n\n\ntf.Variable([1.0,2.0,3.0,4.0])\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)>\n\n\n- tf.constant() 선언후 변환\n\ntf.Variable(tf.constant([1,2,3,4]))\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n\n\n- np 등으로 선언후 변환\n\ntf.Variable(np.array([1,2,3,4]))\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=int64, numpy=array([1, 2, 3, 4])>\n\n\n\n\n타입\n\ntype(tf.Variable([1,2,3,4]))\n\ntensorflow.python.ops.resource_variable_ops.ResourceVariable\n\n\n\n\n인덱싱\n\na=tf.Variable([1,2,3,4])\na\n\n<tf.Variable 'Variable:0' shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)>\n\n\n\na[:2]\n\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>\n\n\n\n\n연산가능\n\na=tf.Variable([1,2,3,4])\nb=tf.Variable([-1,-2,-3,-4])\n\n\na+b\n\n<tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)>\n\n\n\n\ntf.Variable도 쓰기 불편함\n\ntf.Variable([1,2])+tf.Variable([3.14,3.14])\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]\n\n\n\n\ntnp의 은총도 일부만 가능\n\nimport tensorflow.experimental.numpy as tnp \ntnp.experimental_enable_numpy_behavior() \n\n- 알아서 형 변환\n\ntf.Variable([1,2])+tf.Variable([3.14,3.14])\n\n<tf.Tensor: shape=(2,), dtype=float64, numpy=array([4.1400001, 5.1400001])>\n\n\n- .reshape 메소드\n\ntf.Variable([1,2,3,4]).reshape(2,2)\n\nAttributeError: 'ResourceVariable' object has no attribute 'reshape'\n\n\n\n\n대부분의 동작은 tf.constant랑 큰 차이를 모르겠음\n- tf.concat\n\na= tf.Variable([[1,2],[3,4]]) \nb= tf.Variable([[-1,-2],[-3,-4]]) \ntf.concat([a,b],axis=0)\n\n<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[ 1,  2],\n       [ 3,  4],\n       [-1, -2],\n       [-3, -4]], dtype=int32)>\n\n\n- tf.stack\n\na= tf.Variable([[1,2],[3,4]]) \nb= tf.Variable([[-1,-2],[-3,-4]]) \ntf.stack([a,b],axis=0)\n\n<tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\narray([[[ 1,  2],\n        [ 3,  4]],\n\n       [[-1, -2],\n        [-3, -4]]], dtype=int32)>\n\n\n\n\n변수값변경가능(?)\n\na= tf.Variable([1,2,3,4])\nid(a)\n\n140652736059120\n\n\n\na.assign_add([-1,-2,-3,-4])\nid(a)\n\n140652736059120\n\n\n\n\n요약\n- tf.Variable()로 만들어야 하는 뚜렷한 차이는 모르겠음.\n- 애써 tf.Variable()로 만들어도 간단한연산을 하면 그 결과는 tf.constant()로 만든 오브젝트와 동일해짐.\n\n\n\n미분\n\n모티브\n- 예제: 컴퓨터를 이용하여 \\(x=2\\)에서 \\(y=3x^2\\)의 접선의 기울기를 구해보자.\n(손풀이)\n\\[\\frac{dy}{dx}=6x\\]\n이므로 \\(x=2\\)를 대입하면 12이다.\n(컴퓨터를 이용한 풀이)\n단계1\n\nx1=2 \ny1= 3*x1**2 \n\n\nx2=2+0.000000001\ny2= 3*x2**2\n\n\n(y2-y1)/(x2-x1)\n\n12.0\n\n\n단계2\n\ndef f(x):\n    return(3*x**2)\n\n\nf(3)\n\n27\n\n\n\ndef d(f,x):\n    return (f(x+0.000000001)-f(x))/0.000000001\n\n\nd(f,2)\n\n12.000000992884452\n\n\n단계3\n\nd(lambda x: 3*x**2 ,2)\n\n12.000000992884452\n\n\n\nd(lambda x: x**2 ,0)\n\n1e-09\n\n\n단계4\n\\[f(x,y)= x^2 +3y\\]\n\ndef f(x,y):\n    return(x**2 +3*y)\n\n\nd(f,(2,3))\n\nTypeError: can only concatenate tuple (not \"float\") to tuple\n\n\n\n\ntf.GradientTape() 사용방법\n- 예제1: \\(x=2\\)에서 \\(y=3x^2\\)의 도함수값을 구하라.\n\nx=tf.Variable(2.0)\na=tf.constant(3.0)\n\n\nmytape=tf.GradientTape()\nmytape.__enter__() # 기록 시작 \ny=a*x**2 # y=ax^2 = 3x^2\nmytape.__exit__(None,None,None) # 기록 끝 \n\n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n<tf.Tensor: shape=(), dtype=float32, numpy=12.0>\n\n\n- 예제2: 조금 다른예제\n\nx=tf.Variable(2.0)\n#a=tf.constant(3.0)\n\nmytape=tf.GradientTape()\nmytape.__enter__() # 기록 시작 \na=(x/2)*3 ## a=(3/2)x \ny=a*x**2  ## y=ax^2 = (3/2)x^3\nmytape.__exit__(None,None,None) # 기록 끝 \n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n<tf.Tensor: shape=(), dtype=float32, numpy=18.0>\n\n\n\\[a=\\frac{3}{2}x\\] \\[y=ax^2=\\frac{3}{2}x^3\\]\n\\[\\frac{dy}{dx}=\\frac{3}{2} 3x^2\\]\n\n3/2*3*4\n\n18.0\n\n\n- 테이프의 개념 (\\(\\star\\))\n(상황)\n우리가 어려운 미분계산을 컴퓨터에게 부탁하는 상황임. (예를들면 \\(y=3x^2\\)) 컴퓨터에게 부탁을 하기 위해서는 연습장(=테이프)에 \\(y=3x^2\\)이라는 수식을 써서 보여줘야하는데 이때 컴퓨터에게 target이 무엇인지 그리고 무엇으로 미분하고 싶은 것인지를 명시해야함.\n\nmytape = tf.GradientTape(): tf.GradientTape()는 연습장을 만드는 명령어, 만들어진 연습장을 mytape라고 이름을 붙인다.\nmytape.__enter__(): 만들어진 공책을 연다 (=기록할수 있는 상태로 만든다)\na=x/2*3; y=a*x**2: 컴퓨터에게 전달할 수식을 쓴다\nmytape.__exit__(None,None,None): 공책을 닫는다.\nmytape.gradient(y,x): \\(y\\)를 \\(x\\)로 미분하라는 메모를 남기고 컴퓨터에게 전달한다.\n\n- 예제3: 연습장을 언제 열고 닫을지 결정하는건 중요하다.\n\nx=tf.Variable(2.0)\na=(x/2)*3 ## a=(3/2)x\n\nmytape=tf.GradientTape()\nmytape.__enter__() # 기록 시작 \ny=a*x**2  ## y=ax^2 = (3/2)x^3\nmytape.__exit__(None,None,None) # 기록 끝 \n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n<tf.Tensor: shape=(), dtype=float32, numpy=12.0>\n\n\n- 예제4: with문과 함께 쓰는 tf.GradientTape()\n\nx=tf.Variable(2.0)\na=(x/2)*3 \n\n\nwith tf.GradientTape() as mytape:\n    ## with문 시작 \n    y=a*x**2 \n    ## with문 끝 \n\n\nmytape.gradient(y,x) # y를 x로 미분하라.\n\n<tf.Tensor: shape=(), dtype=float32, numpy=12.0>\n\n\n(문법해설)\n아래와 같이 쓴다.\nwith expression as myname:\n    ## with문 시작: myname.__enter__() \n    blabla ~ \n    yadiyadi !! \n    ## with문 끝: myname.__exit__()\n\nexpression 의 실행결과 오브젝트가 생성, 생성된 오브젝트는 myname라고 이름붙임. 이 오브젝트는 .__enter__()와 .__exit__()를 숨겨진 기능으로 포함해야 한다.\nwith문이 시작되면서 myname.__enter__()이 실행된다.\n블라블라와 야디야디가 실행된다.\nwith문이 종료되면서 myname.__exit__()이 실행된다.\n\n- 예제5: 예제2를 with문과 함께 구현\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape() as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\nmytape.gradient(y,x) # y를 x로 미분하라. \n\n<tf.Tensor: shape=(), dtype=float32, numpy=18.0>\n\n\n- 예제6: persistent = True\n(관찰1)\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape() as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nmytape.gradient(y,x) # 2번이상 실행해서 에러를 관측하라\n\nRuntimeError: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n\n\n(관찰2)\n\nx=tf.Variable(2.0)\n\nwith tf.GradientTape(persistent=True) as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nmytape.gradient(y,x) # 2번이상실행해도 에러가 나지않음 \n\n<tf.Tensor: shape=(), dtype=float32, numpy=18.0>\n\n\n- 예제7: watch\n(관찰1)\n\nx=tf.constant(2.0)\n\nwith tf.GradientTape(persistent=True) as mytape:\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\nNone\n\n\n(관찰2)\n\nx=tf.constant(2.0)\nwith tf.GradientTape(persistent=True) as mytape:\n    mytape.watch(x) # 수동감시\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n(관찰3)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제 \n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\nNone\n\n\n(관찰4)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제\n    mytape.watch(x)\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n(관찰5)\n\nx=tf.Variable(2.0)\nwith tf.GradientTape(persistent=True) as mytape: \n    mytape.watch(x)\n    a=(x/2)*3 ## a=(3/2)x \n    y=a*x**2  ## y=ax^2 = (3/2)x^3\n\n\nprint(mytape.gradient(y,x))\n\ntf.Tensor(18.0, shape=(), dtype=float32)\n\n\n- 예제9: 카페예제로 돌아오자.\n- 예제10: 카페예제의 매트릭스 버전\n- 예제11: 위의 예제에서 이론적인 \\(\\boldsymbol{\\beta}\\)의 최적값을 찾아보고 (즉 \\(\\hat{\\boldsymbol{\\beta}}\\)을 찾고) 그곳에서 loss의 미분을 구하라. 구한결과가 \\(\\begin{bmatrix}0 \\\\ 0 \\end{bmatrix}\\) 임을 확인하라."
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-28-(4주차).html",
    "href": "post/Bigdata Analysis/2022-03-28-(4주차).html",
    "title": "04. 경사하강법-1",
    "section": "",
    "text": "- 예제9: 카페예제로 돌아오자. (1주차 강의)\n- 자료 생성\n\n#collapse-hide\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nx=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])\nx\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])>\n\n\n\ntnp.random.seed(43052) \ny= 10.2+ x*2.2 + tnp.random.randn(10) \ny\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([54.98269924, 60.27348365, 61.27621687, 60.53495888, 62.9770905 ,\n       66.32168996, 66.87781372, 71.0050025 , 72.63837337, 77.11143943])>"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-28-(4주차).html#경사하강법",
    "href": "post/Bigdata Analysis/2022-03-28-(4주차).html#경사하강법",
    "title": "04. 경사하강법-1",
    "section": "경사하강법",
    "text": "경사하강법\n- \\(loss = (\\frac {1}{2}\\beta-1)^2\\) 을 최소화 하는 \\(\\beta\\) 를 구해보자\n- 당연히 \\(\\beta=2\\)일 떼 최솟값을 가질 것이다\n- 이것을 컴퓨터로 직접 구해보자\n\n최적화문제\n\n\n방법1: grid search\n- 단순히 베타를 개많이 만들고 loss를 최소화하는 베타를 찾자\n\n알고리즘\n\n\n구현코드\n\nbeta = tnp.linspace(-10,10,1000) \n#beta\n\n\nloss = (1/2*beta-1)**2 \n\n\ntnp.argmin(loss)\n\n<tf.Tensor: shape=(), dtype=int64, numpy=599>\n\n\n\ngrid search로 알아봤을 때 최솟값이 2에 근사하게 나온다\n\n\nbeta[599]\n\n<tf.Tensor: shape=(), dtype=float64, numpy=1.9919919919919913>\n\n\n\n\n그리드서치의 문제점\n- 좀 정확하진 않지만 표본의 수를 늘리면 2의 근접한 값을 찾는다.\n- 비판1: [-10,10]이외에 해가 존재하면? 즉 범위 밖에 존재할 수 가 있음 - 이 예제의 경우는 운좋게 [-10,10]에서 해가 존재했음 - 하지만 임의의 고정된 \\(x,y\\)에 대하여 \\(loss(\\beta)=(x\\beta-y)^2\\) 의 형태의 해가 항상 [-10,10]에서 존재한다는 보장은 없음 - 해결책: 더 넓게 많은 범위를 탐색하자? \\(\\to\\) but, 무한대의 범위에서 할 수 없음\n- 비판2: 위 해결책은 효율적이지 않음 - 알고리즘을 요약하면 결국 -10부터 10까지 작은 간격으로 조금씩 이동하며 loss를 조사하는 것이 grid search의 아이디어 - \\(\\to\\) 생각해보니까 \\(\\beta=2\\)인 순간 \\(loss=(\\frac{1}{2}\\beta-1)^2=0\\)이 되어서 이것보다 작은 최소값은 존재하지 않는다(제곱은 항상 양수이어야 하므로) - \\(\\to\\) 따라서 \\(\\beta=2\\) 이후로는 탐색할 필요가 없다\n\n\n\n방법2: gradient descent\n\n임의의 초기값을 선정하고 \\(loss\\)를 계산한다 (초깃값 셋팅)\n\n\n\\(\\beta = -5 \\to loss(-5) = (-5/2-1)^2 = 12.55\\)\n\n\n(-5/2-1)**2\n\n12.25\n\n\n\n임의의 초기값에서 좌우로 약간씩 이동해보고 \\(loss\\)를 계한한다. (미분에서 최솟값을 찾는 과정)\n\n\\(\\to \\beta = -5.01,\\, \\beta = -4.99\\)\n\n(-5.01 /2 -1)**2,(-4.99 /2 -1)**2\n\n(12.285025, 12.215025)\n\n\n\n(2)의 결과를 보고 어느쪽으로 이동하는 것이 유리한지 따져본다. 그 후 유리한 방향으로 이동한다.\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.plot(beta,loss)\n\n\n\n\n- (2) - (3)의 과정은 \\(\\beta = -5\\) 미분계수를 구한 후 미분계수가 양수이면 왼쪽으로 움직이고 음수이면 오른쪽으로 움직인다고 해석가능\n\n(2) ~ (3) 과정을 반복 후, 어느쪽으로 가도 유리한 지점이 없다면 알고리즘을 멈춘다\n\n\n알고리즘 분석\n- 알고리즘이 멈추는 지점은 \\(\\beta=2\\)이다. 왜냐하면 이경우 왼쪽으로 가도, 오른쪽으로 가도 현재 손실함수값보다 크기 때문.\n\n\n왼쪽/오른쪽중에 어디로 갈지 어떻게 판단하는 과정을 수식화?\n\n오른쪽으로 0.01간다 \\(\\to\\) 미분계수가 음수일 때\n왼쪽으로 0.01간다 \\(\\to\\) 미분계수가 양수일 때\n\n- 그렇다면\n\\(\\beta_{new} =\\beta_{old} + 0.01 \\to \\frac{d\\,loss}{d\\,\\beta_{old}}\\) 가 음수 일때\n\\(\\beta_{new} =\\beta_{old} - 0.01 \\to \\frac{d\\,loss}{d\\,\\beta_{old}}\\) 가 양수 일때\n\n\n혹시 알고리즘을 좀 개선할수 있을까?\n- 동일하게 0.01씩 이동하는게 맞는지 의문\n\nimport numpy as np\n\n\n_beta = np.linspace(-10,5)\nplt.plot(_beta,(_beta/2-1)**2)\n\n\n\n\n- \\(\\beta=-10\\) 일 경우의 접선의 기울기? \\(\\beta=-4\\) 일때 접선의 기울기?\n- \\(\\beta= -10 \\to 기울기는 -6\\)\n- \\(\\beta= -4 \\to 기울기는 -3\\)\n\n위 같은 경우 \\(\\beta = -10\\) 에서 0.01만큼 이동했다면 \\(\\beta=-4\\) 에서 0.005만큼 이동해야함\n즉, 떨어진 만큼 비례해서 조금 더 자신있게 가자는 거임\n\n\\[\\beta_{new}= \\beta_{old} -\\alpha \\left[\\frac {∂}{\\,∂\\beta}loss(\\beta)\\right],\\quad \\alpha>0\\]\n\n\n구현코드\niter1: \\(\\beta=-10\\) 출발\n\nbeta = tf.Variable(-10.0)\n\n\nwith tf.GradientTape(persistent=True) as tape: \n    loss = (beta/2-1)**2 \n\n\ntape.gradient(loss,beta)\n\n<tf.Tensor: shape=(), dtype=float32, numpy=-6.0>\n\n\n\nalpha=0.01/6 \n\n\nbeta.assign_sub(alpha*tape.gradient(loss,beta)) ## variable 로 벼수 선언시 assign을 사용하면 초괴화가 가능\n\n<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.99>\n\n\niter2 \\(\\beta=-9.99\\)\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.99>\n\n\n\nwith tf.GradientTape(persistent=True) as tape: \n    loss = (beta/2-1)**2 \n\n\nbeta.assign_sub(alpha*tape.gradient(loss,beta))\n\n<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.980008>\n\n\nfor 문을 이용 (시도1 : 100번)\n\nbeta = tf.Variable(-10.0) \nalpha=0.01/6 \n\n\nfor k in range(100): \n    with tf.GradientTape(persistent=True) as tape: \n        loss = (beta/2-1)**2 \n    beta.assign_sub(alpha*tape.gradient(loss,beta))\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.040152>\n\n\nfor 문을 이용 (시도2 : 10,000번)\n\nbeta = tf.Variable(-10.0) \nalpha=0.01/6 \n\n\nfor k in range(10000): \n    with tf.GradientTape(persistent=True) as tape: \n        loss = (beta/2-1)**2 \n    beta.assign_sub(alpha*tape.gradient(loss,beta))\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251>\n\n\n\n사실 10,000 번까지 갈 필요도 없다 그냥 \\(\\alpha\\) 값을 크게 키우면 된다. -> 사실 이것도 분류 모델에서는 과적합 문제로 이어질 수 있다.\n\n\n\n학습률\n- \\(\\alpha\\) 값의 변화에 따라서 최적해가 어떻게 수렴하는지 시각화 해보자.\n\n[시각화 코드 예비학습]\n\nplt.plot([1,2,3],[3,4,5],\"ro\")\n\n\n\n\n\n도화지 생성\n\n\nfig = plt.figure()\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n도화지 안의 어떤 틀을 생성\n\n\nax = fig.add_subplot()\n\n\nfig\n\n\n\n\n\nfig.axes[0]\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f7948b12790>\n\n\n\ntype(fig.axes[0])\n\nmatplotlib.axes._subplots.AxesSubplot\n\n\n\nid(fig.axes[0])\n\n140158887339920\n\n\n- 네모틀(ax)의 특수기능(=메소드)중에는 plot이 있음. 이것은 또 어떤 오브젝트를 생성함\npython\n길이가 1인 튜플\na,=1\n\npnts, = ax.plot([1,2,3],[3,4,5],'or') \npnts\n\n<matplotlib.lines.Line2D at 0x7f79489e82d0>\n\n\n\nfig\n\n\n\n\n- pnts 오브젝트: x,y data를 변경해보자.\n\npnts.get_xdata(),pnts.get_ydata()\n\n(array([1, 2, 3]), array([3, 4, 5]))\n\n\n\npnts.get_ydata()\n\narray([3, 4, 5])\n\n\n\npnts.set_ydata([5,5,5])\n\n\npnts.get_ydata()\n\n[5, 5, 5]\n\n\n\nfig\n\n\n\n\n응용 숫자에 변화에 따른 animation을 구현해보자\n\nplt.rcParams[\"animation.html\"]=\"jshtml\"\nfrom matplotlib import animation \n\n\ndef animate(i): \n    if i%2 == 0:\n        pnts.set_ydata([3,4,5])\n    else: \n        pnts.set_ydata([4,4,4])\n\n\nani=animation.FuncAnimation(fig,animate,frames=10)\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n- 위를 적용하여 최적해를 찾는 과정 즉, 수렴과정을 시각화하자\n\n\\(\\beta :-10\\to-9\\to-8\\) 이런식으로 이동한다고 하자\n그에 대한 \\(loss\\)도 저장\n\n\nbeta_lst = [-10.0,-9.00,-8.00] \nloss_lst = [(-10.0/2-1)**2,(-9.00/2-1)**2,(-8.00/2-1)**2]\n\n\nfig = plt.figure() \nax = fig.add_subplot()\n\n\n\n\n\n그 후 \\(\\beta\\) 가 이동할 경로를 그려주자\n\n\n_beta= np.linspace(-15,19)\nax.plot(_beta,(_beta/2-1)**2) \n\n\nfig\n\n\n\n\n\npnts, = ax.plot(beta_lst[0],loss_lst[0],'ro')\n\n\nanimation 구현\n\n\ndef animate(i):\n    pnts.set_xdata(beta_lst[:(i+1)])\n    pnts.set_ydata(loss_lst[:(i+1)])\n\n\nani=animation.FuncAnimation(fig,animate,frames=3) ## frame은 리스트 길이라고 생각\nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n이제 목적대로 구현해보자\n\n1 초깃값 설정\n\nbeta = tf.Variable(10.0)\n\n\nbeta.numpy()\nalpha=0.01/6\n\n\nbeta_lst=[]\nbeta_lst.append(beta.numpy())\n\n\nloss_lst=[]\nloss_lst.append((beta.numpy()/2-1)**2)\n\n2 초깃값을 설정했으니 최적해를 찾아보자\n\nfor k in range(100) :\n    with tf.GradientTape(persistent=True) as tape :\n        tape.watch(beta)\n        loss = (beta/2-1)**2\n    beta.assign_sub(alpha*tape.gradient(loss,beta))\n    beta_lst.append(beta.numpy()) \n    loss_lst.append((beta.numpy()/2-1)**2) \n\n\nfig = plt.figure() # fig 는 도화지 \n\n<Figure size 432x288 with 0 Axes>\n\n\n\nax = fig.add_subplot()\nax.plot(_beta,(_beta/2-1)**2)\npnts, = ax.plot(beta_lst[0],loss_lst[0],'or')\n\n\nani=animation.FuncAnimation(fig,animate,frames=100) \nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n3 \\(\\alpha\\) 값을 살펴보니 깂이 너무 작아서 이동의 폭이 너무 짧음 \\(\\alpha \\to 0.1\\) 로 변경\n\nbeta = tf.Variable(10.0)\nbeta.numpy()\n\nalpha=0.1\n\nbeta_lst=[]\nbeta_lst.append(beta.numpy())\n\nloss_lst=[]\nloss_lst.append((beta.numpy()/2-1)**2)\n\n\nfor k in range(100) :\n    with tf.GradientTape(persistent=True) as tape :\n        tape.watch(beta)\n        loss = (beta/2-1)**2\n    beta.assign_sub(alpha*tape.gradient(loss,beta))\n    beta_lst.append(beta.numpy()) \n    loss_lst.append((beta.numpy()/2-1)**2) \n\n\nfig = plt.figure() # fig 는 도화지 \n\nax = fig.add_subplot()\nax.plot(_beta,(_beta/2-1)**2)\npnts, = ax.plot(beta_lst[0],loss_lst[0],'or')\n\n\n\n\n\nani=animation.FuncAnimation(fig,animate,frames=100) \nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n오 아까보단 빠르당\n\n4 \\(\\alpha \\to 1\\) 로 변경\n\nbeta = tf.Variable(10.0)\nbeta.numpy()\n\nalpha=1\n\nbeta_lst=[]\nbeta_lst.append(beta.numpy())\n\nloss_lst=[]\nloss_lst.append((beta.numpy()/2-1)**2)\n\n\nfor k in range(100) :\n    with tf.GradientTape(persistent=True) as tape :\n        tape.watch(beta)\n        loss = (beta/2-1)**2\n    beta.assign_sub(alpha*tape.gradient(loss,beta))\n    beta_lst.append(beta.numpy()) \n    loss_lst.append((beta.numpy()/2-1)**2) \n\n\nfig = plt.figure() # fig 는 도화지 \n\nax = fig.add_subplot()\nax.plot(_beta,(_beta/2-1)**2)\npnts, = ax.plot(beta_lst[0],loss_lst[0],'or')\n\n\n\n\n\nani=animation.FuncAnimation(fig,animate,frames=100) \nani\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-03-30-(5주차).html",
    "href": "post/Bigdata Analysis/2022-03-30-(5주차).html",
    "title": "05. 경사하강법-2",
    "section": "",
    "text": "imports\n\n#!conda install -c conda-forge python-graphviz -y\n\n/bin/bash: conda: command not found\n\n\n\nimport tensorflow as tf \nimport numpy as np\nimport matplotlib.pyplot as plt \n\n\nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior() \n\n\n\n최적화의 문제\n- \\(loss=(\\frac{1}{2}\\beta-1)^2\\)\n- 기존에 했던 방법은 수식을 알고 있어야 한다는 단점이 있음\n- 그래서 확률적 경사하강법을 이용해서 최적해를 찾는 과제를 수행하였음\n- 그러나 다양한 loss 함수에서 위와 같이 간단히 최적해를 찾는 것은 매우 어려움 \\(\\to\\) 대부분의 함수는 비모수적이기 때문이다!!\n- 그래서 오늘은 그것을 쉽게 해주는 tf.keras.optimizers를 사용해서 최적해를 찾을 거얌\n\n\ntf.keras.optimizers를 이용한 최적화방법\n\n방법1: opt.apply_gradients()를 이용\n\n이전까지의 방법\n\n\nbeta = tf.Variable(-10.0)\nalpha = 0.01/6\n\n\nwith tf.GradientTape() as tape :\n    tape.watch(beta)\n    loss = (beta/2-1)**2\nslope = tape.gradient(loss,beta)\n\n\nbeta.assign_sub(slope*alpha)\n\n<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=-9.99>\n\n\n\n이전 방법 + 새로운 방법 ( - beta.assign_sub(slope*alpha))\n\n\nopt = tf.keras.optimizers.SGD(alpha)\n\n\nopt.apply_gradients() : 베타 값과 slope의 값을 받아 최적해를 구해준다.\n즉, 위식을 통해 수식을 몰라도 최적해를 구할 수 있다.\n아래 과정은 한번에 iteration 임\n\n\nopt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope*alpha)\n\n<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>\n\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.98>\n\n\n\niteration 2\n\n\nwith tf.GradientTape() as tape :\n    tape.watch(beta)\n    loss = (beta/2-1)**2\nslope = tape.gradient(loss,beta)\n\n\nopt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope*alpha)\n\n<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=2>\n\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.9700165>\n\n\n\ninteration 1과 interation 2 의 결과가 달라진 것을 확인하였다.\nfor 문을 이용한다면?\n\n\nalpha = 0.01/6\nbeta =  tf.Variable(-10.0)\nopt = tf.keras.optimizers.SGD(alpha)\n\n\nfor epoc in range(10000) :\n  with tf.GradientTape() as tape :\n    tape.watch(beta)\n    loss = (beta/2-1)**2\n  slope = tape.gradient(loss,beta)\n  opt.apply_gradients([(slope,beta)])\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251>\n\n\n\n잠깐, 주소가 같지만 문법이 다름 근데 걍 그런갑다 하자\n\n\nopt.learning_rate, opt.lr\n\n(<tf.Variable 'SGD/learning_rate:0' shape=() dtype=float32, numpy=0.0016666667>,\n <tf.Variable 'SGD/learning_rate:0' shape=() dtype=float32, numpy=0.0016666667>)\n\n\n\nid(opt.learning_rate), id( opt.lr)\n\n(139921818271120, 139921818271120)\n\n\n\n\n방법2: opt.minimize()\n\n이 함수를 이용하면 gradienttape를 안써도 된다. 핳\n\n\nalpha = 0.01/6\nbeta =  tf.Variable(-10.0)\nopt = tf.keras.optimizers.SGD(alpha)\n\n\nloss 함수 정의\n\\[\\left(\\frac 12\\beta-1\\right)^2\\]\n\nloss_fn = lambda: (beta/2-1)**2 \n\n- iter 1\n\nopt.minimize(loss_fn,beta)\n\n<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>\n\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-9.99>\n\n\n- for\n\nalpha = 0.01/6\nbeta =  tf.Variable(-10.0)\nopt = tf.keras.optimizers.SGD(alpha)\nloss_fn = lambda: (beta/2-1)**2 \nfor epoc in range(10000) :\n    opt.minimize(loss_fn,beta)\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251>\n\n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9971251>\n\n\n\n\n\n\n회귀분석 문제\n- \\({\\bf y} \\approx 2.5 + 4 {\\bf x}\\)\n\ntnp.random.seed(43052)\nN = 200\nx = tnp.linspace(0,1,N) \nepsilon = tnp.random.randn(N)*0.5\ny = 2.5 + 4*x + epsilon\ny_hat = 2.5 + 4*x\n\n\nplt.plot(x,y,'.')\nplt.plot(x,y_hat,'r--')\n\n\n\n\n\n\n이론적 풀이\n\n풀이1: 스칼라버전\n- 포인트 - \\(S_{xx}=\\sum (x-\\bar x)^2\\),\\(\\quad S_{xy}=\\sum (x-\\bar x)(y- \\bar y)\\) - \\(\\hat{\\beta}_0=\\bar y - \\hat{ \\beta_1} \\bar x\\)$,_1= $\n\nSxx = sum((x-x.mean())**2) \nSxy = sum((x-x.mean())*(y-y.mean())) \n\n\nbeta1_hat = Sxy/Sxx \nbeta0_hat = y.mean() - beta1_hat*x.mean()\n\n\nbeta0_hat,beta1_hat\n\n(<tf.Tensor: shape=(), dtype=float64, numpy=2.583667211565867>,\n <tf.Tensor: shape=(), dtype=float64, numpy=3.933034516733169>)\n\n\n\n\n풀이2: 벡터버전\n\nX=tf.stack([tf.ones(N,dtype='float64'),x],axis=1)\ny=y.reshape(N,1) \n\n\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\n\\(\\hat \\beta = (X^{T}X)^{-1} X^{T} y\\)\n\n\ntf.linalg.inv(X.T@X)@ X.T @y \n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[2.58366721],\n       [3.93303452]])>\n\n\n\n\n풀이3: 벡터버전, 손실함수의 도함수이용 (경사하강법을 이용한 풀이)\n(단, 텐서플로우의 미분기능을 사용하지 않음 ) \\(\\to\\) gradienttape X\n\nX=tf.stack([tf.ones(N,dtype='float64'),x],axis=1)\ny=y.reshape(N,1) \nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n- 초기값 설정\n\nbeta_hat = tnp.array([-5.0,10.0]).reshape(2,1)\n\n- 포인트 - \\(loss'(\\beta)=-2X'y +2X'X\\beta\\) - \\(\\beta_{new} = \\beta_{old} - \\alpha \\times loss'(\\beta_{old})\\)\n\nslope = -2*X.T@y + 2*X.T@X@beta_hat\n\n\nslope\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[-1820.07378797],\n       [ -705.77222696]])>\n\n\n\nalpha = 0.001\n\n\nstep = slope * alpha\nstep\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[-1.82007379],\n       [-0.70577223]])>\n\n\n\nfor epoc in range(1000) : \n  slope = -2*X.T@y + 2*X.T@X@beta_hat\n  beta_hat = beta_hat - alpha*slope\n\n\nbeta_hat\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[2.58366721],\n       [3.93303452]])>\n\n\n\n\n\nGradientTape를 이용\n\n풀이1: 벡터버전\n- 포인트\n## 포인트코드1: 그레디언트 테입  \nwith tf.GradientTape() as tape: \n    loss = \n## 포인트코드2: 미분 \nslope = tape.gradient(loss,beta_hat) \n## 포인트코드3: update \nbeta_hat.assign_sub(slope*alph) \n\ny=y.reshape(N,1) # N=200 \nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\n\n\nfor epoc in range(1000):   \n    with tf.GradientTape() as tape: \n        tape.watch(beta_hat)\n        yhat = X@beta_hat\n        loss = (y-yhat).T @ (y-yhat) / N \n    slope = tape.gradient(loss,beta_hat) \n    beta_hat.assign_sub(slope * alpha) \n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이2: 스칼라버전\n- 포인트\n## 포인트코드: 미분\nslope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat])\n\ny=y.reshape(-1) # N=200 \nx.shape,y.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0)\nalpha = 0.1\n\n\nfor epoc in range(1000):   \n    with tf.GradientTape() as tape: \n        yhat = beta0_hat + beta1_hat*x \n        loss = tf.reduce_sum((y-yhat)**2) / N  #loss = sum((y-yhat)**2) / N \n    slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) \n    beta0_hat.assign_sub(slope0 * alpha) \n    beta1_hat.assign_sub(slope1 * alpha) \n\n\nbeta0_hat, beta1_hat \n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.5836616>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.9330447>)\n\n\n\n\n\nGradientTape + opt.apply_gradients\n\n풀이1: 벡터버전\n- 포인트\n## 포인트코드: 업데이트\nopt.apply_gradients([(slope,beta_hat)])  ## pair의 list가 입력 \n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)  \n### tf.keras.optimizers.SGD 와 똑같은 위치에 있는 모듈이다 그냥 똑같은거다.\n\n\nfor epoc in range(1000): \n    with tf.GradientTape() as tape: \n        yhat = X@beta_hat\n        loss = (y-yhat).T @ (y-yhat) / N \n    slope = tape.gradient(loss,beta_hat)  \n    opt.apply_gradients( [(slope,beta_hat),(slope,beta_hat)] )\n\n\n(y-yhat).T @ (y-yhat) / N, beta_hat \n\n(<tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[0.25493942]])>,\n <tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\n array([[2.58366721],\n        [3.93303452]])>)\n\n\n\n\n풀이2: 스칼라버전\n- 포인트\n## 포인트코드: 업데이트 \nopt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)]) ## pair의 list가 입력 \n\ny=y.reshape(-1)\nx.shape,y.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0) \n\n\nalpha=0.1\n\n\nopt = tf.optimizers.SGD(alpha) \n\n\nfor epoc in range(1000): \n    with tf.GradientTape() as tape: \n        yhat = beta0_hat + beta1_hat*x \n        loss = tf.reduce_sum((y-yhat)**2) / N \n    slope0,slope1 = tape.gradient(loss,[beta0_hat,beㅠta1_hat])  \n    opt.apply_gradients( [(slope0,beta0_hat),(slope1,beta1_hat)] )\n\n\nbeta0_hat,beta1_hat\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.58366>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.933048>)\n\n\n\n\n\nopt.minimize\n\n풀이1: 벡터버전, 사용자정의 손실함수 with lambda\n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha) \nloss_fn = lambda: (y-X@beta_hat).T @ (y-X@beta_hat)/N\n\n\n(y-X@beta_hat).T @ (y-X@beta_hat)\n\n<tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[4811.45696758]])>\n\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat) # 미분 + update \n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이2: 스칼라버전, 사용자정의 손실함수 with lambda\n\ny=y.reshape(-1)\nx.shape,y.shape\n\n(TensorShape([200]), TensorShape([200]))\n\n\n\nbeta0_hat = tf.Variable(-5.0)\nbeta1_hat = tf.Variable(10.0) \n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha) \nloss_fn = lambda: tf.reduce_sum((y-beta0_hat - beta1_hat*x)**2)/N\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,[beta0_hat,beta1_hat]) # 미분 + update \n\n\nbeta0_hat, beta1_hat\n\n- 포인트\n## 포인트코드: 미분 & 업데이트 = minimize \nopt.minimize(loss_fn,[beta0_hat,beta1_hat])\n\n\n풀이3: 벡터버전, 사용자정의 (짧은) 손실함수\n- 포인트\n## 포인트코드: 손실함수정의 \ndef loss_fn():\n    return ??\n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\ndef loss_fn():\n    return (y-X@beta_hat).T @ (y-X@beta_hat)/N\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat) # 미분 + update \n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이4: 벡터버전, 사용자정의 (긴) 손실함수\ndef loss_fn():\n    yhat = X@beta_hat \n    loss = (y-yhat).T @ (y-yhat) / N\n    return loss\n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha)\n\n\ndef loss_fn():\n    yhat = X@beta_hat \n    loss = (y-yhat).T @ (y-yhat) / N\n    return loss\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat) # 미분 + update \n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이5: 벡터버전, 사용자정의 손실함수 <- tf.losses.MSE\n- 포인트\n## 포인트코드: 미리구현되어있는 손실함수 이용 \ntf.losses.MSE(y,yhat)\n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha) \n\n\ndef loss_fn():\n    yhat= X@beta_hat\n    loss = tf.losses.MSE(y.reshape(-1),yhat.reshape(-1))\n    return loss\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat) # 미분 + update \n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n풀이6: 벡터버전, 사용자정의 손실함수 <- tf.losses.MeaSquaredError\n- 포인트\n## 포인트코드: 클래스로부터 손실함수 오브젝트 생성 (함수를 찍어내는 클래스) \nmse_fn = tf.losses.MeanSquaredError()\nmse_fn(y,yhat)\n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\nbeta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1))\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[-5.],\n       [10.]])>\n\n\n\nalpha=0.1\nopt = tf.optimizers.SGD(alpha) \n\n\nmseloss_fn = tf.losses.MeanSquaredError()\n\n\nmseloss_fn(y.reshape(-1),yhat.reshape(-1))\n\n<tf.Tensor: shape=(), dtype=float64, numpy=0.25493940711021423>\n\n\n\ndef loss_fn():\n    yhat= X@beta_hat\n    loss = mseloss_fn(y.reshape(-1),yhat.reshape(-1))\n    return loss\n\n\nfor epoc in range(1000):\n    opt.minimize(loss_fn,beta_hat) # 미분 + update \n\n\nbeta_hat\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[2.58366061],\n       [3.93304684]])>\n\n\n\n\n\ntf.keras.Sequential\n- \\(\\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1x_i\\) 의 서로다른 표현\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\ngv(''' \n    \"1\" -> \"beta0_hat + x*beta1_hat,    bias=False\"[label=\"* beta0_hat\"]\n    \"x\" -> \"beta0_hat + x*beta1_hat,    bias=False\"[label=\"* beta1_hat\"]\n    \"beta0_hat + x*beta1_hat,    bias=False\" -> \"yhat\"[label=\"indentity\"]\n    ''')\n\n\n\n\n\ngv('''\n\"x\" -> \"x*beta1_hat,    bias=True\"[label=\"*beta1_hat\"] ;\n\"x*beta1_hat,    bias=True\" -> \"yhat\"[label=\"indentity\"] ''')\n\n\n\n\n\ngv('''\n\"X=[1 x]\" -> \"X@beta_hat,    bias=False\"[label=\"@beta_hat\"] ;\n\"X@beta_hat,    bias=False\" -> \"yhat\"[label=\"indentity\"] ''')\n\n\n\n\n\n풀이1: 벡터버전, 사용자정의 손실함수\n- 포인트\n## 포인트코드1: 네트워크 생성 \nnet = tf.keras.Sequential()\n\n## 포인트코드2: 네트워크의 아키텍처 설계 \nnet.add(tf.keras.layers.Dense(1,input_shape=(2,),use_bias=False)) \n\n## 포인트코드3: 네트워크 컴파일 = 아키텍처 + 손실함수 + 옵티마이저\nnet.compile(opt,loss=loss_fn2)\n\n## 포인트코드4: 미분 & update \nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\ny=y.reshape(N,1)\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(units=1,input_shape=(2,),use_bias=False)) # 아키텍처 설계 = yhat을 만들계획 \n\n\ndef loss_fn2(y,yhat): ## 손실함수의 정의 \n    return (y-yhat).T @ (y-yhat) / N  \n\n\nalpha=0.1\nopt=tf.optimizers.SGD(alpha) ## 옵티마이저의 선택 \n\n\nnet.compile(opt,loss=loss_fn2) ## 컴파일 = 아키텍처 + 손실함수 + 옵티마이저 \n\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) # 미분 & update 의 반복\n\n<keras.callbacks.History at 0x7f42158d8e10>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836728],\n        [3.9330244]], dtype=float32)>]"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-04-11-(6주차).html",
    "href": "post/Bigdata Analysis/2022-04-11-(6주차).html",
    "title": "06. layer",
    "section": "",
    "text": "imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+s + '; }')\n\n\n\n\\(x \\to \\hat{y}\\) 가 되는 과정을 그림으로 그리기\n- 단순회귀분석의 예시 - \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i, \\quad i=1,2,\\dots,n\\)\n(표현1)\n\n#collapse-hide\ngv(''' \n    \"1\" -> \"β̂₀ + xₙ*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"xₙ\" -> \"β̂₀ + xₙ*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + xₙ*β̂₁,    bias=False\" -> \"ŷₙ\"[label=\"identity\"]\n\n    \".\" -> \"....................................\"[label=\"* β̂₀\"]\n    \"..\" -> \"....................................\"[label=\"* β̂₁\"]\n    \"....................................\" -> \"...\"[label=\" \"]\n\n    \"1 \" -> \"β̂₀ + x₂*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x₂\" -> \"β̂₀ + x₂*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x₂*β̂₁,    bias=False\" -> \"ŷ₂\"[label=\"identity\"]\n    \n    \"1  \" -> \"β̂₀ + x₁*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x₁\" -> \"β̂₀ + x₁*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x₁*β̂₁,    bias=False\" -> \"ŷ₁\"[label=\"identity\"]\n''')\n\n\n\n\n- 표현1의 소감? - 교수님이 고생해서 만든것 같음 - 그런데 그냥 다 똑같은 그림의 반복이라 사실 고생한 의미가 없음.\n(표현2)\n- 그냥 아래와 같이 그리고 “모든 \\(i=1,2,3,\\dots,n\\)에 대하여 \\(\\hat{y}_i\\)을 아래의 그림과 같이 그린다”고 하면 될것 같다.\n\n#collapse-hide\ngv(''' \n    \"1\" -> \"β̂₀ + xᵢ*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"xᵢ\" -> \"β̂₀ + xᵢ*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + xᵢ*β̂₁,    bias=False\" -> \"ŷᵢ\"[label=\"identity\"]\n\n''')\n\n\n\n\n(표현3)\n- 그런데 “모든 \\(i=1,2,3,\\dots,n\\)에 대하여 \\(\\hat{y}_i\\)을 아래의 그림과 같이 그린다” 라는 언급자체도 반복할 필요가 없을 것 같다. (어차피 당연히 그럴테니까) 그래서 단순히 아래와 같이 그려도 무방할듯 하다.\n\n#collapse-hide\ngv(''' \n    \"1\" -> \"β̂₀ + x*β̂₁,    bias=False\"[label=\"* β̂₀\"]\n    \"x\" -> \"β̂₀ + x*β̂₁,    bias=False\"[label=\"* β̂₁\"]\n    \"β̂₀ + x*β̂₁,    bias=False\" -> \"ŷ\"[label=\"identity\"]\n\n''')\n\n\n\n\n(표현4)\n- 위의 모델은 아래와 같이 쓸 수 있다. (\\(\\beta_0\\)를 바이어스로 표현)\n\n#collapse-hide\ngv('''\n\"x\" -> \"x*β̂₁,    bias=True\"[label=\"*β̂₁\"] ;\n\"x*β̂₁,    bias=True\" -> \"ŷ\"[label=\"indentity\"] ''')\n\n\n\n\n\n실제로는 이 표현을 많이 사용함\n\n(표현5)\n- 벡터버전으로 표현하면 아래와 같다. 이 경우에는 \\({\\bf X}=[1,x]\\)에 포함된 1이 bias의 역할을 해주므로 bias = False 임.\n\n#collapse-hide\ngv('''\n\"X\" -> \"X@β̂,    bias=False\"[label=\"@β̂\"] ;\n\"X@β̂,    bias=False\" -> \"ŷ\"[label=\"indentity\"] ''')\n\n\n\n\n\n저는 이걸 좋아해요\n\n(표현6)\n- 딥러닝에서는 \\(\\hat{\\boldsymbol{\\beta}}\\) 대신에 \\(\\hat{{\\bf W}}\\)을 라고 표현한다.\n\n#collapse-hide\ngv('''\n\"X\" -> \"X@Ŵ,    bias=False\"[label=\"@Ŵ\"] ;\n\"X@Ŵ,    bias=False\" -> \"ŷ\"[label=\"identity\"] ''')\n\n\n\n\n- 실제로는 표현4 혹은 표현5를 외우면 된다.\n\n\nLayer의 개념\n- (표현4) 혹은 (표현5)의 그림은 레이어로 설명할 수 있다.\n- 레이어는 항상 아래와 같은 규칙을 가진다. - 첫 동그라미는 레이어의 입력이다. - 첫번째 화살표는 선형변환을 의미한다. - 두번째 동그라미는 선형변환의 결과이다. (이때 bias가 false인지 true인지에 따라서 실제 수식이 조금 다름) - 두번째 화살표는 두번째 동그라미에 어떠한 함수 \\(f\\)를 취하는 과정을 의미한다. - 세번째 동그라미는 레이어의 최종출력이다.\n- 엄청 복잡한데, 결국 레이어를 만들때 위의 그림들을 의미하도록 하려면 아래의 4개의 요소만 필요하다. 1. 레이어의 입력차원 2. 선형변환의 결과로 얻어지는 차원 3. 선형변환에서 바이어스를 쓸지? 안쓸지? 4. 함수 \\(f\\)\n- 주목: 1,2가 결정되면 자동으로 \\(\\hat{{\\bf W}}\\)의 차원이 결정된다.\n(예시) - 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=1: \\(\\hat{\\bf W}\\)는 (2,1) 매트릭스 - 레이어의 입력차원=20, 선형변환의 결과로 얻어지는 차원=5: \\(\\hat{\\bf W}\\)는 (20,5) 매트릭스 - 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=50: \\(\\hat{\\bf W}\\)는 (2,50) 매트릭스\n- 주목2: 이중에서 절대 생략불가능 것은 “2. 선형변환의 결과로 얻어지는 차원” 이다. - 레이어의 입력차원: 실제 레이어에 데이터가 들어올 때 데이터의 입력차원을 컴퓨터 스스로 체크하여 \\(\\hat{\\bf W}\\)의 차원을 결정할 수 있음. - 바이어스를 쓸지? 안쓸지? 기본적으로 쓴다고 가정한다. - 함수 \\(f\\): 기본적으로 항등함수를 가정하면 된다.\n\n\nKeras를 이용한 풀이\n- 기본뼈대: net생성 \\(\\to\\) add(layer) \\(\\to\\) compile(opt,loss) \\(\\to\\) fit(data,epochs)\n- 데이터정리\n\\[{\\bf y}\\approx 2.5 +4x\\]\n\ntnp.random.seed(43052)\nN= 200 \nx= tnp.linspace(0,1,N)\nepsilon= tnp.random.randn(N)*0.5 \ny= 2.5+4*x +epsilon\n\n\nX=tf.stack([tf.ones(N,dtype='float64'),x],axis=1)\n\n\n풀이1: 스칼라버전\n(0단계) 데이터정리\n\ny=y.reshape(N,1)\nx=x.reshape(N,1)\nx.shape,y.shape\n\n(TensorShape([200, 1]), TensorShape([200, 1]))\n\n\n(1단계) net 생성\n\nnet = tf.keras.Sequential() \n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1) ## 선형변환의 결과 차원\n# 입력차원? 데이터를 넣어보고 결정, 바이어스=디폴드값을 쓰겠음 (use_bias=true), 함수도 디폴트값을 쓰겠음 (f(x)=x)\nnet.add(layer)\n\n(3단계) net.compile(opt,loss_fn)\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) \n\n(4단계) net.fit(x,y,epochs)\n\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 \n\n<keras.callbacks.History at 0x7ff91540e790>\n\n\n(결과확인)\n\nnet.weights\n\n[<tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[3.9330251]], dtype=float32)>,\n <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([2.5836723], dtype=float32)>]\n\n\n\\[{\\bf y}\\approx 2.5 +4x\\]\n\n\n풀이2: 벡터버전\n(0단계) 데이터정리\n\nX.shape,y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net 생성\n\nnet = tf.keras.Sequential() \n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False) \nnet.add(layer)\n\n(3단계) net.compile(opt,loss_fn)\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) \n\n(4단계) net.fit(x,y,epochs)\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 \n\n<keras.callbacks.History at 0x7ff91550e210>\n\n\n(결과확인)\n\nnet.weights\n\n[<tf.Variable 'dense_1/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836728],\n        [3.9330244]], dtype=float32)>]\n\n\n\n\n잠시문법정리\n- 잠깐 Dense layer를 만드는 코드를 정리해보자.\n\n아래는 모두 같은 코드이다.\n\n\ntf.keras.layers.Dense(1)\ntf.keras.layers.Dense(units=1)\ntf.keras.layers.Dense(units=1,activation=‘linear’) // identity 가 더 맞는것 같은데..\ntf.keras.layers.Dense(units=1,activation=‘linear’,use_bias=True)\n\n\n아래의 코드1,2는 (1)의 코드들과 살짝 다른코드이다. (코드1과 코드2는 같은코드임)\n\n\ntf.keras.layers.Dense(1,input_dim=2) # 코드1\ntf.keras.layers.Dense(1,input_shape=(2,)) # 코드2\n\n\n아래는 사용불가능한 코드이다.\n\n\ntf.keras.layers.Dense(1,input_dim=(2,)) # 코드1\ntf.keras.layers.Dense(1,input_shape=2) # 코드2\n\n- 왜 input_dim이 필요한가?\n\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Dense(1,use_bias=False)) \n\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Dense(1,use_bias=False,input_dim=2))\n\n- net1의 경우 input_dim을 명시해주지 않아 Weight를 알 수 없다\n\nnet1.weights\n\nValueError: ignored\n\n\n\nnet2.weights\n\n[<tf.Variable 'dense_3/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[-1.053657 ],\n        [ 1.3536845]], dtype=float32)>]\n\n\n- 또한 입력차원을 모르기 깨문에 summary값도 알 수 없다.\n\nnet1.summary()\n\nValueError: ignored\n\n\n\nnet2.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_3 (Dense)             (None, 1)                 2         \n                                                                 \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n풀이3: 스칼라버전, 임의의 초기값을 설정\n(0단계) 데이터정리\n\ny=y.reshape(N,1)\nx=x.reshape(N,1)\nx.shape,y.shape\n\n(TensorShape([200, 1]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential() \n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,input_dim=1)\n\n\nnet.add(layer)\n\n\n초기값을 설정\n\nnet.weights\n\n[<tf.Variable 'dense_4/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[0.534932]], dtype=float32)>,\n <tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n\n\n\nnet.get_weights()\n\n[array([[0.534932]], dtype=float32), array([0.], dtype=float32)]\n\n\n\nweight, bias순으로 출력\n\n\nnet.set_weights?\n\n\nlayer_b.set_weights(layer_a.get_weights()) 와 같은방식으로 쓴다는 것이군?\n\n- 한번따라해보자.\n\n_w = net.get_weights()\n_w\n\n[array([[0.534932]], dtype=float32), array([0.], dtype=float32)]\n\n\n\n_w[0]\n\narray([[0.534932]], dtype=float32)\n\n\n\n길이가 2인 리스트이고, 각 원소는 numpy array 임\n\n\nnet.set_weights(\n    [np.array([[10.0]],dtype=np.float32), # weight, β1_hat\n     np.array([-5.0],dtype=np.float32)] # bias, β0_hat \n)\n\n\nnet.weights\n\n[<tf.Variable 'dense_4/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[10.]], dtype=float32)>,\n <tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([-5.], dtype=float32)>]\n\n\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1),tf.losses.MSE) \n\n(4단계) net.fit()\n\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7ff9152a5c50>\n\n\n결과확인\n\nnet.weights\n\n[<tf.Variable 'dense_4/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[3.933048]], dtype=float32)>,\n <tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([2.58366], dtype=float32)>]\n\n\n\n\n풀이4: 벡터버전, 임의의 초기값을 설정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False,input_dim=2) \n\n\nnet.add(layer)\n\n\n초기값을 설정하자\n\nnet.set_weights([np.array([[ -5.0],[10.0]], dtype=np.float32)])\n\n\nnet.get_weights()\n\n[array([[-5.],\n        [10.]], dtype=float32)]\n\n\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1), tf.losses.MSE) \n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7ff9151dbad0>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_5/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.58366 ],\n        [3.933048]], dtype=float32)>]\n\n\n- 사실 실전에서는 초기값을 설정할 필요가 별로 없음.\n\n\n풀이5: 벡터버전 사용자정의 손실함수\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nlayer = tf.keras.layers.Dense(1,use_bias=False) \n\n\nnet.add(layer)\n\n(3단계) net.compile()\n\nloss_fn = lambda y,yhat: (y-yhat).T @ (y-yhat) / N\n\n\nnet.compile(tf.keras.optimizers.SGD(0.1), loss_fn) \n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7ff915103dd0>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_6/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836728],\n        [3.9330244]], dtype=float32)>]\n\n\n\n\n풀이6: 벡터버전, net.compile의 옵션으로 손실함수 지정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\n\n(3단계) net.compile()\n\nnet.compile(tf.keras.optimizers.SGD(0.1), loss='mse') \n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=1000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7ff91502bb50>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_7/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5836728],\n        [3.9330244]], dtype=float32)>]\n\n\n\n\n풀이7: 벡터버전, net.compile의 옵션으로 손실함수 지정 + 옵티마이저 지정\n(0단계) 데이터정리\n\nX.shape, y.shape\n\n(TensorShape([200, 2]), TensorShape([200, 1]))\n\n\n(1단계) net생성\n\nnet = tf.keras.Sequential()\n\n(2단계) net.add(layer)\n\nnet.add(tf.keras.layers.Dense(1,use_bias=False))\n\n(3단계) net.compile()\n\nnet.compile(optimizer='sgd', loss='mse') \n#net.optimizer.lr = tf.Variable(0.1,dtype=tf.float32)\n#net.optimizer.lr = 0.1\n\n(4단계) net.fit()\n\nnet.fit(X,y,epochs=5000,verbose=0,batch_size=N) \n\n<keras.callbacks.History at 0x7ff915117550>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_8/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[2.5842712],\n        [3.9319096]], dtype=float32)>]\n\n\n\n\n\n여러가지 회귀모형의 적합과 학습과정의 모니터링\n\n예제1\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 x_i\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.random.randn(N) \nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*x +epsilon\n\n\nX= np.stack([np.ones(N),x],axis=1)\ny= y.reshape(N,1)\n\n\nplt.plot(x,y,'o') # 관측한 자료 \n\n\n\n\n\nbeta_hat = np.array([-3,-2]).reshape(2,1)\n\n\nyhat = X@beta_hat \n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.reshape(-1),'-') \n\n\n\n\n더 좋은 적합선을 얻기위해서!\n\nslope = (2*X.T@X@beta_hat - 2*X.T@y)/ N \nbeta_hat2 = beta_hat - 0.1*slope  \nyhat2 = X@beta_hat2\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.reshape(-1),'-') \nplt.plot(x,yhat2.reshape(-1),'-') \n\n\n\n\n초록색이 좀 더 나아보인다.\n\nbeta_hat = np.array([-3,-2]).reshape(2,1) \nbeta_hats = beta_hat # beta_hats = beta_hat.copy() 가 더 안전한 코드입니다. \nfor i in range(1,30):\n    yhat = X@beta_hat \n    slope = (2*X.T@X@beta_hat - 2*X.T@y) / N \n    beta_hat = beta_hat - 1.0*slope # 0.1은 적당, 0.3은 쪼금빠르지만 그래도 적당, 0.9는 너무 나간것같음, 1.0 은 수렴안함, 1.2 \n    beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) \n\n\nbeta_hats\n\narray([[-3.        ,  7.12238255, -1.2575366 ,  5.73166742, -0.1555309 ,\n         4.86767499,  0.51106397,  4.36611576,  0.87316777,  4.12348617,\n         1.01165173,  4.07771926,  0.97282343,  4.19586617,  0.77814101,\n         4.46653491,  0.4299822 ,  4.89562729, -0.08537358,  5.50446319,\n        -0.79684366,  6.32975688, -1.74933031,  7.42517729, -3.00603683,\n         8.86442507, -4.6523303 , 10.74592463, -6.80132547, 13.19938129],\n       [-2.        ,  8.70824998,  0.16165717,  6.93399596,  1.62435964,\n         5.72089586,  2.63858056,  4.86387722,  3.37280529,  4.22385379,\n         3.94259478,  3.70397678,  4.43004465,  3.23363047,  4.89701606,\n         2.75741782,  5.39439054,  2.22728903,  5.96886945,  1.59655409,\n         6.66836857,  0.81489407,  7.54676324, -0.17628423,  8.66856437,\n        -1.44867655, 10.11401544, -3.09256176, 11.98507323, -5.22340389]])\n\n\n\nb0hats = beta_hats[0].tolist()\nb1hats = beta_hats[1].tolist()\n\n\nnp.linalg.inv(X.T@X) @ X.T @ y\n\narray([[2.5451404 ],\n       [3.94818596]])\n\n\n\nfrom matplotlib import animation \nplt.rcParams[\"animation.html\"] = \"jshtml\" \n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n<Figure size 864x360 with 0 Axes>\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림 \nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*x) \n# ax2: 오른쪽그림 \nβ0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\nβ0=β0.reshape(-1)\nβ1=β1.reshape(-1)\nloss_fn = lambda b0,b1: np.sum((y-b0-b1*x)**2)\nloss = list(map(loss_fn, β0,β1))\nax2.scatter(β0,β1,loss,alpha=0.02) \nax2.scatter(2.5451404,3.94818596,loss_fn(2.5451404,3.94818596),s=200,marker='*') \n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*x) \n    ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\") \n\nani = animation.FuncAnimation(fig,animate,frames=30) \nani\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\n예제2\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i}\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) +epsilon\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\nX= np.stack([np.ones(N),np.exp(-x)],axis=1)\ny= y.reshape(N,1)\n\n\nbeta_hat = np.array([-3,-2]).reshape(2,1)\nbeta_hats = beta_hat.copy() # shallow copy, deep copy <--- 여름 방학 특강 \nfor i in range(1,30): \n    yhat = X@beta_hat \n    slope = (2*X.T@X@beta_hat - 2*X.T@y) /N \n    beta_hat = beta_hat - 0.05*slope \n    beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) \n\n\nbeta_hats\n\narray([[-3.        , -1.74671631, -0.82428979, -0.14453919,  0.35720029,\n         0.72834869,  1.0036803 ,  1.20869624,  1.36209751,  1.47759851,\n         1.56525696,  1.63244908,  1.68458472,  1.72563174,  1.75850062,\n         1.78532638,  1.80767543,  1.82669717,  1.84323521,  1.85790889,\n         1.8711731 ,  1.88336212,  1.89472176,  1.90543297,  1.91562909,\n         1.92540859,  1.93484428,  1.94399023,  1.9528867 ,  1.96156382],\n       [-2.        , -0.25663415,  1.01939241,  1.95275596,  2.63488171,\n         3.13281171,  3.49570765,  3.75961951,  3.95098231,  4.08918044,\n         4.18842797,  4.2591476 ,  4.30898175,  4.34353413,  4.36691339,\n         4.38213187,  4.39139801,  4.39633075,  4.39811673,  4.3976256 ,\n         4.3954946 ,  4.3921905 ,  4.38805511,  4.3833386 ,  4.37822393,\n         4.37284482,  4.36729887,  4.36165718,  4.35597148,  4.35027923]])\n\n\n\nb0hats= beta_hats[0].tolist()\nb1hats= beta_hats[1].tolist()\n\n\nnp.linalg.inv(X.T@X)@X.T@y\n\narray([[2.46307644],\n       [3.99681332]])\n\n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n<Figure size 864x360 with 0 Axes>\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림 \nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x))\n# ax2: 오른쪽그림 \nβ0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\nβ0=β0.reshape(-1)\nβ1=β1.reshape(-1)\nloss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2)\nloss = list(map(loss_fn, β0,β1))\nax2.scatter(β0,β1,loss,alpha=0.02) \nax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker='*') \n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x))\n    ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\") \n\nani = animation.FuncAnimation(fig,animate,frames=30) \nani\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\n예제3\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i} + \\beta_2 \\cos(5x_i)\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon\n\n\nplt.plot(x,y,'o')\n\n\n\n\n\nX=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) \ny=y.reshape(N,1)\n\n\nbeta_hat = np.array([-3,-2,-1]).reshape(3,1) \nbeta_hats = beta_hat.copy()\nfor i in range(1,30):\n    yhat = X@beta_hat \n    slope = (2*X.T@X@beta_hat -2*X.T@y) /N \n    beta_hat = beta_hat - 0.1 * slope \n    beta_hats= np.concatenate([beta_hats,beta_hat],axis=1)\n\n\nbeta_hats\n\narray([[-3.        , -0.71767532,  0.36255782,  0.89072137,  1.16423101,\n         1.31925078,  1.41819551,  1.48974454,  1.54713983,  1.59655416,\n         1.64091846,  1.68167278,  1.71956758,  1.75503084,  1.78833646,\n         1.81968188,  1.84922398,  1.877096  ,  1.90341567,  1.92828934,\n         1.95181415,  1.97407943,  1.99516755,  2.01515463,  2.0341111 ,\n         2.05210214,  2.06918818,  2.08542523,  2.10086524,  2.11555643],\n       [-2.        ,  1.16947474,  2.64116513,  3.33411605,  3.66880042,\n         3.83768856,  3.92897389,  3.98315095,  4.01888831,  4.04486085,\n         4.06516144,  4.08177665,  4.09571971,  4.10754954,  4.1176088 ,\n         4.12613352,  4.13330391,  4.13926816,  4.14415391,  4.14807403,\n         4.15112966,  4.1534121 ,  4.15500404,  4.15598045,  4.15640936,\n         4.15635249,  4.15586584,  4.15500014,  4.15380139,  4.1523112 ],\n       [-1.        , -0.95492718, -0.66119313, -0.27681968,  0.12788212,\n         0.52254445,  0.89491388,  1.24088224,  1.55993978,  1.85310654,\n         2.12199631,  2.36839745,  2.59408948,  2.8007666 ,  2.99000967,\n         3.16327964,  3.32192026,  3.46716468,  3.60014318,  3.72189116,\n         3.83335689,  3.93540864,  4.02884144,  4.11438316,  4.19270026,\n         4.26440288,  4.33004965,  4.39015202,  4.44517824,  4.49555703]])\n\n\n\nb0hats,b1hats,b2hats = beta_hats\n\n\nnp.linalg.inv(X.T@X) @ X.T @ y\n\narray([[2.46597526],\n       [4.00095138],\n       [5.04161877]])\n\n\n\nfig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12)\n\n<Figure size 864x360 with 0 Axes>\n\n\n\nax1= fig.add_subplot(1,2,1)\nax2= fig.add_subplot(1,2,2,projection='3d')\n# ax1: 왼쪽그림 \nax1.plot(x,y,'o')\nline, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x) + b2hats[0]*np.cos(5*x))\n# ax2: 오른쪽그림 \n# β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing='ij')\n# β0=β0.reshape(-1)\n# β1=β1.reshape(-1)\n# loss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2)\n# loss = list(map(loss_fn, β0,β1))\n# ax2.scatter(β0,β1,loss,alpha=0.02) \n# ax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker='*') \n\ndef animate(i):\n    line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x) + b2hats[i]*np.cos(5*x))\n    # ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=\"grey\") \n\nani = animation.FuncAnimation(fig,animate,frames=30) \nani\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\n예제3: 케라스로 해보자!\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i} + \\beta_2 \\cos(5x_i)\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon\n\n\nX=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) \ny=y.reshape(N,1)\n\n\nnet = tf.keras.Sequential() # 1: 네트워크 생성\nnet.add(tf.keras.layers.Dense(1,use_bias=False)) # 2: add layer \nnet.compile(tf.optimizers.SGD(0.1), loss='mse') # 3: compile\nnet.fit(X,y,epochs=30, batch_size=N) # 4: fit \n\nEpoch 1/30\n1/1 [==============================] - 0s 186ms/step - loss: 82.1027\nEpoch 2/30\n1/1 [==============================] - 0s 11ms/step - loss: 23.9512\nEpoch 3/30\n1/1 [==============================] - 0s 14ms/step - loss: 10.7256\nEpoch 4/30\n1/1 [==============================] - 0s 9ms/step - loss: 7.0664\nEpoch 5/30\n1/1 [==============================] - 0s 6ms/step - loss: 5.5521\nEpoch 6/30\n1/1 [==============================] - 0s 5ms/step - loss: 4.6075\nEpoch 7/30\n1/1 [==============================] - 0s 6ms/step - loss: 3.8836\nEpoch 8/30\n1/1 [==============================] - 0s 7ms/step - loss: 3.2909\nEpoch 9/30\n1/1 [==============================] - 0s 6ms/step - loss: 2.7971\nEpoch 10/30\n1/1 [==============================] - 0s 7ms/step - loss: 2.3838\nEpoch 11/30\n1/1 [==============================] - 0s 6ms/step - loss: 2.0374\nEpoch 12/30\n1/1 [==============================] - 0s 5ms/step - loss: 1.7471\nEpoch 13/30\n1/1 [==============================] - 0s 5ms/step - loss: 1.5038\nEpoch 14/30\n1/1 [==============================] - 0s 6ms/step - loss: 1.2998\nEpoch 15/30\n1/1 [==============================] - 0s 6ms/step - loss: 1.1288\nEpoch 16/30\n1/1 [==============================] - 0s 7ms/step - loss: 0.9854\nEpoch 17/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.8652\nEpoch 18/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.7645\nEpoch 19/30\n1/1 [==============================] - 0s 7ms/step - loss: 0.6800\nEpoch 20/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.6092\nEpoch 21/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.5499\nEpoch 22/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.5001\nEpoch 23/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.4584\nEpoch 24/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.4234\nEpoch 25/30\n1/1 [==============================] - 0s 6ms/step - loss: 0.3941\nEpoch 26/30\n1/1 [==============================] - 0s 12ms/step - loss: 0.3695\nEpoch 27/30\n1/1 [==============================] - 0s 9ms/step - loss: 0.3489\nEpoch 28/30\n1/1 [==============================] - 0s 10ms/step - loss: 0.3316\nEpoch 29/30\n1/1 [==============================] - 0s 12ms/step - loss: 0.3171\nEpoch 30/30\n1/1 [==============================] - 0s 4ms/step - loss: 0.3050\n\n\n<keras.callbacks.History at 0x7ff90f24bd10>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_9/kernel:0' shape=(3, 1) dtype=float32, numpy=\n array([[2.485702 ],\n        [3.9252913],\n        [4.6923084]], dtype=float32)>]\n\n\n\nplt.plot(x,y,'o') \nplt.plot(x,(X@net.weights).reshape(-1),'--')\n\n\n\n\n\n\n\n숙제\n\n예제2: 케라스를 이용하여 아래를 만족하는 적절한 \\(\\beta_0\\)와 \\(\\beta_1\\)을 구하라. 적합결과를 시각화하라. (애니메이션 시각화 X)\nmodel: \\(y_i \\approx \\beta_0 +\\beta_1 e^{-x_i}\\)\n\nnp.random.seed(43052) \nN= 100 \nx= np.linspace(-1,1,N)\nepsilon = np.random.randn(N)*0.5 \ny= 2.5+4*np.exp(-x) +epsilon"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-04-18-(7주차).html",
    "href": "post/Bigdata Analysis/2022-04-18-(7주차).html",
    "title": "07. Adam",
    "section": "",
    "text": "import numpy as np\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-04-18-(7주차).html#piece-wise-linear-regression",
    "href": "post/Bigdata Analysis/2022-04-18-(7주차).html#piece-wise-linear-regression",
    "title": "07. Adam",
    "section": "piece-wise linear regression",
    "text": "piece-wise linear regression\n\n이제까지는 단순선형회귀에 대해서 다루었다면 일정한 지점에서 절단점을 가지는 piecc-wise linear regreesion을 다루어보자\n\n\\[y = x + 0.3\\varepsilon \\quad x\\leq 0\\]\n\\[y = 3.5x + 0.3\\varepsilon\\]\n\nnp.random.seed(2021025)\n\nN = 100\n\nx = np.linspace(-1,1,N)\ne = np.random.normal(size=N)\n\ny = list(map(lambda x,e : x + 0.3*e if x<=0 else 3.5*x +0.3*e,x,e))\ny = np.array(y)\n\n\n시각화\n\nplt.plot(x[x<=0],y[x<=0],\".\")\nplt.plot(x[x>0],y[x>0],\".\")\nplt.legend([\"x<=0\", \"x>0\"])\n\n<matplotlib.legend.Legend at 0x7fee233e4e50>\n\n\n\n\n\n\n\n풀이 1\n\ntip : x,y가 tensor가 아니어도 신경망 적합시 잘 적합된다.\n\n\nx = x.reshape(N,1)\ny = y.reshape(N,1)\n\n\nnet = tf.keras.Sequential()\nlayer = tf.keras.layers.Dense(1)\nnet.add(layer)\n\nnet.compile(tf.optimizers.SGD(0.1),loss = \"mse\")\nnet.fit(x,y,batch_size=N,verbose=0,epochs=1000)\n\n<keras.callbacks.History at 0x7fee1f1e8b50>\n\n\n\nw = net.weights\n\n\nbeta0, beta1 = w[1],w[0]\n\n\nyhat = x*beta1+beta0\nyhat = yhat.reshape(N,)\n\n\nplt.plot(x[x<=0],y[x<=0],\".\")\nplt.plot(x[x>0],y[x>0],\".\")\nplt.plot(x,yhat,\".\")\nplt.legend([\"x<=0\", \"x>0\",\"yhat\"])\n\n<matplotlib.legend.Legend at 0x7fee1f0f8110>\n\n\n\n\n\n\n위 적합한 모형은 틀렸음\n우리가 의도한 모델은 피스와이즈 모델이지 단순선형모델이 아님 \\(\\to\\) 꺽은선 형태의 모델이 형성되어야 한다.\n위 같은 문제를 underfit 의 문제라고 함.\n\n\n\n풀이 2. 비선형 활성화 함수의 도입(Relu)\nRelu\n\\[relu(x) = max(0,x)\\]\n\n#collapse\ngv('''\n\"x\" -> \"x*w,    bias=True\"[label=\"*w\"] ;\n\"x*w,    bias=True\" -> \"y\"[label=\"relu\"] ''')\n\n\n\n\n\n즉, \\(x\\)가 0보다 작은 녀석들은 전부다 0으로 보내는 활성화함수이다.\n\n\nnet2 = tf.keras.Sequential()\n\nl1 = tf.keras.layers.Dense(1,input_shape=(1,))\n\na1 = tf.keras.layers.Activation(tf.nn.relu)\n\nnet2.add(l1)\nnet2.add(a1)\n\n\nl1의 weight(\\(\\beta_1\\))값이 1보다 크게 나와야 현재 보여주려는 문제 의도대로 풀 수 있다.\n\n\nl1.weights\n\n[<tf.Variable 'dense_3/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[0.47527015]], dtype=float32)>,\n <tf.Variable 'dense_3/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n\n\n\n현재 생성한 네트워크 상황을 확인해보자.\n\n\nu1 = l1(x)\nv1 = a1(u1)\n\n\nplt.plot(x,x,\"--\")\nplt.plot(x,u1,\"--\")\nplt.plot(x,v1,\"--\")\nplt.legend([\"x=x\",\"y=linear x\",\"relu (y)\"])\n\n<matplotlib.legend.Legend at 0x7fee1efb1990>\n\n\n\n\n\n\n오 표현하려는 relu 함수처럼 잘 표시된 것 같다.\n\n이제 모델을 컴파일 하고 적합해보자!!\n\nnet2.compile(tf.optimizers.SGD(0.1),loss=\"MSE\")\nnet2.fit(x,y,N,1000,0)\n\n<keras.callbacks.History at 0x7fee1ed30f50>\n\n\n\nyhat = net2(x)\nyhat = yhat.reshape(N,) ## 이건 굳이 안해줘도 되나 습관화하자 (목적에 맞게 차원변환)\n\n\nplt.plot(x[x>=0],y[x>=0],\".\",label=\"x>=0\")\nplt.plot(x[x<0],y[x<0],\".\",label=\"x<0\")\nplt.plot(x,yhat,\"--\",label=\"hat y\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7feea848ca10>\n\n\n\n\n\n\n그런데… 아직도 \\(x<0\\)인 부분에서는 모델 적합이 잘 되지 않은 것 같다.\nwhy? relu 함수의 특성상 \\(relu(y) = 0, (y<0)\\) 으로 전부 보냈기 때문!\n이제 선형변환된 값들이 0 이하로 떨어지는 부분들을 어떻게 하면 잘 적합시킬 수 있는지 해결하자!\n해결책 : \\(\\hat {y}\\) 가 2개가 있으면?\n\n\n\n풀이 3\n\n목표 : \\(\\hat y\\)를 2개 만들자,즉 \\(\\hat y \\to (N,2)\\)\n위의 의도대로 하려면 하나의 \\(\\bf X\\)를 받아 2개의 출력이 나와야한다!!\n\n\nnet3 = tf.keras.Sequential()\n\nl1 = tf.keras.layers.Dense(2,input_shape=(1,))\na1 = tf.keras.layers.Activation(tf.nn.relu)\n\nnet3.add(l1)\nnet3.add(a1)\n\n(현재 네트워크 상황 확인)\n\nl1.weights\n\n[<tf.Variable 'dense_50/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[-1.1642166,  0.6747991]], dtype=float32)>,\n <tf.Variable 'dense_50/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n\n\n\n## collapse\nfig,(ax1,ax2) = plt.subplots(1,2)\nfig.set_figwidth(10)\nfig.set_figheight(5)\n\nax1.plot(x,x,\"--\")\nax1.plot(x,l1(x),\"--\")\nax1.legend([\"x\",\"l1=w1*x+b\",\"l2=w2*x+b\"])\n\nax2.plot(x,x,\"--\")\nax2.plot(x,a1(l1(x)),\"--\")\nax2.legend([\"x\",\"al(l1)\",\"al(l2)\"])\n\n<matplotlib.legend.Legend at 0x7fee129f4e10>\n\n\n\n\n\n\n위의 문제점\n\n1. 우리의 의도대로 relu를 지나서 \\(\\hat y\\) 를 2차원으로 만들었다..\n2. 근데 차피 relu를 지나면 al(l1),al(l2)가 0보다 작으면 \\(\\hat y\\)는 전부 0이 나올 것임\n3. 해결책 : 노드를 추가해서 al(l1),al(l2)를 입력으로 받아 선형모형을 다시 만들자\n4. 또한 \\(\\hat y\\)의 차원의 수를 의도적으로 2차원으로 만들었으니 다시 1차원으로 변경해주자!\n\n즉, 입력차원 2, 출력차원 1로 변환해주는 노드를 추가\n\npython\ntemp  = tf.keras.Sequential()\n_l1 = tf.keras.layers.Dense(2,input_shape=(1,))\n_a1 = tf.keras.layers.Activation(tf.nn.relu)\n_l2 = tf.keras.layers.Dense(2,input_shape=(2,))\n\ntemp.add(_l1)\ntemp.add(_a1)\ntemp.add(_l2)\n\ntemp.summary()\n\nModel: \"sequential_21\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_26 (Dense)            (None, 2)                 4         \n                                                                 \n activation_15 (Activation)  (None, 2)                 0         \n                                                                 \n dense_27 (Dense)            (None, 2)                 6         \n                                                                 \n=================================================================\nTotal params: 10\nTrainable params: 10\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nl2 = tf.keras.layers.Dense(1,input_shape=(2,))\n\n\nnet3.add(l2)\n\n\nl2.weights\n\n[<tf.Variable 'dense_51/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[-0.54625785],\n        [ 0.949442  ]], dtype=float32)>,\n <tf.Variable 'dense_51/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n\n\n\nnet3.summary()\n\nModel: \"sequential_34\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_50 (Dense)            (None, 2)                 4         \n                                                                 \n activation_28 (Activation)  (None, 2)                 0         \n                                                                 \n dense_51 (Dense)            (None, 1)                 3         \n                                                                 \n=================================================================\nTotal params: 7\nTrainable params: 7\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nnet3.compile(tf.optimizers.SGD(0.1),loss=\"mse\")\nnet3.fit(x,y,N,1000,0)\n\n<keras.callbacks.History at 0x7fee12a87950>\n\n\n\nyhat = net3(x)\nyhat = yhat.reshape(N,)\n\n(네트워크의 변화과정 확인)\n\nl1_1=l1(x)[:,[0]].reshape(N,)\nl1_2=l1(x)[:,[1]].reshape(N,)\n\n\nl2.weights\n\n[<tf.Variable 'dense_51/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[-0.734917 ],\n        [ 1.8227489]], dtype=float32)>,\n <tf.Variable 'dense_51/bias:0' shape=(1,) dtype=float32, numpy=array([-0.05703843], dtype=float32)>]\n\n\n\nfig,(ax1,ax2,ax3) = plt.subplots(1,3)\nfig.set_figwidth(14b)\nfig.set_figheight(5)\n\nax1.plot(x,y,\".\")\nax1.plot(x,l1_1,\"--\")\nax1.plot(x,l1_2,\"--\")\nax1.legend([\"y\",\"l1\",\"l2\"])\nax1.set_title(\"1st linear regreesion\")\n\nax2.plot(x,y,\".\")\nax2.plot(x,a1(l1(x)),\"--\")\nax2.set_title(\"2st relu\")\nax2.legend([\"y\",\"relu(l1)\",\"relu(l2)\"])\n\nax3.plot(x,y,\".\")\nax3.plot(x,l2(a1(l1(x))),\"--\")\nax3.set_title(\"3rd linear regression\")\nax3.legend([\"y\",\"l2\"])\n\n<matplotlib.legend.Legend at 0x7fee12862550>\n\n\n\n\n\n\n표본의 수가 작아서 그런가 의도한대로 잘 안나오다가 한 10번 돌려서 나온 것 같음..\n즉. 여기까지 요약하자면\n2개의 출력을 가지는 linear \\(\\to\\) relu \\(\\to\\) 1차원의 linear\n근데 풀이 3의 실패하는 경우도 존재함.\n\n\nnp.random.seed(4)\nN = 100\n\nx = np.linspace(-1,1,N)\ne = np.random.normal(size=N)\n\ny = list(map(lambda x,e : x + 0.3*e if x<=0 else 3.5*x +0.3*e,x,e))\ny = np.array(y)\n\nx= x.reshape(N,1)\ny= y.reshape(N,1)\n\n\ntf.random.set_seed(2)\n\nnet3 = tf.keras.Sequential()\n####\nnet3.add(tf.keras.layers.Dense(2))\nnet3.add(tf.keras.layers.Activation(\"relu\"))\nnet3.add(tf.keras.layers.Dense(1))\n####\nnet3.compile(optimizer = tf.optimizers.SGD(0.1),loss=\"mse\")\n\nnet3.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7fee1d2ff490>\n\n\n\nl2.weights\n\n[<tf.Variable 'dense_51/kernel:0' shape=(2, 1) dtype=float32, numpy=\n array([[-0.734917 ],\n        [ 1.8227489]], dtype=float32)>,\n <tf.Variable 'dense_51/bias:0' shape=(1,) dtype=float32, numpy=array([-0.05703843], dtype=float32)>]\n\n\n\n그냥 이거는 교수님 코드 따라서 친건데 밑에 그래프하고 똑같음\n\n\n#collapse\nfig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4) \nfig.set_figwidth(16) \nax1.plot(x,y,'.')\nax1.plot(x,l1(x)[:,0],'--r')\nax1.plot(x,l1(x)[:,1],'--b')\nax2.plot(x,y,'.')\nax2.plot(x,a1(l1(x))[:,0],'--r')\nax2.plot(x,a1(l1(x))[:,1],'--b')\nax3.plot(x,y,'.')\nax3.plot(x,a1(l1(x))[:,0]*(-0.734917),'--r')\nax3.plot(x,a1(l1(x))[:,1]*(1.8227489)+(-0.05703843),'--b')\nax4.plot(x,y,'.')\nax4.plot(x,a1(l1(x))[:,0]*(-0.734917)+a1(l1(x))[:,1]*(1.8227489)+(-0.05703843),'--')\n\n\n\n\n\n#collapse\nfig,(ax1,ax2,ax3) = plt.subplots(1,3)\n\nfig.set_figwidth(14)\nfig.set_figheight(5)\n\nax1.plot(x,y,\".\")\nax1.plot(x,l1_1,\"--\")\nax1.plot(x,l1_2,\"--\")\nax1.legend([\"y\",\"l1\",\"l2\"])\nax1.set_title(\"1st linear regreesion\")\n\nax2.plot(x,y,\".\")\nax2.plot(x,a1(l1(x)),\"--\")\nax2.set_title(\"2st relu\")\nax2.legend([\"y\",\"relu(l1)\",\"relu(l2)\"])\n\nax3.plot(x,y,\".\")\nax3.plot(x,l2(a1(l1(x))),\"--\")\nax3.set_title(\"3rd linear regression\")\nax3.legend([\"y\",\"l2\"])\n\n<matplotlib.legend.Legend at 0x7fee1c33b250>\n\n\n\n\n\n\n사실 위 경우는 2개의 linear가 둘 다 일을 잘하고 있는 것임\n근데 가끔 둘 중에 한 개가 일을 제대로 하지 못하는 경우가 발생 (예를 들어 선형변환된 선중에 하나가 계속 0값을 가지면?)\n즉, 현재 학습한 에포크에서 우리가 만든 모델이 최적상태이다. \\(\\to\\) 완전한 예측은 난 못해 이런느낌\n이를 global minimum(로스를 최소하는)을 찾지 못하고 local minimum(현재 조건에서 최적화한)에 빠졌다고 한다.\n7주차 공부한 피스와이즈는 DNN 이라고 할 수 있다."
  },
  {
    "objectID": "post/Bigdata Analysis/2022-04-18-(7주차).html#logistic",
    "href": "post/Bigdata Analysis/2022-04-18-(7주차).html#logistic",
    "title": "07. Adam",
    "section": "logistic",
    "text": "logistic\n\n결론부터 말하자면 로지스틱은 MSE대신 BCE(Binary cross entropy)를 손실함수로 사용한다.\n왜? 로지스틱의 손실함수 BCE인 경우 convex(2차함수모양)하기 때문에 글로벌 미니멈을 잘 찾을 수 있음!!\n\n\n예제\n\nN = 2000\n\nx = np.linspace(-1,1,N).reshape(N,1)\n\n초기 가중치 설정 및 베르누이 분포\n\nw0 = -1\nw1 = 5\n\nu = w0+x*w1\n\nv = tf.nn.sigmoid(u)\ny = tf.constant(np.random.binomial(1,v))\n\n\nplt.plot(x,y,\".\",alpha=0.2)\nplt.plot(x,v,\"--\")\nplt.legend([\"y\",\"p(y=1)\"])\n\n<matplotlib.legend.Legend at 0x7fee1851fe90>\n\n\n\n\n\n\n케라스를 이용하여 구현\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1,input_shape=(1,),activation=\"sigmoid\"))\nnet.compile(tf.optimizers.SGD(0.1),loss='binary_crossentropy' )\n\n\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7fee12459b90>\n\n\n\nfig, (ax1,ax2) = plt.subplots(1,2)\nfig.set_figwidth(12)\n\nax1.plot(x,y,\".\",alpha=0.2)\nax1.plot(x,v,\"--\")\nax1.legend([\"y\",\"p(y=1)\"])\nax1.set_title(\"not keras\")\n\nax2.plot(x,y,\".\",alpha=0.2)\nax2.plot(x,net(x),\"--\")\nax2.legend([\"y\",\"p(y=1)\"])\nax2.set_title(\"with keras\")\n\nText(0.5, 1.0, 'with keras')\n\n\n\n\n\n\n\nMSE vs BCE\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Dense(1,input_shape=(1,),activation=\"sigmoid\"))\nnet2.compile(tf.optimizers.SGD(0.1),loss='mse' )\n\n\nnet2.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7fee11a3e2d0>\n\n\n\nplt.plot(x,y,\".\",alpha=0.2)\nplt.plot(x,net(x),\"--\")\nplt.plot(x,net2(x),\"--\")\nplt.legend([\"y\",\"BCE\",\"MSE\"])\nplt.title(\"BCE vs MSE\")\n\nText(0.5, 1.0, 'BCE vs MSE')\n\n\n\n\n\n\n로지스틱 회귀모형의 특성상 \\(x\\)는 0을 기점으로 약간 지그재그에 가까울수록 좋은 모형인데\nBCE가 확실히 MSE에 비해서 지그재그에 가까워 보인다!\n즉 BCE는 동일한 조건(epoch,가중치)에서 BCE가 MSE보다 적합이 뛰어나다.\n\n\nMSE, BCE loss 비교\n\nmseloss_fn = lambda y,yhat: tf.reduce_mean((y-yhat)**2)\nbceloss_fn = lambda y,yhat: -tf.reduce_mean(y*tnp.log(yhat) + (1-y)*tnp.log(1-yhat))\n\n\ndef loss_fn1(w0,w1):\n    u = w0+w1*x \n    yhat = np.exp(u)/(np.exp(u)+1)\n    return mseloss_fn(y,yhat) \n\ndef loss_fn2(w0,w1):\n    u = w0+w1*x \n    yhat = np.exp(u)/(np.exp(u)+1)\n    return bceloss_fn(y,yhat)\n\n\nw0, w1 = np.meshgrid(np.arange(-10,3,0.2), np.arange(-1,10,0.2), indexing='ij')\nw0, w1 = w0.reshape(-1), w1.reshape(-1)\n\n\nloss1 = list(map(loss_fn1,w0,w1))\nloss2 = list(map(loss_fn2,w0,w1))\n\n\nw0.shape,w1.shape\n\n((3575,), (3575,))\n\n\n\nfig = plt.figure()\nfig.set_figwidth(9)\nfig.set_figheight(9)\nax1=fig.add_subplot(1,2,1,projection='3d')\nax2=fig.add_subplot(1,2,2,projection='3d')\nax1.elev=15\nax2.elev=15\nax1.azim=75\nax2.azim=75\nax1.scatter(w0,w1,loss1,s=0.1)\nax1.set_title(\"MSE Loss\")\nax2.scatter(w0,w1,loss2,s=0.1) \nax2.set_title(\"BCE Loss\")\n\nText(0.5, 0.92, 'BCE Loss')\n\n\n\n\n\n\n로스를 확인한 결과 오른쪽 그림이 더 최적해에 잘 수렴할 것 같음\n이번엔 동일한 기준을 설정해놓고 확인해보자.\n\n- 파라미터 : (w0,w1) = (-3.0,1.0), epoch 15 마다 관찰\n\nx.shape\n\n(2000, 1)\n\n\n\ntf.ones(N,dtype=tf.float64).reshape(N,1)\n\n<tf.Tensor: shape=(2000, 1), dtype=float64, numpy=\narray([[1.],\n       [1.],\n       [1.],\n       ...,\n       [1.],\n       [1.],\n       [1.]])>\n\n\n\nX = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1)\n\n\nX.shape\n\nTensorShape([2000, 2])\n\n\n\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet1.compile(tf.keras.optimizers.SGD(0.1),loss=\"mse\")\nnet1.fit(X,y,N,1,0)\n\n<keras.callbacks.History at 0x7fee0a4f27d0>\n\n\n\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet2.compile(tf.keras.optimizers.SGD(0.1),loss=\"binary_crossentropy\")\nnet2.fit(X,y,N,1,0)\n\n<keras.callbacks.History at 0x7fee0a47add0>\n\n\n\n에포크 한번 안덜리면 set_weights못함…\n\n\nnet1.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\nnet2.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nnet_mse.get_weights(), net_bce.get_weights()\n\n([array([[-3.],\n         [-1.]], dtype=float32)], [array([[-3.],\n         [-1.]], dtype=float32)])\n\n\n이제 15 epoch마다 기록해보자\n\nmse_hat = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\nbce_hat = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\n\n\ntf.concat([mse_hat,bce_hat],axis=1)\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-3., -3.],\n       [-1., -1.]], dtype=float32)>\n\n\n\nnet1.weights[0]\n\n<tf.Variable 'dense_116/kernel:0' shape=(2, 1) dtype=float32, numpy=\narray([[-1.4075234 ],\n       [ 0.26052466]], dtype=float32)>\n\n\n\nfor i in range(29) :\n    net1.fit(X,y,N,15,0)\n    net2.fit(X,y,N,15,0)\n    mse_hat = tf.concat([mse_hat,net1.weights[0]],axis=1)\n    bce_hat = tf.concat([bce_hat,net2.weights[0]],axis=1)\n\n\n\n시각화\n\nfrom matplotlib import animation\nplt.rcParams[\"animation.html\"] = \"jshtml\"\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"SGD, Winit=(-3,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r'); \nline3, = ax3.plot(x,1/(1+np.exp(-X@mse_hat[:,0])),'--b')\nax3.set_title(\"MSE\")\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@bce_hat[:,0])),'--b')\nax4.set_title(\"BCE\")\ndef animate(i):\n    _w0_mse,_w1_mse = mse_hat[:,i]\n    _w0_bce,_w1_bce = bce_hat[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@mse_hat[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@bce_hat[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\nMSE의 경우 초기 기울기가 크지 않아 loss가 0으로 가는 속도가 매우 느리다.\n즉 동일한 기준에서 BCE loss가 성능이 더좋다!!\n\n\n\nAdam 옵티마이저\n\nnet_mse = tf.keras.Sequential()\nnet_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn) \nnet_mse.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 438ms/step - loss: 0.3187\n\n\n<keras.callbacks.History at 0x7fee07c90f90>\n\n\n\nnet_bce = tf.keras.Sequential()\nnet_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid')) \nnet_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn) \nnet_bce.fit(X,y,epochs=1,batch_size=N)\n\n1/1 [==============================] - 0s 495ms/step - loss: 0.8539\n\n\n<keras.callbacks.History at 0x7fee07a886d0>\n\n\n\nnet_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\nnet_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)])\n\n\nWhat_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\nWhat_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)\n\n\nfor k in range(29): \n    net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0)\n    What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) \n    What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1)\n\n\nfig = plt.figure()\nfig.set_figwidth(6)\nfig.set_figheight(6)\nfig.suptitle(\"Adam, Winit=(-3,-1)\")\nax1=fig.add_subplot(2,2,1,projection='3d')\nax2=fig.add_subplot(2,2,2,projection='3d')\nax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75\nax3=fig.add_subplot(2,2,3)\nax4=fig.add_subplot(2,2,4)\n\nax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color='red',marker='*',s=200)\nax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color='red',marker='*',s=200)\n\nax3.plot(x,y,','); ax3.plot(x,v,'--r'); \nline3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),'--b')\nax4.plot(x,y,','); ax4.plot(x,v,'--r')\nline4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),'--b')\n\ndef animate(i):\n    _w0_mse,_w1_mse = What_mse[:,i]\n    _w0_bce,_w1_bce = What_bce[:,i]\n    ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color='gray')\n    ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color='gray')\n    line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i])))\n    line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i])))\n\nani = animation.FuncAnimation(fig, animate, frames=30)\nplt.close()\nani\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\nAdam의 경우 손실함수 미분시 자동으로 scale을 조정해주고 계산된 기울기 값을 누적시켜 반영하기 때문에 잘 적합됨을 보인다.\n그렇다고 손실함수 상관없이 Adam을 무조건으로 선호하면 안된다.\n아무리 Adam이라고 해도 초기값 설정이 좋지 않으면 로컬미니멈에서 빠져나오기 힘들다.\n따라서 경우에 맞게 일차적으로 손실함수를 결정한 후 그 후에 옵티마이저를 결정하는 것이 옳은 판단!!"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-04-27-중간고사 해설.html",
    "href": "post/Bigdata Analysis/2022-04-27-중간고사 해설.html",
    "title": "08. Mid term",
    "section": "",
    "text": "imports\n\nimport numpy as np\nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt \n\n\n\n1. 경사하강법과 tf.GradientTape()의 사용방법 (30점)\n- 문제 의도 : 손실함수와 로그우도함수, LSE와 MLE의 개념\n(1) 아래는 \\(X_i \\overset{iid}{\\sim} N(3,2^2)\\) 를 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tnp.random.randn(10000)*2+3\nx\n\n<tf.Tensor: shape=(10000,), dtype=float64, numpy=\narray([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n        5.01072291, -1.13050477])>\n\n\n함수 \\(L(\\mu,\\sigma)\\)을 최대화하는 \\((\\mu,\\sigma)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\mu\\)의 초기값은 2로 \\(\\sigma\\)의 초기값은 3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x_i-\\mu}{\\sigma})^2}\\]\n(풀이)\n\nsigma = tf.Variable(3.0) \nmu = tf.Variable(2.0)\n\n\nwith tf.GradientTape() as tape: \n    pdf = 1/sigma * tnp.exp(-0.5*((x-mu)/sigma)**2)\n    logL = tf.reduce_sum(tnp.log(pdf) ) \ntape.gradient(logL,[mu,sigma]) \n\n[<tf.Tensor: shape=(), dtype=float32, numpy=1129.3353>,\n <tf.Tensor: shape=(), dtype=float32, numpy=-1488.3431>]\n\n\n\nfor i in range(1000):\n    with tf.GradientTape() as tape: \n        pdf = 1/sigma * tnp.exp(-0.5*((x-mu)/sigma)**2)\n        logL = tf.reduce_sum(tnp.log(pdf) ) \n    slope1, slope2 = tape.gradient(logL,[mu,sigma]) \n    mu.assign_add(slope1* 0.1/10000) # N=10000 \n    sigma.assign_add(slope2* 0.1/10000) \n\n\nmu,sigma\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0163972>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.9870595>)\n\n\n(2) 아래는 \\(X_i \\overset{iid}{\\sim} Ber(0.8)\\)을 생성하는 코드이다.\n\ntf.random.set_seed(43052)\nx= tf.constant(np.random.binomial(1,0.8,(10000,)))\nx\n\n<tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 1, 1, 1])>\n\n\n함수 \\(L(p)\\)을 최대화하는 \\(p\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(p\\)의 초기값은 0.3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=p^{x_i}(1-p)^{1-x_i}\\]\n(풀이)\n\np=tf.Variable(0.3) \nfor i in range(1000):\n    with tf.GradientTape() as tape: \n        pdf = p**x * (1-p)**(1-x) \n        logL = tf.reduce_sum(tnp.log(pdf)) \n    slope = tape.gradient(logL,p) \n    p.assign_add(slope* 0.1/10000) # N=10000 \n\n\np\n\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.802>\n\n\n(3) 아래의 모형에 따라서 \\(\\{Y_i\\}_{i=1}^{10000}\\)를 생성하는 코드를 작성하라. - \\(Y_i \\overset{iid}{\\sim} N(\\mu_i,1)\\) - \\(\\mu_i = \\beta_0 + \\beta_1 x_i = 0.5 + 2 x_i\\) , where \\(x_i = \\frac{i}{10000}\\)\n함수 \\(L(\\beta_0,\\beta_1)\\)을 최대화하는 \\((\\beta_0,\\beta_1)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\beta_0,\\beta_1\\)의 초기값은 모두 1로 설정할 것)\n\\[L(\\beta_0,\\beta_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(y_i)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y_i-\\mu_i)^2}, \\quad \\mu_i=\\beta_0+\\beta_1 x_i\\]\n(풀이)\n\nx= tf.constant(np.arange(1,10001)/10000)\ny= tnp.random.randn(10000) + (0.5 + 2*x) \n\n\nbeta0= tf.Variable(1.0)\nbeta1= tf.Variable(1.0) \nfor i in range(2000):\n    with tf.GradientTape() as tape: \n        mu = beta0 + beta1*x \n        pdf = tnp.exp(-0.5*(y-mu)**2)\n        logL = tf.reduce_sum(tnp.log(pdf)) \n    slope1, slope2 = tape.gradient(logL,[beta0,beta1]) \n    beta0.assign_add(slope1* 0.1/10000) # N=10000 \n    beta1.assign_add(slope2* 0.1/10000) \n\n\nbeta0, beta1\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.5553082>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.8987025>)\n\n\n\n\n2. 회귀분석의 이론적해와 tf.keras.optimizer 이용방법 (20점)\n- 문제 의도 : 효율적으로 학습률을 찾는 방법, 미니 배치의 개념 + 확률적 경사하강법\n아래와 같은 선형모형을 고려하자.\n\\[y_i = \\beta_0 + \\beta_1 x_i +\\epsilon_i.\\]\n이때 오차항은 정규분포로 가정한다. 즉 \\(\\epsilon_i \\overset{iid}{\\sim} N(0,\\sigma^2)\\)라고 가정한다.\n관측데이터가 아래와 같을때 아래의 물음에 답하라.\n\n#x= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])\n\nX= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4],\n              [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]])\ny= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , \n              63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286]).reshape(10,1)\n\n(1) MSE loss를 최소화 하는 \\(\\beta_0,\\beta_1\\)의 해석해를 구하라.\n(풀이)\n\ntf.linalg.inv(X.T @ X ) @ X.T @ y\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[9.94457323],\n       [2.21570461]])>\n\n\n(2) 경사하강법과 MSE loss의 도함수를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것.\n(풀이)\n\nbeta= tnp.array([5,10]).reshape(2,1) \n\n\nfor i in range(50000): \n    beta = beta - 0.0015 * (-2*X.T @y + 2*X.T@X@beta)/10 \n\n\nbeta\n\n<tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[9.28579424],\n       [2.24168098]])>\n\n\n(3) tf.keras.optimizers의 apply_gradients()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n(풀이)\n\nbeta = tf.Variable(tnp.array([5.0,10.0]).reshape(2,1)) \nopt = tf.optimizers.SGD(0.0015) \nfor i in range(50000): \n    with tf.GradientTape() as tape: \n        loss = (y-X@beta).T @ (y-X@beta) / 10 \n    slope = tape.gradient(loss,beta) \n    opt.apply_gradients([(slope,beta)]) \n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[9.28579425],\n       [2.24168098]])>\n\n\n(4) tf.keras.optimizers의 minimize()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n(풀이)\n\nbeta = tf.Variable(tnp.array([5.0,10.0]).reshape(2,1)) \nopt = tf.optimizers.SGD(0.0015) \nloss_fn = lambda: (y-X@beta).T @ (y-X@beta) / 10 \nfor i in range(50000): \n    opt.minimize(loss_fn,beta)  \n\n\nbeta\n\n<tf.Variable 'Variable:0' shape=(2, 1) dtype=float64, numpy=\narray([[9.28579425],\n       [2.24168098]])>\n\n\n\n\n3. keras를 이용한 풀이 (30점)\n- 문제 의도 : 복잡한 모형과 오버피팅의 개념\n(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i, \\quad i=0,1,\\dots, 999\\]\n여기에서 \\(t_i=\\frac{2\\pi i}{1000}\\) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.array(range(1000))* np.pi/1000\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.2)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2,3,1,0,0,0.5 이다)\n(풀이)\n\ny = y.reshape(1000,1)\nx1 = np.cos(t) \nx2 = np.cos(2*t)\nx3 = np.cos(3*t)\nx4 = np.cos(4*t)\nx5 = np.cos(5*t)\nX = tf.stack([x1,x2,x3,x4,x5],axis=1)\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(loss='mse',optimizer='sgd') \nnet.fit(X,y,batch_size=1000, epochs = 1000, verbose=0) \n\n<keras.callbacks.History at 0x7fb471478050>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense/kernel:0' shape=(5, 1) dtype=float32, numpy=\n array([[ 3.0008404e+00],\n        [ 1.0067019e+00],\n        [ 1.8562071e-03],\n        [-3.8460975e-03],\n        [ 4.9710521e-01]], dtype=float32)>,\n <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([-2.0122595], dtype=float32)>]\n\n\n(2) 아래와 같은 모형을 고려하자.\n\\[y_i \\sim Ber(\\pi_i), ~ \\text{where} ~ \\pi_i=\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\]\n위의 모형에서 관측한 데이터는 아래와 같다.\n\ntf.random.set_seed(43052)\nx = tnp.linspace(-1,1,2000) \ny = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64) \nplt.plot(x,y,'.',alpha=0.05)\n\n\n\n\ntf.keras를 이용하여 \\(w_0,w_1\\)을 추정하라. (참고: \\(w_0, w_1\\)에 대한 참값은 -1과 5이다.)\n(풀이)\n\nx= x.reshape(2000,1) \ny= y.reshape(2000,1) \n\n\nnet= tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='sgd', loss= tf.losses.binary_crossentropy) \nnet.fit(x,y,epochs=10000,batch_size=2000, verbose=0)\n\n<keras.callbacks.History at 0x7fb4719abb10>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_1/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[4.232856]], dtype=float32)>,\n <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([-0.90837014], dtype=float32)>]\n\n\n\nplt.plot(y,'.')\nplt.plot(net(x),'--')\n\n\n\n\n\n\n4. Piecewise-linear regression (15점)\n- 분석결과를 뜯어보는 방법\n아래의 모형을 고려하자.\nmodel: \\(y_i=\\begin{cases} x_i +0.3\\epsilon_i & x\\leq 0 \\\\ 3.5x_i +0.3\\epsilon_i & x>0 \\end{cases}\\)\n아래는 위의 모형에서 생성한 샘플이다.\n\n## data \nnp.random.seed(43052)\nN=100\nx= np.linspace(-1,1,N).reshape(N,1)\ny= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x<0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1)\n\n(1) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n$ = _0+_1x $\n\n\ntf.random.set_seed(43054) \nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감\n\n<keras.callbacks.History at 0x7fb4712b94d0>\n\n\n케라스에 의해 추정된 \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\)을 구하라.\n\nnet.weights\n\n[<tf.Variable 'dense_2/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[2.2616348]], dtype=float32)>,\n <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.6069048], dtype=float32)>]\n\n\n(풀이)\n\n\\(\\hat{\\beta}_0= 0.6069048\\)\n\\(\\hat{\\beta}_1= 2.2616348\\)\n\n(2) 다음은 \\((x_i,y_i)\\)를 아래와 같은 아키텍처로 적합시키는 코드이다.\n\n\\(\\boldsymbol{u}= x\\boldsymbol{W}^{(1)}+\\boldsymbol{b}^{(1)}\\)\n\\(\\boldsymbol{v}= \\text{relu}(u)\\)\n\\(yhat= \\boldsymbol{v}\\boldsymbol{W}^{(2)}+b^{(2)}\\)\n\n\ntf.random.set_seed(43056) \n## 1단계\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(2))\nnet.add(tf.keras.layers.Activation('relu')) \nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer=tf.optimizers.SGD(0.1),loss='mse')\nnet.fit(x,y,epochs=1000,verbose=0,batch_size=N)\n\n<keras.callbacks.History at 0x7f14b3dc7490>\n\n\n\\({\\boldsymbol u}\\)를 이용하여 \\({\\boldsymbol v}\\)를 만드는 코드와 \\({\\boldsymbol v}\\)를 이용하여 \\(yhat\\)를 만드는 코드를 작성하라.\n(풀이)\n\nu=net.layers[0](x)\nv=net.layers[1](u) \nyhat=net.layers[2](v) \n\n(3) 아래는 (1)-(2)번 모형에 대한 discussion이다. 올바른 것을 모두 골라라.\n(곤이) (2) 모형은 활성화함수로 relu를 사용하였다.\n(철용) (1) 모형에서 추정해야할 파라메터의 수는 2개이다.\n(아귀) (2) 모형이 (1) 모형보다 복잡한 모형이다.\n(짝귀) (1) 의 모형은 오버피팅의 위험이 있다.\n\n\n5. 다음을 잘 읽고 참과 거짓을 판단하라. (5점)\n(1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제나 전역최소해를 찾을 수 있다.\n(2) tf.GradeintTape()는 경사하강법을 이용하여 최적점을 찾아주는 tool이다.\n\n그냥 미분해주는 계산기이다.\n\n(3) 학습률이 크다는 것은 파라메터는 1회 업데이트 하는 양이 크다는 것을 의미한다.\n(4) 학습률이 크면 학습파라메터의 수렴속도가 빨라지지만 때때로 과적합에 빠질 수도 있다.\n\n과적합은 모형의 복잡도와 관려있는 것이지 학습률과는 연관이 없다\n\n(5) 단순회귀분석에서 MSE loss를 최소화 하는 해는 경사하강법을 이용하지 않아도 해석적으로 구할 수 있다."
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-01-(9주차).html",
    "href": "post/Bigdata Analysis/2022-05-01-(9주차).html",
    "title": "09. MLE",
    "section": "",
    "text": "imports\n\nimport numpy as np\nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt \n\n\n\n우도함수와 최대우도추정량\n\n베이스 : 우도함수는 특정 모수가 주어졌을 때 추출된 샘플들이 얻어질 확률이지 해당 모수의 값에대한 확률을 의미하지 않는다.\n\n(예제)\n\\(X_i \\overset{iid}{\\sim} Ber(p)\\)에서 얻은 샘플이 아래와 같다고 하자.\n\nx=[0,1,0,1] \nx\n\n[0, 1, 0, 1]\n\n\n\\(p\\)는 얼마라고 볼 수 있는가? –> 0.5\n왜?? \\(p\\)가 0.5라고 주장할 수 있는 이론적 근거, 혹은 논리체계가 무엇인가?\n- suppose: \\(p=0.1\\) 이라고 하자.\n그렇다면 \\((x_1,x_2,x_3,x_4)=(0,1,0,1)\\)와 같은 샘플이 얻어질 확률이 아래와 같다.\n\n0.9 * 0.1 * 0.9 * 0.1 ## 우도함수 계산값\n\n0.008100000000000001\n\n\n- suppose: \\(p=0.2\\) 이라고 하자.\n그렇다면 \\((x_1,x_2,x_3,x_4)=(0,1,0,1)\\)와 같은 샘플이 얻어질 확률이 아래와 같다.\n\n0.8 * 0.2 * 0.8 * 0.2 ## 우도함수 계산값\n\n0.025600000000000008\n\n\n- 질문1: \\(p=0.1\\)인것 같냐? 아니면 \\(p=0.2\\)인것 같냐? -> 두 가지 경우 중 \\(p=0.2\\) 가 답인 것 같다. - 왜?? \\(p=0.2\\)일 확률이 더 크다! \\(\\to\\) 이렇게 말하기 어려움\n\n왜 어렵냐?? \\(\\to\\) 확률이 더 크다! 확률 이라는 단어를 함부로 쓸 수가 없다\n\n\n(여기서 잠깐 중요한것) 확률이라는 말을 함부로 쓸 수 없다.\n- 0.0256은 “\\(p=0.2\\)일 경우 샘플 (0,1,0,1)이 얻어질 확률”이지 “\\(p=0.2\\)일 확률”은 아니다.\\((\\star\\star\\star)\\)\n“\\(p=0.2\\)인 확률” 이라는 개념이 성립하려면 아래코드에서 sum([(1-p)*p*(1-p)*p for p in _plist])이 1보다는 작아야 한다. (그런데 1보다 크다)\n즉, 아래와 같이 p=0.499일 때 샘플 (0,1,0,1)이 얻어질 확률\n\n(1-0.499)*0.499*(1-0.499)*0.499\n\n0.06249950000099999\n\n\n\n_plist = np.linspace(0.499,0.501,1000) \n_prob=[(1-p)*p*(1-p)*p for p in _plist]\n\n\n_prob[:1] # p 가 0.499일확률\n_prob[2] # p가 0.499 + epsilon\n\n0.06249950399697206\n\n\n아래와 같이 p의 합이 1보다 크기 때문에 \\(p=0.1\\,\\, or \\,\\, p=0.2\\)일 확률이다 라고 정의할수 없다.\n\nsum(_prob)\n\n62.49983299986714\n\n\n따라서 우도함수(가능도)는 주어진 확률값에 대한 임의의 샘플들이 얻어질 확률이라고 정의하자.\n- 확률이라는 말을 쓸 수 없지만 확률의 느낌은 있음 -> 가능도라는 말을 쓰자. - 0.0256 \\(=\\) \\(p\\)가 0.2일 경우 샘플 (0,1,0,1)이 얻어질 확률 \\(\\to\\) \\(p\\)가 0.2일 가능도\n\n- 다시 질문1로 돌아가자! - 질문1: \\(p=0.1\\)인 것 같냐? 아니면 \\(p=0.2\\)인 것 같냐? -> 답 \\(p=0.2\\) -> 왜? \\(p=0.2\\)인 가능도가 더 크니까! - 질문2: \\(p=0.2\\)인 것 같냐? 아니면 \\(p=0.3\\)인 것 같냐? -> 답 \\(p=0.3\\) -> 왜? \\(p=0.3\\)인 가능도가 더 크니까!\n- 궁극의 질문: \\(p\\)가 뭐일 것 같아? - \\(p\\)가 입력으로 들어가면 가능도가 계산되는 함수를 만들자. - 그 함수를 최대화하는 \\(p\\)를 찾자. - 그 \\(p\\)가 궁극의 질문에 대한 대답이 된다.\n- 잠깐 용어정리 - 가능도함수 \\(=\\) 우도함수 \\(=\\) likelihood function \\(:=\\) \\(L(p)\\) - \\(p\\)의 maximum likelihood estimator \\(=\\) p의 MLE \\(=\\) \\(\\hat{p}^{mle}\\) \\(=\\) \\(\\text{argmax}_p L(p)\\) \\(=\\) \\(\\hat{p}\\)\n\n\n중간고사 1번\n(1) \\(N(\\mu,\\sigma)\\)에서 얻은 샘플이 아래와 같다고 할때 \\(\\mu,\\sigma\\)의 MLE를 구하여라.\n<tf.Tensor: shape=(10000,), dtype=float64, numpy=\narray([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n        5.01072291, -1.13050477])>\n(2) \\(Ber(p)\\)에서 얻은 샘플이 아래와 같다고 할 때 \\(p\\)의 MLE를 구하여라.\n<tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 0, 0, 1])>\n(3) \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\), \\(\\epsilon_i \\overset{iid}{\\sim} N(0,1)\\) 일때 \\((\\beta_0,\\beta_1)\\)의 MLE를 구하여라. (회귀모형)\n(풀이) 가능도함수\n\\[L(\\beta_0,\\beta_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(y_i)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}(y_i-\\mu_i)^2}, \\quad \\mu_i=\\beta_0+\\beta_1 x_i\\]\n를 최대화하는 \\(\\beta_0,\\beta_1\\)을 구하면된다. 그런데 이것은 아래를 최소화하는 \\(\\beta_0,\\beta_1\\)을 구하는 것과 같다.\n\\[-\\log L(\\beta_0,\\beta_1) = \\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2\\]\n위의 식은 SSE와 같다. 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 MSE를 최소화하는 \\(\\beta_0,\\beta_1\\)을 구하면 된다.\n중간고사 1-(3)의 다른 풀이\nstep1: 생성\n\nx= tf.constant(np.arange(1,10001)/10000)\ny= tnp.random.randn(10000) + (0.5 + 2*x) \n\nstep2: minimize MSEloss (원래는 maximize log-likelihood)\n\nmaximize likelihood였던 문제를 minimize MSEloss로 바꾸어도 되는근거? 주어진 함수(=가능도함수)를 최대화하는 \\(\\beta_0,\\beta_1\\)은 MSE를 최소화하는 \\(\\beta_0,\\beta_1\\)과 동일하므로\n\n\nbeta0= tf.Variable(1.0)\nbeta1= tf.Variable(1.0) \nfor i in range(2000):\n    with tf.GradientTape() as tape: \n        #minus_log_likelihood = tf.reduce_sum((y-beta0-beta1*x)**2)\n        loss =  tf.reduce_sum((y-beta0-beta1*x)**2)\n    slope1, slope2 = tape.gradient(loss,[beta0,beta1]) \n    beta0.assign_sub(slope1* 0.1/10000) # N=10000 \n    beta1.assign_sub(slope2* 0.1/10000) \n\n\nbeta0,beta1\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.4626273>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0632904>)\n\n\n- 문제를 풀면서 생각해보니 손실함수는 -로그가능도함수로 선택하면 될 것 같다? - 손실함수를 선택하는 기준이 -로그가능도함수만 존재하는 것은 아니나 대부분 그러하긴함\n(4) 출제하지 못한 중간고사 문제\n아래의 모형을 생각하자. - \\(Y_i \\overset{iid}{\\sim} Ber(\\pi_i)\\) - \\(\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}=\\frac{\\exp(-1+5x_i)}{1+\\exp(-1+5x_i)}\\)\n아래는 위의 모형에서 얻은 샘플이다.\n\nx = tnp.linspace(-1,1,2000)\npi = tnp.exp(-1+5*x) / (1+tnp.exp(-1+5*x))\ny = np.random.binomial(1,pi)\ny = tf.constant(y)\n\n함수 \\(L(w_0,w_1)\\)을 최대화하는 \\((w_0,w_1)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\((w_0,w_1)\\)의 초기값은 모두 0.1로 설정할 것)\n\\[L(w_0,w_1)=\\prod_{i=1}^{n}f(y_i), \\quad f(x_i)={\\pi_i}^{y_i}(1-\\pi_i)^{1-y_i},\\quad \\pi_i=\\text{sigmoid}(w_0+w_1x_i)\\]\n(풀이1)\n\nw0hat = tf.Variable(1.0) \nw1hat = tf.Variable(1.0) \n\n\nfor i in range(1000): \n    with tf.GradientTape() as tape: \n        pihat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x))\n        pdf = pihat**y * (1-pihat)**(1-y) \n        logL = tf.reduce_mean(tnp.log(pdf)) \n    slope1,slope2 = tape.gradient(logL,[w0hat,w1hat])\n    w0hat.assign_add(slope1*0.1) \n    w1hat.assign_add(slope2*0.1) \n\n\nw0hat,w1hat\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.8487661>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.1949835>)\n\n\n(해석) - 로지스틱에서 가능도함수와 BCEloss의 관계\n\\(L(w_0,w_1)\\)를 최대화하는 \\(w_0,w_1\\)은 아래를 최소화하는 \\(w_0,w_1\\)와 같다.\n\\[-\\log L(w_0,w_1) = - \\sum_{i=1}^{n}\\big(y_i \\log(\\pi_i) + (1-y_i)\\log(1-\\pi_i)\\big)\\]\n이것은 최적의 \\(w_0,w_1\\)을 \\(\\hat{w}_0,\\hat{w}_1\\)이라고 하면 \\(\\hat{\\pi}_i=\\frac{\\exp(\\hat{w}_0+\\hat{w}_1x_i)}{1+\\exp(\\hat{w}_0+\\hat{w}_1x_i)}=\\hat{y}_i\\)이 되고 따라서 위의 식은 \\(n\\times\\)BCEloss의 형태임을 쉽게 알 수 있다.\n결국 로지스틱 모형에서 \\((w_0,w_1)\\)의 MLE를 구하기 위해서는 BCEloss를 최소화하는 \\((w_0,w_1)\\)을 구하면 된다!\n(풀이2)\n\nw0hat = tf.Variable(1.0) \nw1hat = tf.Variable(1.0) \n\n\nfor i in range(1000): \n    with tf.GradientTape() as tape: \n        yhat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x))\n        loss = tf.losses.binary_crossentropy(y,yhat)\n    slope1,slope2 = tape.gradient(loss,[w0hat,w1hat])\n    w0hat.assign_sub(slope1*0.1) \n    w1hat.assign_sub(slope2*0.1) \n\n\nw0hat,w1hat\n\n(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.8487661>,\n <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.1949835>)\n\n\n\n\n손실함수의 설계 (선택)\n- 회귀분석이든 로지스틱이든 손실함수는 minus_log_likelihood 로 선택한다. - 그런데 (오차항이 정규분포인) 회귀분석 일때는 minus_log_likelihood 가 MSEloss가 되고 - 로지스틱일때는 minus_log_likelihood 가 BCEloss가 된다\n- minus_log_likelihood가 손실함수를 선택하는 유일한 기준은 아니다. <— 참고만하세요, 이 수업에서는 안중요합니다. - 오차항이 대칭이고 서로독립이며 등분산 가정을 만족하는 어떠한 분포에서의 회귀모형이 있다고 하자. 이 회귀모형에서 \\(\\hat{\\beta}\\)은 여전히 MSEloss를 최소화하는 \\(\\beta\\)를 구함으로써 얻을 수 있다.\n\n그러나 샘플 추출시 임의의 분포 가정을 가정하기 때문에 분포를 모를 경우 minus_log_likelihood를 구할 수 없다. 그러면 왜 이것을 썻는가? 위 같은 분포 가정을 하여기 때문이다!!\n이 경우 MSEloss를 쓰는 이론적근거? \\(\\hat{\\beta}\\)이 BLUE(Best Linear Unbiased Estimator)가 되기 때문임 (가우스-마코프정리)"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-02-lecture.html",
    "href": "post/Bigdata Analysis/2022-05-02-lecture.html",
    "title": "10. 확률적 경사하강법",
    "section": "",
    "text": "import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow.experimental.numpy as tnp\n\n\ntf.config.experimental.list_physical_devices()\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-02-lecture.html#중간고사-관련-잡담",
    "href": "post/Bigdata Analysis/2022-05-02-lecture.html#중간고사-관련-잡담",
    "title": "10. 확률적 경사하강법",
    "section": "중간고사 관련 잡담",
    "text": "중간고사 관련 잡담\n\n중간고사 3번문제\n- 특이한 모형 : 오버핏이 일어날 수 없는 모형이다.\n- 회귀분석은 과적합이 안된다! \\(\\to\\) 알아서 \\(n\\)이 커질수록 유의미하지 않은 변수들을 거슬러줌 * 모형이 스스로 변수에 coffecient에 대한 유의성 검정을 수행 따라서 과적합 이슈가 없다 * keypoint : 무조건 변수가 많다고 과적합이 일어나지 않음 * 빨강, 노랑, 파랑으로 모든 색깔을 표현 가능(\\(\\star\\star\\star\\))\n\n\n중간고사 1-(3)번 문제"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-02-lecture.html#경사하강법과-확률적경사하강법",
    "href": "post/Bigdata Analysis/2022-05-02-lecture.html#경사하강법과-확률적경사하강법",
    "title": "10. 확률적 경사하강법",
    "section": "경사하강법과 확률적경사하강법",
    "text": "경사하강법과 확률적경사하강법\n\n확률적 경사하강법을 오늘부터 배울 거임!!\n\n\nver1: 모든 샘플을 사용하여 slope계산 (Gradient decent)\n\n기본까지 했던 방법\n10개의 샘플을 관측!\n\n(epoch1) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) \\(loss=\\sum_{i=1}^{10}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\n\n\nver2: 하나의 샘플만 사용하여 slope계산(stochastic gradient descent)\n\n(epoch이 3번이면 for문이 30번돌아감)\n\n(epoch1) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_2)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) - \\(loss=(y_1-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_2-\\beta_0-\\beta_1x_1)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - … - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n\n총 쏘는거 생각 : ver2는 동일한 총알이 주어졌을 때 그냥 막 쏘는거임\n동일 iteration 대비 효율이 좋은 것은 ver1이다.\n\n왜냐 ver1의경우 iteration == epoch이기 때문!\n\n동일 epoch 대비 효율이 좋은 것도 ver1이다.\n정확도 또한 ver1이 더 높을 것이다. 그러나 이터레이셧 횟수는 ver2가 더 많을 것이다.\nver1은 모든 샘플을 고려해 한번에 기울기를 계산(질로 승부), ver2는 양으로 승부하는 것이라고 생각한다.\n\n\n\nver3: \\(m(\\leq n)\\)개의 샘플만 사용하여 slope계산\n\\(m=3\\)이라고 하자.\n(epoch1) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n(epoch2) - \\(loss=\\sum_{i=1}^{3}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=4}^{6}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=\\sum_{i=7}^{9}(y_i-\\beta_0-\\beta_1x_i)^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\) - \\(loss=(y_{10}-\\beta_0-\\beta_1x_{10})^2 \\quad \\to \\quad slope \\quad \\to \\quad update\\)\n…\n\nVer 3의 겨우 파라미터 업데이트시 한번도 사용이 안되는 샘플이 존재할 수 가 있다.\n\n\n\n용어의 정리\n\n옛날 (좀 더 엄밀)\n- ver1: gradient descent, batch gradient descent\n- ver2: stochastic gradient descent (확률적 경사 하강법)\n- ver3: mini-batch gradient descent, mini-batch stochastic gradient descent\n\n\n요즘\n- ver1: gradient descent\n- ver2: stochastic gradient descent with batch size = 1\n- ver3: stochastic gradient descent (확률적 경사하강법) - https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고.\nnote: 이렇게 많이 쓰는 이유? ver1,2는 사실상 없는 방법이므로\n\n\n\nver1,2,3 이외에 좀 더 지저분한 것들이 있다.\n- ver2,3에서 샘플을 셔플할 수도 있다.\n- ver3에서 일부 샘플이 학습에 참여 안하는 버전도 있다.\n- 개인적 생각: 크게3개정도만 알면 괜찮고 나머지는 그렇게 유의미하지 않아보인다.\n\n\nDiscussion\n- 핵심개념 - 메모리사용량: ver1 > ver3 > ver2 (한번에 한개의 파라미터를 업데이트 할 때!!) - 계산속도: ver1 > ver3 > ver2 (한번에 한개의 파라미터를 업데이트 할 때!!) - local-min에 갇힘: ver1> ver3 > ver2 (이건 알고리즘 구성의 차이)\n* ver1 은 local-min을 잘 찾는다 * ver2, ver3은 운좋게 local-min을 탈출한다\n- 본질: GPU 메모리가 한정되어 있어서 ver1을 쓰지는 못한다. GPU 메모리를 가장 적게쓰는것은 ver2인데 이것은 너무 불안정하다.\n- 틀리진 않지만 어색한 블로그 정리 내용들 - 경사하강법은 종종 국소최소점에 갇히는 문제가 있다. 이를 해결하기 위해서 등장한 방법이 확률적 경사하강법이다.(X) * 틀린말은 아니나 그것을 의도하고 만든 것은 아님, 가끔 그럴 때도 있는 것이지 확률적 경사하강법을 쓴다고 local_minimum에 빠지지 않는 것은 아니다. - 경사하강법은 계산시간이 오래걸린다(O). 계산을 빠르게 하기 위해서 등장한 방법이 확률적 경사하강법이다.(O) * 1회 업데이트는 빠르게 계산하나, 하지만 그것이 최적의 \\(\\beta\\)를 빠르게 얻을 수 있는 것은 아님\n\n결론\n\n확률적 경사하강법을 쓰는 이유는 메모리적 문제이다. 메모리를 아끼기 위해 우리는 tf.Variable을 이용한다. 그런데 Gradient를 계산하려면 loss를 계산해야 하고 샘플 (y,yhat)을 전부 메모리상에 올려야한다.\n위 같은 경우 GPU가 샘플의 일부만 올라갈 수 있다면 미니배치방법인 확률적 경사하강법을 사용해야기 때문에 확률적경사하강법을 우리가 사용하는 이유다."
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-02-lecture.html#fashion_mnist-모듈",
    "href": "post/Bigdata Analysis/2022-05-02-lecture.html#fashion_mnist-모듈",
    "title": "10. 확률적 경사하강법",
    "section": "fashion_mnist 모듈",
    "text": "fashion_mnist 모듈\n\ntf.keras.datasets.fashion_mnist.load_data()\n\ntype(tf.keras.datasets.fashion_mnist)\n\nmodule\n\n\n\n\n데이터생성 및 탐색\n- tf.keras.datasets.fashion_mnist.load_data()를 이용한 데이터 생성\n\ntype(tf.keras.datasets.fashion_mnist.load_data)\n\nfunction\n\n\n\n__call__ : 오브젝트가 숨겨져있음 \\(\\to\\) 괄호를 열고 닫으면 오브젝트가 생성됨\nenter 와 exit가 없으면 with를 같이 쓸 수 없음\n\n\ndir(tf.keras.datasets.fashion_mnist.load_data)\n\n['__annotations__',\n '__call__',\n '__class__',\n '__closure__',\n '__code__',\n '__defaults__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__get__',\n '__getattribute__',\n '__globals__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__kwdefaults__',\n '__le__',\n '__lt__',\n '__module__',\n '__name__',\n '__ne__',\n '__new__',\n '__qualname__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '_keras_api_names',\n '_keras_api_names_v1']\n\n\n\ntf.keras.datasets.fashion_mnist.load_data??\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n32768/29515 [=================================] - 0s 0us/step\n40960/29515 [=========================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26427392/26421880 [==============================] - 0s 0us/step\n26435584/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n16384/5148 [===============================================================================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4423680/4422102 [==============================] - 0s 0us/step\n4431872/4422102 [==============================] - 0s 0us/step\n\n\n\nx_train.shape, y_train.shape\n\n((60000, 28, 28), (60000,))\n\n\n\nx_test.shape, y_test.shape\n\n((10000, 28, 28), (10000,))\n\n\n\n코드를 바로 뜯어보고 구글링은 최후 수단으로 하자\n\n\n\n데이터구조\n\n#x_train[0] ## 첫번째 관측치\n\n\nx_train[0].shape\n\n(28, 28)\n\n\n\nplt.imshow(x_train[0])\n\n<matplotlib.image.AxesImage at 0x7f44628357d0>\n\n\n\n\n\n\nnp.unique(y_train)\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)\n\n\n\nnp.where(y_train==9)\n\n(array([    0,    11,    15, ..., 59932, 59970, 59978]),)\n\n\n\n오 왠지 11번째 데이터도 신발일 것 같에\n\n\nplt.imshow(x_train[11])\n\n<matplotlib.image.AxesImage at 0x7f446239ba50>\n\n\n\n\n\n- \\(\\bf{X} : (n,28,28)\\)\n- \\(y\\) : 각 이미지의 라벨 데이터"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-02-lecture.html#예제1",
    "href": "post/Bigdata Analysis/2022-05-02-lecture.html#예제1",
    "title": "10. 확률적 경사하강법",
    "section": "예제1",
    "text": "예제1\n\n데이터 정리\n- \\(y=0,1\\)에 대응하는 이미지만 정리하자. (우리가 배운건 로지스틱이니깐)\n\ny=y_train[(y_train == 0) | (y_train == 1)].reshape(-1,1)\nyy= y_test[(y_test == 0) | (y_test == 1)].reshape(-1,1)\n\n\nX=x_train[(y_train == 0) | (y_train == 1)]\nXX= x_test[(y_test == 0) | (y_test== 1)]\n\n\nX.shape\n\n(12000, 28, 28)\n\n\n\ny.shape\n\n(12000, 1)\n\n\n\n28*28\n\n784\n\n\n\nX=x_train[(y_train == 0) | (y_train == 1)].reshape(-1,784)\nXX= x_test[(y_test == 0) | (y_test== 1)].reshape(-1,784)\n\n\nX.shape,XX.shape\n\n((12000, 784), (2000, 784))\n\n\n\n\n풀이1: 은닉층을 포함한 신경망 // epochs=100\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \n    \"x784\" -> \"node1\"\n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node30\"\n    \"x2\" -> \"node30\"\n    \"..\" -> \"node30\"\n    \"x784\" -> \"node30\"\n\n\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"y\"\n    \"node2\" -> \"y\"\n    \"...\" -> \"y\"\n    \"node30\" -> \"y\"\n    label = \"Layer 2: sigmoid\"\n}\n''')\n\n\n\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='sgd',loss=tf.losses.binary_crossentropy)\nnet.fit(X,y,epochs=100,batch_size=12000,verbose=0) \n\n<keras.callbacks.History at 0x7f44601b7990>\n\n\n\nloss는 미분하기위한 정보지 성능에 대한 정보는 아니다.\ntrain\n\n\nnp.mean((net(X)>0.5) == y.reshape(12000,1))\n\n0.5000833333333333\n\n\n\ntest\n\n\nnp.mean((net(XX)>0.5) == yy.reshape(2000,1))\n\n0.5\n\n\n\nlocal_min에 빠져서 못 나오는 것임(verbose=1로하고 loss를 살펴볼 줄 알아야한다)\n초기값 문제가 아닌 옵티마이저의 문제임\\((\\star\\star\\star)\\)\n\n\n\n풀이2: 옵티마이저 개선\n\ntf.random.set_seed(43051)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy)\nnet.fit(X,y,epochs=100,batch_size=12000,verbose=0) \n\n<keras.callbacks.History at 0x7f445e8ef250>\n\n\n\nnp.mean((net(X)>0.5) == y.reshape(12000,1))\n\n0.9919166666666667\n\n\n\nnp.mean((net(XX)>0.5) == yy.reshape(2000,1))\n\n0.9855\n\n\n\n\n풀이3: 컴파일시 metrics=[‘accuracy’] 추가\n\ntf.random.set_seed(43055)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy,metrics=['accuracy'])\nnet.fit(X,y,epochs=100,batch_size=12000) \n\nEpoch 1/100\n1/1 [==============================] - 0s 486ms/step - loss: 100.9425 - accuracy: 0.4988\nEpoch 2/100\n1/1 [==============================] - 0s 82ms/step - loss: 44.4441 - accuracy: 0.3741\nEpoch 3/100\n1/1 [==============================] - 0s 75ms/step - loss: 29.2322 - accuracy: 0.4321\nEpoch 4/100\n1/1 [==============================] - 0s 109ms/step - loss: 22.6921 - accuracy: 0.5399\nEpoch 5/100\n1/1 [==============================] - 0s 69ms/step - loss: 8.7741 - accuracy: 0.7321\nEpoch 6/100\n1/1 [==============================] - 0s 63ms/step - loss: 4.6409 - accuracy: 0.8516\nEpoch 7/100\n1/1 [==============================] - 0s 65ms/step - loss: 5.2642 - accuracy: 0.8711\nEpoch 8/100\n1/1 [==============================] - 0s 69ms/step - loss: 6.1993 - accuracy: 0.8771\nEpoch 9/100\n1/1 [==============================] - 0s 66ms/step - loss: 6.5543 - accuracy: 0.8845\nEpoch 10/100\n1/1 [==============================] - 0s 68ms/step - loss: 6.3454 - accuracy: 0.8953\nEpoch 11/100\n1/1 [==============================] - 0s 66ms/step - loss: 5.7887 - accuracy: 0.9062\nEpoch 12/100\n1/1 [==============================] - 0s 67ms/step - loss: 5.1074 - accuracy: 0.9168\nEpoch 13/100\n1/1 [==============================] - 0s 69ms/step - loss: 4.4821 - accuracy: 0.9276\nEpoch 14/100\n1/1 [==============================] - 0s 71ms/step - loss: 3.9864 - accuracy: 0.9359\nEpoch 15/100\n1/1 [==============================] - 0s 64ms/step - loss: 3.6388 - accuracy: 0.9402\nEpoch 16/100\n1/1 [==============================] - 0s 71ms/step - loss: 3.4077 - accuracy: 0.9412\nEpoch 17/100\n1/1 [==============================] - 0s 65ms/step - loss: 3.2687 - accuracy: 0.9415\nEpoch 18/100\n1/1 [==============================] - 0s 72ms/step - loss: 3.1829 - accuracy: 0.9404\nEpoch 19/100\n1/1 [==============================] - 0s 63ms/step - loss: 3.1196 - accuracy: 0.9388\nEpoch 20/100\n1/1 [==============================] - 0s 71ms/step - loss: 3.0526 - accuracy: 0.9380\nEpoch 21/100\n1/1 [==============================] - 0s 65ms/step - loss: 2.9635 - accuracy: 0.9371\nEpoch 22/100\n1/1 [==============================] - 0s 80ms/step - loss: 2.8397 - accuracy: 0.9376\nEpoch 23/100\n1/1 [==============================] - 0s 66ms/step - loss: 2.6812 - accuracy: 0.9389\nEpoch 24/100\n1/1 [==============================] - 0s 65ms/step - loss: 2.4916 - accuracy: 0.9396\nEpoch 25/100\n1/1 [==============================] - 0s 81ms/step - loss: 2.2804 - accuracy: 0.9408\nEpoch 26/100\n1/1 [==============================] - 0s 65ms/step - loss: 2.0630 - accuracy: 0.9433\nEpoch 27/100\n1/1 [==============================] - 0s 74ms/step - loss: 1.8600 - accuracy: 0.9470\nEpoch 28/100\n1/1 [==============================] - 0s 70ms/step - loss: 1.6744 - accuracy: 0.9488\nEpoch 29/100\n1/1 [==============================] - 0s 68ms/step - loss: 1.5003 - accuracy: 0.9510\nEpoch 30/100\n1/1 [==============================] - 0s 65ms/step - loss: 1.3529 - accuracy: 0.9531\nEpoch 31/100\n1/1 [==============================] - 0s 74ms/step - loss: 1.2575 - accuracy: 0.9542\nEpoch 32/100\n1/1 [==============================] - 0s 68ms/step - loss: 1.1763 - accuracy: 0.9553\nEpoch 33/100\n1/1 [==============================] - 0s 69ms/step - loss: 1.0853 - accuracy: 0.9567\nEpoch 34/100\n1/1 [==============================] - 0s 78ms/step - loss: 0.9978 - accuracy: 0.9587\nEpoch 35/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.9337 - accuracy: 0.9603\nEpoch 36/100\n1/1 [==============================] - 0s 65ms/step - loss: 0.8893 - accuracy: 0.9617\nEpoch 37/100\n1/1 [==============================] - 0s 69ms/step - loss: 0.8503 - accuracy: 0.9627\nEpoch 38/100\n1/1 [==============================] - 0s 66ms/step - loss: 0.8154 - accuracy: 0.9632\nEpoch 39/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.7843 - accuracy: 0.9642\nEpoch 40/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.7548 - accuracy: 0.9654\nEpoch 41/100\n1/1 [==============================] - 0s 65ms/step - loss: 0.7288 - accuracy: 0.9663\nEpoch 42/100\n1/1 [==============================] - 0s 70ms/step - loss: 0.7061 - accuracy: 0.9674\nEpoch 43/100\n1/1 [==============================] - 0s 65ms/step - loss: 0.6844 - accuracy: 0.9687\nEpoch 44/100\n1/1 [==============================] - 0s 66ms/step - loss: 0.6640 - accuracy: 0.9693\nEpoch 45/100\n1/1 [==============================] - 0s 77ms/step - loss: 0.6427 - accuracy: 0.9710\nEpoch 46/100\n1/1 [==============================] - 0s 75ms/step - loss: 0.6187 - accuracy: 0.9716\nEpoch 47/100\n1/1 [==============================] - 0s 65ms/step - loss: 0.5933 - accuracy: 0.9723\nEpoch 48/100\n1/1 [==============================] - 0s 75ms/step - loss: 0.5693 - accuracy: 0.9730\nEpoch 49/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.5471 - accuracy: 0.9733\nEpoch 50/100\n1/1 [==============================] - 0s 70ms/step - loss: 0.5253 - accuracy: 0.9737\nEpoch 51/100\n1/1 [==============================] - 0s 65ms/step - loss: 0.5031 - accuracy: 0.9744\nEpoch 52/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.4805 - accuracy: 0.9750\nEpoch 53/100\n1/1 [==============================] - 0s 66ms/step - loss: 0.4572 - accuracy: 0.9767\nEpoch 54/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.4368 - accuracy: 0.9775\nEpoch 55/100\n1/1 [==============================] - 0s 73ms/step - loss: 0.4180 - accuracy: 0.9778\nEpoch 56/100\n1/1 [==============================] - 0s 69ms/step - loss: 0.3991 - accuracy: 0.9783\nEpoch 57/100\n1/1 [==============================] - 0s 62ms/step - loss: 0.3828 - accuracy: 0.9791\nEpoch 58/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.3701 - accuracy: 0.9793\nEpoch 59/100\n1/1 [==============================] - 0s 78ms/step - loss: 0.3568 - accuracy: 0.9793\nEpoch 60/100\n1/1 [==============================] - 0s 66ms/step - loss: 0.3426 - accuracy: 0.9805\nEpoch 61/100\n1/1 [==============================] - 0s 78ms/step - loss: 0.3286 - accuracy: 0.9813\nEpoch 62/100\n1/1 [==============================] - 0s 70ms/step - loss: 0.3165 - accuracy: 0.9822\nEpoch 63/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.3051 - accuracy: 0.9827\nEpoch 64/100\n1/1 [==============================] - 0s 73ms/step - loss: 0.2929 - accuracy: 0.9827\nEpoch 65/100\n1/1 [==============================] - 0s 69ms/step - loss: 0.2836 - accuracy: 0.9827\nEpoch 66/100\n1/1 [==============================] - 0s 70ms/step - loss: 0.2753 - accuracy: 0.9830\nEpoch 67/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.2664 - accuracy: 0.9835\nEpoch 68/100\n1/1 [==============================] - 0s 65ms/step - loss: 0.2574 - accuracy: 0.9843\nEpoch 69/100\n1/1 [==============================] - 0s 66ms/step - loss: 0.2493 - accuracy: 0.9845\nEpoch 70/100\n1/1 [==============================] - 0s 69ms/step - loss: 0.2411 - accuracy: 0.9852\nEpoch 71/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.2335 - accuracy: 0.9852\nEpoch 72/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.2267 - accuracy: 0.9852\nEpoch 73/100\n1/1 [==============================] - 0s 75ms/step - loss: 0.2198 - accuracy: 0.9858\nEpoch 74/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.2137 - accuracy: 0.9866\nEpoch 75/100\n1/1 [==============================] - 0s 71ms/step - loss: 0.2075 - accuracy: 0.9866\nEpoch 76/100\n1/1 [==============================] - 0s 71ms/step - loss: 0.2013 - accuracy: 0.9873\nEpoch 77/100\n1/1 [==============================] - 0s 69ms/step - loss: 0.1954 - accuracy: 0.9875\nEpoch 78/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.1903 - accuracy: 0.9879\nEpoch 79/100\n1/1 [==============================] - 0s 69ms/step - loss: 0.1850 - accuracy: 0.9883\nEpoch 80/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.1801 - accuracy: 0.9882\nEpoch 81/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.1751 - accuracy: 0.9886\nEpoch 82/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.1708 - accuracy: 0.9893\nEpoch 83/100\n1/1 [==============================] - 0s 65ms/step - loss: 0.1671 - accuracy: 0.9893\nEpoch 84/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.1629 - accuracy: 0.9893\nEpoch 85/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.1592 - accuracy: 0.9898\nEpoch 86/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.1553 - accuracy: 0.9898\nEpoch 87/100\n1/1 [==============================] - 0s 78ms/step - loss: 0.1514 - accuracy: 0.9898\nEpoch 88/100\n1/1 [==============================] - 0s 71ms/step - loss: 0.1479 - accuracy: 0.9900\nEpoch 89/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.1441 - accuracy: 0.9900\nEpoch 90/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.1409 - accuracy: 0.9899\nEpoch 91/100\n1/1 [==============================] - 0s 68ms/step - loss: 0.1373 - accuracy: 0.9902\nEpoch 92/100\n1/1 [==============================] - 0s 70ms/step - loss: 0.1340 - accuracy: 0.9902\nEpoch 93/100\n1/1 [==============================] - 0s 71ms/step - loss: 0.1305 - accuracy: 0.9904\nEpoch 94/100\n1/1 [==============================] - 0s 66ms/step - loss: 0.1275 - accuracy: 0.9909\nEpoch 95/100\n1/1 [==============================] - 0s 71ms/step - loss: 0.1242 - accuracy: 0.9908\nEpoch 96/100\n1/1 [==============================] - 0s 69ms/step - loss: 0.1213 - accuracy: 0.9908\nEpoch 97/100\n1/1 [==============================] - 0s 67ms/step - loss: 0.1184 - accuracy: 0.9910\nEpoch 98/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.1157 - accuracy: 0.9911\nEpoch 99/100\n1/1 [==============================] - 0s 71ms/step - loss: 0.1132 - accuracy: 0.9912\nEpoch 100/100\n1/1 [==============================] - 0s 64ms/step - loss: 0.1110 - accuracy: 0.9917\n\n\n<keras.callbacks.History at 0x7f44600a41d0>\n\n\n\nnet.evaluate(X,y)\n\n375/375 [==============================] - 1s 2ms/step - loss: 0.1086 - accuracy: 0.9918\n\n\n[0.10858089476823807, 0.9917500019073486]\n\n\n\nnet.evaluate(XX,yy)\n\n63/63 [==============================] - 0s 2ms/step - loss: 0.2933 - accuracy: 0.9800\n\n\n[0.2932808995246887, 0.9800000190734863]\n\n\n\n\n풀이4: 확률적경사하강법 이용 // epochs=10 (Ver2)\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential() \nnet.add(tf.keras.layers.Dense(30,activation='relu'))\nnet.add(tf.keras.layers.Dense(1,activation='sigmoid'))\nnet.compile(optimizer='adam',loss=tf.losses.binary_crossentropy,metrics=['accuracy'])\nnet.fit(X,y,epochs=10,batch_size=120) \n\nEpoch 1/10\n100/100 [==============================] - 1s 3ms/step - loss: 5.6484 - accuracy: 0.9418\nEpoch 2/10\n100/100 [==============================] - 0s 4ms/step - loss: 0.5078 - accuracy: 0.9793\nEpoch 3/10\n100/100 [==============================] - 0s 3ms/step - loss: 0.3784 - accuracy: 0.9818\nEpoch 4/10\n100/100 [==============================] - 0s 3ms/step - loss: 0.3390 - accuracy: 0.9828\nEpoch 5/10\n100/100 [==============================] - 0s 4ms/step - loss: 0.2474 - accuracy: 0.9857\nEpoch 6/10\n100/100 [==============================] - 0s 3ms/step - loss: 0.2116 - accuracy: 0.9870\nEpoch 7/10\n100/100 [==============================] - 0s 3ms/step - loss: 0.1743 - accuracy: 0.9889\nEpoch 8/10\n100/100 [==============================] - 0s 3ms/step - loss: 0.1374 - accuracy: 0.9899\nEpoch 9/10\n100/100 [==============================] - 0s 3ms/step - loss: 0.1570 - accuracy: 0.9891\nEpoch 10/10\n100/100 [==============================] - 0s 4ms/step - loss: 0.1097 - accuracy: 0.9915\n\n\n<keras.callbacks.History at 0x7f445ff81490>\n\n\n\nnet.evaluate(X,y)\n\n375/375 [==============================] - 1s 2ms/step - loss: 0.0889 - accuracy: 0.9933\n\n\n[0.08887288719415665, 0.9932500123977661]\n\n\n\nnet.evaluate(XX,yy)\n\n63/63 [==============================] - 0s 2ms/step - loss: 0.2973 - accuracy: 0.9845\n\n\n[0.2972556948661804, 0.984499990940094]\n\n\n\n풀이 4는 정확도가 초기값이 너무 좋음\n왜 업데이트를 한번의 에폭에서 현재 100번을 수행했기 때문!!\n배치사이즈는 보통 메모리에 맞추어 올린다"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-08-lecture.html",
    "href": "post/Bigdata Analysis/2022-05-08-lecture.html",
    "title": "11. softmax",
    "section": "",
    "text": "import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow.experimental.numpy as tnp\n\n\ntf.config.experimental.list_physical_devices()\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-08-lecture.html#평가지표",
    "href": "post/Bigdata Analysis/2022-05-08-lecture.html#평가지표",
    "title": "11. softmax",
    "section": "평가지표",
    "text": "평가지표\n\n다양한 평가지표들\n- 의문: 왜 다양한 평가지표가 필요한가? (accuray면 끝나는거 아닌가? 더 이상 뭐가 필요해?)\n- 여러가지 평가지표들: https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values - 이걸 다 암기하는건 불가능함. - 몇 개만 뽑아서 암기하고 왜 쓰는지만 생각해보고 넘어가자!\n\n\nconfusion matrix의 이해\n- 표1\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\nTP\nFN\n\n\n안나감(실제)\nFP\nTN\n\n\n\n- 표2 (책에없음)\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\n$(y,)= $ (O,O)\n$(y,)= $(O,X)\n\n\n안나감(실제)\n$(y,)= $(X,O)\n$(y,)= $(X,X)\n\n\n\n- 표3 (책에없음)\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n퇴사(실제)\nTP, \\(\\# O/O\\)\nFN, \\(\\#O/X\\)\n\n\n안나감(실제)\nFP, \\(\\#X/O\\)\nTN, \\(\\#X/X\\)\n\n\n\n\n암기법, (1) 두번째 글자를 그대로 쓴다 (2) 첫글자가 T이면 분류를 제대로한것, 첫글자가 F이면 분류를 잘못한것\n\n- 표4 (위키등에 있음)\n\n\n\n\n\n\n\n\n\n\n퇴사(예측)\n안나감(예측)\n\n\n\n\n\n퇴사(실제)\nTP, \\(\\# O/O\\)\nFN, \\(\\# O/X\\)\nSensitivity(민감도)=Recall(재현율)=\\(\\frac{TP}{TP+FN}\\)=\\(\\frac{\\#O/O}{\\# O/O+ \\#O/X}\\)\n\n\n안나감(실제)\nFP, \\(\\# X/O\\)\nTN, \\(\\# X/X\\)\n\n\n\n\nPrecision(프리시즌)=\\(\\frac{TP}{TP+FP}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# X/O}\\)\n\nAccuracy(애큐러시)=\\(\\frac{TP+TN}{total}\\)=\\(\\frac{\\#O/O+\\# X/X}{total}\\)\n\n\n\n\n\n상황극\n- 최규빈은 입사하여 “퇴사자 예측시스템”의 개발에 들어갔다.\n- 자료의 특성상 대부분의 사람이 퇴사하지 않고 회사에 잘 다닌다. 즉 1000명이 있으면 10명정도 퇴사한다.\n\n\nAccuracy\n- 정의: Accuracy(애큐러시)=\\(\\frac{TP+TN}{total}\\)=\\(\\frac{\\#O/O+ \\#X/X}{total}\\) - 한국말로는 정확도, 정분류율이라고 한다. - 한국말이 헷갈리므로 그냥 영어를 외우는게 좋다. (어차피 Keras에서 옵션도 영어로 넣음)\n- (상확극 시점1) 왜 애큐러시는 불충분한가? - 회사: 퇴사자예측프로그램 개발해 - 최규빈: 귀찮은데 다 나간다고 하자! -> 99퍼의 accuracy\n\n모델에 사용한 파라메터 = 0. 그런데 애큐러시 = 99! 이거 엄청 좋은 모형이다?\n\n\n\nSensitivity(민감도), Recall(재현율), True Positive Rate(TPR)\n- 정의: Sensitivity(민감도)=Recall(재현율)=\\(\\frac{TP}{TP+FN}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# O/X}\\) - 분모: 실제 O인 관측치 수 - 분자: 실제 O를 O라고 예측한 관측치 수 - 뜻: 실제 O를 O라고 예측한 비율\n- (상황극 시점2) recall을 봐야하는 이유 - 인사팀: 실제 퇴사자를 퇴사자로 예측해야 의미가 있음! 우리는 퇴사할것 같은 10명을 찍어달란 의미였어요! (그래야 면담을 하든 할거아냐!) - 최규빈: 가볍고(=파라메터 적고) 잘 맞추는 모형 만들어 달라면서요?\n\n인사팀: (고민중..) 사실 생각해보니까 이 경우는 애큐러시는 의미가 없네. 실제 나간 사람 중 최규빈이 나간다고 한 사람이 몇인지 카운트 하는게 더 의미가 있겠다. 우리는 앞으로 리컬(혹은 민감도)를 보겠다!\n\n\n예시1: 실제로 퇴사한 10명중 최규빈이 나간다고 찍은 사람이 5명이면 리컬이 50%\n\n\n예시2: 최규빈이 아무도 나가지 않는다고 예측해버린다? 실제 10명중에서 최규빈이 나간다고 적중시킨사람은 0명이므로 이 경우 리컬은 0%\n\n\n결론: 우리가 필요한건 recall이니까 앞으로 recall을 가져와! accuracy는 큰 의미없어. (그래도 명색이 모델인데 accuracy가 90은 되면 좋겠다)\n\n\n\nPrecision\n- 정의: Precision(프리시즌)=\\(\\frac{TP}{TP+FP}\\)=\\(\\frac{\\# O/O}{\\# O/O+\\# X/O}\\) - 분모: O라고 예측한 관측치 - 분자: O라고 예측한 관측치중 진짜 O인 관측치 - 뜻: O라고 예측한 관측치중 진짜 O인 비율\n- (상황극 시점3) recall 만으로 불충분한 이유\n\n최규빈: 에휴.. 귀찮은데 그냥 좀만 수틀리면 다 나갈것 같다고 해야겠다. -> 한 100명 나간다고 했음 -> 실제로 최규빈이 찍은 100명중에 10명이 다 나감!\n\n\n이 경우 애큐러시는 91%, 리컬은 100% (퇴사자 10명을 일단은 다 맞췄으므로).\n\n\n인사팀: (화가 많이 남) 멀쩡한 사람까지 다 퇴사할 것 같다고 하면 어떡해요? 최규빈 연구원이 나간다고 한 100명중에 실제로 10명만 나갔어요.\n인사팀: 마치 총으로 과녁중앙에 맞춰 달라고 했더니 기관총을 가져와서 한번 긁은것이랑 뭐가 달라요? 맞추는게 문제가 아니고 precision이 너무 낮아요.\n최규빈: accuracy 90% 이상, recall은 높을수록 좋다는게 주문 아니었나요?\n인사팀: (고민중..) 앞으로는 recall과 함께 precision도 같이 제출하세요. precision은 당신이 나간다고 한 사람중에 실제 나간사람의 비율을 의미해요. 이 경우는 \\(\\frac{10}{100}\\)이니까 precision이 10%입니다. (속마음: recall 올리겠다고 무작정 너무 많이 예측하지 말란 말이야!)\n\n\n\nF1 score\n- 정의: recall과 precision의 조화평균\n- (상황극 시점4) recall, precision을 모두 고려\n\n최규빈: recall/precision을 같이 내는건 좋은데요, 둘은 trade off의 관계에 있습니다. 물론 둘다 올리는 모형이 있다면 좋지만 그게 쉽지는 않아요. 보통은 precision을 올리려면 recall이 희생되는 면이 있고요, recall을 올리려고 하면 precision이 다소 떨어집니다.\n최규빈: 평가기준이 애매하다는 의미입니다. 모형1,2가 있는데 모형1은 모형2보다 precision이 약간 좋고 대신 recall이 떨어진다면 모형1이 좋은것입니까? 아니면 모형2가 좋은것입니까?\n인사팀: 그렇다면 둘을 평균내서 F1score를 계산해서 제출해주세요.\n\n\n\nSpecificity(특이도), False Positive Rate(FPR)\n- 정의:\n\nSpecificity(특이도)=\\(\\frac{TN}{FP+TN}\\)=\\(\\frac{\\# X/X}{\\# X/O+\\# X/X}\\)\nFalse Positive Rate (FPR) = 1-Specificity(특이도) = \\(\\frac{FP}{FP+TN}\\)=\\(\\frac{\\# X/O}{\\# X/O+\\# X/X}\\)\n\n- 의미: FPR = 오해해서 미안해, recall(=TPR)을 올리려고 보니 어쩔 수 없었어 ㅠㅠ - specificity는 안나간 사람을 안나갔다고 찾아낸 비율인데 별로 안중요하다. - FPR은 recall을 올리기 위해서 “실제로는 회사 잘 다니고 있는 사람 중 최규빈이 나갈것 같다고 찍은 사람들” 의 비율이다.\n\n즉 생사람잡은 비율.. 오해해서 미안한 사람의 비율..\n\n\n\nROC curve\n- 정의: \\(x\\)축=FPR, \\(y\\)축=TPR 을 그린 커브\n- 의미: - 결국 “오해해서 미안해 vs recall”을 그린 곡선이 ROC커브이다. - 생각해보면 오해하는 사람이 많을수록 당연히 recall은 올라간다. 따라서 우상향하는 곡선이다. - 오해한 사람이 매우 적은데 recall이 우수하면 매우 좋은 모형이다. 그래서 초반부터 ROC값이 급격하게 올라가면 좋은 모형이다."
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-08-lecture.html#fashion_mnist-revisit",
    "href": "post/Bigdata Analysis/2022-05-08-lecture.html#fashion_mnist-revisit",
    "title": "11. softmax",
    "section": "fashion_mnist (revisit)",
    "text": "fashion_mnist (revisit)\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nx_train.shape\n\n(60000, 28, 28)\n\n\n이미지의 차원이 단지 (28,28)이라는 것은 흑백이미지라는 뜻이다.\n\nplt.imshow(x_train[0]) \n\n<matplotlib.image.AxesImage at 0x7f2ad4a04990>\n\n\n\n\n\n\n아닌데요?! 칼라인데요?! -> 흑백이다. 그냥 밝을수록 노란색, 어두울수록 남색으로 표현한것 뿐임 (colormap이 viridis일 뿐임)\n\n일반적으로 분석할 이미지는 칼라를 의미하는 채널도 포함할테니 아래와 같이 자료형을 정리하는게 일반적으로 이미지 자료를 분석하는 정석적인 처리방법이다.\n\nX = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nX.shape\n\nTensorShape([60000, 28, 28, 1])\n\n\nkeras에서 이미지자료는 (관측치수,픽셀,픽셀,채널)과 같은 형식을 가진다\n예를들어 256*256 size인 칼라이미지(채널수=3)가 10개 있다면 X.shape은 (10,256,256,3)이다."
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-08-lecture.html#x의-차원이-관측치수픽셀픽셀채널일-경우-dnn-쓰기",
    "href": "post/Bigdata Analysis/2022-05-08-lecture.html#x의-차원이-관측치수픽셀픽셀채널일-경우-dnn-쓰기",
    "title": "11. softmax",
    "section": "X의 차원이 (관측치수,픽셀,픽셀,채널)일 경우 DNN 쓰기",
    "text": "X의 차원이 (관측치수,픽셀,픽셀,채널)일 경우 DNN 쓰기\n\n(예제1) X -> Dense(30,relu) -> Dense(10,softmax):=> y\n- 이러한 아키텍처를 돌리기 위해서는 X의 shape을 미리 바꿔야 했었다. 혹시 바꾸지 않는 방법도 있을까?\n\nflttn = tf.keras.layers.Flatten()\n\n\nX.shape\n\nTensorShape([60000, 28, 28, 1])\n\n\n\nflttn(X).shape\n\nTensorShape([60000, 784])\n\n\n펴진다? 즉 X.reshape(-1,784)와 같은 기능!\n- 근데 이거 레이어다? 즉 네트워크에 add 할 수 있다는 의미!\n\ntf.random.set_seed(43052)\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Flatten())\nnet1.add(tf.keras.layers.Dense(30,activation='relu'))\nnet1.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet1.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet1.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 2.5431 - accuracy: 0.4038\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.2042 - accuracy: 0.5173\nEpoch 3/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.0222 - accuracy: 0.5878\nEpoch 4/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.9312 - accuracy: 0.6192\nEpoch 5/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.8973 - accuracy: 0.6270\n\n\n<keras.callbacks.History at 0x7f2ad4ad0850>\n\n\n\nnet1.layers\n\n[<keras.layers.core.flatten.Flatten at 0x7f2ad4a3e5d0>,\n <keras.layers.core.dense.Dense at 0x7f2ad4a3e9d0>,\n <keras.layers.core.dense.Dense at 0x7f2ad4c22790>]\n\n\n\nnet1.layers[0](X) \n\n<tf.Tensor: shape=(60000, 784), dtype=float32, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>\n\n\n\nnet1.layers[1](net1.layers[0](X)) # 출력이 30이니까~ + 렐루를 거쳐서 0또는 양수인 모습!\n\n<tf.Tensor: shape=(60000, 30), dtype=float32, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>\n\n\n\nnet1.layers[2](net1.layers[1](net1.layers[0](X))) # 최종출력 10차원, 각각은 확률을 의미하게 된다. \n\n<tf.Tensor: shape=(60000, 10), dtype=float32, numpy=\narray([[0.0000000e+00, 2.8076959e-37, 0.0000000e+00, ..., 3.8582063e-04,\n        0.0000000e+00, 9.9960333e-01],\n       [2.0941226e-01, 1.4367345e-03, 2.3821327e-01, ..., 5.0225304e-03,\n        2.5805481e-02, 3.0450234e-03],\n       [3.0291098e-01, 1.3899502e-02, 2.2462834e-02, ..., 2.5782569e-15,\n        2.1044535e-04, 2.6176517e-10],\n       ...,\n       [6.8422541e-02, 2.0148051e-01, 1.2838944e-05, ..., 0.0000000e+00,\n        5.6517130e-10, 8.8627344e-27],\n       [2.0941226e-01, 1.4367345e-03, 2.3821327e-01, ..., 5.0225304e-03,\n        2.5805481e-02, 3.0450234e-03],\n       [1.0189731e-34, 0.0000000e+00, 0.0000000e+00, ..., 1.7085880e-05,\n        2.8456826e-20, 5.3099287e-05]], dtype=float32)>\n\n\n`-`` (참고) metrics=[‘accuracy’] 대신에 이렇게 해도된다~\n\ntf.random.set_seed(43052)\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Flatten())\nnet1.add(tf.keras.layers.Dense(30,activation='relu'))\nnet1.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet1.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=[tf.metrics.CategoricalAccuracy()])\nnet1.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 5s 2ms/step - loss: 2.5431 - categorical_accuracy: 0.4038\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.2042 - categorical_accuracy: 0.5173\nEpoch 3/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.0222 - categorical_accuracy: 0.5878\nEpoch 4/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.9312 - categorical_accuracy: 0.6192\nEpoch 5/5\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.8973 - categorical_accuracy: 0.6270\n\n\n<keras.callbacks.History at 0x7f2ad3b95310>\n\n\n\nid(tf.metrics.CategoricalAccuracy), id(tf.keras.metrics.CategoricalAccuracy)\n\n(115791520, 115791520)\n\n\n\n주소가 똑같다!!\n주의사항: tf.metrics.Accuracy() 말고 tf.metrics.CategoricalAccuracy() 를 써야함\n(참고2) 메트릭을 추가할수도 있다\n\n\ntf.random.set_seed(43052)\nnet1 = tf.keras.Sequential()\nnet1.add(tf.keras.layers.Flatten())\nnet1.add(tf.keras.layers.Dense(30,activation='relu'))\nnet1.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet1.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=[tf.metrics.CategoricalAccuracy(),tf.metrics.Recall()])\nnet1.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 49s 2ms/step - loss: 2.5431 - categorical_accuracy: 0.4038 - recall: 0.2949\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.2042 - categorical_accuracy: 0.5173 - recall: 0.4078\nEpoch 3/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.0222 - categorical_accuracy: 0.5878 - recall: 0.4774\nEpoch 4/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.9312 - categorical_accuracy: 0.6192 - recall: 0.5059\nEpoch 5/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.8973 - categorical_accuracy: 0.6270 - recall: 0.5128\n\n\n<keras.callbacks.History at 0x7f2acdaafd90>\n\n\nrecall을 추가하면 test set의 성능평가에도 리콜을 볼 수 있다.\n\nnet1.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 2ms/step - loss: 0.9399 - categorical_accuracy: 0.6308 - recall: 0.5122\n\n\n[0.9398639798164368, 0.6308000087738037, 0.5121999979019165]\n\n\n\n\n(예제2) X -> Dense(500,relu) -> Dense(500,relu) -> Dense(10,softmax):=>y\n- 다른 모형으로 적합해보기\n\ntf.random.set_seed(43052)\nnet2 = tf.keras.Sequential()\nnet2.add(tf.keras.layers.Flatten())\nnet2.add(tf.keras.layers.Dense(500,activation='relu'))\nnet2.add(tf.keras.layers.Dense(500,activation='relu'))\nnet2.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet2.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet2.fit(X,y,epochs=5)\n\nEpoch 1/5\n1875/1875 [==============================] - 15s 8ms/step - loss: 2.2713 - accuracy: 0.7542\nEpoch 2/5\n1875/1875 [==============================] - 14s 7ms/step - loss: 0.6264 - accuracy: 0.7955\nEpoch 3/5\n1875/1875 [==============================] - 14s 7ms/step - loss: 0.5365 - accuracy: 0.8209\nEpoch 4/5\n1875/1875 [==============================] - 14s 7ms/step - loss: 0.4477 - accuracy: 0.8425\nEpoch 5/5\n1875/1875 [==============================] - 14s 7ms/step - loss: 0.4127 - accuracy: 0.8548\n\n\n<keras.callbacks.History at 0x7f2ad49ae390>\n\n\n\nnet2.fit(XX,yy)\n\n313/313 [==============================] - 3s 9ms/step - loss: 0.4584 - accuracy: 0.8366\n\n\n<keras.callbacks.History at 0x7f2ad50e3ad0>\n\n\n- 위 모형은 epoch을 늘려도 큰 변화가 없다.\n\n\n(예제3) 아주 복잡한 DNN\n\ntf.random.set_seed(43052)\nnet3 = tf.keras.Sequential()\nnet3.add(tf.keras.layers.Flatten())\nnet3.add(tf.keras.layers.Dense(500,activation='relu'))\nnet3.add(tf.keras.layers.Dense(500,activation='relu'))\nnet3.add(tf.keras.layers.Dense(500,activation='relu'))\nnet3.add(tf.keras.layers.Dense(500,activation='relu'))\nnet3.add(tf.keras.layers.Dense(500,activation='relu'))\nnet3.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet3.compile(loss=tf.losses.categorical_crossentropy, optimizer='adam',metrics=['accuracy'])\nnet3.fit(X,y,epochs=10)\n\nEpoch 1/10\n1875/1875 [==============================] - 31s 16ms/step - loss: 0.9820 - accuracy: 0.7955\nEpoch 2/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.4518 - accuracy: 0.8384\nEpoch 3/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.4163 - accuracy: 0.8536\nEpoch 4/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.3869 - accuracy: 0.8636\nEpoch 5/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.3759 - accuracy: 0.8677\nEpoch 6/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.3616 - accuracy: 0.8737\nEpoch 7/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.3443 - accuracy: 0.8794\nEpoch 8/10\n1875/1875 [==============================] - 29s 16ms/step - loss: 0.3355 - accuracy: 0.8816\nEpoch 9/10\n1875/1875 [==============================] - 29s 16ms/step - loss: 0.3335 - accuracy: 0.8831\nEpoch 10/10\n1875/1875 [==============================] - 36s 19ms/step - loss: 0.3182 - accuracy: 0.8866\n\n\n<keras.callbacks.History at 0x7f2ad50be210>\n\n\n\nnet3.evaluate(XX,yy)\n\n313/313 [==============================] - 2s 5ms/step - loss: 0.3748 - accuracy: 0.8718\n\n\n[0.37482383847236633, 0.8718000054359436]"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-08-lecture.html#발상의-전환",
    "href": "post/Bigdata Analysis/2022-05-08-lecture.html#발상의-전환",
    "title": "11. softmax",
    "section": "발상의 전환",
    "text": "발상의 전환\n\n현재 신경망을 복잡하게 해봤지만 늘어난 파라미터 대비 성능개선이 크지 않다.\n다른 방법을 이용해보자.\nMaxPooling2D, MaxPool2D를 이용\n\n\nid(tf.keras.layers.MaxPooling2D), id(tf.keras.layers.MaxPool2D)\n\n(114701472, 114701472)\n\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nmp = tf.keras.layers.MaxPool2D() # pool size의 디폴트는 (2,2)\n\n-테스트1: (2,2) 이미지\n임의의 흑백이미지 생성\n\nXXX = tnp.arange(1*2*2*1).reshape(1,2,2,1)\nXXX\n\n<tf.Tensor: shape=(1, 2, 2, 1), dtype=int64, numpy=\narray([[[[0],\n         [1]],\n\n        [[2],\n         [3]]]])>\n\n\n\nXXX.reshape(1,2,2) # 채널때문에 살짝 헷갈리지만 실제로는 이렇게 생긴 이미지! \n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[0, 1],\n        [2, 3]]])>\n\n\n\nmp(XXX) # 4개중에 제일 큰 값이 나오는 것 같다.\n\n<tf.Tensor: shape=(1, 1, 1, 1), dtype=int64, numpy=array([[[[3]]]])>\n\n\n-테스트2: (4,4) 이미지로 변경\n\nXXX = tnp.arange(1*4*4*1).reshape(1,4,4,1)\nXXX,XXX.reshape(1,4,4)\n\n(<tf.Tensor: shape=(1, 4, 4, 1), dtype=int64, numpy=\n array([[[[ 0],\n          [ 1],\n          [ 2],\n          [ 3]],\n \n         [[ 4],\n          [ 5],\n          [ 6],\n          [ 7]],\n \n         [[ 8],\n          [ 9],\n          [10],\n          [11]],\n \n         [[12],\n          [13],\n          [14],\n          [15]]]])>, <tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy=\n array([[[ 0,  1,  2,  3],\n         [ 4,  5,  6,  7],\n         [ 8,  9, 10, 11],\n         [12, 13, 14, 15]]])>)\n\n\n\nmp(XXX),mp(XXX).reshape(1,2,2) ## 4개 의 값중 가장 큰 값을 반환\n\n(<tf.Tensor: shape=(1, 2, 2, 1), dtype=int64, numpy=\n array([[[[ 5],\n          [ 7]],\n \n         [[13],\n          [15]]]])>, <tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\n array([[[ 5,  7],\n         [13, 15]]])>)\n\n\n- 테스트3: (6,6) 이미지 + pool_size = (3,3)\n\nXXX = tnp.arange(1*6*6*1).reshape(1,6,6,1)\nXXX.reshape(1,6,6)\n\n<tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]]])>\n\n\n\nmp(XXX).reshape(1,3,3) # 왜 (2,2)씩...? \n\n<tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy=\narray([[[ 7,  9, 11],\n        [19, 21, 23],\n        [31, 33, 35]]])>\n\n\n\nmp3 = tf.keras.layers.MaxPool2D(pool_size=(3,3))\nmp3(XXX).reshape(1,2,2)\n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[14, 17],\n        [32, 35]]])>\n\n\n\n모형적합\n\nnet4 = tf.keras.Sequential()\nnet4.add(tf.keras.layers.Con)"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-11-(11주차).html",
    "href": "post/Bigdata Analysis/2022-05-11-(11주차).html",
    "title": "12. Max pooling, CNN",
    "section": "",
    "text": "import tensorflow as tf \nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-11-(11주차).html#maxpooling",
    "href": "post/Bigdata Analysis/2022-05-11-(11주차).html#maxpooling",
    "title": "12. Max pooling, CNN",
    "section": "maxpooling",
    "text": "maxpooling\n\n테스트 1\n\nm = tf.keras.layers.MaxPool2D()\n\n\nXXX = tnp.arange(1*4*4*1)\nXXX\n\n<tf.Tensor: shape=(16,), dtype=int64, numpy=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])>\n\n\n\nXXX = XXX.reshape(1,4,4,1) ## 컬러이미지\nXXX\n\n<tf.Tensor: shape=(1, 4, 4, 1), dtype=int64, numpy=\narray([[[[ 0],\n         [ 1],\n         [ 2],\n         [ 3]],\n\n        [[ 4],\n         [ 5],\n         [ 6],\n         [ 7]],\n\n        [[ 8],\n         [ 9],\n         [10],\n         [11]],\n\n        [[12],\n         [13],\n         [14],\n         [15]]]])>\n\n\n\nXXX.reshape(1,4,4) ## 흑백이미지\n\n<tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15]]])>\n\n\n\nm(XXX)\n\n<tf.Tensor: shape=(1, 2, 2, 1), dtype=int64, numpy=\narray([[[[ 5],\n         [ 7]],\n\n        [[13],\n         [15]]]])>\n\n\n\nm(XXX).reshape(1,2,2)\n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[ 5,  7],\n        [13, 15]]])>\n\n\n\n눈치껏 XXX.reshape(1,4,4)에서 각각의 2 x 2에서 가장 큰 값을 찾아주는 것으로 생각하자.\n즉, 2 x 2의 window를 만들고 그 중 가장 기록 후 윈도우를 움직이면서 이 과정을 반복한다.\n\n\n\n테스트 2\n\nXXX = tnp.arange(1*6*6*1).reshape(1,6,6,1)\nXXX.reshape(1,6,6)\n\n<tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]]])>\n\n\n\nm(XXX).reshape(1,3,3)\n\n<tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy=\narray([[[ 7,  9, 11],\n        [19, 21, 23],\n        [31, 33, 35]]])>\n\n\n\n윈도우 크기 2 x 2는 디폴트 값이다!!\n\n\n\n테스트 3\n\nm=tf.keras.layers.MaxPool2D(pool_size=(3,3))\n\n\nXXX = tnp.arange(1*6*6*1).reshape(1,6,6,1)\nXXX.reshape(1,6,6)\n\n<tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11],\n        [12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23],\n        [24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]]])>\n\n\n\nm(XXX).reshape(1,2,2)\n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[14, 17],\n        [32, 35]]])>\n\n\n\n\n테스트 4\n\nm=tf.keras.layers.MaxPool2D(pool_size=(2,2))\n\n\nXXX = tnp.arange(1*5*5*1).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n<tf.Tensor: shape=(1, 5, 5), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19],\n        [20, 21, 22, 23, 24]]])>\n\n\n\n차원이 안맞는 부분은 버리고 계산한다.\n\n\nm(XXX).reshape(1,2,2) ## 어? 뒤에 나머지 5행과 5열은 버리고 계산한다....\n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[ 6,  8],\n        [16, 18]]])>\n\n\n\n# ?tf.keras.layers.MaxPool2D\n\n\npadding 옵션을 same으로 주면 남는 차원을 버리지 않음\n\n\nm=tf.keras.layers.MaxPool2D(pool_size=(2,2),padding=\"same\")\n\n\nm(XXX).reshape(1,3,3) \n\n<tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy=\narray([[[ 6,  8,  9],\n        [16, 18, 19],\n        [21, 23, 24]]])>\n\n\n\n\n테스트 5\n\n관측치가 2개 채널(색)은 흑백\n\n\nm=tf.keras.layers.MaxPool2D(pool_size=(2,2))\n\n\nXXX = tnp.arange(2*4*4*1).reshape(2,4,4,1)\nXXX.reshape(2,4,4)\n\n<tf.Tensor: shape=(2, 4, 4), dtype=int64, numpy=\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15]],\n\n       [[16, 17, 18, 19],\n        [20, 21, 22, 23],\n        [24, 25, 26, 27],\n        [28, 29, 30, 31]]])>\n\n\n\nm(XXX).reshape(2,2,2) \n\n<tf.Tensor: shape=(2, 2, 2), dtype=int64, numpy=\narray([[[ 5,  7],\n        [13, 15]],\n\n       [[21, 23],\n        [29, 31]]])>\n\n\n\n\n테스트 6\n\nXXX = tnp.arange(1*4*4*3).reshape(1,4,4,3)\n\n\nXXX[:,:,:,0] ## 첫번째 채널\n\n<tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy=\narray([[[ 0,  3,  6,  9],\n        [12, 15, 18, 21],\n        [24, 27, 30, 33],\n        [36, 39, 42, 45]]])>\n\n\n\nm(XXX)\n\n<tf.Tensor: shape=(1, 2, 2, 3), dtype=int64, numpy=\narray([[[[15, 16, 17],\n         [21, 22, 23]],\n\n        [[39, 40, 41],\n         [45, 46, 47]]]])>\n\n\n\nm(XXX)[:,:,:,0] ## 첫번째 채널\n\n<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=\narray([[[15, 21],\n        [39, 45]]])>"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-11-(11주차).html#convolution-2d",
    "href": "post/Bigdata Analysis/2022-05-11-(11주차).html#convolution-2d",
    "title": "12. Max pooling, CNN",
    "section": "Convolution 2D",
    "text": "Convolution 2D\n- 테스트 1\n- 레이어 생성\n\ncv = tf.keras.layers.Conv2D(1, kernel_size = (2,2))\n\n- XXX 생성\n\nXXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1) ### 4 x 4 픽셀을 가진 이미지 생성\nXXX.reshape(1,4,4) \n\n<tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]]])>\n\n\n\n아래가 에러가 나는 이유\n\n데이터 타입이 int가 아닌 float이어야한다.\n또한 차원을 XXX.reshape(1,4,4)로 입력하면 에러가 난다.\n\n\n\ncv(XXX).reshape(1,3,3)\n\n<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[-4.290876 , -4.5067925, -4.722709 ],\n        [-5.154542 , -5.3704586, -5.5863748],\n        [-6.0182076, -6.234124 , -6.45004  ]]], dtype=float32)>\n\n\n\n사칙연산, max 이런 형태로 데이터가 변환되서 출력 되는 것이 아닌 것 같음\n또한, 레이어의 가중치가 랜덤으로 배정되는 것 같음\n\n- 코드 정리 + 시드통일\n\ntf.random.set_seed(43052)\ncv = tf.keras.layers.Conv2D(1, kernel_size = (2,2))\nXXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1)\nprint(XXX.reshape(1,4,4)) \nprint(cv(XXX).reshape(1,3,3))\n\ntf.Tensor(\n[[[ 0.  1.  2.  3.]\n  [ 4.  5.  6.  7.]\n  [ 8.  9. 10. 11.]\n  [12. 13. 14. 15.]]], shape=(1, 4, 4), dtype=float64)\ntf.Tensor(\n[[[ -4.125754   -5.312817   -6.4998803]\n  [ -8.874006  -10.0610695 -11.248133 ]\n  [-13.622259  -14.809322  -15.996386 ]]], shape=(1, 3, 3), dtype=float32)\n\n\n\ndense layer를 생각해보면 가중치가 랜덤으로 배정되기 때문에 시드를 고정해주었다.\nweight를 한번 찍어보자\n\n\ncv.weights\n\n[<tf.Variable 'conv2d_7/kernel:0' shape=(2, 2, 1, 1) dtype=float32, numpy=\n array([[[[-0.13014299]],\n \n         [[-0.23927206]]],\n \n \n        [[[-0.20175874]],\n \n         [[-0.6158894 ]]]], dtype=float32)>,\n <tf.Variable 'conv2d_7/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n\n\n\ntype(cv.weights) ## 길이가 2인 리스트가 출력\n\nlist\n\n\n\ncv.weights[0]\n\n<tf.Variable 'conv2d_7/kernel:0' shape=(2, 2, 1, 1) dtype=float32, numpy=\narray([[[[-0.13014299]],\n\n        [[-0.23927206]]],\n\n\n       [[[-0.20175874]],\n\n        [[-0.6158894 ]]]], dtype=float32)>\n\n\n\nprint(XXX.reshape(1,4,4)) \ntf.reshape(cv.weights[0],(2,2))\n\ntf.Tensor(\n[[[ 0.  1.  2.  3.]\n  [ 4.  5.  6.  7.]\n  [ 8.  9. 10. 11.]\n  [12. 13. 14. 15.]]], shape=(1, 4, 4), dtype=float64)\n\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.13014299, -0.23927206],\n       [-0.20175874, -0.6158894 ]], dtype=float32)>\n\n\n- 연산과정\n\n0*-0.13014299 + 1*-0.23927206 + 4*-0.20175874 + 5*-0.6158894 +0 ## 0 은 bias\n\n-4.1257540200000005\n\n\n\nprint(cv(XXX).reshape(1,3,3))\n\ntf.Tensor(\n[[[ -4.125754   -5.312817   -6.4998803]\n  [ -8.874006  -10.0610695 -11.248133 ]\n  [-13.622259  -14.809322  -15.996386 ]]], shape=(1, 3, 3), dtype=float32)\n\n\n- weight를 변경해보자\n\ncv.get_weights()[0].shape\n\n(2, 2, 1, 1)\n\n\n\nw = tnp.array([1/4,1/4,1/4,1/4],dtype=tf.float32).reshape(2,2,1,1)\nb = tnp.array([3],dtype=tf.float32)\n\n\ncv.set_weights([w,b])\n\n\nXXX.reshape(1,4,4)\n\n<tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]]])>\n\n\nw는 0,1,4,5를 평균내기위한 weight 값으로 설정한 것이다.\n\ncv(XXX).reshape(1,3,3)\n\n<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[ 5.5,  6.5,  7.5],\n        [ 9.5, 10.5, 11.5],\n        [13.5, 14.5, 15.5]]], dtype=float32)>\n\n\n\ntnp.mean([0,1,4,5])+3,tnp.mean([1,2,5,6])+3,tnp.mean([2,3,6,7])+3\n\n(<tf.Tensor: shape=(), dtype=float64, numpy=5.5>,\n <tf.Tensor: shape=(), dtype=float64, numpy=6.5>,\n <tf.Tensor: shape=(), dtype=float64, numpy=7.5>)\n\n\n\nConv 2D 요약\n\nsize = (2,2)인 윈도우를 만든다.\nXXX에 윈도우를 통과시켜서 (2,2)크기의 sub XXX를 얻음, sub XXX의 각 원소에 conv2d.weights[0]의 각 원소를 element-wise하게 곱한다.\n(2)의 결과를 모두 더한다.\n위 과정을 window를 이동시키면서 반복!\n\n- 테스트 2\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,(3,3))\nXXX = tnp.arange(1*5*5*1,dtype=tf.float64).reshape(1,5,5,1)\n\n\nXXX.reshape(1,5,5)\n\n<tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy=\narray([[[ 0.,  1.,  2.,  3.,  4.],\n        [ 5.,  6.,  7.,  8.,  9.],\n        [10., 11., 12., 13., 14.],\n        [15., 16., 17., 18., 19.],\n        [20., 21., 22., 23., 24.]]])>\n\n\n\ntf.reshape(cnv.weights[0],(3,3))\n\n<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.08676198, -0.1595147 , -0.13450584],\n       [-0.4105929 , -0.38366908,  0.07744962],\n       [-0.09255642,  0.4915564 ,  0.20828158]], dtype=float32)>\n\n\n\ncnv(XXX).reshape(1,3,3)\n\n<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=\narray([[[ 2.7395768 ,  2.2492635 ,  1.7589504 ],\n        [ 0.28801066, -0.20230258, -0.6926158 ],\n        [-2.1635566 , -2.6538715 , -3.1441827 ]]], dtype=float32)>\n\n\n\ntf.reduce_sum(XXX.reshape(1,5,5)[0,:3,:3] * tf.reshape(cnv.weights[0],(3,3)))\n\n<tf.Tensor: shape=(), dtype=float64, numpy=2.739577144384384>\n\n\n- 테스트 3\n\nXXX = tf.constant([[3,3,2,1,0],[0,0,1,3,1],[3,1,2,2,3],[2,0,0,2,2],[2,0,0,0,1]],dtype=tf.float64).reshape(1,5,5,1)\nXXX.reshape(1,5,5)\n\n<tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy=\narray([[[3., 3., 2., 1., 0.],\n        [0., 0., 1., 3., 1.],\n        [3., 1., 2., 2., 3.],\n        [2., 0., 0., 2., 2.],\n        [2., 0., 0., 0., 1.]]])>\n\n\n\n_w = tf.constant([[0,1,2],[2,2,0],[0,1,2]],dtype = tf.float64)\n_b = tf.constant([0],dtype=tf.float64)\n\n\ncnv= tf.keras.layers.Conv2D(1, kernel_size = (3,3))\n\n\ncnv.set_weights([_w,_b])\n\nValueError: ignored\n\n\n- 테스트 4\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(1,kernel_size=(2,2))\nXXX \n\n\n\n- 테스트 3,4 다시…\n- 테스트 5\n\n관측치는 고정이고 출력채널이 바뀔 때\n\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,kernel_size=(2,2)) ## 출력채널이 1인 경우 값이 1개 나옴 element wise 정확히 알아야한다\nXXX = tnp.arange(1*2*2*1,dtype=tf.float64).reshape(1,2,2,1)\n\n\nprint(XXX.reshape(1,2,2))\n\ntf.Tensor(\n[[[0. 1.]\n  [2. 3.]]], shape=(1, 2, 2), dtype=float64)\n\n\n\ncnv(XXX)\n\n<tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=\narray([[[[ 1.048703  , -1.0948269 , -0.04476056, -1.7289025 ]]]],\n      dtype=float32)>\n\n\n\ncnv.weights[0] # (2 x 2)커널의크기 // 1은 XXX의 채널 수 // 4는 con(XXX)의 채널수\n\n<tf.Variable 'conv2d_28/kernel:0' shape=(2, 2, 1, 4) dtype=float32, numpy=\narray([[[[-0.08230966, -0.15132892, -0.12760344, -0.38952267]],\n\n        [[-0.36398047,  0.07347518, -0.08780673,  0.46633136]]],\n\n\n       [[[ 0.19759327, -0.46042526, -0.15406173, -0.34838456]],\n\n        [[ 0.33916563, -0.08248386,  0.11705655, -0.49948823]]]],\n      dtype=float32)>\n\n\n\ncnv.weights[0][...,0].reshape(2,2) ## conv(XXX)의 첫 번째 채널의 weight 값\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.08230966, -0.36398047],\n       [ 0.19759327,  0.33916563]], dtype=float32)>\n\n\n\ntf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2)) ## 이는 Conv 2D를 거쳐 나오는 첫번째 출력 값\n\n<tf.Tensor: shape=(), dtype=float64, numpy=1.0487029552459717>\n\n\n\n채널을 뻥튀기한다 \\(\\to\\) 아무의미없다 그냥 늘리는 것이다 $ $ 그냥 파라미터를 많이 써서 뻥튀기하는 것이다.\n원래 채널은 3개(색깔)지만 그냥 모델을 복잡하게 해서 뻥튀기를 하는 것이다.\n\n- 테스트 6\n\nX의 채널이 여러개고, Conv의 채널도 여러개 일때\n\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,kernel_size=(2,2))\nXXX = tnp.array([1]*1*2*2*1,dtype=tf.float64).reshape(1,2,2,1)\n\n\nprint(XXX.reshape(1,2,2))\n\ntf.Tensor(\n[[[1. 1.]\n  [1. 1.]]], shape=(1, 2, 2), dtype=float64)\n\n\n\n위와같이 XXX를 설정하면 cnv(XXX)의 결과는 단지 cnv의 weigth들의 합이 된다.\n\n\ncnv(XXX)\n\n<tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=\narray([[[[ 0.09046876, -0.6207628 , -0.25241536, -0.7710641 ]]]],\n      dtype=float32)>\n\n\n\ncnv.weights[0] # (2 x 2)커널의크기 // 1은 XXX의 채널 수 // 4는 con(XXX)의 채널수\n\n<tf.Variable 'conv2d_29/kernel:0' shape=(2, 2, 1, 4) dtype=float32, numpy=\narray([[[[-0.08230966, -0.15132892, -0.12760344, -0.38952267]],\n\n        [[-0.36398047,  0.07347518, -0.08780673,  0.46633136]]],\n\n\n       [[[ 0.19759327, -0.46042526, -0.15406173, -0.34838456]],\n\n        [[ 0.33916563, -0.08248386,  0.11705655, -0.49948823]]]],\n      dtype=float32)>\n\n\n\ntf.reduce_sum(cnv.weights[0][...,0]) == tf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2))\n\n<tf.Tensor: shape=(), dtype=bool, numpy=True>\n\n\n\n계산결과를 확인하기 쉽게 만든 약간의 트릭\n\n\ntf.random.set_seed(43052)\ncnv = tf.keras.layers.Conv2D(4,kernel_size=(2,2))\nXXX = tnp.array([1]*1*2*2*3,dtype=tf.float64).reshape(1,2,2,3) ## X의 채널 차원 수를 늘림\n\n\ncnv(XXX)\n\n<tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=\narray([[[[ 0.3297621, -0.4498347, -1.0487393, -1.580095 ]]]],\n      dtype=float32)>\n\n\n\ncnv.weights ## (2 x 2)는 커널 사이즈 // 3은 XXX의 채널 // 4는 conv(xxx) 출력 채널\n\n[<tf.Variable 'conv2d_34/kernel:0' shape=(2, 2, 3, 4) dtype=float32, numpy=\n array([[[[-0.06956434, -0.12789628, -0.10784459, -0.32920673],\n          [-0.30761963,  0.06209785, -0.07421023,  0.3941219 ],\n          [ 0.16699678, -0.38913035, -0.13020593, -0.29443866]],\n \n         [[ 0.28664726, -0.0697116 ,  0.09893084, -0.4221446 ],\n          [-0.23161241, -0.16410837, -0.36420006,  0.12424195],\n          [-0.14245945,  0.36286396, -0.10751781,  0.1733647 ]]],\n \n \n        [[[ 0.02764335,  0.15547717, -0.42024496, -0.31893867],\n          [ 0.22414821,  0.3619454 , -0.00282967, -0.3503708 ],\n          [ 0.4610079 , -0.17417148,  0.00401336, -0.29777044]],\n \n         [[-0.1620284 , -0.42066965, -0.01578814, -0.4240524 ],\n          [ 0.37925082,  0.24236053,  0.3949356 , -0.20996472],\n          [-0.30264795, -0.28889188, -0.3237777 ,  0.37506342]]]],\n       dtype=float32)>,\n <tf.Variable 'conv2d_34/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n\n\n\ncnv.weights[0][...,0] ### cnv(XXX)의 첫번째 채널을 얻기위해 사용되는 weight값\n\n<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=\narray([[[-0.06956434, -0.30761963,  0.16699678],\n        [ 0.28664726, -0.23161241, -0.14245945]],\n\n       [[ 0.02764335,  0.22414821,  0.4610079 ],\n        [-0.1620284 ,  0.37925082, -0.30264795]]], dtype=float32)>\n\n\n\ntf.reduce_sum(cnv.weights[0][...,0]) ## 오 cnv(XXX)의 첫번째 값과 같다. 즉 첫번째 채널의 결과 ## 이거 시험에 안나오니 정리만!!\n## 채널이 뻥튀기 될 때 뻥튀기 된 채널이 아무의미가 없고 유의미한 피쳐가 있을 것이라고 기대하는 것이다 즉 그중에 강아지인지, 고양이지인지 그중에 하나만 걸려라의 느낌의 뻥튀기\n\n<tf.Tensor: shape=(), dtype=float32, numpy=0.32976213>\n\n\n\nprint(tf.reduce_sum(cnv.weights[0][...,1])) ## 두 번째 cnv(XXX)\nprint(tf.reduce_sum(cnv.weights[0][...,2])) ## 두 번째 cnv(XXX)\nprint(tf.reduce_sum(cnv.weights[0][...,3])) ## 두 번째 cnv(XXX)\n\ntf.Tensor(-0.44983464, shape=(), dtype=float32)\ntf.Tensor(-1.0487392, shape=(), dtype=float32)\ntf.Tensor(-1.5800952, shape=(), dtype=float32)\n\n\n\nw_red = cnv.weights[0][...,0][...,0] ## X의 입력 채널\nw_green = cnv.weights[0][...,0][...,1] ## X의 입력 채널\nw_blue = cnv.weights[0][...,0][...,2] ## X의 입력 채널\n\n\nw_red\n\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[-0.06956434,  0.28664726],\n       [ 0.02764335, -0.1620284 ]], dtype=float32)>\n\n\n\nXXX[...,0]*w_red  +  XXX[...,1] * w_green  + XXX[...,2] * w_blue \n\n<tf.Tensor: shape=(1, 2, 2), dtype=float64, numpy=\narray([[[-0.2101872 , -0.08742461],\n        [ 0.71279946, -0.08542553]]])>\n\n\n\nXXX[...,0] ## X_red\n\n<tf.Tensor: shape=(1, 2, 2), dtype=float64, numpy=\narray([[[1., 1.],\n        [1., 1.]]])>\n\n\n\n만약 layer의 acitvation을 relu로 잡으면 음의값은 전부 0으로 바뀐다."
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-11-(11주차).html#텐서플로우를-공부하기-좋은-교재-혹은-참고자료",
    "href": "post/Bigdata Analysis/2022-05-11-(11주차).html#텐서플로우를-공부하기-좋은-교재-혹은-참고자료",
    "title": "12. Max pooling, CNN",
    "section": "텐서플로우를 공부하기 좋은 교재 혹은 참고자료",
    "text": "텐서플로우를 공부하기 좋은 교재 혹은 참고자료\n\n텐서플로우 교재\n- 교재1: http://www.kyobobook.co.kr/product/detailViewEng.laf?mallGb=ENG&ejkGb=ENG&barcode=9781838823412 - 장점: 텐서플로우 2.0을 다룬 교재, 기본적 내용을 간략히 소개. 다양한 분야를 커버. - 단점: 내용이 조금 산만함 (잘 안읽힘). 코드가 예쁘지 않음. 글을 진짜 못씀.\n- 교재2: https://www.amazon.com/Advanced-Deep-Learning-TensorFlow-Keras/dp/1838821651/ref=pd_sbs_sccl_2_2/139-5369471-9501726?pd_rd_w=PLPU2&pf_rd_p=3676f086-9496-4fd7-8490-77cf7f43f846&pf_rd_r=019BZNCP8B6RHXPKXF0T&pd_rd_r=2dea359c-c5d4-4404-86d8-cb1f5f934607&pd_rd_wg=98kmk&pd_rd_i=1838821651&psc=1\n\n장점: 코드가 예쁘다. 예제가 잘 구성됨. 참신하다. 내용에 깊이가 있다.\n단점: 입문용 내용이 아예 없음\n\n- 교재3: 텐서플로 딥러닝 프로그래밍 (김동근/가메출판사) - 장점: 한글교재, 그래도 교재구성에 흐름이 있다. 공식문서의 가이드내용도 포함 (대부분은 튜토리얼 수준만 포함) - 단점: 공식문서내용 그냥 거의 그대로 베낌.\n\n\n공식홈페이지\n- 튜토리얼: https://www.tensorflow.org/tutorials?hl=ko - 탭눌러서 초보자~고급 모두 읽어보면 좋다. - 간단한 모형실습들 제공. - 대부분의 교재는 튜토리얼의 내용을 베껴서 출판한다. (외국교재도 포함!)\n- 가이드: https://www.tensorflow.org/guide?hl=ko - 왜 파이토치가 아니고 텐서플로우를 써야하는가? 에 대한 대답들 - 모형에 대한 설명보다 프로그램 자체에 대한 이해도를 높여준다.\n- API: https://www.tensorflow.org/versions?hl=ko - tf의 다양한 모듈구조를 확인 - 파이썬에서 도움말 치면 나오는 것들 + \\(\\alpha\\) - 교재에서 확인불가능한 아주 디테일한 부분까지 확인가능\n\ntf.GradientTape?\n\n\nInit signature: tf.GradientTape(persistent=False, watch_accessed_variables=True)\nDocstring:     \nRecord operations for automatic differentiation.\nOperations are recorded if they are executed within this context manager and\nat least one of their inputs is being \"watched\".\nTrainable variables (created by `tf.Variable` or `tf.compat.v1.get_variable`,\nwhere `trainable=True` is default in both cases) are automatically watched.\nTensors can be manually watched by invoking the `watch` method on this context\nmanager.\nFor example, consider the function `y = x * x`. The gradient at `x = 3.0` can\nbe computed as:\n>>> x = tf.constant(3.0)\n>>> with tf.GradientTape() as g:\n...   g.watch(x)\n...   y = x * x\n>>> dy_dx = g.gradient(y, x)\n>>> print(dy_dx)\ntf.Tensor(6.0, shape=(), dtype=float32)\nGradientTapes can be nested to compute higher-order derivatives. For example,\n>>> x = tf.constant(5.0)\n>>> with tf.GradientTape() as g:\n...   g.watch(x)\n...   with tf.GradientTape() as gg:\n...     gg.watch(x)\n...     y = x * x\n...   dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x\n>>> d2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2\n>>> print(dy_dx)\ntf.Tensor(10.0, shape=(), dtype=float32)\n>>> print(d2y_dx2)\ntf.Tensor(2.0, shape=(), dtype=float32)\nBy default, the resources held by a GradientTape are released as soon as\nGradientTape.gradient() method is called. To compute multiple gradients over\nthe same computation, create a persistent gradient tape. This allows multiple\ncalls to the gradient() method as resources are released when the tape object\nis garbage collected. For example:\n>>> x = tf.constant(3.0)\n>>> with tf.GradientTape(persistent=True) as g:\n...   g.watch(x)\n...   y = x * x\n...   z = y * y\n>>> dz_dx = g.gradient(z, x)  # (4*x^3 at x = 3)\n>>> print(dz_dx)\ntf.Tensor(108.0, shape=(), dtype=float32)\n>>> dy_dx = g.gradient(y, x)\n>>> print(dy_dx)\ntf.Tensor(6.0, shape=(), dtype=float32)\nBy default GradientTape will automatically watch any trainable variables that\nare accessed inside the context. If you want fine grained control over which\nvariables are watched you can disable automatic tracking by passing\n`watch_accessed_variables=False` to the tape constructor:\n>>> x = tf.Variable(2.0)\n>>> w = tf.Variable(5.0)\n>>> with tf.GradientTape(\n...     watch_accessed_variables=False, persistent=True) as tape:\n...   tape.watch(x)\n...   y = x ** 2  # Gradients will be available for `x`.\n...   z = w ** 3  # No gradients will be available as `w` isn't being watched.\n>>> dy_dx = tape.gradient(y, x)\n>>> print(dy_dx)\ntf.Tensor(4.0, shape=(), dtype=float32)\n>>> # No gradients will be available as `w` isn't being watched.\n>>> dz_dy = tape.gradient(z, w)\n>>> print(dz_dy)\nNone\nNote that when using models you should ensure that your variables exist when\nusing `watch_accessed_variables=False`. Otherwise it's quite easy to make your\nfirst iteration not have any gradients:\n```python\na = tf.keras.layers.Dense(32)\nb = tf.keras.layers.Dense(32)\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\n  tape.watch(a.variables)  # Since `a.build` has not been called at this point\n                           # `a.variables` will return an empty list and the\n                           # tape will not be watching anything.\n  result = b(a(inputs))\n  tape.gradient(result, a.variables)  # The result of this computation will be\n                                      # a list of `None`s since a's variables\n                                      # are not being watched.\nNote that only tensors with real or complex dtypes are differentiable. Init docstring: Creates a new GradientTape. Args: persistent: Boolean controlling whether a persistent gradient tape is created. False by default, which means at most one call can be made to the gradient() method on this object. watch_accessed_variables: Boolean controlling whether the tape will automatically watch any (trainable) variables accessed while the tape is active. Defaults to True meaning gradients can be requested from any result computed in the tape derived from reading a trainable Variable. If False users must explicitly watch any Variables they want to request gradients from. File: ~/anaconda3/envs/tfgpu/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py Type: type Subclasses: LossScaleGradientTape\n\n```\n\n\n\n\n\n텐서플로우가 아닌 그냥 딥러닝 이론교재\n- Deep Learning (이안굿펠로우): https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=171345378 - 장점: 그나마 잘 쓰여진 전통있는 교재, 교수님들 책꽃이에 하나씩 꼽혀있었음. 방대한 내용다룸. 깊이있음. 틀린것 없음. 무료 - 단점: 번역이 쓰레기임. 내용이 너무 어려움. (이해를 하라고 쓴 설명이 아님) - 무료제공: https://www.deeplearningbook.org/\n- 기계학습 (오일석) - 장점: 이해가 잘된다. 꽤 넓은 분야를 다룬다. 비교적 간단한 예제로 개념을 설명한다.\n\n\nCNN의 시작: 알렉스넷\n- 만화: https://wedatalab.tistory.com/71\n\n\n2d convolution"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-23-(12주차).html#imports",
    "href": "post/Bigdata Analysis/2022-05-23-(12주차).html#imports",
    "title": "13. CNN-2",
    "section": "imports",
    "text": "imports\n\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-23-(12주차).html#cnn",
    "href": "post/Bigdata Analysis/2022-05-23-(12주차).html#cnn",
    "title": "13. CNN-2",
    "section": "CNN",
    "text": "CNN\n\nCONV의 역할\n- 2D 에서 이미지 처리파트 * CONV - MAXPOOL - CONV - MAXPOOL\n- DNN 파트 * Flatten - Dense\n- 데이터생성 (그냥 흑백대비 데이터)\n\n_X1 = tnp.ones([50,25])*10 \n_X1\n\n<tf.Tensor: shape=(50, 25), dtype=float64, numpy=\narray([[10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       ...,\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.],\n       [10., 10., 10., ..., 10., 10., 10.]])>\n\n\n\n_X2 = tnp.zeros([50,25])*10 \n_X2\n\n<tf.Tensor: shape=(50, 25), dtype=float64, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])>\n\n\n\ntf.concat([_X1,_X2],axis=1)\n\n<tf.Tensor: shape=(50, 50), dtype=float64, numpy=\narray([[10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       ...,\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.],\n       [10., 10., 10., ...,  0.,  0.,  0.]])>\n\n\n\n_noise = tnp.random.randn(50*50).reshape(50,50)\n_noise\n\n<tf.Tensor: shape=(50, 50), dtype=float64, numpy=\narray([[ 0.37129319, -0.22218732,  2.47718277, ...,  0.49074486,\n         0.83356932,  0.45569771],\n       [-0.71583146, -0.11528848, -1.54078337, ..., -1.10061299,\n         0.38703597,  0.10322716],\n       [ 1.52172142,  1.81161957, -0.16138102, ..., -1.27557782,\n        -0.39697905,  1.96703026],\n       ...,\n       [ 1.34529249, -0.33572408, -1.13030603, ...,  0.84900934,\n        -1.70690337, -0.55816   ],\n       [-0.78912273, -1.80655893,  0.16310892, ...,  0.8075891 ,\n        -2.06762249,  0.84053733],\n       [-1.06669861, -0.04952165,  0.40685118, ..., -0.100907  ,\n         1.56423262, -1.63625911]])>\n\n\n\nXXX = tf.concat([_X1,_X2],axis=1) + _noise\n\n\nXXX=XXX.reshape(1,50,50,1)\n\n\nplt.imshow(XXX.reshape(50,50),cmap='gray')\n\n<matplotlib.image.AxesImage at 0x7fbc5d008750>\n\n\n\n\n\n- conv layer 생성\n\nconv = tf.keras.layers.Conv2D(2,(2,2)) \n\n\nconv.weights # 처음에는 가중치가 없음 \n\n[]\n\n\n\nconv(XXX) # 가중치를 만들기 위해서 XXX를 conv에 한번 통과시킴\nconv.weights # 이제 가중치가 생김\n\n[<tf.Variable 'conv2d/kernel:0' shape=(2, 2, 1, 2) dtype=float32, numpy=\n array([[[[ 0.4503097 , -0.6546046 ]],\n \n         [[ 0.53834134,  0.05130672]]],\n \n \n        [[[-0.34205168,  0.12724537]],\n \n         [[-0.39912695,  0.36518508]]]], dtype=float32)>,\n <tf.Variable 'conv2d/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n\n\n- 가중치의 값을 확인해보자.\n\nconv.weights[0] # kernel에 해당하는것 \n\n<tf.Variable 'conv2d/kernel:0' shape=(2, 2, 1, 2) dtype=float32, numpy=\narray([[[[ 0.4503097 , -0.6546046 ]],\n\n        [[ 0.53834134,  0.05130672]]],\n\n\n       [[[-0.34205168,  0.12724537]],\n\n        [[-0.39912695,  0.36518508]]]], dtype=float32)>\n\n\n\nconv.weights[1] # bias에 해당하는것 \n\n<tf.Variable 'conv2d/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>\n\n\n- 필터값을 원하는 것으로 변경해보자.\n\nw0 = [[0.25,0.25],[0.25,0.25]] # 잡티를 제거하는 효과를 준다. \nw1 = [[-1.0,1.0],[-1.0,1.0]] # 경계를 찾기 좋아보이는 필터이다. (엣지검출)\n\n\nnp.array(w1)\n\narray([[-1.,  1.],\n       [-1.,  1.]])\n\n\n\nw=np.concatenate([np.array(w0).reshape(2,2,1,1),np.array(w1).reshape(2,2,1,1)],axis=-1)\nw\n\narray([[[[ 0.25, -1.  ]],\n\n        [[ 0.25,  1.  ]]],\n\n\n       [[[ 0.25, -1.  ]],\n\n        [[ 0.25,  1.  ]]]])\n\n\n\nb= np.array([0.0,0.0])\nb\n\narray([0., 0.])\n\n\n\nconv.set_weights([w,b])\nconv.get_weights()\n\n[array([[[[ 0.25, -1.  ]],\n \n         [[ 0.25,  1.  ]]],\n \n \n        [[[ 0.25, -1.  ]],\n \n         [[ 0.25,  1.  ]]]], dtype=float32), array([0., 0.], dtype=float32)]\n\n\n\n첫번째는 평균을 구하는 필터\n두번째는 엣지를 검출하는 필터\n\n- 필터를 넣은 결과를 확인\n\nXXX0=conv(XXX)[...,0] # 채널0\nXXX0\n\n<tf.Tensor: shape=(1, 49, 49), dtype=float32, numpy=\narray([[[ 9.829496  , 10.149731  , 10.115938  , ..., -0.4450039 ,\n          0.1526843 ,  0.44488257],\n        [10.625555  ,  9.998542  ,  9.816621  , ..., -1.199796  ,\n         -0.5965334 ,  0.5150786 ],\n        [10.125448  ,  9.55331   ,  9.920209  , ..., -0.09245254,\n         -0.50081515, -0.37436587],\n        ...,\n        [10.242724  , 10.06068   ,  9.8106785 , ...,  0.59299064,\n          0.25008035, -0.6558946 ],\n        [ 9.603472  ,  9.22263   ,  9.8764715 , ...,  0.17234075,\n         -0.52948177, -0.8730371 ],\n        [ 9.072025  ,  9.678471  ,  9.743685  , ..., -0.26665327,\n          0.05082306, -0.3247779 ]]], dtype=float32)>\n\n\n\nXXX1=conv(XXX)[...,1] # 채널1\nXXX1\n\n<tf.Tensor: shape=(1, 49, 49), dtype=float32, numpy=\narray([[[ 7.0648193e-03,  1.2738743e+00, -1.4090481e+00, ...,\n          5.6027925e-01,  1.8304735e+00, -6.6168052e-01],\n        [ 8.9044189e-01, -3.3984947e+00,  2.6708107e+00, ...,\n          4.6802759e-02,  2.3662477e+00,  2.0802004e+00],\n        [ 1.2678986e+00, -3.5564499e+00,  5.0240440e+00, ...,\n         -5.1247388e-01, -1.1209767e+00,  1.6267738e+00],\n        ...,\n        [-8.2698441e-01,  9.8808289e-02, -1.0988159e+00, ...,\n          2.4145689e+00, -3.7862101e+00,  1.6231036e-01],\n        [-2.6984520e+00,  1.1750851e+00,  1.4402809e+00, ...,\n          2.6238339e+00, -5.4311242e+00,  4.0569029e+00],\n        [-2.5749207e-04,  2.4260387e+00, -2.1651821e+00, ...,\n          2.4799771e+00, -1.2100719e+00, -2.9233193e-01]]], dtype=float32)>\n\n\n- 각 채널을 시각화\n\nfig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2)\n\n\n\n\n\nax1.imshow(XXX.reshape(50,50),cmap='gray')\n\n<matplotlib.image.AxesImage at 0x7fbc5bca1910>\n\n\n\nax3.imshow(XXX0.reshape(49,49),cmap='gray')\n\n<matplotlib.image.AxesImage at 0x7fbc5bbfb1d0>\n\n\n\nax4.imshow(XXX1.reshape(49,49),cmap='gray')\n\n<matplotlib.image.AxesImage at 0x7fbc5bbca450>\n\n\n\nfig\n\n\n\n\n\n2사분면: 원래이미지\n3사분면: 원래이미지 -> 평균을 의미하는 conv적용\n4사분면: 원래이미지 -> 엣지를 검출하는 conv적용\n\n- conv(XXX)의 각 채널에 한번더 conv를 통과시켜보자\n\nconv(XXX0.reshape(1,49,49,1))[...,0] ### XXX0 -> 평균필터 <=> XXX -> 평균필터 -> 평균필터 \nconv(XXX0.reshape(1,49,49,1))[...,1] ### XXX0 -> 엣지필터 <=> XXX -> 평균필터 -> 엣지필터 \nconv(XXX1.reshape(1,49,49,1))[...,0] ### XXX1 -> 평균필터 <=> XXX -> 엣지필터 -> 평균필터 \nconv(XXX1.reshape(1,49,49,1))[...,1] ### XXX1 -> 엣지필터 <=> XXX -> 엣지필터 -> 엣지필터 \n\n<tf.Tensor: shape=(1, 48, 48), dtype=float32, numpy=\narray([[[ -3.0221272 ,   3.386383  ,  -4.2654552 , ...,  -1.2773508 ,\n           3.5896392 ,  -2.778201  ],\n        [ -9.113285  ,  14.649799  , -11.062517  , ...,  -2.4062634 ,\n           1.710942  ,   2.4617033 ],\n        [ -5.9759903 ,  10.901724  ,  -7.9709587 , ...,  -2.7940087 ,\n          -0.18495584,   1.7879481 ],\n        ...,\n        [  2.469574  ,  -3.258545  ,   4.4745617 , ...,   6.023678  ,\n          -9.406242  ,   3.7584436 ],\n        [  4.7993298 ,  -0.93242836,  -5.0819206 , ...,   7.839739  ,\n         -14.255737  ,  13.436548  ],\n        [  6.2998333 ,  -4.326025  ,  -2.8787212 , ...,   9.661198  ,\n         -11.7450075 ,  10.405767  ]]], dtype=float32)>\n\n\n\nfig,ax =plt.subplots(3,4)\n\n\n\n\n\nax[0][0].imshow(XXX.reshape(50,50),cmap='gray') # 원래이미지\n\n<matplotlib.image.AxesImage at 0x7fbc5b8ab210>\n\n\n\nax[1][0].imshow(XXX0.reshape(49,49),cmap='gray') # 원래이미지 -> 평균필터 \nax[1][1].imshow(XXX1.reshape(49,49),cmap='gray') # 원래이미지 -> 엣지필터\n\n<matplotlib.image.AxesImage at 0x7fbc5b87edd0>\n\n\n\nax[2][0].imshow(conv(XXX0.reshape(1,49,49,1))[...,0].reshape(48,48),cmap='gray') # 원래이미지 -> 평균필터 -> 평균필터 \nax[2][1].imshow(conv(XXX0.reshape(1,49,49,1))[...,1].reshape(48,48),cmap='gray') # 원래이미지 -> 평균필터 -> 엣지필터\nax[2][2].imshow(conv(XXX1.reshape(1,49,49,1))[...,0].reshape(48,48),cmap='gray') # 원래이미지 -> 엣지필터 -> 평균필터 \nax[2][3].imshow(conv(XXX1.reshape(1,49,49,1))[...,1].reshape(48,48),cmap='gray') # 원래이미지 -> 엣지필터 -> 엣지필터\n\n<matplotlib.image.AxesImage at 0x7fbc5b8d6250>\n\n\n\nfig.set_figheight(8)\nfig.set_figwidth(16)\nfig.tight_layout()\nfig\n\n\n\n\n- 요약 - conv의 weight에 따라서 엣지를 검출하는 필터가 만들어지기도 하고 스무딩의 역할을 하는 필터가 만들어지기도 한다. 그리고 우리는 의미를 알 수 없지만 어떠한 역할을 하는 필터가 만들어질 것이다. - 이것들을 조합하다보면 우연히 이미지를 분류하기에 유리한 특징을 뽑아내는 weight가 맞춰질 수도 있겠다. - 컨볼루션 레이어의 역할은 이미지의 특징을 추출한다. - 채널수를 많이 만들고 다양한 웨이트 조합을 실험하다보면 보다 복잡한 이미지의 특징을 추출할 수 도있다.\n\n우리가 설정한 에지는 수직선 에지를 찾는데 유리하다.\n\n\nplt.imshow(XXX.reshape(50,50))\n\n<matplotlib.image.AxesImage at 0x7fbc5765a510>\n\n\n\n\n\n\n아래와 같은 이미지에서 설정한 에지에서 적절한 경계를 찾는것이 힘들다\n\n\nplt.imshow(XXX.reshape(50,50).T)\n\n<matplotlib.image.AxesImage at 0x7fbc575b47d0>\n\n\n\n\n\n- 참고: 스트라이드, 패딩 - 스트라이드: 컨볼루션을 수행할 때 윈도우가 1칸씩 이동하는 것이 아니라 2~3칸씩 이동함 - 패딩: 이미지의 가장자리에 정당한 값을 넣어서 (예를들어 0) 컨볼루션을 수행. 따라서 컨볼루션 연산 이후에도 이미지의 크기가 줄어들지 않도록 방지한다.\n\n\nMAXPOOL\n- 기본적역할: 이미지의 크기를 줄이는 것 - 이미지의 크기를 줄여야하는 이유? 어차피 최종적으로 출력값을 클래스 개수 (ex. 10차원)으로 줄어야하므로 - 이미지의 크기를 줄이면서도 동시에 아주 크리티컬한 특징은 손실없이 유지하고 싶다~\n- 점점 작은 이미지가 되면서 중요한 특징들은 살아남지만 그렇지 않으면 죽는다. (캐리커쳐 느낌)\n- 평균이 아니라 max를 쓴 이유는? 그냥 평균보다 나을것이라고 생각했음.. - 그런데 사실은 꼭 그렇지만은 않아서 최근에는 꼭 맥스풀링을 고집하진 않는 추세 (평균풀링도 많이씀)\n\n\nCNN 아키텍처의 표현방법\n- 아래와 같이 아키텍처의 다이어그램형태로 표현하고 굳이 노드별로 이미지를 그리진 않음\n\n- 물론 아래와 같이 그리는 경우도 있음\n\n\n\nDiscusstion about CNN\n- 격자형태로 배열된 자료를 처리하는데 특화된 신경망이다. - 시계열 (1차원격자), 이미지 (2차원격자)\n- 실제응용에서 엄청난 성공을 거두었다.\n- 이름의 유래는 컨볼루션이라는 수학적 연산을 사용했기 때문 - 컨볼루션은 조금 특별한 선형변환이다.\n- 신경과학의 원리가 심층학습에 영향을 미친 사례이다.\n\n\nCNN의 모티브\n- 희소성 + 매개변수의 공유 - 다소 철학적인 모티브임 - 희소성: 이미지를 분석하여 특징을 뽑아낼때 부분부분의 특징만 뽑으면 된다는 의미 - 매개변수의 공유: 한 채널에는 하나의 역할을 하는 커널을 설계하면 된다는 의미 (스무딩이든 엣징이든). 즉 어떤지역은 스무딩, 어떤지역은 엣징을 할 필요가 없이 한채널에서는 엣징만, 다른채널에서는 스무딩만 수행한뒤 여러채널을 조합해서 이해하면 된다.\n- 매개변수 공유효과로 인해서 파라메터가 확 줄어든다.\n(예시) (1,6,6,1) -> (1,5,5,2) - MLP방식이면 (36,50) 의 차원을 가진 매트릭스가 필요함 => 1800개의 매개변수 필요 - CNN은 8개의 매개변수 필요\n\n\nCNN 신경망의 기본구조\n- 기본유닛 - conv - activation - pooling - conv - conv - activation - pooling"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-23-(12주차).html#모형의-성능을-올리기-위한-노력들",
    "href": "post/Bigdata Analysis/2022-05-23-(12주차).html#모형의-성능을-올리기-위한-노력들",
    "title": "13. CNN-2",
    "section": "모형의 성능을 올리기 위한 노력들",
    "text": "모형의 성능을 올리기 위한 노력들\n\ndropout\n- 아래의 예제를 복습하자.\n\nnp.random.seed(43052)\nx = np.linspace(0,1,100).reshape(100,1)\ny = np.random.normal(loc=0,scale=0.01,size=(100,1))\nplt.plot(x,y)\n\n\n\n\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x,y,epochs=5000,verbose=0,batch_size=100)\n\n2022-05-23 19:33:01.211991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n<keras.callbacks.History at 0x7f1b9528feb0>\n\n\n\nplt.plot(x,y)\nplt.plot(x,net(x),'--')\n\n\n\n\n- train/test로 나누어서 생각해보자.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80)\n\n<keras.callbacks.History at 0x7f1b881f9840>\n\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\n\n\n\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\nplt.plot(x[80:],net(x[80:]),'--')\n\n\n\n\n\ntrain에서 추세를 따라가는게 좋은게 아니다 \\(\\to\\) 그냥 직선으로 핏하는거 이외에는 다 오버핏이다.\n\n- 매 에폭마다 적당히 80%의 노드들을 빼고 학습하자 \\(\\to\\) 너무 잘 학습되는 문제는 생기지 않을 것이다 (과적합이 방지될것이다?)\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2048,activation='relu'))\nnet.add(tf.keras.layers.Dropout(0.8))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\nnet.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80)\n\n<keras.callbacks.History at 0x7f1b80381a50>\n\n\n\nplt.plot(x,y)\nplt.plot(x[:80],net(x[:80]),'--')\nplt.plot(x[80:],net(x[80:]),'--')\n\n\n\n\n- 드랍아웃에 대한 summary - 직관: 특정노드를 랜덤으로 off시키면 학습이 방해되어 오히려 과적합이 방지되는 효과가 있다 (그렇지만 진짜 중요한 특징이라면 랜덤으로 off 되더라도 어느정도는 학습될 듯) - note: 드랍아웃을 쓰면 오버핏이 줄어드는건 맞지만 완전히 없어지는건 아니다. - note: 오버핏을 줄이는 유일한 방법이 드랍아웃만 있는것도 아니며, 드랍아웃이 오버핏을 줄이는 가장 효과적인 방법도 아니다 (최근에는 dropout보다 batch nomalization을 사용하는 추세임)\n\n\ntrain / val / test\n- data\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n32768/29515 [=================================] - 0s 0us/step\n40960/29515 [=========================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26427392/26421880 [==============================] - 0s 0us/step\n26435584/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n16384/5148 [===============================================================================================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4423680/4422102 [==============================] - 0s 0us/step\n4431872/4422102 [==============================] - 0s 0us/step\n\n\n\nX= x_train.reshape(-1,28,28,1)/255 ## 입력이 0~255 -> 0~1로 표준화 시키는 효과 + float으로 자료형이 바뀜 \ny = tf.keras.utils.to_categorical(y_train)\nXX = x_test.reshape(-1,28,28,1)/255\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\nimport tensorflow as tf\n\n\ntf.config.experimental.list_physical_devices('GPU')\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\n#collapse_output\ncb1 = tf.keras.callbacks.TensorBoard()\nnet.fit(X,y,epochs=5,batch_size=200,validation_split=0.2,callbacks=cb1,verbose=1) \n\nEpoch 1/5\n240/240 [==============================] - 1s 3ms/step - loss: 0.0677 - accuracy: 0.9765 - val_loss: 0.6648 - val_accuracy: 0.8743\nEpoch 2/5\n240/240 [==============================] - 1s 3ms/step - loss: 0.0685 - accuracy: 0.9769 - val_loss: 0.6766 - val_accuracy: 0.8745\nEpoch 3/5\n240/240 [==============================] - 1s 3ms/step - loss: 0.0702 - accuracy: 0.9748 - val_loss: 0.6618 - val_accuracy: 0.8766\nEpoch 4/5\n240/240 [==============================] - 1s 3ms/step - loss: 0.0647 - accuracy: 0.9778 - val_loss: 0.6751 - val_accuracy: 0.8758\nEpoch 5/5\n240/240 [==============================] - 1s 3ms/step - loss: 0.0659 - accuracy: 0.9775 - val_loss: 0.6805 - val_accuracy: 0.8746\n\n\n<keras.callbacks.History at 0x7fdf70127910>\n\n\n- 텐서보드 여는 방법1\n\n%reload_ext tensorboard\n# 주피터노트북 (혹은 주피터랩)에서 텐서보드를 임베딩하여 넣을 수 있도록 도와주는 매직펑션\n\n\n#\n# !rm -rf logs\n# !kill 313799\n\n\n#\n# %tensorboard --logdir logs --host 0.0.0.0\n%tensorboard --logdir logs # <-- 실습에서는 이렇게 하면됩니다. \n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n프로세스 죽이기\n\n!kill 1424\n\n\n\n폴더안에 파일 지우기\n\n!rm -rf logs\n\n(참고사항) 파이썬 3.10의 경우 아래의 수정이 필요\n?/python3.10/site-packages/tensorboard/_vendor/html5lib/_trie/_base.py 을 열고\nfrom collections import Mapping ### 수정전\nfrom collections.abc import Mapping ### 수정후 \n와 같이 수정한다.\n\n왜냐하면 파이썬 3.10부터 from collections import Mapping 가 동작하지 않고 from collections.abc import Mapping 가 동작하도록 문법이 바뀜\n\n- 텐서보드를 실행하는 방법2\n\n#\n# !tensorboard --logdir logs --host 0.0.0.0\n# !tensorboard --logdir logs # <-- 실습에서는 이렇게 하면됩니다. \n\n\n\n\n조기종료\n- 텐서보드를 살펴보니 특정에폭 이후에는 오히려 과적합이 진행되는 듯 하다 (학습할수록 손해인듯 하다) \\(\\to\\) 그 특정에폭까지만 학습해보자\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 과적합좀 시키려고 \nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가 \nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 4ms/step - loss: 0.5483 - accuracy: 0.8134 - val_loss: 0.4027 - val_accuracy: 0.8546\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.3568 - accuracy: 0.8671 - val_loss: 0.3531 - val_accuracy: 0.8712\nEpoch 3/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.3210 - accuracy: 0.8799 - val_loss: 0.3477 - val_accuracy: 0.8733\nEpoch 4/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2971 - accuracy: 0.8876 - val_loss: 0.3502 - val_accuracy: 0.8776\n\n\n<keras.callbacks.History at 0x7f1b80086650>\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2791 - accuracy: 0.8935 - val_loss: 0.3224 - val_accuracy: 0.8820\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2619 - accuracy: 0.8999 - val_loss: 0.3498 - val_accuracy: 0.8779\n\n\n<keras.callbacks.History at 0x7f1b24290a90>\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2491 - accuracy: 0.9043 - val_loss: 0.3641 - val_accuracy: 0.8711\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2328 - accuracy: 0.9110 - val_loss: 0.3282 - val_accuracy: 0.8848\nEpoch 3/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2254 - accuracy: 0.9151 - val_loss: 0.3280 - val_accuracy: 0.8843\nEpoch 4/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2144 - accuracy: 0.9177 - val_loss: 0.3191 - val_accuracy: 0.8925\nEpoch 5/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2074 - accuracy: 0.9223 - val_loss: 0.3152 - val_accuracy: 0.8949\nEpoch 6/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1952 - accuracy: 0.9250 - val_loss: 0.3322 - val_accuracy: 0.8863\n\n\n<keras.callbacks.History at 0x7f1b242c1660>\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1908 - accuracy: 0.9257 - val_loss: 0.3513 - val_accuracy: 0.8836\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1799 - accuracy: 0.9304 - val_loss: 0.3376 - val_accuracy: 0.8901\nEpoch 3/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1712 - accuracy: 0.9346 - val_loss: 0.3568 - val_accuracy: 0.8894\n\n\n<keras.callbacks.History at 0x7f1b24302230>\n\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1591 - accuracy: 0.9367 - val_loss: 0.3995 - val_accuracy: 0.8780\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1552 - accuracy: 0.9398 - val_loss: 0.3469 - val_accuracy: 0.8917\nEpoch 3/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1481 - accuracy: 0.9423 - val_loss: 0.3726 - val_accuracy: 0.8853\n\n\n<keras.callbacks.History at 0x7f1b24136e00>\n\n\n- 몇 번 좀 참았다가 멈추면 좋겠다.\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 과적합좀 시키려고 \nnet.add(tf.keras.layers.Dense(5000,activation='relu')) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가 \nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\n#\n#cb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=5) # 좀더 참다가 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) \n\nEpoch 1/200\n240/240 [==============================] - 1s 4ms/step - loss: 0.5475 - accuracy: 0.8139 - val_loss: 0.4219 - val_accuracy: 0.8453\nEpoch 2/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.3575 - accuracy: 0.8676 - val_loss: 0.3647 - val_accuracy: 0.8712\nEpoch 3/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.3219 - accuracy: 0.8792 - val_loss: 0.3559 - val_accuracy: 0.8710\nEpoch 4/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2990 - accuracy: 0.8883 - val_loss: 0.3448 - val_accuracy: 0.8808\nEpoch 5/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2759 - accuracy: 0.8966 - val_loss: 0.3337 - val_accuracy: 0.8792\nEpoch 6/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2621 - accuracy: 0.9004 - val_loss: 0.3220 - val_accuracy: 0.8841\nEpoch 7/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2478 - accuracy: 0.9074 - val_loss: 0.3302 - val_accuracy: 0.8858\nEpoch 8/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2342 - accuracy: 0.9110 - val_loss: 0.3150 - val_accuracy: 0.8904\nEpoch 9/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2261 - accuracy: 0.9144 - val_loss: 0.3117 - val_accuracy: 0.8932\nEpoch 10/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2116 - accuracy: 0.9200 - val_loss: 0.3345 - val_accuracy: 0.8888\nEpoch 11/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.2081 - accuracy: 0.9207 - val_loss: 0.3344 - val_accuracy: 0.8867\nEpoch 12/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1956 - accuracy: 0.9255 - val_loss: 0.3158 - val_accuracy: 0.8975\nEpoch 13/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1863 - accuracy: 0.9275 - val_loss: 0.3302 - val_accuracy: 0.8934\nEpoch 14/200\n240/240 [==============================] - 1s 3ms/step - loss: 0.1764 - accuracy: 0.9324 - val_loss: 0.3717 - val_accuracy: 0.8859\n\n\n<keras.callbacks.History at 0x7f1b24301960>\n\n\n- 텐서보드로 그려보자?\n\n#\n# %tensorboard --logdir logs --host 0.0.0.0 \n# 아무것도 안나온다 -> 왜? cb1을 써야 텐서보드가 나옴\n\n- 조기종료와 텐서보드를 같이 쓰려면?\n\ntf.random.set_seed(43052)\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu')) \nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer='adam',loss=tf.losses.categorical_crossentropy,metrics='accuracy')\n\n\ncb1 = tf.keras.callbacks.TensorBoard()\ncb2 = tf.keras.callbacks.EarlyStopping(patience=7) # 좀더 참다가 멈추어라 \nnet.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=[cb1,cb2]) \n\nEpoch 1/200\n240/240 [==============================] - 0s 1ms/step - loss: 0.7184 - accuracy: 0.7581 - val_loss: 0.5077 - val_accuracy: 0.8276\nEpoch 2/200\n240/240 [==============================] - 0s 890us/step - loss: 0.4752 - accuracy: 0.8386 - val_loss: 0.4793 - val_accuracy: 0.8342\nEpoch 3/200\n240/240 [==============================] - 0s 899us/step - loss: 0.4304 - accuracy: 0.8517 - val_loss: 0.4386 - val_accuracy: 0.8497\nEpoch 4/200\n240/240 [==============================] - 0s 880us/step - loss: 0.4048 - accuracy: 0.8582 - val_loss: 0.4029 - val_accuracy: 0.8603\nEpoch 5/200\n240/240 [==============================] - 0s 923us/step - loss: 0.3832 - accuracy: 0.8669 - val_loss: 0.3932 - val_accuracy: 0.8619\nEpoch 6/200\n240/240 [==============================] - 0s 934us/step - loss: 0.3697 - accuracy: 0.8705 - val_loss: 0.3842 - val_accuracy: 0.8657\nEpoch 7/200\n240/240 [==============================] - 0s 900us/step - loss: 0.3569 - accuracy: 0.8759 - val_loss: 0.3844 - val_accuracy: 0.8668\nEpoch 8/200\n240/240 [==============================] - 0s 889us/step - loss: 0.3482 - accuracy: 0.8774 - val_loss: 0.3679 - val_accuracy: 0.8708\nEpoch 9/200\n240/240 [==============================] - 0s 912us/step - loss: 0.3387 - accuracy: 0.8799 - val_loss: 0.3602 - val_accuracy: 0.8719\nEpoch 10/200\n240/240 [==============================] - 0s 923us/step - loss: 0.3299 - accuracy: 0.8820 - val_loss: 0.3610 - val_accuracy: 0.8748\nEpoch 11/200\n240/240 [==============================] - 0s 853us/step - loss: 0.3229 - accuracy: 0.8858 - val_loss: 0.3574 - val_accuracy: 0.8717\nEpoch 12/200\n240/240 [==============================] - 0s 904us/step - loss: 0.3157 - accuracy: 0.8873 - val_loss: 0.3572 - val_accuracy: 0.8743\nEpoch 13/200\n240/240 [==============================] - 0s 890us/step - loss: 0.3106 - accuracy: 0.8899 - val_loss: 0.3545 - val_accuracy: 0.8761\nEpoch 14/200\n240/240 [==============================] - 0s 911us/step - loss: 0.3046 - accuracy: 0.8914 - val_loss: 0.3493 - val_accuracy: 0.8759\nEpoch 15/200\n240/240 [==============================] - 0s 921us/step - loss: 0.3011 - accuracy: 0.8928 - val_loss: 0.3483 - val_accuracy: 0.8776\nEpoch 16/200\n240/240 [==============================] - 0s 937us/step - loss: 0.2988 - accuracy: 0.8935 - val_loss: 0.3733 - val_accuracy: 0.8716\nEpoch 17/200\n240/240 [==============================] - 0s 892us/step - loss: 0.2925 - accuracy: 0.8947 - val_loss: 0.3481 - val_accuracy: 0.8768\nEpoch 18/200\n240/240 [==============================] - 0s 933us/step - loss: 0.2880 - accuracy: 0.8951 - val_loss: 0.3396 - val_accuracy: 0.8801\nEpoch 19/200\n240/240 [==============================] - 0s 957us/step - loss: 0.2827 - accuracy: 0.8982 - val_loss: 0.3439 - val_accuracy: 0.8798\nEpoch 20/200\n240/240 [==============================] - 0s 881us/step - loss: 0.2791 - accuracy: 0.8986 - val_loss: 0.3489 - val_accuracy: 0.8779\nEpoch 21/200\n240/240 [==============================] - 0s 886us/step - loss: 0.2765 - accuracy: 0.9007 - val_loss: 0.3350 - val_accuracy: 0.8823\nEpoch 22/200\n240/240 [==============================] - 0s 912us/step - loss: 0.2709 - accuracy: 0.9016 - val_loss: 0.3350 - val_accuracy: 0.8812\nEpoch 23/200\n240/240 [==============================] - 0s 908us/step - loss: 0.2688 - accuracy: 0.9029 - val_loss: 0.3374 - val_accuracy: 0.8820\nEpoch 24/200\n240/240 [==============================] - 0s 930us/step - loss: 0.2658 - accuracy: 0.9041 - val_loss: 0.3445 - val_accuracy: 0.8805\nEpoch 25/200\n240/240 [==============================] - 0s 872us/step - loss: 0.2607 - accuracy: 0.9058 - val_loss: 0.3383 - val_accuracy: 0.8822\nEpoch 26/200\n240/240 [==============================] - 0s 928us/step - loss: 0.2607 - accuracy: 0.9056 - val_loss: 0.3415 - val_accuracy: 0.8811\nEpoch 27/200\n240/240 [==============================] - 0s 927us/step - loss: 0.2576 - accuracy: 0.9068 - val_loss: 0.3402 - val_accuracy: 0.8814\nEpoch 28/200\n240/240 [==============================] - 0s 905us/step - loss: 0.2525 - accuracy: 0.9098 - val_loss: 0.3469 - val_accuracy: 0.8802\n\n\n<keras.callbacks.History at 0x7f1b24217a00>\n\n\n\n# \n# 조기종료가 구현된 그림이 출력\n# %tensorboard --logdir logs --host 0.0.0.0 \n\n\n\n하이퍼파라메터 선택\n- 하이퍼파라메터 설정\n\nfrom tensorboard.plugins.hparams import api as hp\n\n\na=net.evaluate(XX,yy)\n\n313/313 [==============================] - 0s 859us/step - loss: 0.3803 - accuracy: 0.8704\n\n\n\n!rm -rf logs\nfor u in [50,5000]: \n    for d in [0.0,0.5]: \n        for o in ['adam','sgd']:\n            logdir = 'logs/hpguebin_{}_{}_{}'.format(u,d,o)\n            with tf.summary.create_file_writer(logdir).as_default():\n                net = tf.keras.Sequential()\n                net.add(tf.keras.layers.Flatten())\n                net.add(tf.keras.layers.Dense(u,activation='relu'))\n                net.add(tf.keras.layers.Dropout(d))\n                net.add(tf.keras.layers.Dense(10,activation='softmax'))\n                net.compile(optimizer=o,loss=tf.losses.categorical_crossentropy,metrics=['accuracy','Recall'])\n                cb3 = hp.KerasCallback(logdir, {'유닛수':u, '드랍아웃비율':d, '옵티마이저':o})\n                net.fit(X,y,epochs=3,callbacks=cb3)\n                _rslt=net.evaluate(XX,yy)\n                _mymetric=_rslt[1]*0.8 + _rslt[2]*0.2  \n                tf.summary.scalar('애큐러시와리컬의가중평균(테스트셋)', _mymetric, step=1) \n\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5255 - accuracy: 0.8180 - recall: 0.7546\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.3993 - accuracy: 0.8588 - recall: 0.8294\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.3648 - accuracy: 0.8698 - recall: 0.8443\n313/313 [==============================] - 0s 830us/step - loss: 0.4063 - accuracy: 0.8545 - recall: 0.8286\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.7744 - accuracy: 0.7503 - recall: 0.5797\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5204 - accuracy: 0.8223 - recall: 0.7565\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4742 - accuracy: 0.8369 - recall: 0.7859\n313/313 [==============================] - 0s 828us/step - loss: 0.4899 - accuracy: 0.8304 - recall: 0.7831\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.7502 - accuracy: 0.7356 - recall: 0.6115\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5738 - accuracy: 0.7923 - recall: 0.7133\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5473 - accuracy: 0.8037 - recall: 0.7321\n313/313 [==============================] - 0s 865us/step - loss: 0.4319 - accuracy: 0.8448 - recall: 0.7919\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 1.0932 - accuracy: 0.6228 - recall: 0.3971\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.7616 - accuracy: 0.7388 - recall: 0.5956\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6828 - accuracy: 0.7684 - recall: 0.6478\n313/313 [==============================] - 0s 894us/step - loss: 0.5265 - accuracy: 0.8180 - recall: 0.7353\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4777 - accuracy: 0.8292 - recall: 0.7890\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.3603 - accuracy: 0.8682 - recall: 0.8427\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.3197 - accuracy: 0.8817 - recall: 0.8605\n313/313 [==============================] - 0s 846us/step - loss: 0.3803 - accuracy: 0.8628 - recall: 0.8428\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6685 - accuracy: 0.7883 - recall: 0.6444\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4815 - accuracy: 0.8372 - recall: 0.7781\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4408 - accuracy: 0.8498 - recall: 0.8021\n313/313 [==============================] - 0s 859us/step - loss: 0.4634 - accuracy: 0.8390 - recall: 0.7962\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5708 - accuracy: 0.7991 - recall: 0.7556\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4418 - accuracy: 0.8393 - recall: 0.8057\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4091 - accuracy: 0.8514 - recall: 0.8211\n313/313 [==============================] - 0s 850us/step - loss: 0.3937 - accuracy: 0.8587 - recall: 0.8238\nEpoch 1/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.6930 - accuracy: 0.7752 - recall: 0.6338\nEpoch 2/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.5048 - accuracy: 0.8274 - recall: 0.7651\nEpoch 3/3\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.4608 - accuracy: 0.8417 - recall: 0.7910\n313/313 [==============================] - 0s 854us/step - loss: 0.4625 - accuracy: 0.8396 - recall: 0.7957\n\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-23-(12주차).html#숙제",
    "href": "post/Bigdata Analysis/2022-05-23-(12주차).html#숙제",
    "title": "13. CNN-2",
    "section": "숙제",
    "text": "숙제\n- 아래의 네트워크에서 옵티마이저를 adam, sgd를 선택하여 각각 적합시켜보고 testset의 loss를 성능비교를 하라. epoch은 5정도로 설정하라.\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Flatten())\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(50,activation='relu'))\nnet.add(tf.keras.layers.Dense(10,activation='softmax'))\nnet.compile(optimizer=???,loss=tf.losses.categorical_crossentropy,metrics=['accuracy','Recall'])"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-05-30-(13주차).html",
    "href": "post/Bigdata Analysis/2022-05-30-(13주차).html",
    "title": "13. model fitting",
    "section": "",
    "text": "imports\n\nimport numpy as np\nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\nimport matplotlib.pyplot as plt \n\n\n%load_ext tensorboard\n\n\n\n오버피팅\n\n오버피팅으로 착각하기 쉬운 상황\n3-(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i\\]\n여기에서 \\(t=(t_1,\\dots,t_{1000})=\\) np.linspace(0,5,1000) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.linspace(0,5,1000)\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.1)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2,3,1,0,0,0.5 이다)\n(풀이)\n- 다시 풀어보자\n\ny = y.reshape(1000,1)\nx1 = np.cos(t) \nx2 = np.cos(2*t)\nx3 = np.cos(3*t)\nx4 = np.cos(4*t)\nx5 = np.cos(5*t)\nX = tf.stack([x1,x2,x3,x4,x5],axis=1)\n\n2022-05-25 13:34:57.890242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(loss='mse',optimizer='adam')\nnet.fit(X,y,epochs=500,batch_size=100, validation_split=0.45, callbacks=tf.keras.callbacks.TensorBoard()) \n# 텐서보드를 이용한 시각화기능 추가 \n# validation_split 이용\n\nEpoch 1/500\n6/6 [==============================] - 0s 9ms/step - loss: 6.2200 - val_loss: 16.9707\nEpoch 2/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.1682 - val_loss: 16.9098\nEpoch 3/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.1168 - val_loss: 16.8487\nEpoch 4/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.0665 - val_loss: 16.7881\nEpoch 5/500\n6/6 [==============================] - 0s 3ms/step - loss: 6.0170 - val_loss: 16.7266\nEpoch 6/500\n6/6 [==============================] - 0s 4ms/step - loss: 5.9663 - val_loss: 16.6675\nEpoch 7/500\n6/6 [==============================] - 0s 4ms/step - loss: 5.9185 - val_loss: 16.6086\nEpoch 8/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.8679 - val_loss: 16.5515\nEpoch 9/500\n6/6 [==============================] - 0s 4ms/step - loss: 5.8200 - val_loss: 16.4926\nEpoch 10/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.7724 - val_loss: 16.4347\nEpoch 11/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.7247 - val_loss: 16.3748\nEpoch 12/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.6759 - val_loss: 16.3188\nEpoch 13/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.6296 - val_loss: 16.2615\nEpoch 14/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.5824 - val_loss: 16.2049\nEpoch 15/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.5361 - val_loss: 16.1485\nEpoch 16/500\n6/6 [==============================] - 0s 4ms/step - loss: 5.4899 - val_loss: 16.0936\nEpoch 17/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.4436 - val_loss: 16.0364\nEpoch 18/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.3988 - val_loss: 15.9806\nEpoch 19/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.3545 - val_loss: 15.9251\nEpoch 20/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.3087 - val_loss: 15.8708\nEpoch 21/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.2653 - val_loss: 15.8172\nEpoch 22/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.2204 - val_loss: 15.7660\nEpoch 23/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.1774 - val_loss: 15.7140\nEpoch 24/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.1352 - val_loss: 15.6609\nEpoch 25/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.0921 - val_loss: 15.6077\nEpoch 26/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.0496 - val_loss: 15.5556\nEpoch 27/500\n6/6 [==============================] - 0s 3ms/step - loss: 5.0074 - val_loss: 15.5049\nEpoch 28/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.9658 - val_loss: 15.4536\nEpoch 29/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.9243 - val_loss: 15.4012\nEpoch 30/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.8829 - val_loss: 15.3500\nEpoch 31/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.8414 - val_loss: 15.2996\nEpoch 32/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.8004 - val_loss: 15.2499\nEpoch 33/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.7603 - val_loss: 15.1995\nEpoch 34/500\n6/6 [==============================] - 0s 4ms/step - loss: 4.7195 - val_loss: 15.1498\nEpoch 35/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.6804 - val_loss: 15.1006\nEpoch 36/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.6396 - val_loss: 15.0534\nEpoch 37/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.6005 - val_loss: 15.0050\nEpoch 38/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.5618 - val_loss: 14.9572\nEpoch 39/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.5234 - val_loss: 14.9099\nEpoch 40/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.4840 - val_loss: 14.8627\nEpoch 41/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.4467 - val_loss: 14.8146\nEpoch 42/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.4087 - val_loss: 14.7684\nEpoch 43/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.3711 - val_loss: 14.7220\nEpoch 44/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.3334 - val_loss: 14.6753\nEpoch 45/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.2970 - val_loss: 14.6294\nEpoch 46/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.2603 - val_loss: 14.5826\nEpoch 47/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.2232 - val_loss: 14.5386\nEpoch 48/500\n6/6 [==============================] - 0s 4ms/step - loss: 4.1877 - val_loss: 14.4930\nEpoch 49/500\n6/6 [==============================] - 0s 4ms/step - loss: 4.1513 - val_loss: 14.4489\nEpoch 50/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.1169 - val_loss: 14.4047\nEpoch 51/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.0811 - val_loss: 14.3595\nEpoch 52/500\n6/6 [==============================] - 0s 3ms/step - loss: 4.0459 - val_loss: 14.3128\nEpoch 53/500\n6/6 [==============================] - 0s 4ms/step - loss: 4.0111 - val_loss: 14.2681\nEpoch 54/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.9770 - val_loss: 14.2218\nEpoch 55/500\n6/6 [==============================] - 0s 4ms/step - loss: 3.9416 - val_loss: 14.1764\nEpoch 56/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.9088 - val_loss: 14.1324\nEpoch 57/500\n6/6 [==============================] - 0s 4ms/step - loss: 3.8748 - val_loss: 14.0885\nEpoch 58/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.8411 - val_loss: 14.0468\nEpoch 59/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.8085 - val_loss: 14.0052\nEpoch 60/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.7749 - val_loss: 13.9633\nEpoch 61/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.7425 - val_loss: 13.9234\nEpoch 62/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.7106 - val_loss: 13.8829\nEpoch 63/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.6800 - val_loss: 13.8406\nEpoch 64/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.6468 - val_loss: 13.8024\nEpoch 65/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.6158 - val_loss: 13.7614\nEpoch 66/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.5847 - val_loss: 13.7200\nEpoch 67/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.5536 - val_loss: 13.6788\nEpoch 68/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.5231 - val_loss: 13.6385\nEpoch 69/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.4927 - val_loss: 13.6000\nEpoch 70/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.4631 - val_loss: 13.5597\nEpoch 71/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.4322 - val_loss: 13.5206\nEpoch 72/500\n6/6 [==============================] - 0s 2ms/step - loss: 3.4026 - val_loss: 13.4812\nEpoch 73/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.3729 - val_loss: 13.4425\nEpoch 74/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.3431 - val_loss: 13.4058\nEpoch 75/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.3148 - val_loss: 13.3676\nEpoch 76/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.2857 - val_loss: 13.3286\nEpoch 77/500\n6/6 [==============================] - 0s 2ms/step - loss: 3.2565 - val_loss: 13.2909\nEpoch 78/500\n6/6 [==============================] - 0s 2ms/step - loss: 3.2284 - val_loss: 13.2534\nEpoch 79/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.1992 - val_loss: 13.2167\nEpoch 80/500\n6/6 [==============================] - 0s 4ms/step - loss: 3.1722 - val_loss: 13.1812\nEpoch 81/500\n6/6 [==============================] - 0s 4ms/step - loss: 3.1444 - val_loss: 13.1452\nEpoch 82/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.1169 - val_loss: 13.1083\nEpoch 83/500\n6/6 [==============================] - 0s 2ms/step - loss: 3.0901 - val_loss: 13.0695\nEpoch 84/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.0625 - val_loss: 13.0314\nEpoch 85/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.0359 - val_loss: 12.9948\nEpoch 86/500\n6/6 [==============================] - 0s 3ms/step - loss: 3.0088 - val_loss: 12.9592\nEpoch 87/500\n6/6 [==============================] - 0s 4ms/step - loss: 2.9823 - val_loss: 12.9249\nEpoch 88/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.9566 - val_loss: 12.8900\nEpoch 89/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.9307 - val_loss: 12.8534\nEpoch 90/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.9050 - val_loss: 12.8185\nEpoch 91/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.8792 - val_loss: 12.7831\nEpoch 92/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.8542 - val_loss: 12.7482\nEpoch 93/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.8289 - val_loss: 12.7135\nEpoch 94/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.8041 - val_loss: 12.6790\nEpoch 95/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.7795 - val_loss: 12.6438\nEpoch 96/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.7546 - val_loss: 12.6110\nEpoch 97/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.7307 - val_loss: 12.5763\nEpoch 98/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.7060 - val_loss: 12.5430\nEpoch 99/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.6825 - val_loss: 12.5093\nEpoch 100/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.6584 - val_loss: 12.4760\nEpoch 101/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.6350 - val_loss: 12.4419\nEpoch 102/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.6117 - val_loss: 12.4088\nEpoch 103/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.5882 - val_loss: 12.3750\nEpoch 104/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.5657 - val_loss: 12.3433\nEpoch 105/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.5425 - val_loss: 12.3114\nEpoch 106/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.5202 - val_loss: 12.2804\nEpoch 107/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.4979 - val_loss: 12.2476\nEpoch 108/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.4755 - val_loss: 12.2145\nEpoch 109/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.4537 - val_loss: 12.1828\nEpoch 110/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.4314 - val_loss: 12.1527\nEpoch 111/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.4102 - val_loss: 12.1212\nEpoch 112/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3888 - val_loss: 12.0904\nEpoch 113/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3681 - val_loss: 12.0618\nEpoch 114/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3466 - val_loss: 12.0304\nEpoch 115/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3263 - val_loss: 12.0002\nEpoch 116/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.3053 - val_loss: 11.9704\nEpoch 117/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.2849 - val_loss: 11.9400\nEpoch 118/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.2647 - val_loss: 11.9106\nEpoch 119/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.2438 - val_loss: 11.8820\nEpoch 120/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.2241 - val_loss: 11.8520\nEpoch 121/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.2039 - val_loss: 11.8224\nEpoch 122/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.1843 - val_loss: 11.7940\nEpoch 123/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.1648 - val_loss: 11.7655\nEpoch 124/500\n6/6 [==============================] - 0s 4ms/step - loss: 2.1454 - val_loss: 11.7361\nEpoch 125/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.1262 - val_loss: 11.7072\nEpoch 126/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.1071 - val_loss: 11.6805\nEpoch 127/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.0886 - val_loss: 11.6536\nEpoch 128/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.0695 - val_loss: 11.6254\nEpoch 129/500\n6/6 [==============================] - 0s 2ms/step - loss: 2.0515 - val_loss: 11.5969\nEpoch 130/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.0327 - val_loss: 11.5688\nEpoch 131/500\n6/6 [==============================] - 0s 3ms/step - loss: 2.0150 - val_loss: 11.5409\nEpoch 132/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9966 - val_loss: 11.5148\nEpoch 133/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9790 - val_loss: 11.4873\nEpoch 134/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9609 - val_loss: 11.4611\nEpoch 135/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9438 - val_loss: 11.4336\nEpoch 136/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9268 - val_loss: 11.4073\nEpoch 137/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.9093 - val_loss: 11.3819\nEpoch 138/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.8922 - val_loss: 11.3557\nEpoch 139/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.8756 - val_loss: 11.3285\nEpoch 140/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.8586 - val_loss: 11.3015\nEpoch 141/500\n6/6 [==============================] - 0s 2ms/step - loss: 1.8418 - val_loss: 11.2759\nEpoch 142/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.8256 - val_loss: 11.2502\nEpoch 143/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.8090 - val_loss: 11.2256\nEpoch 144/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7927 - val_loss: 11.1994\nEpoch 145/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7769 - val_loss: 11.1727\nEpoch 146/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7608 - val_loss: 11.1471\nEpoch 147/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7451 - val_loss: 11.1210\nEpoch 148/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.7294 - val_loss: 11.0955\nEpoch 149/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.7142 - val_loss: 11.0686\nEpoch 150/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6985 - val_loss: 11.0437\nEpoch 151/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6833 - val_loss: 11.0186\nEpoch 152/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6682 - val_loss: 10.9943\nEpoch 153/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.6531 - val_loss: 10.9703\nEpoch 154/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6386 - val_loss: 10.9462\nEpoch 155/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6239 - val_loss: 10.9234\nEpoch 156/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.6093 - val_loss: 10.9001\nEpoch 157/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5949 - val_loss: 10.8762\nEpoch 158/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5811 - val_loss: 10.8518\nEpoch 159/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5667 - val_loss: 10.8276\nEpoch 160/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5530 - val_loss: 10.8025\nEpoch 161/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5387 - val_loss: 10.7807\nEpoch 162/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5250 - val_loss: 10.7566\nEpoch 163/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.5115 - val_loss: 10.7325\nEpoch 164/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4977 - val_loss: 10.7099\nEpoch 165/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4846 - val_loss: 10.6866\nEpoch 166/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4714 - val_loss: 10.6642\nEpoch 167/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4582 - val_loss: 10.6410\nEpoch 168/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4451 - val_loss: 10.6192\nEpoch 169/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4321 - val_loss: 10.5966\nEpoch 170/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4194 - val_loss: 10.5742\nEpoch 171/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.4066 - val_loss: 10.5521\nEpoch 172/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3942 - val_loss: 10.5301\nEpoch 173/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3817 - val_loss: 10.5074\nEpoch 174/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3695 - val_loss: 10.4855\nEpoch 175/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3575 - val_loss: 10.4629\nEpoch 176/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.3451 - val_loss: 10.4417\nEpoch 177/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3332 - val_loss: 10.4193\nEpoch 178/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3214 - val_loss: 10.3978\nEpoch 179/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.3094 - val_loss: 10.3760\nEpoch 180/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2979 - val_loss: 10.3535\nEpoch 181/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2867 - val_loss: 10.3313\nEpoch 182/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2751 - val_loss: 10.3107\nEpoch 183/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2640 - val_loss: 10.2911\nEpoch 184/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.2527 - val_loss: 10.2700\nEpoch 185/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2418 - val_loss: 10.2492\nEpoch 186/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.2310 - val_loss: 10.2279\nEpoch 187/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.2201 - val_loss: 10.2073\nEpoch 188/500\n6/6 [==============================] - 0s 2ms/step - loss: 1.2098 - val_loss: 10.1864\nEpoch 189/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1989 - val_loss: 10.1660\nEpoch 190/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1886 - val_loss: 10.1447\nEpoch 191/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1780 - val_loss: 10.1253\nEpoch 192/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1680 - val_loss: 10.1046\nEpoch 193/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.1579 - val_loss: 10.0847\nEpoch 194/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1477 - val_loss: 10.0651\nEpoch 195/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1379 - val_loss: 10.0455\nEpoch 196/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.1280 - val_loss: 10.0270\nEpoch 197/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.1183 - val_loss: 10.0083\nEpoch 198/500\n6/6 [==============================] - 0s 4ms/step - loss: 1.1086 - val_loss: 9.9880\nEpoch 199/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0990 - val_loss: 9.9680\nEpoch 200/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0896 - val_loss: 9.9479\nEpoch 201/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0802 - val_loss: 9.9284\nEpoch 202/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0708 - val_loss: 9.9092\nEpoch 203/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0616 - val_loss: 9.8902\nEpoch 204/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0527 - val_loss: 9.8713\nEpoch 205/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0435 - val_loss: 9.8533\nEpoch 206/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0346 - val_loss: 9.8335\nEpoch 207/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0260 - val_loss: 9.8141\nEpoch 208/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0172 - val_loss: 9.7948\nEpoch 209/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0084 - val_loss: 9.7757\nEpoch 210/500\n6/6 [==============================] - 0s 3ms/step - loss: 1.0000 - val_loss: 9.7570\nEpoch 211/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9918 - val_loss: 9.7374\nEpoch 212/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9832 - val_loss: 9.7190\nEpoch 213/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9752 - val_loss: 9.6991\nEpoch 214/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9669 - val_loss: 9.6805\nEpoch 215/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9588 - val_loss: 9.6630\nEpoch 216/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9509 - val_loss: 9.6449\nEpoch 217/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9429 - val_loss: 9.6260\nEpoch 218/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9351 - val_loss: 9.6078\nEpoch 219/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9272 - val_loss: 9.5889\nEpoch 220/500\n6/6 [==============================] - 0s 2ms/step - loss: 0.9194 - val_loss: 9.5704\nEpoch 221/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9117 - val_loss: 9.5517\nEpoch 222/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.9043 - val_loss: 9.5328\nEpoch 223/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8967 - val_loss: 9.5144\nEpoch 224/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8893 - val_loss: 9.4963\nEpoch 225/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8820 - val_loss: 9.4785\nEpoch 226/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8748 - val_loss: 9.4600\nEpoch 227/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8675 - val_loss: 9.4414\nEpoch 228/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8605 - val_loss: 9.4222\nEpoch 229/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.8535 - val_loss: 9.4037\nEpoch 230/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8464 - val_loss: 9.3854\nEpoch 231/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8396 - val_loss: 9.3677\nEpoch 232/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8328 - val_loss: 9.3485\nEpoch 233/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8262 - val_loss: 9.3306\nEpoch 234/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.8196 - val_loss: 9.3123\nEpoch 235/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8130 - val_loss: 9.2945\nEpoch 236/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8064 - val_loss: 9.2777\nEpoch 237/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.8002 - val_loss: 9.2597\nEpoch 238/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7938 - val_loss: 9.2424\nEpoch 239/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.7875 - val_loss: 9.2246\nEpoch 240/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.7812 - val_loss: 9.2078\nEpoch 241/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7752 - val_loss: 9.1906\nEpoch 242/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7690 - val_loss: 9.1731\nEpoch 243/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.7630 - val_loss: 9.1547\nEpoch 244/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7570 - val_loss: 9.1369\nEpoch 245/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7511 - val_loss: 9.1204\nEpoch 246/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7454 - val_loss: 9.1022\nEpoch 247/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7395 - val_loss: 9.0852\nEpoch 248/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7338 - val_loss: 9.0677\nEpoch 249/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7282 - val_loss: 9.0500\nEpoch 250/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7227 - val_loss: 9.0341\nEpoch 251/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7172 - val_loss: 9.0179\nEpoch 252/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7118 - val_loss: 9.0014\nEpoch 253/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7064 - val_loss: 8.9845\nEpoch 254/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.7011 - val_loss: 8.9669\nEpoch 255/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6958 - val_loss: 8.9504\nEpoch 256/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6907 - val_loss: 8.9342\nEpoch 257/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6855 - val_loss: 8.9176\nEpoch 258/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6804 - val_loss: 8.9012\nEpoch 259/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6755 - val_loss: 8.8848\nEpoch 260/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.6705 - val_loss: 8.8678\nEpoch 261/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6655 - val_loss: 8.8516\nEpoch 262/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6606 - val_loss: 8.8351\nEpoch 263/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6559 - val_loss: 8.8178\nEpoch 264/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6510 - val_loss: 8.8016\nEpoch 265/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6464 - val_loss: 8.7851\nEpoch 266/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.6419 - val_loss: 8.7681\nEpoch 267/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.6372 - val_loss: 8.7525\nEpoch 268/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6328 - val_loss: 8.7359\nEpoch 269/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6284 - val_loss: 8.7178\nEpoch 270/500\n6/6 [==============================] - 0s 2ms/step - loss: 0.6240 - val_loss: 8.7002\nEpoch 271/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6195 - val_loss: 8.6830\nEpoch 272/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6153 - val_loss: 8.6657\nEpoch 273/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6109 - val_loss: 8.6506\nEpoch 274/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6067 - val_loss: 8.6332\nEpoch 275/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.6025 - val_loss: 8.6157\nEpoch 276/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5983 - val_loss: 8.5999\nEpoch 277/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5942 - val_loss: 8.5836\nEpoch 278/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5901 - val_loss: 8.5670\nEpoch 279/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5861 - val_loss: 8.5503\nEpoch 280/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5822 - val_loss: 8.5338\nEpoch 281/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5782 - val_loss: 8.5173\nEpoch 282/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5743 - val_loss: 8.5005\nEpoch 283/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5705 - val_loss: 8.4849\nEpoch 284/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5666 - val_loss: 8.4695\nEpoch 285/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5630 - val_loss: 8.4534\nEpoch 286/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5593 - val_loss: 8.4368\nEpoch 287/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5556 - val_loss: 8.4202\nEpoch 288/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5521 - val_loss: 8.4029\nEpoch 289/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5485 - val_loss: 8.3865\nEpoch 290/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.5449 - val_loss: 8.3706\nEpoch 291/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5416 - val_loss: 8.3542\nEpoch 292/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5380 - val_loss: 8.3383\nEpoch 293/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5346 - val_loss: 8.3220\nEpoch 294/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.5313 - val_loss: 8.3061\nEpoch 295/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5279 - val_loss: 8.2898\nEpoch 296/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5245 - val_loss: 8.2738\nEpoch 297/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5213 - val_loss: 8.2570\nEpoch 298/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5180 - val_loss: 8.2404\nEpoch 299/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5149 - val_loss: 8.2240\nEpoch 300/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5117 - val_loss: 8.2085\nEpoch 301/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5086 - val_loss: 8.1927\nEpoch 302/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5055 - val_loss: 8.1769\nEpoch 303/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.5026 - val_loss: 8.1606\nEpoch 304/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4995 - val_loss: 8.1443\nEpoch 305/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4966 - val_loss: 8.1277\nEpoch 306/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4936 - val_loss: 8.1108\nEpoch 307/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4907 - val_loss: 8.0935\nEpoch 308/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4879 - val_loss: 8.0770\nEpoch 309/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4850 - val_loss: 8.0609\nEpoch 310/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4822 - val_loss: 8.0452\nEpoch 311/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4794 - val_loss: 8.0294\nEpoch 312/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4768 - val_loss: 8.0134\nEpoch 313/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4741 - val_loss: 7.9976\nEpoch 314/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4715 - val_loss: 7.9818\nEpoch 315/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4689 - val_loss: 7.9666\nEpoch 316/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4663 - val_loss: 7.9510\nEpoch 317/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4638 - val_loss: 7.9347\nEpoch 318/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4612 - val_loss: 7.9191\nEpoch 319/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4587 - val_loss: 7.9027\nEpoch 320/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4561 - val_loss: 7.8862\nEpoch 321/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4536 - val_loss: 7.8708\nEpoch 322/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4512 - val_loss: 7.8552\nEpoch 323/500\n6/6 [==============================] - 0s 2ms/step - loss: 0.4487 - val_loss: 7.8394\nEpoch 324/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4463 - val_loss: 7.8227\nEpoch 325/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4439 - val_loss: 7.8063\nEpoch 326/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4416 - val_loss: 7.7906\nEpoch 327/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4393 - val_loss: 7.7742\nEpoch 328/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4370 - val_loss: 7.7577\nEpoch 329/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4348 - val_loss: 7.7415\nEpoch 330/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4326 - val_loss: 7.7239\nEpoch 331/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4304 - val_loss: 7.7076\nEpoch 332/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4282 - val_loss: 7.6921\nEpoch 333/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4261 - val_loss: 7.6763\nEpoch 334/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4240 - val_loss: 7.6604\nEpoch 335/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4220 - val_loss: 7.6445\nEpoch 336/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4198 - val_loss: 7.6292\nEpoch 337/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4179 - val_loss: 7.6131\nEpoch 338/500\n6/6 [==============================] - 0s 2ms/step - loss: 0.4158 - val_loss: 7.5977\nEpoch 339/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4138 - val_loss: 7.5814\nEpoch 340/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4118 - val_loss: 7.5642\nEpoch 341/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4099 - val_loss: 7.5479\nEpoch 342/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4079 - val_loss: 7.5325\nEpoch 343/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4060 - val_loss: 7.5169\nEpoch 344/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4040 - val_loss: 7.5016\nEpoch 345/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.4022 - val_loss: 7.4851\nEpoch 346/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.4003 - val_loss: 7.4683\nEpoch 347/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3985 - val_loss: 7.4519\nEpoch 348/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3966 - val_loss: 7.4350\nEpoch 349/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3948 - val_loss: 7.4181\nEpoch 350/500\n6/6 [==============================] - 0s 2ms/step - loss: 0.3930 - val_loss: 7.4023\nEpoch 351/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3912 - val_loss: 7.3859\nEpoch 352/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3895 - val_loss: 7.3701\nEpoch 353/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3877 - val_loss: 7.3544\nEpoch 354/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3860 - val_loss: 7.3373\nEpoch 355/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3842 - val_loss: 7.3210\nEpoch 356/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3825 - val_loss: 7.3045\nEpoch 357/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3808 - val_loss: 7.2885\nEpoch 358/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3791 - val_loss: 7.2724\nEpoch 359/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3775 - val_loss: 7.2565\nEpoch 360/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3758 - val_loss: 7.2409\nEpoch 361/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3742 - val_loss: 7.2244\nEpoch 362/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3727 - val_loss: 7.2077\nEpoch 363/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3710 - val_loss: 7.1918\nEpoch 364/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3695 - val_loss: 7.1761\nEpoch 365/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3679 - val_loss: 7.1599\nEpoch 366/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3664 - val_loss: 7.1431\nEpoch 367/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3649 - val_loss: 7.1263\nEpoch 368/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3634 - val_loss: 7.1100\nEpoch 369/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3619 - val_loss: 7.0939\nEpoch 370/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3604 - val_loss: 7.0774\nEpoch 371/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3589 - val_loss: 7.0615\nEpoch 372/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3575 - val_loss: 7.0453\nEpoch 373/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3561 - val_loss: 7.0284\nEpoch 374/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3547 - val_loss: 7.0119\nEpoch 375/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3533 - val_loss: 6.9952\nEpoch 376/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3519 - val_loss: 6.9800\nEpoch 377/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3505 - val_loss: 6.9635\nEpoch 378/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3491 - val_loss: 6.9466\nEpoch 379/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3477 - val_loss: 6.9295\nEpoch 380/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3464 - val_loss: 6.9124\nEpoch 381/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3452 - val_loss: 6.8957\nEpoch 382/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3438 - val_loss: 6.8792\nEpoch 383/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3425 - val_loss: 6.8631\nEpoch 384/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3412 - val_loss: 6.8465\nEpoch 385/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3399 - val_loss: 6.8299\nEpoch 386/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.3387 - val_loss: 6.8137\nEpoch 387/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3373 - val_loss: 6.7981\nEpoch 388/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.3361 - val_loss: 6.7825\nEpoch 389/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3348 - val_loss: 6.7660\nEpoch 390/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3336 - val_loss: 6.7500\nEpoch 391/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3323 - val_loss: 6.7337\nEpoch 392/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3311 - val_loss: 6.7167\nEpoch 393/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3299 - val_loss: 6.7001\nEpoch 394/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3287 - val_loss: 6.6835\nEpoch 395/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3275 - val_loss: 6.6664\nEpoch 396/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3263 - val_loss: 6.6488\nEpoch 397/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.3252 - val_loss: 6.6320\nEpoch 398/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3240 - val_loss: 6.6150\nEpoch 399/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3228 - val_loss: 6.5981\nEpoch 400/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3217 - val_loss: 6.5811\nEpoch 401/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3207 - val_loss: 6.5645\nEpoch 402/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3195 - val_loss: 6.5477\nEpoch 403/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3184 - val_loss: 6.5309\nEpoch 404/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.3173 - val_loss: 6.5144\nEpoch 405/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3161 - val_loss: 6.4976\nEpoch 406/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3150 - val_loss: 6.4804\nEpoch 407/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.3139 - val_loss: 6.4638\nEpoch 408/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3128 - val_loss: 6.4458\nEpoch 409/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3117 - val_loss: 6.4281\nEpoch 410/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3106 - val_loss: 6.4111\nEpoch 411/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3096 - val_loss: 6.3941\nEpoch 412/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3085 - val_loss: 6.3764\nEpoch 413/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3074 - val_loss: 6.3594\nEpoch 414/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3065 - val_loss: 6.3420\nEpoch 415/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3054 - val_loss: 6.3241\nEpoch 416/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3044 - val_loss: 6.3073\nEpoch 417/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3034 - val_loss: 6.2903\nEpoch 418/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3025 - val_loss: 6.2734\nEpoch 419/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3013 - val_loss: 6.2575\nEpoch 420/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.3004 - val_loss: 6.2409\nEpoch 421/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2994 - val_loss: 6.2242\nEpoch 422/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2984 - val_loss: 6.2075\nEpoch 423/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2974 - val_loss: 6.1904\nEpoch 424/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2964 - val_loss: 6.1737\nEpoch 425/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2954 - val_loss: 6.1569\nEpoch 426/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2945 - val_loss: 6.1407\nEpoch 427/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2936 - val_loss: 6.1236\nEpoch 428/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2926 - val_loss: 6.1073\nEpoch 429/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2916 - val_loss: 6.0908\nEpoch 430/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2908 - val_loss: 6.0741\nEpoch 431/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2898 - val_loss: 6.0571\nEpoch 432/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2889 - val_loss: 6.0399\nEpoch 433/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2879 - val_loss: 6.0221\nEpoch 434/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2870 - val_loss: 6.0051\nEpoch 435/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2861 - val_loss: 5.9884\nEpoch 436/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2851 - val_loss: 5.9713\nEpoch 437/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2843 - val_loss: 5.9546\nEpoch 438/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.2833 - val_loss: 5.9381\nEpoch 439/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.2824 - val_loss: 5.9211\nEpoch 440/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2815 - val_loss: 5.9039\nEpoch 441/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2806 - val_loss: 5.8861\nEpoch 442/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2797 - val_loss: 5.8693\nEpoch 443/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2789 - val_loss: 5.8528\nEpoch 444/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2781 - val_loss: 5.8350\nEpoch 445/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2772 - val_loss: 5.8184\nEpoch 446/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2763 - val_loss: 5.8015\nEpoch 447/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2754 - val_loss: 5.7853\nEpoch 448/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2746 - val_loss: 5.7684\nEpoch 449/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2737 - val_loss: 5.7513\nEpoch 450/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2729 - val_loss: 5.7344\nEpoch 451/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2720 - val_loss: 5.7173\nEpoch 452/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2711 - val_loss: 5.7008\nEpoch 453/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2703 - val_loss: 5.6843\nEpoch 454/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2695 - val_loss: 5.6665\nEpoch 455/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2687 - val_loss: 5.6494\nEpoch 456/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2678 - val_loss: 5.6318\nEpoch 457/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2670 - val_loss: 5.6147\nEpoch 458/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2662 - val_loss: 5.5980\nEpoch 459/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2654 - val_loss: 5.5813\nEpoch 460/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2646 - val_loss: 5.5641\nEpoch 461/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2638 - val_loss: 5.5476\nEpoch 462/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.2630 - val_loss: 5.5310\nEpoch 463/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2622 - val_loss: 5.5145\nEpoch 464/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2613 - val_loss: 5.4979\nEpoch 465/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2605 - val_loss: 5.4806\nEpoch 466/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2597 - val_loss: 5.4643\nEpoch 467/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2589 - val_loss: 5.4476\nEpoch 468/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2581 - val_loss: 5.4310\nEpoch 469/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2574 - val_loss: 5.4142\nEpoch 470/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2566 - val_loss: 5.3978\nEpoch 471/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2558 - val_loss: 5.3805\nEpoch 472/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2550 - val_loss: 5.3637\nEpoch 473/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2542 - val_loss: 5.3466\nEpoch 474/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2535 - val_loss: 5.3306\nEpoch 475/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2527 - val_loss: 5.3135\nEpoch 476/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2519 - val_loss: 5.2972\nEpoch 477/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2511 - val_loss: 5.2801\nEpoch 478/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2504 - val_loss: 5.2632\nEpoch 479/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2495 - val_loss: 5.2459\nEpoch 480/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2487 - val_loss: 5.2288\nEpoch 481/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.2480 - val_loss: 5.2122\nEpoch 482/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2472 - val_loss: 5.1952\nEpoch 483/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2464 - val_loss: 5.1784\nEpoch 484/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2456 - val_loss: 5.1620\nEpoch 485/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2449 - val_loss: 5.1452\nEpoch 486/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2441 - val_loss: 5.1281\nEpoch 487/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2434 - val_loss: 5.1113\nEpoch 488/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2426 - val_loss: 5.0950\nEpoch 489/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2419 - val_loss: 5.0791\nEpoch 490/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2411 - val_loss: 5.0621\nEpoch 491/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2404 - val_loss: 5.0454\nEpoch 492/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2396 - val_loss: 5.0286\nEpoch 493/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2389 - val_loss: 5.0118\nEpoch 494/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2381 - val_loss: 4.9955\nEpoch 495/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2374 - val_loss: 4.9795\nEpoch 496/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.2367 - val_loss: 4.9630\nEpoch 497/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.2360 - val_loss: 4.9465\nEpoch 498/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2352 - val_loss: 4.9300\nEpoch 499/500\n6/6 [==============================] - 0s 4ms/step - loss: 0.2344 - val_loss: 4.9141\nEpoch 500/500\n6/6 [==============================] - 0s 3ms/step - loss: 0.2337 - val_loss: 4.8977\n\n\n<keras.callbacks.History at 0x7f70988bbe50>\n\n\n- 결과시각화\n\nplt.plot(y,'.',alpha=0.1)\nplt.plot(net(X),'--')\n\n\n\n\n- 보여준 데이터에서는 잘 맞추는것 같지만 validation에서는 엉망이다. -> 오버피팅인가? -> 텐서보드로 확인\n\n# \n#%tensorboard --logdir logs --host 0.0.0.0 \n\n\n확인결과: 에폭마다 val_loss 가 줄어들고 있기는 하다 (늦게 줄어들뿐) -> 오버피팅이라기보다 val_set에 들어있는 자료를 예측하기에는 보여준 데이터가 불충분하다라고 해석하는것이 더 옳음 (모형자체의 문제는 아님)\n\n- 해결하는 방법? 그냥 더 학습시키면된다.\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(loss='mse',optimizer='adam')\nnet.fit(X,y,epochs=2000,batch_size=100, validation_split=0.45, callbacks=tf.keras.callbacks.TensorBoard()) \n# 텐서보드를 이용한 시각화기능 추가 \n# validation_split 이용\n\nEpoch 1/2000\n6/6 [==============================] - 0s 9ms/step - loss: 7.9563 - val_loss: 17.0027\nEpoch 2/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.9013 - val_loss: 16.9412\nEpoch 3/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.8463 - val_loss: 16.8810\nEpoch 4/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.7913 - val_loss: 16.8207\nEpoch 5/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.7380 - val_loss: 16.7617\nEpoch 6/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.6829 - val_loss: 16.7032\nEpoch 7/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.6308 - val_loss: 16.6446\nEpoch 8/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.5769 - val_loss: 16.5886\nEpoch 9/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.5245 - val_loss: 16.5321\nEpoch 10/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.4717 - val_loss: 16.4760\nEpoch 11/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.4201 - val_loss: 16.4201\nEpoch 12/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.3686 - val_loss: 16.3636\nEpoch 13/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.3162 - val_loss: 16.3079\nEpoch 14/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.2647 - val_loss: 16.2527\nEpoch 15/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.2148 - val_loss: 16.1978\nEpoch 16/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.1634 - val_loss: 16.1427\nEpoch 17/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.1142 - val_loss: 16.0877\nEpoch 18/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.0632 - val_loss: 16.0335\nEpoch 19/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.0148 - val_loss: 15.9786\nEpoch 20/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.9651 - val_loss: 15.9238\nEpoch 21/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.9164 - val_loss: 15.8699\nEpoch 22/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.8670 - val_loss: 15.8155\nEpoch 23/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.8191 - val_loss: 15.7617\nEpoch 24/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.7714 - val_loss: 15.7093\nEpoch 25/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.7231 - val_loss: 15.6568\nEpoch 26/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.6762 - val_loss: 15.6048\nEpoch 27/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.6286 - val_loss: 15.5528\nEpoch 28/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5816 - val_loss: 15.5012\nEpoch 29/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5349 - val_loss: 15.4495\nEpoch 30/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4885 - val_loss: 15.3982\nEpoch 31/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4417 - val_loss: 15.3463\nEpoch 32/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.3956 - val_loss: 15.2957\nEpoch 33/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3508 - val_loss: 15.2442\nEpoch 34/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3053 - val_loss: 15.1941\nEpoch 35/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2593 - val_loss: 15.1447\nEpoch 36/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2142 - val_loss: 15.0943\nEpoch 37/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1706 - val_loss: 15.0446\nEpoch 38/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1261 - val_loss: 14.9966\nEpoch 39/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.0825 - val_loss: 14.9488\nEpoch 40/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.0394 - val_loss: 14.9001\nEpoch 41/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9965 - val_loss: 14.8519\nEpoch 42/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.9530 - val_loss: 14.8035\nEpoch 43/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9106 - val_loss: 14.7552\nEpoch 44/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8678 - val_loss: 14.7074\nEpoch 45/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8262 - val_loss: 14.6600\nEpoch 46/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7841 - val_loss: 14.6128\nEpoch 47/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.7422 - val_loss: 14.5659\nEpoch 48/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7007 - val_loss: 14.5194\nEpoch 49/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6598 - val_loss: 14.4737\nEpoch 50/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6183 - val_loss: 14.4275\nEpoch 51/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5779 - val_loss: 14.3826\nEpoch 52/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5376 - val_loss: 14.3368\nEpoch 53/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.4973 - val_loss: 14.2902\nEpoch 54/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.4572 - val_loss: 14.2453\nEpoch 55/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.4176 - val_loss: 14.2002\nEpoch 56/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3785 - val_loss: 14.1551\nEpoch 57/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3387 - val_loss: 14.1109\nEpoch 58/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2996 - val_loss: 14.0679\nEpoch 59/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2613 - val_loss: 14.0241\nEpoch 60/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2221 - val_loss: 13.9811\nEpoch 61/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1839 - val_loss: 13.9382\nEpoch 62/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1467 - val_loss: 13.8949\nEpoch 63/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1086 - val_loss: 13.8506\nEpoch 64/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0710 - val_loss: 13.8065\nEpoch 65/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0336 - val_loss: 13.7637\nEpoch 66/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.9956 - val_loss: 13.7212\nEpoch 67/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.9589 - val_loss: 13.6790\nEpoch 68/2000\n6/6 [==============================] - 0s 2ms/step - loss: 4.9218 - val_loss: 13.6360\nEpoch 69/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8858 - val_loss: 13.5936\nEpoch 70/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8495 - val_loss: 13.5514\nEpoch 71/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8133 - val_loss: 13.5099\nEpoch 72/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7778 - val_loss: 13.4699\nEpoch 73/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7419 - val_loss: 13.4284\nEpoch 74/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7071 - val_loss: 13.3875\nEpoch 75/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6717 - val_loss: 13.3462\nEpoch 76/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6372 - val_loss: 13.3066\nEpoch 77/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6033 - val_loss: 13.2665\nEpoch 78/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5686 - val_loss: 13.2260\nEpoch 79/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5349 - val_loss: 13.1872\nEpoch 80/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5012 - val_loss: 13.1474\nEpoch 81/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4672 - val_loss: 13.1093\nEpoch 82/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4336 - val_loss: 13.0714\nEpoch 83/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4009 - val_loss: 13.0337\nEpoch 84/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.3675 - val_loss: 12.9957\nEpoch 85/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3348 - val_loss: 12.9574\nEpoch 86/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3025 - val_loss: 12.9182\nEpoch 87/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2700 - val_loss: 12.8800\nEpoch 88/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2382 - val_loss: 12.8425\nEpoch 89/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2063 - val_loss: 12.8047\nEpoch 90/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1745 - val_loss: 12.7663\nEpoch 91/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1428 - val_loss: 12.7295\nEpoch 92/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1119 - val_loss: 12.6919\nEpoch 93/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0804 - val_loss: 12.6557\nEpoch 94/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0493 - val_loss: 12.6196\nEpoch 95/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0188 - val_loss: 12.5834\nEpoch 96/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9878 - val_loss: 12.5464\nEpoch 97/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9580 - val_loss: 12.5099\nEpoch 98/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9274 - val_loss: 12.4748\nEpoch 99/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8977 - val_loss: 12.4391\nEpoch 100/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.8678 - val_loss: 12.4017\nEpoch 101/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8381 - val_loss: 12.3662\nEpoch 102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8084 - val_loss: 12.3317\nEpoch 103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7793 - val_loss: 12.2967\nEpoch 104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7504 - val_loss: 12.2616\nEpoch 105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7214 - val_loss: 12.2255\nEpoch 106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6928 - val_loss: 12.1913\nEpoch 107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6639 - val_loss: 12.1572\nEpoch 108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6359 - val_loss: 12.1229\nEpoch 109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6079 - val_loss: 12.0890\nEpoch 110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5798 - val_loss: 12.0553\nEpoch 111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5520 - val_loss: 12.0209\nEpoch 112/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5245 - val_loss: 11.9872\nEpoch 113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4968 - val_loss: 11.9544\nEpoch 114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4703 - val_loss: 11.9212\nEpoch 115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4430 - val_loss: 11.8894\nEpoch 116/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.4164 - val_loss: 11.8567\nEpoch 117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3899 - val_loss: 11.8250\nEpoch 118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3633 - val_loss: 11.7929\nEpoch 119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3368 - val_loss: 11.7604\nEpoch 120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3108 - val_loss: 11.7281\nEpoch 121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2852 - val_loss: 11.6949\nEpoch 122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2593 - val_loss: 11.6615\nEpoch 123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2339 - val_loss: 11.6295\nEpoch 124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2084 - val_loss: 11.5968\nEpoch 125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1835 - val_loss: 11.5650\nEpoch 126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1585 - val_loss: 11.5338\nEpoch 127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1335 - val_loss: 11.5018\nEpoch 128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1093 - val_loss: 11.4705\nEpoch 129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0847 - val_loss: 11.4392\nEpoch 130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0604 - val_loss: 11.4076\nEpoch 131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0360 - val_loss: 11.3776\nEpoch 132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0123 - val_loss: 11.3460\nEpoch 133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9884 - val_loss: 11.3158\nEpoch 134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9648 - val_loss: 11.2869\nEpoch 135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9414 - val_loss: 11.2564\nEpoch 136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9179 - val_loss: 11.2282\nEpoch 137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8950 - val_loss: 11.1989\nEpoch 138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8720 - val_loss: 11.1695\nEpoch 139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8492 - val_loss: 11.1400\nEpoch 140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8265 - val_loss: 11.1121\nEpoch 141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8047 - val_loss: 11.0833\nEpoch 142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7819 - val_loss: 11.0539\nEpoch 143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7598 - val_loss: 11.0244\nEpoch 144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7381 - val_loss: 10.9954\nEpoch 145/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.7157 - val_loss: 10.9659\nEpoch 146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6944 - val_loss: 10.9374\nEpoch 147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6728 - val_loss: 10.9100\nEpoch 148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6517 - val_loss: 10.8820\nEpoch 149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6303 - val_loss: 10.8545\nEpoch 150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6098 - val_loss: 10.8258\nEpoch 151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5886 - val_loss: 10.7982\nEpoch 152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5681 - val_loss: 10.7702\nEpoch 153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5477 - val_loss: 10.7426\nEpoch 154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5268 - val_loss: 10.7159\nEpoch 155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5070 - val_loss: 10.6879\nEpoch 156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4865 - val_loss: 10.6615\nEpoch 157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4665 - val_loss: 10.6347\nEpoch 158/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.4471 - val_loss: 10.6074\nEpoch 159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4269 - val_loss: 10.5801\nEpoch 160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4074 - val_loss: 10.5535\nEpoch 161/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.3880 - val_loss: 10.5256\nEpoch 162/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.3687 - val_loss: 10.4981\nEpoch 163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3497 - val_loss: 10.4711\nEpoch 164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3308 - val_loss: 10.4447\nEpoch 165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3117 - val_loss: 10.4198\nEpoch 166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2933 - val_loss: 10.3949\nEpoch 167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2749 - val_loss: 10.3696\nEpoch 168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2567 - val_loss: 10.3437\nEpoch 169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2385 - val_loss: 10.3177\nEpoch 170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2204 - val_loss: 10.2909\nEpoch 171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2022 - val_loss: 10.2653\nEpoch 172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1842 - val_loss: 10.2391\nEpoch 173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1669 - val_loss: 10.2139\nEpoch 174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1493 - val_loss: 10.1893\nEpoch 175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1320 - val_loss: 10.1643\nEpoch 176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1146 - val_loss: 10.1393\nEpoch 177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0978 - val_loss: 10.1148\nEpoch 178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0802 - val_loss: 10.0900\nEpoch 179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0637 - val_loss: 10.0662\nEpoch 180/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.0470 - val_loss: 10.0430\nEpoch 181/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.0304 - val_loss: 10.0192\nEpoch 182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0140 - val_loss: 9.9947\nEpoch 183/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.9974 - val_loss: 9.9698\nEpoch 184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9812 - val_loss: 9.9468\nEpoch 185/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.9653 - val_loss: 9.9227\nEpoch 186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9492 - val_loss: 9.8996\nEpoch 187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9333 - val_loss: 9.8767\nEpoch 188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9175 - val_loss: 9.8542\nEpoch 189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9018 - val_loss: 9.8322\nEpoch 190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8864 - val_loss: 9.8097\nEpoch 191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8710 - val_loss: 9.7886\nEpoch 192/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.8560 - val_loss: 9.7661\nEpoch 193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8408 - val_loss: 9.7440\nEpoch 194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8261 - val_loss: 9.7216\nEpoch 195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8109 - val_loss: 9.6998\nEpoch 196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7963 - val_loss: 9.6770\nEpoch 197/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.7817 - val_loss: 9.6547\nEpoch 198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7671 - val_loss: 9.6326\nEpoch 199/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7529 - val_loss: 9.6101\nEpoch 200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7385 - val_loss: 9.5889\nEpoch 201/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7247 - val_loss: 9.5673\nEpoch 202/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.7103 - val_loss: 9.5462\nEpoch 203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6965 - val_loss: 9.5249\nEpoch 204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6832 - val_loss: 9.5034\nEpoch 205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6688 - val_loss: 9.4818\nEpoch 206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6554 - val_loss: 9.4604\nEpoch 207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6420 - val_loss: 9.4388\nEpoch 208/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.6284 - val_loss: 9.4175\nEpoch 209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6153 - val_loss: 9.3953\nEpoch 210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6020 - val_loss: 9.3753\nEpoch 211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5890 - val_loss: 9.3550\nEpoch 212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5760 - val_loss: 9.3347\nEpoch 213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5630 - val_loss: 9.3146\nEpoch 214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5504 - val_loss: 9.2938\nEpoch 215/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.5378 - val_loss: 9.2736\nEpoch 216/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.5254 - val_loss: 9.2528\nEpoch 217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5129 - val_loss: 9.2324\nEpoch 218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5007 - val_loss: 9.2127\nEpoch 219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4883 - val_loss: 9.1932\nEpoch 220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4761 - val_loss: 9.1740\nEpoch 221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4642 - val_loss: 9.1550\nEpoch 222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4522 - val_loss: 9.1361\nEpoch 223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4407 - val_loss: 9.1162\nEpoch 224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4289 - val_loss: 9.0964\nEpoch 225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4172 - val_loss: 9.0773\nEpoch 226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4058 - val_loss: 9.0589\nEpoch 227/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.3944 - val_loss: 9.0388\nEpoch 228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3833 - val_loss: 9.0196\nEpoch 229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3721 - val_loss: 9.0016\nEpoch 230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3607 - val_loss: 8.9826\nEpoch 231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3498 - val_loss: 8.9648\nEpoch 232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3390 - val_loss: 8.9451\nEpoch 233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3282 - val_loss: 8.9263\nEpoch 234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3173 - val_loss: 8.9072\nEpoch 235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3065 - val_loss: 8.8890\nEpoch 236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2960 - val_loss: 8.8710\nEpoch 237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2857 - val_loss: 8.8527\nEpoch 238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2753 - val_loss: 8.8343\nEpoch 239/2000\n6/6 [==============================] - 0s 2ms/step - loss: 1.2650 - val_loss: 8.8167\nEpoch 240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2549 - val_loss: 8.7986\nEpoch 241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2446 - val_loss: 8.7807\nEpoch 242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2348 - val_loss: 8.7634\nEpoch 243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2248 - val_loss: 8.7452\nEpoch 244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2149 - val_loss: 8.7272\nEpoch 245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2051 - val_loss: 8.7098\nEpoch 246/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1953 - val_loss: 8.6919\nEpoch 247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1859 - val_loss: 8.6734\nEpoch 248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1761 - val_loss: 8.6563\nEpoch 249/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1668 - val_loss: 8.6384\nEpoch 250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1575 - val_loss: 8.6211\nEpoch 251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1482 - val_loss: 8.6034\nEpoch 252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1391 - val_loss: 8.5858\nEpoch 253/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1299 - val_loss: 8.5696\nEpoch 254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1210 - val_loss: 8.5526\nEpoch 255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1121 - val_loss: 8.5356\nEpoch 256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1033 - val_loss: 8.5179\nEpoch 257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0945 - val_loss: 8.5020\nEpoch 258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0857 - val_loss: 8.4856\nEpoch 259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0772 - val_loss: 8.4692\nEpoch 260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0686 - val_loss: 8.4531\nEpoch 261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0601 - val_loss: 8.4360\nEpoch 262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0518 - val_loss: 8.4194\nEpoch 263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0433 - val_loss: 8.4037\nEpoch 264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0351 - val_loss: 8.3865\nEpoch 265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0271 - val_loss: 8.3690\nEpoch 266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0190 - val_loss: 8.3524\nEpoch 267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0108 - val_loss: 8.3361\nEpoch 268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0029 - val_loss: 8.3203\nEpoch 269/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9951 - val_loss: 8.3041\nEpoch 270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9873 - val_loss: 8.2878\nEpoch 271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9796 - val_loss: 8.2707\nEpoch 272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9719 - val_loss: 8.2545\nEpoch 273/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9642 - val_loss: 8.2390\nEpoch 274/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9568 - val_loss: 8.2218\nEpoch 275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9494 - val_loss: 8.2058\nEpoch 276/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9420 - val_loss: 8.1896\nEpoch 277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9347 - val_loss: 8.1732\nEpoch 278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9275 - val_loss: 8.1561\nEpoch 279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9202 - val_loss: 8.1401\nEpoch 280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9131 - val_loss: 8.1239\nEpoch 281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9060 - val_loss: 8.1088\nEpoch 282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8990 - val_loss: 8.0931\nEpoch 283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8922 - val_loss: 8.0769\nEpoch 284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8853 - val_loss: 8.0603\nEpoch 285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8784 - val_loss: 8.0448\nEpoch 286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8718 - val_loss: 8.0294\nEpoch 287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8652 - val_loss: 8.0138\nEpoch 288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8584 - val_loss: 7.9985\nEpoch 289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8520 - val_loss: 7.9826\nEpoch 290/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8454 - val_loss: 7.9671\nEpoch 291/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8390 - val_loss: 7.9519\nEpoch 292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8326 - val_loss: 7.9362\nEpoch 293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8263 - val_loss: 7.9212\nEpoch 294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8201 - val_loss: 7.9061\nEpoch 295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8138 - val_loss: 7.8905\nEpoch 296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8079 - val_loss: 7.8750\nEpoch 297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8017 - val_loss: 7.8607\nEpoch 298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7958 - val_loss: 7.8459\nEpoch 299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7899 - val_loss: 7.8306\nEpoch 300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7840 - val_loss: 7.8159\nEpoch 301/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7783 - val_loss: 7.8007\nEpoch 302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7725 - val_loss: 7.7864\nEpoch 303/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.7669 - val_loss: 7.7719\nEpoch 304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7611 - val_loss: 7.7579\nEpoch 305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7556 - val_loss: 7.7428\nEpoch 306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7500 - val_loss: 7.7285\nEpoch 307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7445 - val_loss: 7.7141\nEpoch 308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7391 - val_loss: 7.6997\nEpoch 309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7337 - val_loss: 7.6853\nEpoch 310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7283 - val_loss: 7.6718\nEpoch 311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7231 - val_loss: 7.6580\nEpoch 312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7179 - val_loss: 7.6447\nEpoch 313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7128 - val_loss: 7.6302\nEpoch 314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7077 - val_loss: 7.6174\nEpoch 315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7026 - val_loss: 7.6038\nEpoch 316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6975 - val_loss: 7.5901\nEpoch 317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6925 - val_loss: 7.5768\nEpoch 318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6876 - val_loss: 7.5627\nEpoch 319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6827 - val_loss: 7.5483\nEpoch 320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6779 - val_loss: 7.5341\nEpoch 321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6731 - val_loss: 7.5198\nEpoch 322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6683 - val_loss: 7.5051\nEpoch 323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6636 - val_loss: 7.4905\nEpoch 324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6588 - val_loss: 7.4768\nEpoch 325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6543 - val_loss: 7.4629\nEpoch 326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6497 - val_loss: 7.4488\nEpoch 327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6452 - val_loss: 7.4349\nEpoch 328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6407 - val_loss: 7.4209\nEpoch 329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6361 - val_loss: 7.4075\nEpoch 330/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.6318 - val_loss: 7.3936\nEpoch 331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6273 - val_loss: 7.3804\nEpoch 332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6230 - val_loss: 7.3673\nEpoch 333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6187 - val_loss: 7.3541\nEpoch 334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6146 - val_loss: 7.3411\nEpoch 335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6104 - val_loss: 7.3282\nEpoch 336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6063 - val_loss: 7.3151\nEpoch 337/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.6021 - val_loss: 7.3019\nEpoch 338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5982 - val_loss: 7.2878\nEpoch 339/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.5941 - val_loss: 7.2744\nEpoch 340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5901 - val_loss: 7.2609\nEpoch 341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5862 - val_loss: 7.2473\nEpoch 342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5822 - val_loss: 7.2341\nEpoch 343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5785 - val_loss: 7.2205\nEpoch 344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5746 - val_loss: 7.2082\nEpoch 345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5709 - val_loss: 7.1954\nEpoch 346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5672 - val_loss: 7.1823\nEpoch 347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5635 - val_loss: 7.1690\nEpoch 348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5599 - val_loss: 7.1551\nEpoch 349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5561 - val_loss: 7.1426\nEpoch 350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5527 - val_loss: 7.1290\nEpoch 351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5491 - val_loss: 7.1156\nEpoch 352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5456 - val_loss: 7.1031\nEpoch 353/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5421 - val_loss: 7.0905\nEpoch 354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5387 - val_loss: 7.0773\nEpoch 355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5352 - val_loss: 7.0639\nEpoch 356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5319 - val_loss: 7.0513\nEpoch 357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5285 - val_loss: 7.0387\nEpoch 358/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5251 - val_loss: 7.0263\nEpoch 359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5218 - val_loss: 7.0130\nEpoch 360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5186 - val_loss: 7.0000\nEpoch 361/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5154 - val_loss: 6.9864\nEpoch 362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5121 - val_loss: 6.9732\nEpoch 363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5090 - val_loss: 6.9603\nEpoch 364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5057 - val_loss: 6.9482\nEpoch 365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5027 - val_loss: 6.9357\nEpoch 366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4997 - val_loss: 6.9228\nEpoch 367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4966 - val_loss: 6.9100\nEpoch 368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4936 - val_loss: 6.8972\nEpoch 369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4906 - val_loss: 6.8840\nEpoch 370/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4876 - val_loss: 6.8711\nEpoch 371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4847 - val_loss: 6.8580\nEpoch 372/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4818 - val_loss: 6.8453\nEpoch 373/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4790 - val_loss: 6.8319\nEpoch 374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4760 - val_loss: 6.8195\nEpoch 375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4732 - val_loss: 6.8068\nEpoch 376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4704 - val_loss: 6.7937\nEpoch 377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4676 - val_loss: 6.7809\nEpoch 378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4649 - val_loss: 6.7677\nEpoch 379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4622 - val_loss: 6.7545\nEpoch 380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4595 - val_loss: 6.7420\nEpoch 381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4569 - val_loss: 6.7292\nEpoch 382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4543 - val_loss: 6.7164\nEpoch 383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4517 - val_loss: 6.7039\nEpoch 384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4492 - val_loss: 6.6910\nEpoch 385/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4467 - val_loss: 6.6781\nEpoch 386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4441 - val_loss: 6.6659\nEpoch 387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4416 - val_loss: 6.6535\nEpoch 388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4392 - val_loss: 6.6404\nEpoch 389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4367 - val_loss: 6.6274\nEpoch 390/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4342 - val_loss: 6.6145\nEpoch 391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4318 - val_loss: 6.6011\nEpoch 392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4295 - val_loss: 6.5875\nEpoch 393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4270 - val_loss: 6.5747\nEpoch 394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4248 - val_loss: 6.5615\nEpoch 395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4224 - val_loss: 6.5482\nEpoch 396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4202 - val_loss: 6.5350\nEpoch 397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4178 - val_loss: 6.5221\nEpoch 398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4156 - val_loss: 6.5091\nEpoch 399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4134 - val_loss: 6.4958\nEpoch 400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4112 - val_loss: 6.4832\nEpoch 401/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4090 - val_loss: 6.4701\nEpoch 402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4068 - val_loss: 6.4580\nEpoch 403/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4047 - val_loss: 6.4458\nEpoch 404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4026 - val_loss: 6.4328\nEpoch 405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4005 - val_loss: 6.4201\nEpoch 406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3984 - val_loss: 6.4078\nEpoch 407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3964 - val_loss: 6.3951\nEpoch 408/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3943 - val_loss: 6.3820\nEpoch 409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3923 - val_loss: 6.3687\nEpoch 410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3904 - val_loss: 6.3556\nEpoch 411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3884 - val_loss: 6.3432\nEpoch 412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3864 - val_loss: 6.3316\nEpoch 413/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3845 - val_loss: 6.3191\nEpoch 414/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3826 - val_loss: 6.3064\nEpoch 415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3807 - val_loss: 6.2937\nEpoch 416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3788 - val_loss: 6.2801\nEpoch 417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3770 - val_loss: 6.2673\nEpoch 418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3751 - val_loss: 6.2552\nEpoch 419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3734 - val_loss: 6.2428\nEpoch 420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3716 - val_loss: 6.2297\nEpoch 421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3697 - val_loss: 6.2176\nEpoch 422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3680 - val_loss: 6.2058\nEpoch 423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3662 - val_loss: 6.1930\nEpoch 424/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3645 - val_loss: 6.1798\nEpoch 425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3628 - val_loss: 6.1656\nEpoch 426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3611 - val_loss: 6.1524\nEpoch 427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3593 - val_loss: 6.1409\nEpoch 428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3577 - val_loss: 6.1283\nEpoch 429/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3560 - val_loss: 6.1157\nEpoch 430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3543 - val_loss: 6.1029\nEpoch 431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3527 - val_loss: 6.0910\nEpoch 432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3511 - val_loss: 6.0784\nEpoch 433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3495 - val_loss: 6.0659\nEpoch 434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3479 - val_loss: 6.0532\nEpoch 435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3463 - val_loss: 6.0409\nEpoch 436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3449 - val_loss: 6.0271\nEpoch 437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3433 - val_loss: 6.0139\nEpoch 438/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3417 - val_loss: 6.0021\nEpoch 439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3403 - val_loss: 5.9891\nEpoch 440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3387 - val_loss: 5.9761\nEpoch 441/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3373 - val_loss: 5.9631\nEpoch 442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3358 - val_loss: 5.9510\nEpoch 443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3343 - val_loss: 5.9384\nEpoch 444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3329 - val_loss: 5.9251\nEpoch 445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3315 - val_loss: 5.9124\nEpoch 446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3300 - val_loss: 5.9006\nEpoch 447/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3286 - val_loss: 5.8884\nEpoch 448/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3273 - val_loss: 5.8753\nEpoch 449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3259 - val_loss: 5.8638\nEpoch 450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3246 - val_loss: 5.8502\nEpoch 451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3232 - val_loss: 5.8370\nEpoch 452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3219 - val_loss: 5.8248\nEpoch 453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3205 - val_loss: 5.8125\nEpoch 454/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.3192 - val_loss: 5.8001\nEpoch 455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3179 - val_loss: 5.7872\nEpoch 456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3166 - val_loss: 5.7746\nEpoch 457/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3153 - val_loss: 5.7613\nEpoch 458/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3141 - val_loss: 5.7485\nEpoch 459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3128 - val_loss: 5.7363\nEpoch 460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3115 - val_loss: 5.7242\nEpoch 461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3102 - val_loss: 5.7113\nEpoch 462/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3091 - val_loss: 5.6971\nEpoch 463/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3078 - val_loss: 5.6843\nEpoch 464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3066 - val_loss: 5.6719\nEpoch 465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3054 - val_loss: 5.6590\nEpoch 466/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3042 - val_loss: 5.6457\nEpoch 467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3030 - val_loss: 5.6332\nEpoch 468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3018 - val_loss: 5.6205\nEpoch 469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3007 - val_loss: 5.6081\nEpoch 470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2995 - val_loss: 5.5948\nEpoch 471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2984 - val_loss: 5.5822\nEpoch 472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2973 - val_loss: 5.5698\nEpoch 473/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2962 - val_loss: 5.5573\nEpoch 474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2950 - val_loss: 5.5454\nEpoch 475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2939 - val_loss: 5.5330\nEpoch 476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2929 - val_loss: 5.5206\nEpoch 477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2918 - val_loss: 5.5077\nEpoch 478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2907 - val_loss: 5.4955\nEpoch 479/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2897 - val_loss: 5.4826\nEpoch 480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2886 - val_loss: 5.4703\nEpoch 481/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2875 - val_loss: 5.4572\nEpoch 482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2865 - val_loss: 5.4437\nEpoch 483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2854 - val_loss: 5.4312\nEpoch 484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2844 - val_loss: 5.4180\nEpoch 485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2833 - val_loss: 5.4055\nEpoch 486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2823 - val_loss: 5.3928\nEpoch 487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2812 - val_loss: 5.3801\nEpoch 488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2803 - val_loss: 5.3680\nEpoch 489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2793 - val_loss: 5.3553\nEpoch 490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2784 - val_loss: 5.3432\nEpoch 491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2774 - val_loss: 5.3311\nEpoch 492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2764 - val_loss: 5.3187\nEpoch 493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2755 - val_loss: 5.3060\nEpoch 494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2745 - val_loss: 5.2937\nEpoch 495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2735 - val_loss: 5.2808\nEpoch 496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2726 - val_loss: 5.2676\nEpoch 497/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2717 - val_loss: 5.2543\nEpoch 498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2707 - val_loss: 5.2419\nEpoch 499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2697 - val_loss: 5.2294\nEpoch 500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2688 - val_loss: 5.2162\nEpoch 501/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2679 - val_loss: 5.2035\nEpoch 502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2670 - val_loss: 5.1906\nEpoch 503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2661 - val_loss: 5.1787\nEpoch 504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2653 - val_loss: 5.1656\nEpoch 505/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2643 - val_loss: 5.1536\nEpoch 506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2635 - val_loss: 5.1414\nEpoch 507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2626 - val_loss: 5.1286\nEpoch 508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2618 - val_loss: 5.1159\nEpoch 509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2609 - val_loss: 5.1031\nEpoch 510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2600 - val_loss: 5.0902\nEpoch 511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2592 - val_loss: 5.0771\nEpoch 512/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2583 - val_loss: 5.0634\nEpoch 513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2574 - val_loss: 5.0508\nEpoch 514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2566 - val_loss: 5.0378\nEpoch 515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2558 - val_loss: 5.0251\nEpoch 516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2549 - val_loss: 5.0124\nEpoch 517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2541 - val_loss: 5.0009\nEpoch 518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2533 - val_loss: 4.9884\nEpoch 519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2524 - val_loss: 4.9745\nEpoch 520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2516 - val_loss: 4.9611\nEpoch 521/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2508 - val_loss: 4.9477\nEpoch 522/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2500 - val_loss: 4.9343\nEpoch 523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2492 - val_loss: 4.9215\nEpoch 524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2483 - val_loss: 4.9094\nEpoch 525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2476 - val_loss: 4.8964\nEpoch 526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2468 - val_loss: 4.8839\nEpoch 527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2460 - val_loss: 4.8708\nEpoch 528/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2452 - val_loss: 4.8576\nEpoch 529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2444 - val_loss: 4.8448\nEpoch 530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2437 - val_loss: 4.8320\nEpoch 531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2429 - val_loss: 4.8194\nEpoch 532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2421 - val_loss: 4.8058\nEpoch 533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2413 - val_loss: 4.7929\nEpoch 534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2406 - val_loss: 4.7801\nEpoch 535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2398 - val_loss: 4.7671\nEpoch 536/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2390 - val_loss: 4.7535\nEpoch 537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2382 - val_loss: 4.7403\nEpoch 538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2375 - val_loss: 4.7273\nEpoch 539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2367 - val_loss: 4.7137\nEpoch 540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2360 - val_loss: 4.7006\nEpoch 541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2352 - val_loss: 4.6876\nEpoch 542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2345 - val_loss: 4.6743\nEpoch 543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2337 - val_loss: 4.6610\nEpoch 544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2330 - val_loss: 4.6474\nEpoch 545/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2323 - val_loss: 4.6331\nEpoch 546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2316 - val_loss: 4.6199\nEpoch 547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2308 - val_loss: 4.6071\nEpoch 548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2301 - val_loss: 4.5937\nEpoch 549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2294 - val_loss: 4.5801\nEpoch 550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2287 - val_loss: 4.5669\nEpoch 551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2280 - val_loss: 4.5541\nEpoch 552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2273 - val_loss: 4.5418\nEpoch 553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2266 - val_loss: 4.5291\nEpoch 554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2259 - val_loss: 4.5153\nEpoch 555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2252 - val_loss: 4.5024\nEpoch 556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2245 - val_loss: 4.4886\nEpoch 557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2238 - val_loss: 4.4760\nEpoch 558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2231 - val_loss: 4.4632\nEpoch 559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2225 - val_loss: 4.4498\nEpoch 560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2218 - val_loss: 4.4371\nEpoch 561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2211 - val_loss: 4.4243\nEpoch 562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2205 - val_loss: 4.4102\nEpoch 563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2198 - val_loss: 4.3965\nEpoch 564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2191 - val_loss: 4.3836\nEpoch 565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2184 - val_loss: 4.3712\nEpoch 566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2178 - val_loss: 4.3576\nEpoch 567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2171 - val_loss: 4.3441\nEpoch 568/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2164 - val_loss: 4.3317\nEpoch 569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2158 - val_loss: 4.3184\nEpoch 570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2151 - val_loss: 4.3051\nEpoch 571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2145 - val_loss: 4.2917\nEpoch 572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2138 - val_loss: 4.2786\nEpoch 573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2132 - val_loss: 4.2653\nEpoch 574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2125 - val_loss: 4.2523\nEpoch 575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2119 - val_loss: 4.2398\nEpoch 576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2112 - val_loss: 4.2273\nEpoch 577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2107 - val_loss: 4.2141\nEpoch 578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2100 - val_loss: 4.2012\nEpoch 579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2094 - val_loss: 4.1880\nEpoch 580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2087 - val_loss: 4.1755\nEpoch 581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2081 - val_loss: 4.1625\nEpoch 582/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2075 - val_loss: 4.1492\nEpoch 583/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2068 - val_loss: 4.1359\nEpoch 584/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2062 - val_loss: 4.1222\nEpoch 585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2055 - val_loss: 4.1091\nEpoch 586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2050 - val_loss: 4.0958\nEpoch 587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2043 - val_loss: 4.0840\nEpoch 588/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2037 - val_loss: 4.0713\nEpoch 589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2031 - val_loss: 4.0589\nEpoch 590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2025 - val_loss: 4.0462\nEpoch 591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2018 - val_loss: 4.0341\nEpoch 592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2013 - val_loss: 4.0212\nEpoch 593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2007 - val_loss: 4.0088\nEpoch 594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2001 - val_loss: 3.9966\nEpoch 595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1995 - val_loss: 3.9836\nEpoch 596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1989 - val_loss: 3.9706\nEpoch 597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1982 - val_loss: 3.9582\nEpoch 598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1977 - val_loss: 3.9448\nEpoch 599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1971 - val_loss: 3.9314\nEpoch 600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1964 - val_loss: 3.9187\nEpoch 601/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1959 - val_loss: 3.9060\nEpoch 602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1953 - val_loss: 3.8930\nEpoch 603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1947 - val_loss: 3.8804\nEpoch 604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1941 - val_loss: 3.8667\nEpoch 605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1935 - val_loss: 3.8544\nEpoch 606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1929 - val_loss: 3.8421\nEpoch 607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1924 - val_loss: 3.8289\nEpoch 608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1917 - val_loss: 3.8162\nEpoch 609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1912 - val_loss: 3.8026\nEpoch 610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1906 - val_loss: 3.7897\nEpoch 611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1900 - val_loss: 3.7766\nEpoch 612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1894 - val_loss: 3.7639\nEpoch 613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1888 - val_loss: 3.7507\nEpoch 614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1882 - val_loss: 3.7374\nEpoch 615/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1877 - val_loss: 3.7244\nEpoch 616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1871 - val_loss: 3.7116\nEpoch 617/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1865 - val_loss: 3.6990\nEpoch 618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1859 - val_loss: 3.6861\nEpoch 619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1854 - val_loss: 3.6728\nEpoch 620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1847 - val_loss: 3.6605\nEpoch 621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1842 - val_loss: 3.6471\nEpoch 622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1836 - val_loss: 3.6345\nEpoch 623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1830 - val_loss: 3.6220\nEpoch 624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1825 - val_loss: 3.6088\nEpoch 625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1819 - val_loss: 3.5953\nEpoch 626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1813 - val_loss: 3.5819\nEpoch 627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1807 - val_loss: 3.5693\nEpoch 628/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1802 - val_loss: 3.5565\nEpoch 629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1796 - val_loss: 3.5439\nEpoch 630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1791 - val_loss: 3.5319\nEpoch 631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1785 - val_loss: 3.5202\nEpoch 632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1780 - val_loss: 3.5072\nEpoch 633/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1774 - val_loss: 3.4947\nEpoch 634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1768 - val_loss: 3.4815\nEpoch 635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1763 - val_loss: 3.4685\nEpoch 636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1757 - val_loss: 3.4563\nEpoch 637/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1751 - val_loss: 3.4440\nEpoch 638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1746 - val_loss: 3.4307\nEpoch 639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1741 - val_loss: 3.4180\nEpoch 640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1735 - val_loss: 3.4056\nEpoch 641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1729 - val_loss: 3.3936\nEpoch 642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1724 - val_loss: 3.3813\nEpoch 643/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1719 - val_loss: 3.3682\nEpoch 644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1713 - val_loss: 3.3556\nEpoch 645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1708 - val_loss: 3.3429\nEpoch 646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1702 - val_loss: 3.3306\nEpoch 647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1697 - val_loss: 3.3180\nEpoch 648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1691 - val_loss: 3.3060\nEpoch 649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1686 - val_loss: 3.2933\nEpoch 650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1681 - val_loss: 3.2807\nEpoch 651/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1675 - val_loss: 3.2675\nEpoch 652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1670 - val_loss: 3.2545\nEpoch 653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1664 - val_loss: 3.2423\nEpoch 654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1658 - val_loss: 3.2296\nEpoch 655/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1653 - val_loss: 3.2170\nEpoch 656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1648 - val_loss: 3.2043\nEpoch 657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1642 - val_loss: 3.1921\nEpoch 658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1637 - val_loss: 3.1786\nEpoch 659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1631 - val_loss: 3.1659\nEpoch 660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1626 - val_loss: 3.1534\nEpoch 661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1621 - val_loss: 3.1400\nEpoch 662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1615 - val_loss: 3.1277\nEpoch 663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1610 - val_loss: 3.1148\nEpoch 664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1604 - val_loss: 3.1015\nEpoch 665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1599 - val_loss: 3.0887\nEpoch 666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 3.0764\nEpoch 667/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1588 - val_loss: 3.0640\nEpoch 668/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1583 - val_loss: 3.0516\nEpoch 669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1578 - val_loss: 3.0398\nEpoch 670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1572 - val_loss: 3.0274\nEpoch 671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1567 - val_loss: 3.0159\nEpoch 672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1562 - val_loss: 3.0039\nEpoch 673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1557 - val_loss: 2.9921\nEpoch 674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1551 - val_loss: 2.9799\nEpoch 675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1546 - val_loss: 2.9669\nEpoch 676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1540 - val_loss: 2.9550\nEpoch 677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1535 - val_loss: 2.9428\nEpoch 678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1530 - val_loss: 2.9305\nEpoch 679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1525 - val_loss: 2.9178\nEpoch 680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1519 - val_loss: 2.9059\nEpoch 681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1514 - val_loss: 2.8935\nEpoch 682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1509 - val_loss: 2.8816\nEpoch 683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1504 - val_loss: 2.8687\nEpoch 684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1499 - val_loss: 2.8565\nEpoch 685/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1494 - val_loss: 2.8445\nEpoch 686/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1489 - val_loss: 2.8318\nEpoch 687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1484 - val_loss: 2.8200\nEpoch 688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1479 - val_loss: 2.8081\nEpoch 689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1474 - val_loss: 2.7957\nEpoch 690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1468 - val_loss: 2.7837\nEpoch 691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1463 - val_loss: 2.7715\nEpoch 692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1458 - val_loss: 2.7596\nEpoch 693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1453 - val_loss: 2.7476\nEpoch 694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1448 - val_loss: 2.7354\nEpoch 695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1443 - val_loss: 2.7233\nEpoch 696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1438 - val_loss: 2.7113\nEpoch 697/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1433 - val_loss: 2.6996\nEpoch 698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1428 - val_loss: 2.6874\nEpoch 699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1423 - val_loss: 2.6760\nEpoch 700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1418 - val_loss: 2.6651\nEpoch 701/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1413 - val_loss: 2.6537\nEpoch 702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1408 - val_loss: 2.6435\nEpoch 703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1403 - val_loss: 2.6322\nEpoch 704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1398 - val_loss: 2.6204\nEpoch 705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1394 - val_loss: 2.6084\nEpoch 706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1389 - val_loss: 2.5964\nEpoch 707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1384 - val_loss: 2.5844\nEpoch 708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1378 - val_loss: 2.5730\nEpoch 709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1374 - val_loss: 2.5604\nEpoch 710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1369 - val_loss: 2.5488\nEpoch 711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1364 - val_loss: 2.5374\nEpoch 712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1359 - val_loss: 2.5266\nEpoch 713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1354 - val_loss: 2.5150\nEpoch 714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1350 - val_loss: 2.5034\nEpoch 715/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1345 - val_loss: 2.4924\nEpoch 716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1340 - val_loss: 2.4817\nEpoch 717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1335 - val_loss: 2.4699\nEpoch 718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1331 - val_loss: 2.4581\nEpoch 719/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1326 - val_loss: 2.4463\nEpoch 720/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1321 - val_loss: 2.4342\nEpoch 721/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1316 - val_loss: 2.4222\nEpoch 722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1311 - val_loss: 2.4105\nEpoch 723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1306 - val_loss: 2.3996\nEpoch 724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1302 - val_loss: 2.3888\nEpoch 725/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1297 - val_loss: 2.3774\nEpoch 726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1292 - val_loss: 2.3659\nEpoch 727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1287 - val_loss: 2.3542\nEpoch 728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1283 - val_loss: 2.3425\nEpoch 729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1278 - val_loss: 2.3306\nEpoch 730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1273 - val_loss: 2.3193\nEpoch 731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1269 - val_loss: 2.3079\nEpoch 732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1264 - val_loss: 2.2972\nEpoch 733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1260 - val_loss: 2.2863\nEpoch 734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1255 - val_loss: 2.2755\nEpoch 735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1250 - val_loss: 2.2650\nEpoch 736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1246 - val_loss: 2.2541\nEpoch 737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 2.2434\nEpoch 738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 2.2326\nEpoch 739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1232 - val_loss: 2.2211\nEpoch 740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1227 - val_loss: 2.2096\nEpoch 741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1223 - val_loss: 2.1982\nEpoch 742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1218 - val_loss: 2.1873\nEpoch 743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1213 - val_loss: 2.1772\nEpoch 744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1209 - val_loss: 2.1657\nEpoch 745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1204 - val_loss: 2.1546\nEpoch 746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1200 - val_loss: 2.1438\nEpoch 747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1195 - val_loss: 2.1334\nEpoch 748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1191 - val_loss: 2.1226\nEpoch 749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1186 - val_loss: 2.1122\nEpoch 750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1181 - val_loss: 2.1021\nEpoch 751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1177 - val_loss: 2.0914\nEpoch 752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1173 - val_loss: 2.0800\nEpoch 753/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1168 - val_loss: 2.0693\nEpoch 754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1164 - val_loss: 2.0587\nEpoch 755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1159 - val_loss: 2.0479\nEpoch 756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1154 - val_loss: 2.0373\nEpoch 757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1150 - val_loss: 2.0264\nEpoch 758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1146 - val_loss: 2.0153\nEpoch 759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1141 - val_loss: 2.0050\nEpoch 760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1137 - val_loss: 1.9949\nEpoch 761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1133 - val_loss: 1.9837\nEpoch 762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1128 - val_loss: 1.9731\nEpoch 763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1124 - val_loss: 1.9618\nEpoch 764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1119 - val_loss: 1.9511\nEpoch 765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1115 - val_loss: 1.9405\nEpoch 766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1111 - val_loss: 1.9295\nEpoch 767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1106 - val_loss: 1.9198\nEpoch 768/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1102 - val_loss: 1.9094\nEpoch 769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1098 - val_loss: 1.8989\nEpoch 770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1094 - val_loss: 1.8886\nEpoch 771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1089 - val_loss: 1.8785\nEpoch 772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1085 - val_loss: 1.8684\nEpoch 773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1081 - val_loss: 1.8580\nEpoch 774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1077 - val_loss: 1.8486\nEpoch 775/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1073 - val_loss: 1.8389\nEpoch 776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1069 - val_loss: 1.8292\nEpoch 777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 1.8192\nEpoch 778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1061 - val_loss: 1.8095\nEpoch 779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1057 - val_loss: 1.7997\nEpoch 780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1052 - val_loss: 1.7901\nEpoch 781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1049 - val_loss: 1.7800\nEpoch 782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1044 - val_loss: 1.7697\nEpoch 783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1040 - val_loss: 1.7598\nEpoch 784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1036 - val_loss: 1.7504\nEpoch 785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1032 - val_loss: 1.7409\nEpoch 786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 1.7314\nEpoch 787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1024 - val_loss: 1.7218\nEpoch 788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1020 - val_loss: 1.7124\nEpoch 789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1016 - val_loss: 1.7032\nEpoch 790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 1.6925\nEpoch 791/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1009 - val_loss: 1.6821\nEpoch 792/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1005 - val_loss: 1.6722\nEpoch 793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1001 - val_loss: 1.6626\nEpoch 794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0997 - val_loss: 1.6531\nEpoch 795/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0993 - val_loss: 1.6433\nEpoch 796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0989 - val_loss: 1.6334\nEpoch 797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0985 - val_loss: 1.6241\nEpoch 798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0981 - val_loss: 1.6152\nEpoch 799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0978 - val_loss: 1.6056\nEpoch 800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0974 - val_loss: 1.5965\nEpoch 801/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 1.5872\nEpoch 802/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 1.5778\nEpoch 803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0962 - val_loss: 1.5689\nEpoch 804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0959 - val_loss: 1.5599\nEpoch 805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0955 - val_loss: 1.5510\nEpoch 806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0951 - val_loss: 1.5423\nEpoch 807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0947 - val_loss: 1.5337\nEpoch 808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0944 - val_loss: 1.5249\nEpoch 809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0940 - val_loss: 1.5160\nEpoch 810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0936 - val_loss: 1.5071\nEpoch 811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0933 - val_loss: 1.4979\nEpoch 812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0929 - val_loss: 1.4890\nEpoch 813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0925 - val_loss: 1.4799\nEpoch 814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0921 - val_loss: 1.4709\nEpoch 815/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0918 - val_loss: 1.4622\nEpoch 816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0914 - val_loss: 1.4533\nEpoch 817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0910 - val_loss: 1.4441\nEpoch 818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0907 - val_loss: 1.4352\nEpoch 819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0903 - val_loss: 1.4268\nEpoch 820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0899 - val_loss: 1.4182\nEpoch 821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0896 - val_loss: 1.4094\nEpoch 822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0893 - val_loss: 1.4007\nEpoch 823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0889 - val_loss: 1.3927\nEpoch 824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0885 - val_loss: 1.3842\nEpoch 825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0882 - val_loss: 1.3756\nEpoch 826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0879 - val_loss: 1.3669\nEpoch 827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0875 - val_loss: 1.3588\nEpoch 828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0871 - val_loss: 1.3501\nEpoch 829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0868 - val_loss: 1.3413\nEpoch 830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0864 - val_loss: 1.3330\nEpoch 831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0861 - val_loss: 1.3249\nEpoch 832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0858 - val_loss: 1.3168\nEpoch 833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0854 - val_loss: 1.3080\nEpoch 834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0851 - val_loss: 1.2990\nEpoch 835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0847 - val_loss: 1.2905\nEpoch 836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0844 - val_loss: 1.2821\nEpoch 837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0840 - val_loss: 1.2737\nEpoch 838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0837 - val_loss: 1.2656\nEpoch 839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0834 - val_loss: 1.2577\nEpoch 840/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0831 - val_loss: 1.2491\nEpoch 841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0827 - val_loss: 1.2417\nEpoch 842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0824 - val_loss: 1.2343\nEpoch 843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0820 - val_loss: 1.2269\nEpoch 844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0817 - val_loss: 1.2187\nEpoch 845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0814 - val_loss: 1.2108\nEpoch 846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0811 - val_loss: 1.2027\nEpoch 847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0807 - val_loss: 1.1946\nEpoch 848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0804 - val_loss: 1.1866\nEpoch 849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0801 - val_loss: 1.1785\nEpoch 850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0797 - val_loss: 1.1710\nEpoch 851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0794 - val_loss: 1.1633\nEpoch 852/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0791 - val_loss: 1.1551\nEpoch 853/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0788 - val_loss: 1.1476\nEpoch 854/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0785 - val_loss: 1.1401\nEpoch 855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 1.1325\nEpoch 856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0779 - val_loss: 1.1250\nEpoch 857/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0776 - val_loss: 1.1173\nEpoch 858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0773 - val_loss: 1.1094\nEpoch 859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0769 - val_loss: 1.1020\nEpoch 860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0767 - val_loss: 1.0947\nEpoch 861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 1.0881\nEpoch 862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0760 - val_loss: 1.0809\nEpoch 863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0757 - val_loss: 1.0733\nEpoch 864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0754 - val_loss: 1.0658\nEpoch 865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 1.0587\nEpoch 866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 1.0514\nEpoch 867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0745 - val_loss: 1.0443\nEpoch 868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0742 - val_loss: 1.0366\nEpoch 869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0740 - val_loss: 1.0293\nEpoch 870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0736 - val_loss: 1.0222\nEpoch 871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 1.0156\nEpoch 872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0731 - val_loss: 1.0084\nEpoch 873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0728 - val_loss: 1.0014\nEpoch 874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0725 - val_loss: 0.9942\nEpoch 875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0722 - val_loss: 0.9875\nEpoch 876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.9811\nEpoch 877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0716 - val_loss: 0.9742\nEpoch 878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.9676\nEpoch 879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0711 - val_loss: 0.9613\nEpoch 880/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0708 - val_loss: 0.9546\nEpoch 881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0705 - val_loss: 0.9475\nEpoch 882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0702 - val_loss: 0.9403\nEpoch 883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.9331\nEpoch 884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.9261\nEpoch 885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0694 - val_loss: 0.9197\nEpoch 886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0691 - val_loss: 0.9135\nEpoch 887/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0689 - val_loss: 0.9066\nEpoch 888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0686 - val_loss: 0.8993\nEpoch 889/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.8924\nEpoch 890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8855\nEpoch 891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0678 - val_loss: 0.8796\nEpoch 892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0675 - val_loss: 0.8733\nEpoch 893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0673 - val_loss: 0.8668\nEpoch 894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0670 - val_loss: 0.8608\nEpoch 895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.8547\nEpoch 896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0665 - val_loss: 0.8485\nEpoch 897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0663 - val_loss: 0.8425\nEpoch 898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0660 - val_loss: 0.8362\nEpoch 899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.8303\nEpoch 900/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0655 - val_loss: 0.8244\nEpoch 901/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.8186\nEpoch 902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.8127\nEpoch 903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0648 - val_loss: 0.8066\nEpoch 904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0646 - val_loss: 0.8011\nEpoch 905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0643 - val_loss: 0.7951\nEpoch 906/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0641 - val_loss: 0.7896\nEpoch 907/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7837\nEpoch 908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.7782\nEpoch 909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0634 - val_loss: 0.7722\nEpoch 910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.7668\nEpoch 911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0629 - val_loss: 0.7612\nEpoch 912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.7556\nEpoch 913/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0625 - val_loss: 0.7500\nEpoch 914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0623 - val_loss: 0.7444\nEpoch 915/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0620 - val_loss: 0.7387\nEpoch 916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.7329\nEpoch 917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0616 - val_loss: 0.7272\nEpoch 918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0614 - val_loss: 0.7216\nEpoch 919/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0611 - val_loss: 0.7163\nEpoch 920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.7108\nEpoch 921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.7053\nEpoch 922/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.6999\nEpoch 923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.6943\nEpoch 924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.6890\nEpoch 925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0598 - val_loss: 0.6838\nEpoch 926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.6785\nEpoch 927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0594 - val_loss: 0.6733\nEpoch 928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.6678\nEpoch 929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6624\nEpoch 930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0587 - val_loss: 0.6573\nEpoch 931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0585 - val_loss: 0.6522\nEpoch 932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.6472\nEpoch 933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0581 - val_loss: 0.6422\nEpoch 934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0579 - val_loss: 0.6372\nEpoch 935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0577 - val_loss: 0.6322\nEpoch 936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0575 - val_loss: 0.6270\nEpoch 937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6220\nEpoch 938/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0571 - val_loss: 0.6170\nEpoch 939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.6121\nEpoch 940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0567 - val_loss: 0.6070\nEpoch 941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0565 - val_loss: 0.6022\nEpoch 942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0563 - val_loss: 0.5973\nEpoch 943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0561 - val_loss: 0.5923\nEpoch 944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0559 - val_loss: 0.5877\nEpoch 945/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5830\nEpoch 946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.5779\nEpoch 947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0553 - val_loss: 0.5730\nEpoch 948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0551 - val_loss: 0.5684\nEpoch 949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0549 - val_loss: 0.5634\nEpoch 950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0547 - val_loss: 0.5588\nEpoch 951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0545 - val_loss: 0.5542\nEpoch 952/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0544 - val_loss: 0.5495\nEpoch 953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5449\nEpoch 954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5401\nEpoch 955/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0538 - val_loss: 0.5357\nEpoch 956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0536 - val_loss: 0.5312\nEpoch 957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0534 - val_loss: 0.5266\nEpoch 958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0533 - val_loss: 0.5221\nEpoch 959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.5178\nEpoch 960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5137\nEpoch 961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.5098\nEpoch 962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5058\nEpoch 963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5017\nEpoch 964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.4976\nEpoch 965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.4935\nEpoch 966/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0519 - val_loss: 0.4896\nEpoch 967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4859\nEpoch 968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0516 - val_loss: 0.4814\nEpoch 969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4773\nEpoch 970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4733\nEpoch 971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0511 - val_loss: 0.4691\nEpoch 972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4649\nEpoch 973/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0508 - val_loss: 0.4611\nEpoch 974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.4570\nEpoch 975/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0505 - val_loss: 0.4527\nEpoch 976/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0503 - val_loss: 0.4487\nEpoch 977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4446\nEpoch 978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4406\nEpoch 979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.4368\nEpoch 980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0497 - val_loss: 0.4333\nEpoch 981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0495 - val_loss: 0.4294\nEpoch 982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0494 - val_loss: 0.4259\nEpoch 983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4224\nEpoch 984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.4187\nEpoch 985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0489 - val_loss: 0.4149\nEpoch 986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4111\nEpoch 987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4071\nEpoch 988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.4037\nEpoch 989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0483 - val_loss: 0.4002\nEpoch 990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.3968\nEpoch 991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.3931\nEpoch 992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3897\nEpoch 993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0478 - val_loss: 0.3861\nEpoch 994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3829\nEpoch 995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3797\nEpoch 996/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0474 - val_loss: 0.3764\nEpoch 997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3733\nEpoch 998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3703\nEpoch 999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3669\nEpoch 1000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0468 - val_loss: 0.3635\nEpoch 1001/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.3602\nEpoch 1002/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3569\nEpoch 1003/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3540\nEpoch 1004/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3506\nEpoch 1005/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0462 - val_loss: 0.3476\nEpoch 1006/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3444\nEpoch 1007/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3409\nEpoch 1008/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0458 - val_loss: 0.3378\nEpoch 1009/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3347\nEpoch 1010/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.3315\nEpoch 1011/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0454 - val_loss: 0.3286\nEpoch 1012/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3256\nEpoch 1013/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3225\nEpoch 1014/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3197\nEpoch 1015/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3168\nEpoch 1016/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.3138\nEpoch 1017/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.3111\nEpoch 1018/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3082\nEpoch 1019/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0445 - val_loss: 0.3054\nEpoch 1020/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.3027\nEpoch 1021/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.2998\nEpoch 1022/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2970\nEpoch 1023/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2940\nEpoch 1024/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0439 - val_loss: 0.2913\nEpoch 1025/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0438 - val_loss: 0.2886\nEpoch 1026/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.2858\nEpoch 1027/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 0.2831\nEpoch 1028/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2806\nEpoch 1029/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2782\nEpoch 1030/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0433 - val_loss: 0.2755\nEpoch 1031/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2731\nEpoch 1032/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2702\nEpoch 1033/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2675\nEpoch 1034/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2650\nEpoch 1035/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.2623\nEpoch 1036/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0427 - val_loss: 0.2599\nEpoch 1037/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2575\nEpoch 1038/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0425 - val_loss: 0.2553\nEpoch 1039/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0424 - val_loss: 0.2532\nEpoch 1040/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.2508\nEpoch 1041/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2486\nEpoch 1042/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2461\nEpoch 1043/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2439\nEpoch 1044/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2415\nEpoch 1045/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2393\nEpoch 1046/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2369\nEpoch 1047/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0416 - val_loss: 0.2347\nEpoch 1048/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2325\nEpoch 1049/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2303\nEpoch 1050/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2282\nEpoch 1051/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2259\nEpoch 1052/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2238\nEpoch 1053/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2215\nEpoch 1054/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2191\nEpoch 1055/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2170\nEpoch 1056/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2149\nEpoch 1057/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2129\nEpoch 1058/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2111\nEpoch 1059/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2093\nEpoch 1060/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2075\nEpoch 1061/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2056\nEpoch 1062/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2037\nEpoch 1063/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0403 - val_loss: 0.2018\nEpoch 1064/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1999\nEpoch 1065/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1978\nEpoch 1066/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1958\nEpoch 1067/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1939\nEpoch 1068/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1920\nEpoch 1069/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1904\nEpoch 1070/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1886\nEpoch 1071/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1868\nEpoch 1072/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0397 - val_loss: 0.1851\nEpoch 1073/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1834\nEpoch 1074/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1816\nEpoch 1075/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1799\nEpoch 1076/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1784\nEpoch 1077/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1765\nEpoch 1078/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0393 - val_loss: 0.1747\nEpoch 1079/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1731\nEpoch 1080/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1715\nEpoch 1081/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1699\nEpoch 1082/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1682\nEpoch 1083/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1667\nEpoch 1084/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0389 - val_loss: 0.1650\nEpoch 1085/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1636\nEpoch 1086/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1622\nEpoch 1087/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1606\nEpoch 1088/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1592\nEpoch 1089/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1578\nEpoch 1090/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1563\nEpoch 1091/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1547\nEpoch 1092/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1532\nEpoch 1093/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1517\nEpoch 1094/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1503\nEpoch 1095/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1489\nEpoch 1096/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1475\nEpoch 1097/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0381 - val_loss: 0.1460\nEpoch 1098/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1446\nEpoch 1099/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1434\nEpoch 1100/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1419\nEpoch 1101/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1407\nEpoch 1102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1393\nEpoch 1103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1381\nEpoch 1104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1367\nEpoch 1105/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 0.1355\nEpoch 1106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1343\nEpoch 1107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1333\nEpoch 1108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1321\nEpoch 1109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1308\nEpoch 1110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1295\nEpoch 1111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1283\nEpoch 1112/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1270\nEpoch 1113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1257\nEpoch 1114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1246\nEpoch 1115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1234\nEpoch 1116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1223\nEpoch 1117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1211\nEpoch 1118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1201\nEpoch 1119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1190\nEpoch 1120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1178\nEpoch 1121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1166\nEpoch 1122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1154\nEpoch 1123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1143\nEpoch 1124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1133\nEpoch 1125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1123\nEpoch 1126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1112\nEpoch 1127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1102\nEpoch 1128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1094\nEpoch 1129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1086\nEpoch 1130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1076\nEpoch 1131/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0366 - val_loss: 0.1066\nEpoch 1132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1057\nEpoch 1133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1049\nEpoch 1134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1040\nEpoch 1135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1030\nEpoch 1136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1021\nEpoch 1137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1012\nEpoch 1138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1003\nEpoch 1139/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0364 - val_loss: 0.0995\nEpoch 1140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0988\nEpoch 1141/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0363 - val_loss: 0.0980\nEpoch 1142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0972\nEpoch 1143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0965\nEpoch 1144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0956\nEpoch 1145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0947\nEpoch 1146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0939\nEpoch 1147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0930\nEpoch 1148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0923\nEpoch 1149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0916\nEpoch 1150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0910\nEpoch 1151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0903\nEpoch 1152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0895\nEpoch 1153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0888\nEpoch 1154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0880\nEpoch 1155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0874\nEpoch 1156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0867\nEpoch 1157/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.0858\nEpoch 1158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0851\nEpoch 1159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0844\nEpoch 1160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0837\nEpoch 1161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0830\nEpoch 1162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0824\nEpoch 1163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0818\nEpoch 1164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0812\nEpoch 1165/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0808\nEpoch 1166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0803\nEpoch 1167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0797\nEpoch 1168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0792\nEpoch 1169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0786\nEpoch 1170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0780\nEpoch 1171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0775\nEpoch 1172/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0768\nEpoch 1173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0762\nEpoch 1174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0756\nEpoch 1175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0750\nEpoch 1176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0745\nEpoch 1177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0740\nEpoch 1178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0734\nEpoch 1179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0729\nEpoch 1180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0724\nEpoch 1181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0719\nEpoch 1182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0714\nEpoch 1183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0709\nEpoch 1184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0704\nEpoch 1185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0698\nEpoch 1186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0694\nEpoch 1187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0689\nEpoch 1188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0684\nEpoch 1189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0679\nEpoch 1190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0674\nEpoch 1191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0669\nEpoch 1192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0666\nEpoch 1193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0661\nEpoch 1194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0657\nEpoch 1195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0652\nEpoch 1196/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 0.0648\nEpoch 1197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0644\nEpoch 1198/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 0.0640\nEpoch 1199/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0636\nEpoch 1200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0632\nEpoch 1201/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0627\nEpoch 1202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0623\nEpoch 1203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0621\nEpoch 1204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0617\nEpoch 1205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0614\nEpoch 1206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0611\nEpoch 1207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0607\nEpoch 1208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0604\nEpoch 1209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0601\nEpoch 1210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0597\nEpoch 1211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0594\nEpoch 1212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0590\nEpoch 1213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0587\nEpoch 1214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0583\nEpoch 1215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0581\nEpoch 1216/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 0.0578\nEpoch 1217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0575\nEpoch 1218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0571\nEpoch 1219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0568\nEpoch 1220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0565\nEpoch 1221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0562\nEpoch 1222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0559\nEpoch 1223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0556\nEpoch 1224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0553\nEpoch 1225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0551\nEpoch 1226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0548\nEpoch 1227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0545\nEpoch 1228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0543\nEpoch 1229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0540\nEpoch 1230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0537\nEpoch 1231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0535\nEpoch 1232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0532\nEpoch 1233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0530\nEpoch 1234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0527\nEpoch 1235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0525\nEpoch 1236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0523\nEpoch 1237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0521\nEpoch 1238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0518\nEpoch 1239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0516\nEpoch 1240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0514\nEpoch 1241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0513\nEpoch 1242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0511\nEpoch 1243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0509\nEpoch 1244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0507\nEpoch 1245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0505\nEpoch 1246/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0503\nEpoch 1247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0500\nEpoch 1248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0499\nEpoch 1249/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0497\nEpoch 1250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0495\nEpoch 1251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0493\nEpoch 1252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0491\nEpoch 1253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0489\nEpoch 1254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0488\nEpoch 1255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0486\nEpoch 1256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0484\nEpoch 1257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0482\nEpoch 1258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0480\nEpoch 1259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0479\nEpoch 1260/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0477\nEpoch 1261/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0475\nEpoch 1262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473\nEpoch 1263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473\nEpoch 1264/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0472\nEpoch 1265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0470\nEpoch 1266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0468\nEpoch 1267/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0467\nEpoch 1268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0465\nEpoch 1269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463\nEpoch 1270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462\nEpoch 1271/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0460\nEpoch 1272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458\nEpoch 1273/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0457\nEpoch 1274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456\nEpoch 1275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1276/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0453\nEpoch 1277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452\nEpoch 1278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0451\nEpoch 1279/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0450\nEpoch 1280/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0449\nEpoch 1281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448\nEpoch 1282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0447\nEpoch 1283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446\nEpoch 1284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0445\nEpoch 1285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444\nEpoch 1286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443\nEpoch 1287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442\nEpoch 1288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441\nEpoch 1289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440\nEpoch 1290/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439\nEpoch 1291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438\nEpoch 1292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0437\nEpoch 1293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0437\nEpoch 1294/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0436\nEpoch 1295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435\nEpoch 1296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433\nEpoch 1299/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.0432\nEpoch 1300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432\nEpoch 1301/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0429\nEpoch 1304/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1305/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424\nEpoch 1310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420\nEpoch 1316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420\nEpoch 1317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419\nEpoch 1318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419\nEpoch 1319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418\nEpoch 1320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0416\nEpoch 1323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413\nEpoch 1328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413\nEpoch 1329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1340/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1363/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1370/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1380/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1390/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1401/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1408/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1413/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1421/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1424/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1429/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1438/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1441/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1458/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1463/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1468/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1469/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1473/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1478/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1479/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1481/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1496/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1497/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1498/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1500/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1501/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1505/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1512/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1522/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1523/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1528/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1529/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1532/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1562/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1576/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1577/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1581/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1613/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1617/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1618/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1637/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1643/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1648/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1649/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1651/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1661/2000\n6/6 [==============================] - ETA: 0s - loss: 0.034 - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1665/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1666/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1675/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1676/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1685/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1697/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1701/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1705/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1720/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1721/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1722/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1725/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1734/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1737/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1742/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1763/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1781/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1801/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1806/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1815/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1824/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1833/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1840/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1852/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1854/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1856/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1859/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1874/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1875/2000\n6/6 [==============================] - 0s 5ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1876/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1880/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1882/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1883/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1887/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1889/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1901/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1907/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1913/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1919/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1921/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1922/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1929/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1938/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1945/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1952/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1961/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1970/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1973/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1975/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1983/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 2000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\n\n\n<keras.callbacks.History at 0x7f708bd97e50>\n\n\n\nplt.plot(y,'.',alpha=0.1)\nplt.plot(net(X),'--')\n\n\n\n\n\n# \n#%tensorboard --logdir logs --host 0.0.0.0 \n\n\n이런것은 오버핏이 아님!\n\n- 결론적으로 말해서 위와 같은 net는 설계하였을 경우 val을 빼는 것은 어리석음. (데이터만 버리는 꼴임)\n- 더 많은 데이터를 남겨주면 더 빨리 학습한다.\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(loss='mse',optimizer='adam')\nnet.fit(X,y,epochs=500,batch_size=100, validation_split=0.1, callbacks=tf.keras.callbacks.TensorBoard()) \n\nEpoch 1/500\n9/9 [==============================] - 0s 5ms/step - loss: 11.1529 - val_loss: 17.6322\nEpoch 2/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.0510 - val_loss: 17.4478\nEpoch 3/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.9482 - val_loss: 17.2670\nEpoch 4/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.8465 - val_loss: 17.0850\nEpoch 5/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.7443 - val_loss: 16.9074\nEpoch 6/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.6457 - val_loss: 16.7250\nEpoch 7/500\n9/9 [==============================] - 0s 1ms/step - loss: 10.5456 - val_loss: 16.5480\nEpoch 8/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.4474 - val_loss: 16.3721\nEpoch 9/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.3499 - val_loss: 16.1945\nEpoch 10/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.2516 - val_loss: 16.0212\nEpoch 11/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.1587 - val_loss: 15.8501\nEpoch 12/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.0600 - val_loss: 15.6844\nEpoch 13/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.9677 - val_loss: 15.5179\nEpoch 14/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.8747 - val_loss: 15.3479\nEpoch 15/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.7806 - val_loss: 15.1842\nEpoch 16/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.6911 - val_loss: 15.0164\nEpoch 17/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.5969 - val_loss: 14.8620\nEpoch 18/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.5088 - val_loss: 14.6981\nEpoch 19/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.4182 - val_loss: 14.5400\nEpoch 20/500\n9/9 [==============================] - 0s 1ms/step - loss: 9.3294 - val_loss: 14.3857\nEpoch 21/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.2421 - val_loss: 14.2279\nEpoch 22/500\n9/9 [==============================] - 0s 1ms/step - loss: 9.1551 - val_loss: 14.0740\nEpoch 23/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.0690 - val_loss: 13.9182\nEpoch 24/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.9823 - val_loss: 13.7646\nEpoch 25/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.8961 - val_loss: 13.6238\nEpoch 26/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.8128 - val_loss: 13.4795\nEpoch 27/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.7307 - val_loss: 13.3281\nEpoch 28/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.6474 - val_loss: 13.1848\nEpoch 29/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.5655 - val_loss: 13.0439\nEpoch 30/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.4836 - val_loss: 12.9032\nEpoch 31/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.4041 - val_loss: 12.7584\nEpoch 32/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.3237 - val_loss: 12.6194\nEpoch 33/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.2445 - val_loss: 12.4818\nEpoch 34/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.1652 - val_loss: 12.3472\nEpoch 35/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.0891 - val_loss: 12.2060\nEpoch 36/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.0112 - val_loss: 12.0756\nEpoch 37/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.9331 - val_loss: 11.9493\nEpoch 38/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.8597 - val_loss: 11.8165\nEpoch 39/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.7832 - val_loss: 11.6866\nEpoch 40/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.7085 - val_loss: 11.5609\nEpoch 41/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.6347 - val_loss: 11.4300\nEpoch 42/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.5625 - val_loss: 11.3039\nEpoch 43/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.4886 - val_loss: 11.1786\nEpoch 44/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.4170 - val_loss: 11.0613\nEpoch 45/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.3459 - val_loss: 10.9341\nEpoch 46/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.2742 - val_loss: 10.8175\nEpoch 47/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.2045 - val_loss: 10.6986\nEpoch 48/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.1351 - val_loss: 10.5782\nEpoch 49/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.0656 - val_loss: 10.4625\nEpoch 50/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.9977 - val_loss: 10.3436\nEpoch 51/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.9311 - val_loss: 10.2223\nEpoch 52/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.8624 - val_loss: 10.1109\nEpoch 53/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.7964 - val_loss: 9.9984\nEpoch 54/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.7296 - val_loss: 9.8888\nEpoch 55/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.6645 - val_loss: 9.7811\nEpoch 56/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.5996 - val_loss: 9.6738\nEpoch 57/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.5362 - val_loss: 9.5644\nEpoch 58/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.4717 - val_loss: 9.4555\nEpoch 59/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.4094 - val_loss: 9.3465\nEpoch 60/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.3461 - val_loss: 9.2445\nEpoch 61/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.2842 - val_loss: 9.1374\nEpoch 62/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.2229 - val_loss: 9.0369\nEpoch 63/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.1613 - val_loss: 8.9344\nEpoch 64/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.1003 - val_loss: 8.8330\nEpoch 65/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.0411 - val_loss: 8.7345\nEpoch 66/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.9818 - val_loss: 8.6328\nEpoch 67/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.9227 - val_loss: 8.5349\nEpoch 68/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.8640 - val_loss: 8.4380\nEpoch 69/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.8069 - val_loss: 8.3396\nEpoch 70/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.7490 - val_loss: 8.2460\nEpoch 71/500\n9/9 [==============================] - 0s 1ms/step - loss: 5.6922 - val_loss: 8.1507\nEpoch 72/500\n9/9 [==============================] - 0s 1ms/step - loss: 5.6366 - val_loss: 8.0578\nEpoch 73/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.5798 - val_loss: 7.9645\nEpoch 74/500\n9/9 [==============================] - 0s 1ms/step - loss: 5.5245 - val_loss: 7.8750\nEpoch 75/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.4699 - val_loss: 7.7816\nEpoch 76/500\n9/9 [==============================] - 0s 1ms/step - loss: 5.4155 - val_loss: 7.6949\nEpoch 77/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.3615 - val_loss: 7.6049\nEpoch 78/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.3088 - val_loss: 7.5146\nEpoch 79/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.2546 - val_loss: 7.4298\nEpoch 80/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.2024 - val_loss: 7.3431\nEpoch 81/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.1507 - val_loss: 7.2562\nEpoch 82/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.0991 - val_loss: 7.1726\nEpoch 83/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.0482 - val_loss: 7.0886\nEpoch 84/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.9975 - val_loss: 7.0058\nEpoch 85/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.9472 - val_loss: 6.9263\nEpoch 86/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.8969 - val_loss: 6.8459\nEpoch 87/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.8479 - val_loss: 6.7668\nEpoch 88/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.7994 - val_loss: 6.6891\nEpoch 89/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.7512 - val_loss: 6.6063\nEpoch 90/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.7022 - val_loss: 6.5310\nEpoch 91/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.6547 - val_loss: 6.4561\nEpoch 92/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.6079 - val_loss: 6.3786\nEpoch 93/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.5607 - val_loss: 6.3046\nEpoch 94/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.5144 - val_loss: 6.2286\nEpoch 95/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.4690 - val_loss: 6.1530\nEpoch 96/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.4237 - val_loss: 6.0801\nEpoch 97/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.3780 - val_loss: 6.0086\nEpoch 98/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.3333 - val_loss: 5.9393\nEpoch 99/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.2886 - val_loss: 5.8690\nEpoch 100/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.2452 - val_loss: 5.7963\nEpoch 101/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.2013 - val_loss: 5.7301\nEpoch 102/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.1583 - val_loss: 5.6612\nEpoch 103/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.1153 - val_loss: 5.5936\nEpoch 104/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.0736 - val_loss: 5.5238\nEpoch 105/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.0305 - val_loss: 5.4606\nEpoch 106/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.9894 - val_loss: 5.3935\nEpoch 107/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.9481 - val_loss: 5.3268\nEpoch 108/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.9076 - val_loss: 5.2610\nEpoch 109/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.8664 - val_loss: 5.1996\nEpoch 110/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.8257 - val_loss: 5.1385\nEpoch 111/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.7870 - val_loss: 5.0747\nEpoch 112/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.7474 - val_loss: 5.0111\nEpoch 113/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.7078 - val_loss: 4.9471\nEpoch 114/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.6688 - val_loss: 4.8894\nEpoch 115/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.6306 - val_loss: 4.8294\nEpoch 116/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.5922 - val_loss: 4.7704\nEpoch 117/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.5549 - val_loss: 4.7096\nEpoch 118/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.5174 - val_loss: 4.6514\nEpoch 119/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.4798 - val_loss: 4.5948\nEpoch 120/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.4434 - val_loss: 4.5386\nEpoch 121/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.4072 - val_loss: 4.4816\nEpoch 122/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.3708 - val_loss: 4.4268\nEpoch 123/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.3349 - val_loss: 4.3720\nEpoch 124/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.2994 - val_loss: 4.3174\nEpoch 125/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.2643 - val_loss: 4.2653\nEpoch 126/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.2300 - val_loss: 4.2094\nEpoch 127/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.1955 - val_loss: 4.1579\nEpoch 128/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.1605 - val_loss: 4.1050\nEpoch 129/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.1269 - val_loss: 4.0541\nEpoch 130/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.0935 - val_loss: 4.0013\nEpoch 131/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.0599 - val_loss: 3.9509\nEpoch 132/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.0270 - val_loss: 3.9000\nEpoch 133/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.9942 - val_loss: 3.8525\nEpoch 134/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.9619 - val_loss: 3.8035\nEpoch 135/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.9301 - val_loss: 3.7534\nEpoch 136/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.8981 - val_loss: 3.7043\nEpoch 137/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.8662 - val_loss: 3.6581\nEpoch 138/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.8350 - val_loss: 3.6088\nEpoch 139/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.8040 - val_loss: 3.5631\nEpoch 140/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.7735 - val_loss: 3.5183\nEpoch 141/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.7429 - val_loss: 3.4733\nEpoch 142/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.7129 - val_loss: 3.4262\nEpoch 143/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.6828 - val_loss: 3.3835\nEpoch 144/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.6533 - val_loss: 3.3392\nEpoch 145/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.6238 - val_loss: 3.2961\nEpoch 146/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.5950 - val_loss: 3.2534\nEpoch 147/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.5661 - val_loss: 3.2100\nEpoch 148/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.5375 - val_loss: 3.1676\nEpoch 149/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.5092 - val_loss: 3.1254\nEpoch 150/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.4813 - val_loss: 3.0836\nEpoch 151/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.4537 - val_loss: 3.0419\nEpoch 152/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.4258 - val_loss: 3.0024\nEpoch 153/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3987 - val_loss: 2.9623\nEpoch 154/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3716 - val_loss: 2.9227\nEpoch 155/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3446 - val_loss: 2.8838\nEpoch 156/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.3184 - val_loss: 2.8463\nEpoch 157/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2919 - val_loss: 2.8087\nEpoch 158/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2661 - val_loss: 2.7687\nEpoch 159/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2403 - val_loss: 2.7325\nEpoch 160/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.2147 - val_loss: 2.6946\nEpoch 161/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.1892 - val_loss: 2.6583\nEpoch 162/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.1645 - val_loss: 2.6212\nEpoch 163/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.1394 - val_loss: 2.5858\nEpoch 164/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.1146 - val_loss: 2.5519\nEpoch 165/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.0904 - val_loss: 2.5177\nEpoch 166/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.0662 - val_loss: 2.4819\nEpoch 167/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0422 - val_loss: 2.4472\nEpoch 168/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0186 - val_loss: 2.4146\nEpoch 169/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9950 - val_loss: 2.3812\nEpoch 170/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9717 - val_loss: 2.3481\nEpoch 171/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9486 - val_loss: 2.3145\nEpoch 172/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9259 - val_loss: 2.2815\nEpoch 173/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.9030 - val_loss: 2.2488\nEpoch 174/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.8807 - val_loss: 2.2179\nEpoch 175/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.8587 - val_loss: 2.1859\nEpoch 176/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.8365 - val_loss: 2.1555\nEpoch 177/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.8148 - val_loss: 2.1247\nEpoch 178/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7933 - val_loss: 2.0933\nEpoch 179/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7719 - val_loss: 2.0639\nEpoch 180/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7508 - val_loss: 2.0344\nEpoch 181/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7297 - val_loss: 2.0057\nEpoch 182/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7092 - val_loss: 1.9759\nEpoch 183/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.6885 - val_loss: 1.9478\nEpoch 184/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.6680 - val_loss: 1.9216\nEpoch 185/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.6480 - val_loss: 1.8931\nEpoch 186/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.6280 - val_loss: 1.8653\nEpoch 187/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.6082 - val_loss: 1.8372\nEpoch 188/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5887 - val_loss: 1.8108\nEpoch 189/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.5693 - val_loss: 1.7840\nEpoch 190/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5501 - val_loss: 1.7577\nEpoch 191/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.5310 - val_loss: 1.7306\nEpoch 192/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5123 - val_loss: 1.7052\nEpoch 193/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4938 - val_loss: 1.6806\nEpoch 194/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4752 - val_loss: 1.6546\nEpoch 195/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.4568 - val_loss: 1.6293\nEpoch 196/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4390 - val_loss: 1.6049\nEpoch 197/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4210 - val_loss: 1.5808\nEpoch 198/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4031 - val_loss: 1.5575\nEpoch 199/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3857 - val_loss: 1.5334\nEpoch 200/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3684 - val_loss: 1.5114\nEpoch 201/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3513 - val_loss: 1.4878\nEpoch 202/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3341 - val_loss: 1.4668\nEpoch 203/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3173 - val_loss: 1.4431\nEpoch 204/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3006 - val_loss: 1.4205\nEpoch 205/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2841 - val_loss: 1.3977\nEpoch 206/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2679 - val_loss: 1.3771\nEpoch 207/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2517 - val_loss: 1.3541\nEpoch 208/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.2358 - val_loss: 1.3334\nEpoch 209/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.2198 - val_loss: 1.3130\nEpoch 210/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.2042 - val_loss: 1.2929\nEpoch 211/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.1888 - val_loss: 1.2718\nEpoch 212/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1734 - val_loss: 1.2505\nEpoch 213/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1582 - val_loss: 1.2305\nEpoch 214/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1431 - val_loss: 1.2123\nEpoch 215/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1284 - val_loss: 1.1923\nEpoch 216/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1135 - val_loss: 1.1739\nEpoch 217/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0991 - val_loss: 1.1556\nEpoch 218/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0848 - val_loss: 1.1372\nEpoch 219/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0705 - val_loss: 1.1175\nEpoch 220/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0565 - val_loss: 1.0985\nEpoch 221/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0425 - val_loss: 1.0817\nEpoch 222/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.0286 - val_loss: 1.0638\nEpoch 223/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0150 - val_loss: 1.0465\nEpoch 224/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0016 - val_loss: 1.0295\nEpoch 225/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9882 - val_loss: 1.0121\nEpoch 226/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9751 - val_loss: 0.9955\nEpoch 227/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9620 - val_loss: 0.9794\nEpoch 228/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9490 - val_loss: 0.9625\nEpoch 229/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9363 - val_loss: 0.9461\nEpoch 230/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9237 - val_loss: 0.9302\nEpoch 231/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9113 - val_loss: 0.9141\nEpoch 232/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8988 - val_loss: 0.8987\nEpoch 233/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8868 - val_loss: 0.8831\nEpoch 234/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.8747 - val_loss: 0.8684\nEpoch 235/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8628 - val_loss: 0.8537\nEpoch 236/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8509 - val_loss: 0.8394\nEpoch 237/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.8393 - val_loss: 0.8247\nEpoch 238/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8278 - val_loss: 0.8101\nEpoch 239/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8164 - val_loss: 0.7968\nEpoch 240/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.8051 - val_loss: 0.7825\nEpoch 241/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7940 - val_loss: 0.7689\nEpoch 242/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7831 - val_loss: 0.7554\nEpoch 243/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7721 - val_loss: 0.7424\nEpoch 244/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7614 - val_loss: 0.7292\nEpoch 245/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7507 - val_loss: 0.7159\nEpoch 246/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7403 - val_loss: 0.7026\nEpoch 247/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7300 - val_loss: 0.6907\nEpoch 248/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7197 - val_loss: 0.6785\nEpoch 249/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7096 - val_loss: 0.6669\nEpoch 250/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6996 - val_loss: 0.6546\nEpoch 251/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6896 - val_loss: 0.6430\nEpoch 252/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6799 - val_loss: 0.6311\nEpoch 253/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6702 - val_loss: 0.6192\nEpoch 254/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6608 - val_loss: 0.6076\nEpoch 255/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6513 - val_loss: 0.5967\nEpoch 256/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6420 - val_loss: 0.5856\nEpoch 257/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6329 - val_loss: 0.5744\nEpoch 258/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6238 - val_loss: 0.5640\nEpoch 259/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6148 - val_loss: 0.5537\nEpoch 260/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6060 - val_loss: 0.5435\nEpoch 261/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5972 - val_loss: 0.5333\nEpoch 262/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5886 - val_loss: 0.5234\nEpoch 263/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5801 - val_loss: 0.5131\nEpoch 264/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5717 - val_loss: 0.5038\nEpoch 265/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5634 - val_loss: 0.4936\nEpoch 266/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5552 - val_loss: 0.4847\nEpoch 267/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5470 - val_loss: 0.4754\nEpoch 268/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5391 - val_loss: 0.4663\nEpoch 269/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5312 - val_loss: 0.4569\nEpoch 270/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 0.4483\nEpoch 271/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5157 - val_loss: 0.4397\nEpoch 272/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5081 - val_loss: 0.4307\nEpoch 273/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5007 - val_loss: 0.4224\nEpoch 274/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4933 - val_loss: 0.4148\nEpoch 275/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4860 - val_loss: 0.4066\nEpoch 276/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4788 - val_loss: 0.3987\nEpoch 277/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4717 - val_loss: 0.3905\nEpoch 278/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4647 - val_loss: 0.3831\nEpoch 279/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4577 - val_loss: 0.3754\nEpoch 280/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4509 - val_loss: 0.3683\nEpoch 281/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4442 - val_loss: 0.3608\nEpoch 282/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4376 - val_loss: 0.3536\nEpoch 283/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4310 - val_loss: 0.3468\nEpoch 284/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4245 - val_loss: 0.3395\nEpoch 285/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4182 - val_loss: 0.3326\nEpoch 286/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4120 - val_loss: 0.3255\nEpoch 287/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4057 - val_loss: 0.3190\nEpoch 288/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3996 - val_loss: 0.3124\nEpoch 289/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3936 - val_loss: 0.3065\nEpoch 290/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3876 - val_loss: 0.3002\nEpoch 291/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3818 - val_loss: 0.2941\nEpoch 292/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3760 - val_loss: 0.2880\nEpoch 293/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3703 - val_loss: 0.2820\nEpoch 294/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.2763\nEpoch 295/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3591 - val_loss: 0.2704\nEpoch 296/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3537 - val_loss: 0.2649\nEpoch 297/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3483 - val_loss: 0.2594\nEpoch 298/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3430 - val_loss: 0.2540\nEpoch 299/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3378 - val_loss: 0.2489\nEpoch 300/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3326 - val_loss: 0.2438\nEpoch 301/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3275 - val_loss: 0.2386\nEpoch 302/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3225 - val_loss: 0.2335\nEpoch 303/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3176 - val_loss: 0.2288\nEpoch 304/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3128 - val_loss: 0.2238\nEpoch 305/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3080 - val_loss: 0.2190\nEpoch 306/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3032 - val_loss: 0.2144\nEpoch 307/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2986 - val_loss: 0.2099\nEpoch 308/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2941 - val_loss: 0.2054\nEpoch 309/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2895 - val_loss: 0.2010\nEpoch 310/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2851 - val_loss: 0.1969\nEpoch 311/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2807 - val_loss: 0.1927\nEpoch 312/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2764 - val_loss: 0.1885\nEpoch 313/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2722 - val_loss: 0.1847\nEpoch 314/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2680 - val_loss: 0.1807\nEpoch 315/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2639 - val_loss: 0.1769\nEpoch 316/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2598 - val_loss: 0.1731\nEpoch 317/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2559 - val_loss: 0.1694\nEpoch 318/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2519 - val_loss: 0.1658\nEpoch 319/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2480 - val_loss: 0.1622\nEpoch 320/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2443 - val_loss: 0.1590\nEpoch 321/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2405 - val_loss: 0.1555\nEpoch 322/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2368 - val_loss: 0.1522\nEpoch 323/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2332 - val_loss: 0.1488\nEpoch 324/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2296 - val_loss: 0.1457\nEpoch 325/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2261 - val_loss: 0.1428\nEpoch 326/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2227 - val_loss: 0.1396\nEpoch 327/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2192 - val_loss: 0.1366\nEpoch 328/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2159 - val_loss: 0.1337\nEpoch 329/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2126 - val_loss: 0.1312\nEpoch 330/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2094 - val_loss: 0.1284\nEpoch 331/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2062 - val_loss: 0.1255\nEpoch 332/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2030 - val_loss: 0.1228\nEpoch 333/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2000 - val_loss: 0.1203\nEpoch 334/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1969 - val_loss: 0.1179\nEpoch 335/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1940 - val_loss: 0.1153\nEpoch 336/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1910 - val_loss: 0.1129\nEpoch 337/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1881 - val_loss: 0.1107\nEpoch 338/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1853 - val_loss: 0.1082\nEpoch 339/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1825 - val_loss: 0.1060\nEpoch 340/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1797 - val_loss: 0.1039\nEpoch 341/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1770 - val_loss: 0.1018\nEpoch 342/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1744 - val_loss: 0.0996\nEpoch 343/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1718 - val_loss: 0.0976\nEpoch 344/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1692 - val_loss: 0.0956\nEpoch 345/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1667 - val_loss: 0.0939\nEpoch 346/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1642 - val_loss: 0.0918\nEpoch 347/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1618 - val_loss: 0.0900\nEpoch 348/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1594 - val_loss: 0.0882\nEpoch 349/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1570 - val_loss: 0.0865\nEpoch 350/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1547 - val_loss: 0.0849\nEpoch 351/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1524 - val_loss: 0.0833\nEpoch 352/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1502 - val_loss: 0.0815\nEpoch 353/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1480 - val_loss: 0.0800\nEpoch 354/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1458 - val_loss: 0.0785\nEpoch 355/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1437 - val_loss: 0.0769\nEpoch 356/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1416 - val_loss: 0.0755\nEpoch 357/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1396 - val_loss: 0.0741\nEpoch 358/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1376 - val_loss: 0.0728\nEpoch 359/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1356 - val_loss: 0.0715\nEpoch 360/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1337 - val_loss: 0.0702\nEpoch 361/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1317 - val_loss: 0.0691\nEpoch 362/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1298 - val_loss: 0.0678\nEpoch 363/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1280 - val_loss: 0.0667\nEpoch 364/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1262 - val_loss: 0.0654\nEpoch 365/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.0643\nEpoch 366/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1227 - val_loss: 0.0632\nEpoch 367/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 0.0622\nEpoch 368/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1193 - val_loss: 0.0612\nEpoch 369/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1177 - val_loss: 0.0601\nEpoch 370/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1160 - val_loss: 0.0592\nEpoch 371/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1144 - val_loss: 0.0582\nEpoch 372/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1129 - val_loss: 0.0574\nEpoch 373/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1114 - val_loss: 0.0565\nEpoch 374/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1099 - val_loss: 0.0556\nEpoch 375/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1084 - val_loss: 0.0548\nEpoch 376/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1069 - val_loss: 0.0541\nEpoch 377/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1055 - val_loss: 0.0533\nEpoch 378/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1041 - val_loss: 0.0525\nEpoch 379/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1028 - val_loss: 0.0518\nEpoch 380/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1014 - val_loss: 0.0511\nEpoch 381/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1001 - val_loss: 0.0504\nEpoch 382/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0988 - val_loss: 0.0498\nEpoch 383/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0975 - val_loss: 0.0492\nEpoch 384/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0963 - val_loss: 0.0486\nEpoch 385/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0951 - val_loss: 0.0480\nEpoch 386/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0939 - val_loss: 0.0474\nEpoch 387/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0927 - val_loss: 0.0469\nEpoch 388/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0915 - val_loss: 0.0463\nEpoch 389/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0904 - val_loss: 0.0458\nEpoch 390/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0893 - val_loss: 0.0453\nEpoch 391/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0882 - val_loss: 0.0448\nEpoch 392/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0871 - val_loss: 0.0444\nEpoch 393/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0861 - val_loss: 0.0439\nEpoch 394/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0851 - val_loss: 0.0435\nEpoch 395/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0841 - val_loss: 0.0431\nEpoch 396/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.0427\nEpoch 397/500\n9/9 [==============================] - ETA: 0s - loss: 0.088 - 0s 2ms/step - loss: 0.0821 - val_loss: 0.0423\nEpoch 398/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0812 - val_loss: 0.0420\nEpoch 399/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0416\nEpoch 400/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0793 - val_loss: 0.0413\nEpoch 401/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0784 - val_loss: 0.0410\nEpoch 402/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0775 - val_loss: 0.0407\nEpoch 403/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0767 - val_loss: 0.0403\nEpoch 404/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0758 - val_loss: 0.0401\nEpoch 405/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0750 - val_loss: 0.0398\nEpoch 406/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0742 - val_loss: 0.0395\nEpoch 407/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0734 - val_loss: 0.0392\nEpoch 408/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0726 - val_loss: 0.0390\nEpoch 409/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0718 - val_loss: 0.0388\nEpoch 410/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0711 - val_loss: 0.0386\nEpoch 411/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0704 - val_loss: 0.0384\nEpoch 412/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0696 - val_loss: 0.0382\nEpoch 413/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0689 - val_loss: 0.0380\nEpoch 414/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0682 - val_loss: 0.0378\nEpoch 415/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0676 - val_loss: 0.0376\nEpoch 416/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0669 - val_loss: 0.0374\nEpoch 417/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0663 - val_loss: 0.0373\nEpoch 418/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0656 - val_loss: 0.0371\nEpoch 419/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0650 - val_loss: 0.0370\nEpoch 420/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0644 - val_loss: 0.0369\nEpoch 421/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0638 - val_loss: 0.0367\nEpoch 422/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0632 - val_loss: 0.0366\nEpoch 423/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0626 - val_loss: 0.0365\nEpoch 424/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0620 - val_loss: 0.0364\nEpoch 425/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0615 - val_loss: 0.0363\nEpoch 426/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0609 - val_loss: 0.0362\nEpoch 427/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0604 - val_loss: 0.0361\nEpoch 428/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0599 - val_loss: 0.0360\nEpoch 429/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0594 - val_loss: 0.0359\nEpoch 430/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0589 - val_loss: 0.0358\nEpoch 431/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0358\nEpoch 432/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0357\nEpoch 433/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0574 - val_loss: 0.0356\nEpoch 434/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0570 - val_loss: 0.0356\nEpoch 435/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0565 - val_loss: 0.0355\nEpoch 436/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0561 - val_loss: 0.0355\nEpoch 437/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0556 - val_loss: 0.0354\nEpoch 438/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0552 - val_loss: 0.0354\nEpoch 439/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0548 - val_loss: 0.0353\nEpoch 440/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0544 - val_loss: 0.0353\nEpoch 441/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0540 - val_loss: 0.0353\nEpoch 442/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0536 - val_loss: 0.0352\nEpoch 443/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0532 - val_loss: 0.0352\nEpoch 444/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0528 - val_loss: 0.0352\nEpoch 445/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0525 - val_loss: 0.0352\nEpoch 446/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0521 - val_loss: 0.0351\nEpoch 447/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0518 - val_loss: 0.0351\nEpoch 448/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0514 - val_loss: 0.0351\nEpoch 449/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0511 - val_loss: 0.0351\nEpoch 450/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0507 - val_loss: 0.0351\nEpoch 451/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0504 - val_loss: 0.0350\nEpoch 452/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0501 - val_loss: 0.0350\nEpoch 453/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0498 - val_loss: 0.0350\nEpoch 454/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0495 - val_loss: 0.0350\nEpoch 455/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0492 - val_loss: 0.0350\nEpoch 456/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0489 - val_loss: 0.0350\nEpoch 457/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0486 - val_loss: 0.0350\nEpoch 458/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0483 - val_loss: 0.0350\nEpoch 459/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0480 - val_loss: 0.0350\nEpoch 460/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0478 - val_loss: 0.0350\nEpoch 461/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0475 - val_loss: 0.0350\nEpoch 462/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0473 - val_loss: 0.0350\nEpoch 463/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0470 - val_loss: 0.0350\nEpoch 464/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0468 - val_loss: 0.0350\nEpoch 465/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0465 - val_loss: 0.0350\nEpoch 466/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0463 - val_loss: 0.0350\nEpoch 467/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0460 - val_loss: 0.0350\nEpoch 468/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0458 - val_loss: 0.0350\nEpoch 469/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0456 - val_loss: 0.0350\nEpoch 470/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0454 - val_loss: 0.0350\nEpoch 471/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0452 - val_loss: 0.0351\nEpoch 472/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0449 - val_loss: 0.0351\nEpoch 473/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.0351\nEpoch 474/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0445 - val_loss: 0.0351\nEpoch 475/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0443 - val_loss: 0.0351\nEpoch 476/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0442 - val_loss: 0.0351\nEpoch 477/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0440 - val_loss: 0.0351\nEpoch 478/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0438 - val_loss: 0.0351\nEpoch 479/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0436 - val_loss: 0.0351\nEpoch 480/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0434 - val_loss: 0.0351\nEpoch 481/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0432 - val_loss: 0.0351\nEpoch 482/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0431 - val_loss: 0.0351\nEpoch 483/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0429 - val_loss: 0.0351\nEpoch 484/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0428 - val_loss: 0.0352\nEpoch 485/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0426 - val_loss: 0.0352\nEpoch 486/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0424 - val_loss: 0.0352\nEpoch 487/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0423 - val_loss: 0.0352\nEpoch 488/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0421 - val_loss: 0.0352\nEpoch 489/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0420 - val_loss: 0.0352\nEpoch 490/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0418 - val_loss: 0.0352\nEpoch 491/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0417 - val_loss: 0.0352\nEpoch 492/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.0352\nEpoch 493/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0414 - val_loss: 0.0352\nEpoch 494/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0413 - val_loss: 0.0352\nEpoch 495/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0412 - val_loss: 0.0352\nEpoch 496/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0411 - val_loss: 0.0353\nEpoch 497/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 0.0353\nEpoch 498/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0408 - val_loss: 0.0353\nEpoch 499/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0407 - val_loss: 0.0353\nEpoch 500/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0406 - val_loss: 0.0353\n\n\n<keras.callbacks.History at 0x7f708be9a470>\n\n\n\nplt.plot(y,'.',alpha=0.1)\nplt.plot(net(X),'--')\n\n\n\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0 \n\n\n\n텐서보드: 적합결과 시각화\n- 시각화결과는 모두 텐서보드에서 보고 싶다! 적합결과를 보여주는 fig 오브젝트를 텐서보드에 끼워넣어서 출력하는 방법을 알아보자.\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(loss='mse',optimizer='adam')\nnet.fit(X,y,epochs=500,batch_size=100, validation_split=0.1, callbacks=tf.keras.callbacks.TensorBoard()) \n\nEpoch 1/500\n9/9 [==============================] - 0s 5ms/step - loss: 11.9832 - val_loss: 10.8663\nEpoch 2/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.8783 - val_loss: 10.7961\nEpoch 3/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.7750 - val_loss: 10.7292\nEpoch 4/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.6712 - val_loss: 10.6628\nEpoch 5/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.5688 - val_loss: 10.5916\nEpoch 6/500\n9/9 [==============================] - 0s 1ms/step - loss: 11.4671 - val_loss: 10.5235\nEpoch 7/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.3650 - val_loss: 10.4527\nEpoch 8/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.2672 - val_loss: 10.3818\nEpoch 9/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.1664 - val_loss: 10.3141\nEpoch 10/500\n9/9 [==============================] - 0s 2ms/step - loss: 11.0684 - val_loss: 10.2464\nEpoch 11/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.9698 - val_loss: 10.1806\nEpoch 12/500\n9/9 [==============================] - 0s 1ms/step - loss: 10.8727 - val_loss: 10.1120\nEpoch 13/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.7777 - val_loss: 10.0403\nEpoch 14/500\n9/9 [==============================] - 0s 1ms/step - loss: 10.6817 - val_loss: 9.9739\nEpoch 15/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.5871 - val_loss: 9.9000\nEpoch 16/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.4931 - val_loss: 9.8320\nEpoch 17/500\n9/9 [==============================] - 0s 1ms/step - loss: 10.3994 - val_loss: 9.7659\nEpoch 18/500\n9/9 [==============================] - 0s 1ms/step - loss: 10.3062 - val_loss: 9.7024\nEpoch 19/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.2162 - val_loss: 9.6333\nEpoch 20/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.1243 - val_loss: 9.5670\nEpoch 21/500\n9/9 [==============================] - 0s 2ms/step - loss: 10.0331 - val_loss: 9.5003\nEpoch 22/500\n9/9 [==============================] - 0s 1ms/step - loss: 9.9449 - val_loss: 9.4318\nEpoch 23/500\n9/9 [==============================] - 0s 1ms/step - loss: 9.8568 - val_loss: 9.3642\nEpoch 24/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.7675 - val_loss: 9.2992\nEpoch 25/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.6794 - val_loss: 9.2379\nEpoch 26/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.5940 - val_loss: 9.1682\nEpoch 27/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.5071 - val_loss: 9.1010\nEpoch 28/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.4226 - val_loss: 9.0350\nEpoch 29/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.3376 - val_loss: 8.9707\nEpoch 30/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.2533 - val_loss: 8.9077\nEpoch 31/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.1692 - val_loss: 8.8398\nEpoch 32/500\n9/9 [==============================] - 0s 2ms/step - loss: 9.0872 - val_loss: 8.7771\nEpoch 33/500\n9/9 [==============================] - 0s 1ms/step - loss: 9.0056 - val_loss: 8.7131\nEpoch 34/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.9235 - val_loss: 8.6456\nEpoch 35/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.8424 - val_loss: 8.5801\nEpoch 36/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.7621 - val_loss: 8.5169\nEpoch 37/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.6824 - val_loss: 8.4507\nEpoch 38/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.6039 - val_loss: 8.3903\nEpoch 39/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.5253 - val_loss: 8.3273\nEpoch 40/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.4475 - val_loss: 8.2648\nEpoch 41/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.3703 - val_loss: 8.2014\nEpoch 42/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.2945 - val_loss: 8.1372\nEpoch 43/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.2177 - val_loss: 8.0751\nEpoch 44/500\n9/9 [==============================] - 0s 1ms/step - loss: 8.1433 - val_loss: 8.0141\nEpoch 45/500\n9/9 [==============================] - 0s 2ms/step - loss: 8.0678 - val_loss: 7.9512\nEpoch 46/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.9936 - val_loss: 7.8923\nEpoch 47/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.9200 - val_loss: 7.8267\nEpoch 48/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.8475 - val_loss: 7.7637\nEpoch 49/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.7742 - val_loss: 7.7034\nEpoch 50/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.7027 - val_loss: 7.6417\nEpoch 51/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.6314 - val_loss: 7.5816\nEpoch 52/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.5596 - val_loss: 7.5203\nEpoch 53/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.4893 - val_loss: 7.4622\nEpoch 54/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.4199 - val_loss: 7.4046\nEpoch 55/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.3517 - val_loss: 7.3413\nEpoch 56/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.2821 - val_loss: 7.2826\nEpoch 57/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.2138 - val_loss: 7.2205\nEpoch 58/500\n9/9 [==============================] - 0s 1ms/step - loss: 7.1461 - val_loss: 7.1610\nEpoch 59/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.0787 - val_loss: 7.1049\nEpoch 60/500\n9/9 [==============================] - 0s 2ms/step - loss: 7.0124 - val_loss: 7.0466\nEpoch 61/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.9463 - val_loss: 6.9873\nEpoch 62/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.8811 - val_loss: 6.9300\nEpoch 63/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.8151 - val_loss: 6.8720\nEpoch 64/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.7508 - val_loss: 6.8148\nEpoch 65/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.6870 - val_loss: 6.7572\nEpoch 66/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.6227 - val_loss: 6.7017\nEpoch 67/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.5596 - val_loss: 6.6439\nEpoch 68/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.4973 - val_loss: 6.5899\nEpoch 69/500\n9/9 [==============================] - 0s 1ms/step - loss: 6.4353 - val_loss: 6.5328\nEpoch 70/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.3738 - val_loss: 6.4786\nEpoch 71/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.3121 - val_loss: 6.4191\nEpoch 72/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.2514 - val_loss: 6.3663\nEpoch 73/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.1910 - val_loss: 6.3117\nEpoch 74/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.1314 - val_loss: 6.2588\nEpoch 75/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.0724 - val_loss: 6.2038\nEpoch 76/500\n9/9 [==============================] - 0s 2ms/step - loss: 6.0138 - val_loss: 6.1501\nEpoch 77/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.9543 - val_loss: 6.0937\nEpoch 78/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.8974 - val_loss: 6.0370\nEpoch 79/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.8396 - val_loss: 5.9847\nEpoch 80/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.7821 - val_loss: 5.9322\nEpoch 81/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.7258 - val_loss: 5.8811\nEpoch 82/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.6695 - val_loss: 5.8268\nEpoch 83/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.6137 - val_loss: 5.7750\nEpoch 84/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.5585 - val_loss: 5.7240\nEpoch 85/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.5036 - val_loss: 5.6720\nEpoch 86/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.4493 - val_loss: 5.6208\nEpoch 87/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.3949 - val_loss: 5.5691\nEpoch 88/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.3418 - val_loss: 5.5173\nEpoch 89/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.2885 - val_loss: 5.4681\nEpoch 90/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.2360 - val_loss: 5.4166\nEpoch 91/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.1837 - val_loss: 5.3662\nEpoch 92/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.1319 - val_loss: 5.3159\nEpoch 93/500\n9/9 [==============================] - 0s 2ms/step - loss: 5.0797 - val_loss: 5.2649\nEpoch 94/500\n9/9 [==============================] - 0s 1ms/step - loss: 5.0286 - val_loss: 5.2175\nEpoch 95/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.9783 - val_loss: 5.1691\nEpoch 96/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.9281 - val_loss: 5.1196\nEpoch 97/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.8785 - val_loss: 5.0692\nEpoch 98/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.8286 - val_loss: 5.0219\nEpoch 99/500\n9/9 [==============================] - 0s 1ms/step - loss: 4.7799 - val_loss: 4.9733\nEpoch 100/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.7310 - val_loss: 4.9264\nEpoch 101/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.6824 - val_loss: 4.8787\nEpoch 102/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.6351 - val_loss: 4.8324\nEpoch 103/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.5871 - val_loss: 4.7854\nEpoch 104/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.5403 - val_loss: 4.7400\nEpoch 105/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.4937 - val_loss: 4.6915\nEpoch 106/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.4473 - val_loss: 4.6465\nEpoch 107/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.4013 - val_loss: 4.5990\nEpoch 108/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.3553 - val_loss: 4.5538\nEpoch 109/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.3105 - val_loss: 4.5088\nEpoch 110/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.2656 - val_loss: 4.4612\nEpoch 111/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.2210 - val_loss: 4.4165\nEpoch 112/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.1765 - val_loss: 4.3754\nEpoch 113/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.1330 - val_loss: 4.3317\nEpoch 114/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.0899 - val_loss: 4.2881\nEpoch 115/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.0470 - val_loss: 4.2423\nEpoch 116/500\n9/9 [==============================] - 0s 2ms/step - loss: 4.0040 - val_loss: 4.1998\nEpoch 117/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.9619 - val_loss: 4.1579\nEpoch 118/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.9198 - val_loss: 4.1154\nEpoch 119/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.8786 - val_loss: 4.0717\nEpoch 120/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.8369 - val_loss: 4.0288\nEpoch 121/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.7964 - val_loss: 3.9861\nEpoch 122/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.7558 - val_loss: 3.9436\nEpoch 123/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.7153 - val_loss: 3.9022\nEpoch 124/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.6752 - val_loss: 3.8638\nEpoch 125/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.6362 - val_loss: 3.8228\nEpoch 126/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.5969 - val_loss: 3.7823\nEpoch 127/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.5579 - val_loss: 3.7407\nEpoch 128/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.5196 - val_loss: 3.7022\nEpoch 129/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.4812 - val_loss: 3.6638\nEpoch 130/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.4437 - val_loss: 3.6228\nEpoch 131/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.4061 - val_loss: 3.5835\nEpoch 132/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.3685 - val_loss: 3.5470\nEpoch 133/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.3320 - val_loss: 3.5076\nEpoch 134/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.2952 - val_loss: 3.4692\nEpoch 135/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.2594 - val_loss: 3.4310\nEpoch 136/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.2230 - val_loss: 3.3932\nEpoch 137/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.1874 - val_loss: 3.3545\nEpoch 138/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.1525 - val_loss: 3.3177\nEpoch 139/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.1174 - val_loss: 3.2801\nEpoch 140/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.0823 - val_loss: 3.2447\nEpoch 141/500\n9/9 [==============================] - 0s 1ms/step - loss: 3.0488 - val_loss: 3.2077\nEpoch 142/500\n9/9 [==============================] - 0s 2ms/step - loss: 3.0140 - val_loss: 3.1742\nEpoch 143/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.9804 - val_loss: 3.1380\nEpoch 144/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.9471 - val_loss: 3.1027\nEpoch 145/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.9140 - val_loss: 3.0676\nEpoch 146/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.8808 - val_loss: 3.0307\nEpoch 147/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.8484 - val_loss: 2.9959\nEpoch 148/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.8164 - val_loss: 2.9628\nEpoch 149/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.7844 - val_loss: 2.9261\nEpoch 150/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.7524 - val_loss: 2.8912\nEpoch 151/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.7214 - val_loss: 2.8600\nEpoch 152/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.6900 - val_loss: 2.8276\nEpoch 153/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.6593 - val_loss: 2.7939\nEpoch 154/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.6286 - val_loss: 2.7605\nEpoch 155/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.5985 - val_loss: 2.7283\nEpoch 156/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.5685 - val_loss: 2.6961\nEpoch 157/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.5390 - val_loss: 2.6633\nEpoch 158/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.5093 - val_loss: 2.6319\nEpoch 159/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.4803 - val_loss: 2.6009\nEpoch 160/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.4516 - val_loss: 2.5689\nEpoch 161/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.4225 - val_loss: 2.5390\nEpoch 162/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3946 - val_loss: 2.5096\nEpoch 163/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3666 - val_loss: 2.4766\nEpoch 164/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.3385 - val_loss: 2.4479\nEpoch 165/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.3107 - val_loss: 2.4180\nEpoch 166/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2840 - val_loss: 2.3889\nEpoch 167/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2567 - val_loss: 2.3590\nEpoch 168/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2301 - val_loss: 2.3314\nEpoch 169/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.2035 - val_loss: 2.3024\nEpoch 170/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.1773 - val_loss: 2.2721\nEpoch 171/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.1512 - val_loss: 2.2444\nEpoch 172/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.1257 - val_loss: 2.2158\nEpoch 173/500\n9/9 [==============================] - 0s 1ms/step - loss: 2.0998 - val_loss: 2.1893\nEpoch 174/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0749 - val_loss: 2.1608\nEpoch 175/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0499 - val_loss: 2.1329\nEpoch 176/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0252 - val_loss: 2.1053\nEpoch 177/500\n9/9 [==============================] - 0s 2ms/step - loss: 2.0007 - val_loss: 2.0789\nEpoch 178/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9762 - val_loss: 2.0535\nEpoch 179/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9523 - val_loss: 2.0267\nEpoch 180/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9286 - val_loss: 1.9995\nEpoch 181/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.9051 - val_loss: 1.9733\nEpoch 182/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.8815 - val_loss: 1.9489\nEpoch 183/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.8588 - val_loss: 1.9230\nEpoch 184/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.8357 - val_loss: 1.8970\nEpoch 185/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.8133 - val_loss: 1.8730\nEpoch 186/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.7908 - val_loss: 1.8493\nEpoch 187/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.7688 - val_loss: 1.8231\nEpoch 188/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7467 - val_loss: 1.7991\nEpoch 189/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.7249 - val_loss: 1.7762\nEpoch 190/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.7032 - val_loss: 1.7522\nEpoch 191/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.6821 - val_loss: 1.7295\nEpoch 192/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.6611 - val_loss: 1.7063\nEpoch 193/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.6404 - val_loss: 1.6835\nEpoch 194/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.6197 - val_loss: 1.6604\nEpoch 195/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5993 - val_loss: 1.6368\nEpoch 196/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5791 - val_loss: 1.6148\nEpoch 197/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.5591 - val_loss: 1.5931\nEpoch 198/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.5395 - val_loss: 1.5701\nEpoch 199/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.5196 - val_loss: 1.5490\nEpoch 200/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.5003 - val_loss: 1.5291\nEpoch 201/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.4815 - val_loss: 1.5062\nEpoch 202/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4623 - val_loss: 1.4843\nEpoch 203/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4435 - val_loss: 1.4639\nEpoch 204/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4250 - val_loss: 1.4437\nEpoch 205/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.4067 - val_loss: 1.4226\nEpoch 206/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3884 - val_loss: 1.4033\nEpoch 207/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.3705 - val_loss: 1.3828\nEpoch 208/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3529 - val_loss: 1.3636\nEpoch 209/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3351 - val_loss: 1.3454\nEpoch 210/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3178 - val_loss: 1.3249\nEpoch 211/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.3007 - val_loss: 1.3058\nEpoch 212/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2838 - val_loss: 1.2861\nEpoch 213/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2668 - val_loss: 1.2681\nEpoch 214/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2502 - val_loss: 1.2499\nEpoch 215/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.2336 - val_loss: 1.2314\nEpoch 216/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.2174 - val_loss: 1.2133\nEpoch 217/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.2013 - val_loss: 1.1955\nEpoch 218/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1853 - val_loss: 1.1776\nEpoch 219/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.1697 - val_loss: 1.1601\nEpoch 220/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.1541 - val_loss: 1.1430\nEpoch 221/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1388 - val_loss: 1.1252\nEpoch 222/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.1236 - val_loss: 1.1084\nEpoch 223/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.1084 - val_loss: 1.0921\nEpoch 224/500\n9/9 [==============================] - ETA: 0s - loss: 1.099 - 0s 1ms/step - loss: 1.0936 - val_loss: 1.0763\nEpoch 225/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0789 - val_loss: 1.0605\nEpoch 226/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.0643 - val_loss: 1.0438\nEpoch 227/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.0500 - val_loss: 1.0274\nEpoch 228/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.0359 - val_loss: 1.0106\nEpoch 229/500\n9/9 [==============================] - 0s 2ms/step - loss: 1.0217 - val_loss: 0.9951\nEpoch 230/500\n9/9 [==============================] - 0s 1ms/step - loss: 1.0079 - val_loss: 0.9800\nEpoch 231/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9942 - val_loss: 0.9651\nEpoch 232/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9807 - val_loss: 0.9507\nEpoch 233/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9674 - val_loss: 0.9347\nEpoch 234/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9539 - val_loss: 0.9199\nEpoch 235/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9408 - val_loss: 0.9063\nEpoch 236/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9279 - val_loss: 0.8911\nEpoch 237/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.9152 - val_loss: 0.8767\nEpoch 238/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.9026 - val_loss: 0.8635\nEpoch 239/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8902 - val_loss: 0.8494\nEpoch 240/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8778 - val_loss: 0.8349\nEpoch 241/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8655 - val_loss: 0.8223\nEpoch 242/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8536 - val_loss: 0.8099\nEpoch 243/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8417 - val_loss: 0.7958\nEpoch 244/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.8298 - val_loss: 0.7832\nEpoch 245/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.8182 - val_loss: 0.7705\nEpoch 246/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.8068 - val_loss: 0.7583\nEpoch 247/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7955 - val_loss: 0.7460\nEpoch 248/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7844 - val_loss: 0.7334\nEpoch 249/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7732 - val_loss: 0.7206\nEpoch 250/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7623 - val_loss: 0.7089\nEpoch 251/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7516 - val_loss: 0.6966\nEpoch 252/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7409 - val_loss: 0.6849\nEpoch 253/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7304 - val_loss: 0.6738\nEpoch 254/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.7199 - val_loss: 0.6619\nEpoch 255/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.7097 - val_loss: 0.6512\nEpoch 256/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6997 - val_loss: 0.6401\nEpoch 257/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6896 - val_loss: 0.6294\nEpoch 258/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6798 - val_loss: 0.6186\nEpoch 259/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6700 - val_loss: 0.6080\nEpoch 260/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.5974\nEpoch 261/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6508 - val_loss: 0.5873\nEpoch 262/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6415 - val_loss: 0.5765\nEpoch 263/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6322 - val_loss: 0.5665\nEpoch 264/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6231 - val_loss: 0.5565\nEpoch 265/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.6140 - val_loss: 0.5466\nEpoch 266/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.6052 - val_loss: 0.5370\nEpoch 267/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5964 - val_loss: 0.5284\nEpoch 268/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5876 - val_loss: 0.5181\nEpoch 269/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.5088\nEpoch 270/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5706 - val_loss: 0.4992\nEpoch 271/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5622 - val_loss: 0.4903\nEpoch 272/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5541 - val_loss: 0.4816\nEpoch 273/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5459 - val_loss: 0.4731\nEpoch 274/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5379 - val_loss: 0.4642\nEpoch 275/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5300 - val_loss: 0.4553\nEpoch 276/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.5222 - val_loss: 0.4471\nEpoch 277/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5145 - val_loss: 0.4391\nEpoch 278/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.5069 - val_loss: 0.4313\nEpoch 279/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4994 - val_loss: 0.4231\nEpoch 280/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4921 - val_loss: 0.4154\nEpoch 281/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4847 - val_loss: 0.4079\nEpoch 282/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4776 - val_loss: 0.4006\nEpoch 283/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4704 - val_loss: 0.3929\nEpoch 284/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.4634 - val_loss: 0.3851\nEpoch 285/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4565 - val_loss: 0.3780\nEpoch 286/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4497 - val_loss: 0.3709\nEpoch 287/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4430 - val_loss: 0.3632\nEpoch 288/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4364 - val_loss: 0.3567\nEpoch 289/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4298 - val_loss: 0.3499\nEpoch 290/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4234 - val_loss: 0.3432\nEpoch 291/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4171 - val_loss: 0.3366\nEpoch 292/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4108 - val_loss: 0.3304\nEpoch 293/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.4047 - val_loss: 0.3237\nEpoch 294/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3985 - val_loss: 0.3174\nEpoch 295/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3926 - val_loss: 0.3115\nEpoch 296/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3867 - val_loss: 0.3055\nEpoch 297/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3808 - val_loss: 0.2996\nEpoch 298/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3751 - val_loss: 0.2935\nEpoch 299/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3694 - val_loss: 0.2875\nEpoch 300/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3639 - val_loss: 0.2818\nEpoch 301/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3583 - val_loss: 0.2761\nEpoch 302/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3530 - val_loss: 0.2708\nEpoch 303/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3476 - val_loss: 0.2655\nEpoch 304/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3423 - val_loss: 0.2605\nEpoch 305/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3372 - val_loss: 0.2553\nEpoch 306/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3320 - val_loss: 0.2504\nEpoch 307/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.3270 - val_loss: 0.2452\nEpoch 308/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3221 - val_loss: 0.2401\nEpoch 309/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3171 - val_loss: 0.2351\nEpoch 310/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3124 - val_loss: 0.2308\nEpoch 311/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3076 - val_loss: 0.2260\nEpoch 312/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.3030 - val_loss: 0.2216\nEpoch 313/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2983 - val_loss: 0.2171\nEpoch 314/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2938 - val_loss: 0.2126\nEpoch 315/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2894 - val_loss: 0.2082\nEpoch 316/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2850 - val_loss: 0.2039\nEpoch 317/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2806 - val_loss: 0.1997\nEpoch 318/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2764 - val_loss: 0.1957\nEpoch 319/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2722 - val_loss: 0.1917\nEpoch 320/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2681 - val_loss: 0.1879\nEpoch 321/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2640 - val_loss: 0.1839\nEpoch 322/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2600 - val_loss: 0.1802\nEpoch 323/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2560 - val_loss: 0.1765\nEpoch 324/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2522 - val_loss: 0.1728\nEpoch 325/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2483 - val_loss: 0.1690\nEpoch 326/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2446 - val_loss: 0.1656\nEpoch 327/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2409 - val_loss: 0.1623\nEpoch 328/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2372 - val_loss: 0.1589\nEpoch 329/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2337 - val_loss: 0.1556\nEpoch 330/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2301 - val_loss: 0.1525\nEpoch 331/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2266 - val_loss: 0.1493\nEpoch 332/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2232 - val_loss: 0.1462\nEpoch 333/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2199 - val_loss: 0.1431\nEpoch 334/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2165 - val_loss: 0.1402\nEpoch 335/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2133 - val_loss: 0.1375\nEpoch 336/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.2101 - val_loss: 0.1348\nEpoch 337/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2069 - val_loss: 0.1317\nEpoch 338/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2038 - val_loss: 0.1289\nEpoch 339/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.2008 - val_loss: 0.1262\nEpoch 340/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1978 - val_loss: 0.1238\nEpoch 341/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1948 - val_loss: 0.1213\nEpoch 342/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1919 - val_loss: 0.1188\nEpoch 343/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1891 - val_loss: 0.1166\nEpoch 344/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1863 - val_loss: 0.1142\nEpoch 345/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1835 - val_loss: 0.1118\nEpoch 346/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1808 - val_loss: 0.1095\nEpoch 347/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1781 - val_loss: 0.1073\nEpoch 348/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1755 - val_loss: 0.1052\nEpoch 349/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1729 - val_loss: 0.1031\nEpoch 350/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1704 - val_loss: 0.1009\nEpoch 351/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1679 - val_loss: 0.0989\nEpoch 352/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1654 - val_loss: 0.0971\nEpoch 353/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1630 - val_loss: 0.0952\nEpoch 354/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1606 - val_loss: 0.0934\nEpoch 355/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1583 - val_loss: 0.0914\nEpoch 356/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1560 - val_loss: 0.0896\nEpoch 357/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1537 - val_loss: 0.0880\nEpoch 358/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1515 - val_loss: 0.0863\nEpoch 359/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1494 - val_loss: 0.0845\nEpoch 360/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1472 - val_loss: 0.0830\nEpoch 361/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1451 - val_loss: 0.0815\nEpoch 362/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1430 - val_loss: 0.0799\nEpoch 363/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1410 - val_loss: 0.0784\nEpoch 364/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1390 - val_loss: 0.0770\nEpoch 365/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1370 - val_loss: 0.0755\nEpoch 366/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1351 - val_loss: 0.0741\nEpoch 367/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1333 - val_loss: 0.0728\nEpoch 368/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1314 - val_loss: 0.0715\nEpoch 369/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1295 - val_loss: 0.0704\nEpoch 370/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1278 - val_loss: 0.0691\nEpoch 371/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1260 - val_loss: 0.0679\nEpoch 372/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1242 - val_loss: 0.0667\nEpoch 373/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1225 - val_loss: 0.0655\nEpoch 374/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1209 - val_loss: 0.0645\nEpoch 375/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1193 - val_loss: 0.0635\nEpoch 376/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1176 - val_loss: 0.0624\nEpoch 377/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1161 - val_loss: 0.0614\nEpoch 378/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1145 - val_loss: 0.0603\nEpoch 379/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1129 - val_loss: 0.0594\nEpoch 380/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1115 - val_loss: 0.0584\nEpoch 381/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1100 - val_loss: 0.0575\nEpoch 382/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1086 - val_loss: 0.0567\nEpoch 383/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.1071 - val_loss: 0.0559\nEpoch 384/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1058 - val_loss: 0.0551\nEpoch 385/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1044 - val_loss: 0.0543\nEpoch 386/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1030 - val_loss: 0.0536\nEpoch 387/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1017 - val_loss: 0.0528\nEpoch 388/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.1004 - val_loss: 0.0520\nEpoch 389/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0992 - val_loss: 0.0513\nEpoch 390/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0979 - val_loss: 0.0507\nEpoch 391/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0967 - val_loss: 0.0500\nEpoch 392/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0955 - val_loss: 0.0495\nEpoch 393/500\n9/9 [==============================] - 0s 4ms/step - loss: 0.0943 - val_loss: 0.0489\nEpoch 394/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0931 - val_loss: 0.0482\nEpoch 395/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0920 - val_loss: 0.0477\nEpoch 396/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0909 - val_loss: 0.0471\nEpoch 397/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0898 - val_loss: 0.0465\nEpoch 398/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0887 - val_loss: 0.0461\nEpoch 399/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0877 - val_loss: 0.0456\nEpoch 400/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0866 - val_loss: 0.0451\nEpoch 401/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0856 - val_loss: 0.0446\nEpoch 402/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0846 - val_loss: 0.0442\nEpoch 403/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0836 - val_loss: 0.0437\nEpoch 404/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0827 - val_loss: 0.0433\nEpoch 405/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0817 - val_loss: 0.0430\nEpoch 406/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0808 - val_loss: 0.0425\nEpoch 407/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0799 - val_loss: 0.0422\nEpoch 408/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0790 - val_loss: 0.0418\nEpoch 409/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0782 - val_loss: 0.0415\nEpoch 410/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0773 - val_loss: 0.0411\nEpoch 411/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0765 - val_loss: 0.0408\nEpoch 412/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0756 - val_loss: 0.0405\nEpoch 413/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0749 - val_loss: 0.0403\nEpoch 414/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0740 - val_loss: 0.0400\nEpoch 415/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0733 - val_loss: 0.0397\nEpoch 416/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0725 - val_loss: 0.0394\nEpoch 417/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0718 - val_loss: 0.0392\nEpoch 418/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0710 - val_loss: 0.0389\nEpoch 419/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0703 - val_loss: 0.0387\nEpoch 420/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0696 - val_loss: 0.0385\nEpoch 421/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0689 - val_loss: 0.0383\nEpoch 422/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0682 - val_loss: 0.0381\nEpoch 423/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0676 - val_loss: 0.0379\nEpoch 424/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0669 - val_loss: 0.0377\nEpoch 425/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0663 - val_loss: 0.0376\nEpoch 426/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0657 - val_loss: 0.0374\nEpoch 427/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0650 - val_loss: 0.0372\nEpoch 428/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0644 - val_loss: 0.0371\nEpoch 429/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0638 - val_loss: 0.0370\nEpoch 430/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0633 - val_loss: 0.0368\nEpoch 431/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0627 - val_loss: 0.0367\nEpoch 432/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0621 - val_loss: 0.0366\nEpoch 433/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0616 - val_loss: 0.0365\nEpoch 434/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0611 - val_loss: 0.0364\nEpoch 435/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0605 - val_loss: 0.0363\nEpoch 436/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0600 - val_loss: 0.0362\nEpoch 437/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0595 - val_loss: 0.0361\nEpoch 438/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0590 - val_loss: 0.0360\nEpoch 439/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0585 - val_loss: 0.0359\nEpoch 440/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0580 - val_loss: 0.0358\nEpoch 441/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0357\nEpoch 442/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0571 - val_loss: 0.0357\nEpoch 443/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0567 - val_loss: 0.0356\nEpoch 444/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0562 - val_loss: 0.0355\nEpoch 445/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0558 - val_loss: 0.0355\nEpoch 446/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0554 - val_loss: 0.0354\nEpoch 447/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0550 - val_loss: 0.0354\nEpoch 448/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0546 - val_loss: 0.0353\nEpoch 449/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0542 - val_loss: 0.0353\nEpoch 450/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0538 - val_loss: 0.0353\nEpoch 451/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0534 - val_loss: 0.0352\nEpoch 452/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0530 - val_loss: 0.0352\nEpoch 453/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0527 - val_loss: 0.0352\nEpoch 454/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0523 - val_loss: 0.0351\nEpoch 455/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0519 - val_loss: 0.0351\nEpoch 456/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0516 - val_loss: 0.0351\nEpoch 457/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0513 - val_loss: 0.0351\nEpoch 458/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0509 - val_loss: 0.0350\nEpoch 459/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0506 - val_loss: 0.0350\nEpoch 460/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0503 - val_loss: 0.0350\nEpoch 461/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0500 - val_loss: 0.0350\nEpoch 462/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0497 - val_loss: 0.0350\nEpoch 463/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0494 - val_loss: 0.0350\nEpoch 464/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0491 - val_loss: 0.0350\nEpoch 465/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0488 - val_loss: 0.0350\nEpoch 466/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0485 - val_loss: 0.0350\nEpoch 467/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0482 - val_loss: 0.0350\nEpoch 468/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0479 - val_loss: 0.0350\nEpoch 469/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.0350\nEpoch 470/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0474 - val_loss: 0.0350\nEpoch 471/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0472 - val_loss: 0.0350\nEpoch 472/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0469 - val_loss: 0.0350\nEpoch 473/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0467 - val_loss: 0.0350\nEpoch 474/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0464 - val_loss: 0.0350\nEpoch 475/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0462 - val_loss: 0.0350\nEpoch 476/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0460 - val_loss: 0.0350\nEpoch 477/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0457 - val_loss: 0.0350\nEpoch 478/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0455 - val_loss: 0.0350\nEpoch 479/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0453 - val_loss: 0.0350\nEpoch 480/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0451 - val_loss: 0.0350\nEpoch 481/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0449 - val_loss: 0.0350\nEpoch 482/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.0350\nEpoch 483/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0445 - val_loss: 0.0350\nEpoch 484/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0443 - val_loss: 0.0350\nEpoch 485/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0441 - val_loss: 0.0350\nEpoch 486/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0439 - val_loss: 0.0350\nEpoch 487/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0437 - val_loss: 0.0350\nEpoch 488/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0436 - val_loss: 0.0350\nEpoch 489/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0434 - val_loss: 0.0351\nEpoch 490/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0432 - val_loss: 0.0351\nEpoch 491/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0430 - val_loss: 0.0351\nEpoch 492/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0429 - val_loss: 0.0351\nEpoch 493/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0427 - val_loss: 0.0351\nEpoch 494/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0425 - val_loss: 0.0351\nEpoch 495/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0424 - val_loss: 0.0351\nEpoch 496/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0422 - val_loss: 0.0351\nEpoch 497/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0421 - val_loss: 0.0351\nEpoch 498/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0419 - val_loss: 0.0351\nEpoch 499/500\n9/9 [==============================] - 0s 2ms/step - loss: 0.0418 - val_loss: 0.0351\nEpoch 500/500\n9/9 [==============================] - 0s 1ms/step - loss: 0.0417 - val_loss: 0.0351\n\n\n<keras.callbacks.History at 0x7f70988babc0>\n\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0 \n\n- 끼워넣을 오브젝트 만들기\n\nfig, ax = plt.subplots() \nax.plot(y,'.',alpha=0.2)\nax.plot(net(X),'--')\n\n\n\n\n\nfig\n\n\n\n\n- 이제 fig 오브젝트를 끼워넣을 코드를 구성하자. (공식홈페이지 참고)\n\nhttps://www.tensorflow.org/tensorboard/image_summaries\n\n\n# 이코드는 한번만 실행\n#from datetime import datetime\nimport io\nlogdir = \"logs\" \n#logdir = \"logs\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ndef plot_to_image(fig): # 사용자가 지정한 그림오브젝트 fig를 넣으면 텐서보드에 끼워넣을수 있는 형태로 출력해주는 함수 \n    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n    # Save the plot to a PNG in memory.\n    buf = io.BytesIO()\n    fig.savefig(buf, format='png')\n    # Closing the figure prevents it from being displayed directly inside\n    # the notebook.\n    plt.close(fig)\n    buf.seek(0)\n    # Convert PNG buffer to TF image\n    image = tf.image.decode_png(buf.getvalue(), channels=4)\n    # Add the batch dimension\n    image = tf.expand_dims(image, 0)\n    return image\n\n\nwith tf.summary.create_file_writer(logdir).as_default():\n    tf.summary.image(\"적합결과시각화\", plot_to_image(fig), step=0)\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0\n\n\n\n\n학습과정분석\n\n텐서보드: 가중치 시각화\n- 에폭별로 가중치가 수렴하는 모양을 보고 싶다.\n3-(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i\\]\n여기에서 \\(t=(t_1,\\dots,t_{1000})=\\) np.linspace(0,5,1000) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.linspace(0,5,1000)\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.1)\n\n\n\n\n- 학습을 진행하면서 가중치가 어떻게 업데이트 되는지 시각화하자.\n\n# y = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\n\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\ncb1= tf.keras.callbacks.TensorBoard(update_freq='epoch',histogram_freq=100)\nnet.fit(X,y,epochs=2000, batch_size=100, validation_split=0.45,callbacks=cb1)\n\nEpoch 1/2000\n6/6 [==============================] - 0s 10ms/step - loss: 10.5761 - val_loss: 13.6319\nEpoch 2/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.5050 - val_loss: 13.5579\nEpoch 3/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.4352 - val_loss: 13.4839\nEpoch 4/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.3643 - val_loss: 13.4108\nEpoch 5/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.2954 - val_loss: 13.3387\nEpoch 6/2000\n6/6 [==============================] - 0s 4ms/step - loss: 10.2266 - val_loss: 13.2667\nEpoch 7/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.1574 - val_loss: 13.1948\nEpoch 8/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.0893 - val_loss: 13.1239\nEpoch 9/2000\n6/6 [==============================] - 0s 3ms/step - loss: 10.0220 - val_loss: 13.0539\nEpoch 10/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.9545 - val_loss: 12.9839\nEpoch 11/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.8885 - val_loss: 12.9143\nEpoch 12/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.8218 - val_loss: 12.8446\nEpoch 13/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.7546 - val_loss: 12.7763\nEpoch 14/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.6900 - val_loss: 12.7072\nEpoch 15/2000\n6/6 [==============================] - 0s 4ms/step - loss: 9.6251 - val_loss: 12.6390\nEpoch 16/2000\n6/6 [==============================] - 0s 4ms/step - loss: 9.5592 - val_loss: 12.5710\nEpoch 17/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.4956 - val_loss: 12.5030\nEpoch 18/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.4315 - val_loss: 12.4345\nEpoch 19/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.3670 - val_loss: 12.3678\nEpoch 20/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.3029 - val_loss: 12.3018\nEpoch 21/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.2407 - val_loss: 12.2358\nEpoch 22/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.1777 - val_loss: 12.1697\nEpoch 23/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.1158 - val_loss: 12.1035\nEpoch 24/2000\n6/6 [==============================] - 0s 3ms/step - loss: 9.0525 - val_loss: 12.0391\nEpoch 25/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.9924 - val_loss: 11.9733\nEpoch 26/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.9292 - val_loss: 11.9101\nEpoch 27/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.8694 - val_loss: 11.8461\nEpoch 28/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.8079 - val_loss: 11.7831\nEpoch 29/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.7476 - val_loss: 11.7193\nEpoch 30/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.6889 - val_loss: 11.6557\nEpoch 31/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.6272 - val_loss: 11.5937\nEpoch 32/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.5683 - val_loss: 11.5312\nEpoch 33/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.5101 - val_loss: 11.4687\nEpoch 34/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.4503 - val_loss: 11.4082\nEpoch 35/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.3924 - val_loss: 11.3462\nEpoch 36/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.3351 - val_loss: 11.2848\nEpoch 37/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.2768 - val_loss: 11.2241\nEpoch 38/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.2203 - val_loss: 11.1644\nEpoch 39/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.1620 - val_loss: 11.1063\nEpoch 40/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.1069 - val_loss: 11.0478\nEpoch 41/2000\n6/6 [==============================] - 0s 3ms/step - loss: 8.0510 - val_loss: 10.9894\nEpoch 42/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.9946 - val_loss: 10.9322\nEpoch 43/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.9396 - val_loss: 10.8745\nEpoch 44/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.8851 - val_loss: 10.8172\nEpoch 45/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.8309 - val_loss: 10.7601\nEpoch 46/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.7758 - val_loss: 10.7029\nEpoch 47/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.7227 - val_loss: 10.6449\nEpoch 48/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.6674 - val_loss: 10.5891\nEpoch 49/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.6149 - val_loss: 10.5327\nEpoch 50/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.5614 - val_loss: 10.4775\nEpoch 51/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.5091 - val_loss: 10.4221\nEpoch 52/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.4576 - val_loss: 10.3669\nEpoch 53/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.4050 - val_loss: 10.3126\nEpoch 54/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.3522 - val_loss: 10.2590\nEpoch 55/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.3023 - val_loss: 10.2050\nEpoch 56/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.2503 - val_loss: 10.1516\nEpoch 57/2000\n6/6 [==============================] - 0s 4ms/step - loss: 7.2003 - val_loss: 10.0971\nEpoch 58/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.1490 - val_loss: 10.0436\nEpoch 59/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.0988 - val_loss: 9.9912\nEpoch 60/2000\n6/6 [==============================] - 0s 3ms/step - loss: 7.0486 - val_loss: 9.9394\nEpoch 61/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.9988 - val_loss: 9.8870\nEpoch 62/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.9508 - val_loss: 9.8347\nEpoch 63/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.9000 - val_loss: 9.7832\nEpoch 64/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.8507 - val_loss: 9.7309\nEpoch 65/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.8023 - val_loss: 9.6794\nEpoch 66/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.7536 - val_loss: 9.6278\nEpoch 67/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.7058 - val_loss: 9.5765\nEpoch 68/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.6573 - val_loss: 9.5260\nEpoch 69/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.6098 - val_loss: 9.4752\nEpoch 70/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5625 - val_loss: 9.4245\nEpoch 71/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5147 - val_loss: 9.3755\nEpoch 72/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4692 - val_loss: 9.3261\nEpoch 73/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4218 - val_loss: 9.2776\nEpoch 74/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3757 - val_loss: 9.2286\nEpoch 75/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3297 - val_loss: 9.1803\nEpoch 76/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2845 - val_loss: 9.1314\nEpoch 77/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2383 - val_loss: 9.0835\nEpoch 78/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1934 - val_loss: 9.0348\nEpoch 79/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1484 - val_loss: 8.9875\nEpoch 80/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1032 - val_loss: 8.9405\nEpoch 81/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.0604 - val_loss: 8.8922\nEpoch 82/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.0153 - val_loss: 8.8456\nEpoch 83/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9708 - val_loss: 8.7999\nEpoch 84/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9286 - val_loss: 8.7539\nEpoch 85/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8848 - val_loss: 8.7093\nEpoch 86/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8427 - val_loss: 8.6642\nEpoch 87/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.8006 - val_loss: 8.6189\nEpoch 88/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7591 - val_loss: 8.5736\nEpoch 89/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7162 - val_loss: 8.5291\nEpoch 90/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6751 - val_loss: 8.4846\nEpoch 91/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6328 - val_loss: 8.4410\nEpoch 92/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5924 - val_loss: 8.3969\nEpoch 93/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.5510 - val_loss: 8.3528\nEpoch 94/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5104 - val_loss: 8.3092\nEpoch 95/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.4697 - val_loss: 8.2662\nEpoch 96/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.4291 - val_loss: 8.2228\nEpoch 97/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3892 - val_loss: 8.1801\nEpoch 98/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3491 - val_loss: 8.1378\nEpoch 99/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3104 - val_loss: 8.0953\nEpoch 100/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2704 - val_loss: 8.0537\nEpoch 101/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2316 - val_loss: 8.0121\nEpoch 102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1927 - val_loss: 7.9702\nEpoch 103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1547 - val_loss: 7.9291\nEpoch 104/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.1153 - val_loss: 7.8887\nEpoch 105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0776 - val_loss: 7.8474\nEpoch 106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0398 - val_loss: 7.8072\nEpoch 107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0018 - val_loss: 7.7670\nEpoch 108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.9650 - val_loss: 7.7268\nEpoch 109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.9272 - val_loss: 7.6866\nEpoch 110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8898 - val_loss: 7.6479\nEpoch 111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8535 - val_loss: 7.6090\nEpoch 112/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8171 - val_loss: 7.5696\nEpoch 113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7808 - val_loss: 7.5310\nEpoch 114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7459 - val_loss: 7.4919\nEpoch 115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7091 - val_loss: 7.4538\nEpoch 116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6743 - val_loss: 7.4156\nEpoch 117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6391 - val_loss: 7.3770\nEpoch 118/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.6037 - val_loss: 7.3395\nEpoch 119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5691 - val_loss: 7.3022\nEpoch 120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5346 - val_loss: 7.2650\nEpoch 121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5012 - val_loss: 7.2272\nEpoch 122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4662 - val_loss: 7.1898\nEpoch 123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4323 - val_loss: 7.1527\nEpoch 124/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.3984 - val_loss: 7.1157\nEpoch 125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3651 - val_loss: 7.0795\nEpoch 126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3322 - val_loss: 7.0431\nEpoch 127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2990 - val_loss: 7.0075\nEpoch 128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2665 - val_loss: 6.9722\nEpoch 129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2339 - val_loss: 6.9372\nEpoch 130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2018 - val_loss: 6.9018\nEpoch 131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1699 - val_loss: 6.8666\nEpoch 132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1382 - val_loss: 6.8318\nEpoch 133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1064 - val_loss: 6.7976\nEpoch 134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0750 - val_loss: 6.7637\nEpoch 135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0437 - val_loss: 6.7300\nEpoch 136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0137 - val_loss: 6.6962\nEpoch 137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9831 - val_loss: 6.6622\nEpoch 138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9520 - val_loss: 6.6287\nEpoch 139/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.9218 - val_loss: 6.5953\nEpoch 140/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.8921 - val_loss: 6.5620\nEpoch 141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8617 - val_loss: 6.5292\nEpoch 142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8319 - val_loss: 6.4965\nEpoch 143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8020 - val_loss: 6.4639\nEpoch 144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7732 - val_loss: 6.4315\nEpoch 145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7435 - val_loss: 6.3991\nEpoch 146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7143 - val_loss: 6.3669\nEpoch 147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6857 - val_loss: 6.3345\nEpoch 148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6562 - val_loss: 6.3026\nEpoch 149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6286 - val_loss: 6.2697\nEpoch 150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5990 - val_loss: 6.2380\nEpoch 151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5708 - val_loss: 6.2063\nEpoch 152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5426 - val_loss: 6.1751\nEpoch 153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5147 - val_loss: 6.1438\nEpoch 154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4869 - val_loss: 6.1125\nEpoch 155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4598 - val_loss: 6.0812\nEpoch 156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4324 - val_loss: 6.0509\nEpoch 157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4057 - val_loss: 6.0204\nEpoch 158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3787 - val_loss: 5.9906\nEpoch 159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3523 - val_loss: 5.9610\nEpoch 160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3257 - val_loss: 5.9312\nEpoch 161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2997 - val_loss: 5.9018\nEpoch 162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2740 - val_loss: 5.8723\nEpoch 163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2478 - val_loss: 5.8435\nEpoch 164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2222 - val_loss: 5.8143\nEpoch 165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1972 - val_loss: 5.7853\nEpoch 166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1719 - val_loss: 5.7565\nEpoch 167/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.1462 - val_loss: 5.7283\nEpoch 168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1213 - val_loss: 5.7000\nEpoch 169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0966 - val_loss: 5.6719\nEpoch 170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0715 - val_loss: 5.6443\nEpoch 171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0475 - val_loss: 5.6166\nEpoch 172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0236 - val_loss: 5.5888\nEpoch 173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9992 - val_loss: 5.5612\nEpoch 174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9749 - val_loss: 5.5341\nEpoch 175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9514 - val_loss: 5.5070\nEpoch 176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9280 - val_loss: 5.4798\nEpoch 177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9046 - val_loss: 5.4530\nEpoch 178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8809 - val_loss: 5.4262\nEpoch 179/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.8579 - val_loss: 5.3988\nEpoch 180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8347 - val_loss: 5.3721\nEpoch 181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8117 - val_loss: 5.3455\nEpoch 182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7892 - val_loss: 5.3190\nEpoch 183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7668 - val_loss: 5.2926\nEpoch 184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7439 - val_loss: 5.2664\nEpoch 185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7219 - val_loss: 5.2400\nEpoch 186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6998 - val_loss: 5.2143\nEpoch 187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6775 - val_loss: 5.1886\nEpoch 188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6560 - val_loss: 5.1632\nEpoch 189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6344 - val_loss: 5.1375\nEpoch 190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6128 - val_loss: 5.1125\nEpoch 191/2000\n6/6 [==============================] - 0s 2ms/step - loss: 2.5919 - val_loss: 5.0868\nEpoch 192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5703 - val_loss: 5.0617\nEpoch 193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5491 - val_loss: 5.0373\nEpoch 194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5285 - val_loss: 5.0125\nEpoch 195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5071 - val_loss: 4.9879\nEpoch 196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4868 - val_loss: 4.9628\nEpoch 197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4659 - val_loss: 4.9382\nEpoch 198/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.4455 - val_loss: 4.9135\nEpoch 199/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.4248 - val_loss: 4.8893\nEpoch 200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4049 - val_loss: 4.8653\nEpoch 201/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3847 - val_loss: 4.8420\nEpoch 202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3649 - val_loss: 4.8186\nEpoch 203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3451 - val_loss: 4.7950\nEpoch 204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3261 - val_loss: 4.7714\nEpoch 205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3060 - val_loss: 4.7487\nEpoch 206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2871 - val_loss: 4.7257\nEpoch 207/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.2681 - val_loss: 4.7027\nEpoch 208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2489 - val_loss: 4.6803\nEpoch 209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2303 - val_loss: 4.6579\nEpoch 210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2115 - val_loss: 4.6353\nEpoch 211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1931 - val_loss: 4.6133\nEpoch 212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1750 - val_loss: 4.5909\nEpoch 213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1568 - val_loss: 4.5686\nEpoch 214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1387 - val_loss: 4.5462\nEpoch 215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1208 - val_loss: 4.5243\nEpoch 216/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1031 - val_loss: 4.5023\nEpoch 217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0855 - val_loss: 4.4811\nEpoch 218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0677 - val_loss: 4.4596\nEpoch 219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0503 - val_loss: 4.4384\nEpoch 220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0331 - val_loss: 4.4170\nEpoch 221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0158 - val_loss: 4.3960\nEpoch 222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9988 - val_loss: 4.3747\nEpoch 223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9816 - val_loss: 4.3535\nEpoch 224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9646 - val_loss: 4.3322\nEpoch 225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9478 - val_loss: 4.3112\nEpoch 226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9309 - val_loss: 4.2906\nEpoch 227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9147 - val_loss: 4.2702\nEpoch 228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8978 - val_loss: 4.2495\nEpoch 229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8817 - val_loss: 4.2287\nEpoch 230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8654 - val_loss: 4.2084\nEpoch 231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8493 - val_loss: 4.1881\nEpoch 232/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.8336 - val_loss: 4.1677\nEpoch 233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8177 - val_loss: 4.1479\nEpoch 234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8023 - val_loss: 4.1278\nEpoch 235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7865 - val_loss: 4.1084\nEpoch 236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7710 - val_loss: 4.0889\nEpoch 237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7556 - val_loss: 4.0694\nEpoch 238/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.7405 - val_loss: 4.0502\nEpoch 239/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.7254 - val_loss: 4.0309\nEpoch 240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7103 - val_loss: 4.0120\nEpoch 241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6959 - val_loss: 3.9928\nEpoch 242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6808 - val_loss: 3.9739\nEpoch 243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6661 - val_loss: 3.9554\nEpoch 244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6518 - val_loss: 3.9367\nEpoch 245/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.6372 - val_loss: 3.9182\nEpoch 246/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.6232 - val_loss: 3.8994\nEpoch 247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6087 - val_loss: 3.8812\nEpoch 248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5946 - val_loss: 3.8630\nEpoch 249/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5808 - val_loss: 3.8450\nEpoch 250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5669 - val_loss: 3.8269\nEpoch 251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5532 - val_loss: 3.8090\nEpoch 252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5396 - val_loss: 3.7908\nEpoch 253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5259 - val_loss: 3.7723\nEpoch 254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5123 - val_loss: 3.7543\nEpoch 255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4990 - val_loss: 3.7366\nEpoch 256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4855 - val_loss: 3.7190\nEpoch 257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4724 - val_loss: 3.7013\nEpoch 258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4592 - val_loss: 3.6839\nEpoch 259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4462 - val_loss: 3.6666\nEpoch 260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4335 - val_loss: 3.6491\nEpoch 261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4207 - val_loss: 3.6321\nEpoch 262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4078 - val_loss: 3.6151\nEpoch 263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3955 - val_loss: 3.5979\nEpoch 264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3828 - val_loss: 3.5807\nEpoch 265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3705 - val_loss: 3.5637\nEpoch 266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3581 - val_loss: 3.5469\nEpoch 267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3462 - val_loss: 3.5302\nEpoch 268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3340 - val_loss: 3.5136\nEpoch 269/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.3218 - val_loss: 3.4973\nEpoch 270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3100 - val_loss: 3.4807\nEpoch 271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2982 - val_loss: 3.4645\nEpoch 272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2865 - val_loss: 3.4483\nEpoch 273/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.2750 - val_loss: 3.4321\nEpoch 274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2634 - val_loss: 3.4163\nEpoch 275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2521 - val_loss: 3.4002\nEpoch 276/2000\n6/6 [==============================] - 0s 2ms/step - loss: 1.2408 - val_loss: 3.3844\nEpoch 277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2293 - val_loss: 3.3688\nEpoch 278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2182 - val_loss: 3.3532\nEpoch 279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2075 - val_loss: 3.3375\nEpoch 280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1963 - val_loss: 3.3221\nEpoch 281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1854 - val_loss: 3.3067\nEpoch 282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1747 - val_loss: 3.2914\nEpoch 283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1640 - val_loss: 3.2761\nEpoch 284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1534 - val_loss: 3.2609\nEpoch 285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1429 - val_loss: 3.2457\nEpoch 286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1323 - val_loss: 3.2309\nEpoch 287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1218 - val_loss: 3.2161\nEpoch 288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1116 - val_loss: 3.2014\nEpoch 289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1014 - val_loss: 3.1869\nEpoch 290/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.0913 - val_loss: 3.1723\nEpoch 291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0815 - val_loss: 3.1577\nEpoch 292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0712 - val_loss: 3.1432\nEpoch 293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0614 - val_loss: 3.1287\nEpoch 294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0515 - val_loss: 3.1145\nEpoch 295/2000\n6/6 [==============================] - 0s 2ms/step - loss: 1.0418 - val_loss: 3.1001\nEpoch 296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0321 - val_loss: 3.0862\nEpoch 297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0224 - val_loss: 3.0722\nEpoch 298/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.0130 - val_loss: 3.0578\nEpoch 299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0034 - val_loss: 3.0437\nEpoch 300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9940 - val_loss: 3.0300\nEpoch 301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9848 - val_loss: 3.0160\nEpoch 302/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9756 - val_loss: 3.0022\nEpoch 303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9666 - val_loss: 2.9887\nEpoch 304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9575 - val_loss: 2.9754\nEpoch 305/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9486 - val_loss: 2.9620\nEpoch 306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9398 - val_loss: 2.9483\nEpoch 307/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9309 - val_loss: 2.9350\nEpoch 308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9222 - val_loss: 2.9220\nEpoch 309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9136 - val_loss: 2.9090\nEpoch 310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9052 - val_loss: 2.8960\nEpoch 311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8967 - val_loss: 2.8830\nEpoch 312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8884 - val_loss: 2.8703\nEpoch 313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8800 - val_loss: 2.8577\nEpoch 314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8718 - val_loss: 2.8449\nEpoch 315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8636 - val_loss: 2.8324\nEpoch 316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8555 - val_loss: 2.8196\nEpoch 317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8475 - val_loss: 2.8069\nEpoch 318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8396 - val_loss: 2.7941\nEpoch 319/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8316 - val_loss: 2.7815\nEpoch 320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8237 - val_loss: 2.7691\nEpoch 321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8159 - val_loss: 2.7563\nEpoch 322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8081 - val_loss: 2.7440\nEpoch 323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8005 - val_loss: 2.7317\nEpoch 324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7930 - val_loss: 2.7191\nEpoch 325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7853 - val_loss: 2.7069\nEpoch 326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7778 - val_loss: 2.6946\nEpoch 327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7704 - val_loss: 2.6826\nEpoch 328/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.7630 - val_loss: 2.6707\nEpoch 329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7557 - val_loss: 2.6586\nEpoch 330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7484 - val_loss: 2.6466\nEpoch 331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7412 - val_loss: 2.6347\nEpoch 332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7342 - val_loss: 2.6225\nEpoch 333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7270 - val_loss: 2.6108\nEpoch 334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7201 - val_loss: 2.5992\nEpoch 335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7131 - val_loss: 2.5877\nEpoch 336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7063 - val_loss: 2.5761\nEpoch 337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6995 - val_loss: 2.5646\nEpoch 338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 2.5533\nEpoch 339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6861 - val_loss: 2.5420\nEpoch 340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6795 - val_loss: 2.5309\nEpoch 341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6730 - val_loss: 2.5197\nEpoch 342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6665 - val_loss: 2.5087\nEpoch 343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6600 - val_loss: 2.4978\nEpoch 344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6537 - val_loss: 2.4868\nEpoch 345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6473 - val_loss: 2.4760\nEpoch 346/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.6411 - val_loss: 2.4649\nEpoch 347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6348 - val_loss: 2.4543\nEpoch 348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6285 - val_loss: 2.4438\nEpoch 349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6225 - val_loss: 2.4331\nEpoch 350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6164 - val_loss: 2.4223\nEpoch 351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6103 - val_loss: 2.4117\nEpoch 352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6044 - val_loss: 2.4009\nEpoch 353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5984 - val_loss: 2.3903\nEpoch 354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5925 - val_loss: 2.3800\nEpoch 355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5867 - val_loss: 2.3697\nEpoch 356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5812 - val_loss: 2.3593\nEpoch 357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5754 - val_loss: 2.3490\nEpoch 358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5698 - val_loss: 2.3388\nEpoch 359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5642 - val_loss: 2.3287\nEpoch 360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5587 - val_loss: 2.3189\nEpoch 361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5533 - val_loss: 2.3090\nEpoch 362/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5478 - val_loss: 2.2990\nEpoch 363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5424 - val_loss: 2.2892\nEpoch 364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5372 - val_loss: 2.2795\nEpoch 365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5320 - val_loss: 2.2696\nEpoch 366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5268 - val_loss: 2.2600\nEpoch 367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5215 - val_loss: 2.2505\nEpoch 368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5165 - val_loss: 2.2408\nEpoch 369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5113 - val_loss: 2.2315\nEpoch 370/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5064 - val_loss: 2.2219\nEpoch 371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5013 - val_loss: 2.2122\nEpoch 372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4964 - val_loss: 2.2026\nEpoch 373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4914 - val_loss: 2.1931\nEpoch 374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4866 - val_loss: 2.1835\nEpoch 375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4818 - val_loss: 2.1744\nEpoch 376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4772 - val_loss: 2.1650\nEpoch 377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4723 - val_loss: 2.1559\nEpoch 378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4677 - val_loss: 2.1469\nEpoch 379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4631 - val_loss: 2.1378\nEpoch 380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4585 - val_loss: 2.1288\nEpoch 381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4540 - val_loss: 2.1199\nEpoch 382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4496 - val_loss: 2.1110\nEpoch 383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4452 - val_loss: 2.1021\nEpoch 384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4408 - val_loss: 2.0934\nEpoch 385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4365 - val_loss: 2.0848\nEpoch 386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4322 - val_loss: 2.0760\nEpoch 387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4278 - val_loss: 2.0677\nEpoch 388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4237 - val_loss: 2.0591\nEpoch 389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4196 - val_loss: 2.0505\nEpoch 390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4154 - val_loss: 2.0419\nEpoch 391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4113 - val_loss: 2.0335\nEpoch 392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4072 - val_loss: 2.0253\nEpoch 393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4031 - val_loss: 2.0169\nEpoch 394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3992 - val_loss: 2.0086\nEpoch 395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3952 - val_loss: 2.0003\nEpoch 396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3913 - val_loss: 1.9921\nEpoch 397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3875 - val_loss: 1.9839\nEpoch 398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3837 - val_loss: 1.9756\nEpoch 399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3798 - val_loss: 1.9674\nEpoch 400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3760 - val_loss: 1.9594\nEpoch 401/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3723 - val_loss: 1.9515\nEpoch 402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3687 - val_loss: 1.9434\nEpoch 403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3650 - val_loss: 1.9356\nEpoch 404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3613 - val_loss: 1.9277\nEpoch 405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3578 - val_loss: 1.9199\nEpoch 406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3543 - val_loss: 1.9120\nEpoch 407/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3508 - val_loss: 1.9041\nEpoch 408/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3474 - val_loss: 1.8962\nEpoch 409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3438 - val_loss: 1.8886\nEpoch 410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3405 - val_loss: 1.8808\nEpoch 411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3370 - val_loss: 1.8730\nEpoch 412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3337 - val_loss: 1.8654\nEpoch 413/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3304 - val_loss: 1.8578\nEpoch 414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3271 - val_loss: 1.8502\nEpoch 415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3238 - val_loss: 1.8428\nEpoch 416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3207 - val_loss: 1.8351\nEpoch 417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3175 - val_loss: 1.8276\nEpoch 418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3144 - val_loss: 1.8201\nEpoch 419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3113 - val_loss: 1.8128\nEpoch 420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3082 - val_loss: 1.8056\nEpoch 421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3052 - val_loss: 1.7985\nEpoch 422/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3022 - val_loss: 1.7914\nEpoch 423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2992 - val_loss: 1.7842\nEpoch 424/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2963 - val_loss: 1.7773\nEpoch 425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2935 - val_loss: 1.7702\nEpoch 426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2906 - val_loss: 1.7632\nEpoch 427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2877 - val_loss: 1.7563\nEpoch 428/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2850 - val_loss: 1.7494\nEpoch 429/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2822 - val_loss: 1.7424\nEpoch 430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2794 - val_loss: 1.7355\nEpoch 431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2767 - val_loss: 1.7285\nEpoch 432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2740 - val_loss: 1.7217\nEpoch 433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2713 - val_loss: 1.7148\nEpoch 434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2687 - val_loss: 1.7078\nEpoch 435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2660 - val_loss: 1.7011\nEpoch 436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2634 - val_loss: 1.6943\nEpoch 437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2609 - val_loss: 1.6876\nEpoch 438/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2583 - val_loss: 1.6811\nEpoch 439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2559 - val_loss: 1.6744\nEpoch 440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2534 - val_loss: 1.6676\nEpoch 441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2509 - val_loss: 1.6609\nEpoch 442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2485 - val_loss: 1.6541\nEpoch 443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2461 - val_loss: 1.6475\nEpoch 444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2437 - val_loss: 1.6411\nEpoch 445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2414 - val_loss: 1.6346\nEpoch 446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2391 - val_loss: 1.6283\nEpoch 447/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2367 - val_loss: 1.6220\nEpoch 448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2345 - val_loss: 1.6157\nEpoch 449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2322 - val_loss: 1.6094\nEpoch 450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2300 - val_loss: 1.6031\nEpoch 451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2278 - val_loss: 1.5968\nEpoch 452/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2256 - val_loss: 1.5905\nEpoch 453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2234 - val_loss: 1.5844\nEpoch 454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2213 - val_loss: 1.5781\nEpoch 455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2192 - val_loss: 1.5718\nEpoch 456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2171 - val_loss: 1.5656\nEpoch 457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2151 - val_loss: 1.5595\nEpoch 458/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2130 - val_loss: 1.5536\nEpoch 459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2111 - val_loss: 1.5476\nEpoch 460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2091 - val_loss: 1.5415\nEpoch 461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2071 - val_loss: 1.5357\nEpoch 462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2052 - val_loss: 1.5299\nEpoch 463/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2033 - val_loss: 1.5241\nEpoch 464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2014 - val_loss: 1.5181\nEpoch 465/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1995 - val_loss: 1.5122\nEpoch 466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1976 - val_loss: 1.5063\nEpoch 467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1958 - val_loss: 1.5006\nEpoch 468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1940 - val_loss: 1.4947\nEpoch 469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1922 - val_loss: 1.4890\nEpoch 470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1904 - val_loss: 1.4832\nEpoch 471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1887 - val_loss: 1.4777\nEpoch 472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1869 - val_loss: 1.4722\nEpoch 473/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1852 - val_loss: 1.4668\nEpoch 474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1835 - val_loss: 1.4614\nEpoch 475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1819 - val_loss: 1.4560\nEpoch 476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1803 - val_loss: 1.4504\nEpoch 477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1787 - val_loss: 1.4450\nEpoch 478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1771 - val_loss: 1.4397\nEpoch 479/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1755 - val_loss: 1.4342\nEpoch 480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1739 - val_loss: 1.4290\nEpoch 481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1724 - val_loss: 1.4235\nEpoch 482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1709 - val_loss: 1.4181\nEpoch 483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1693 - val_loss: 1.4129\nEpoch 484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1679 - val_loss: 1.4075\nEpoch 485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1664 - val_loss: 1.4023\nEpoch 486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1649 - val_loss: 1.3974\nEpoch 487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1635 - val_loss: 1.3922\nEpoch 488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1621 - val_loss: 1.3870\nEpoch 489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1606 - val_loss: 1.3818\nEpoch 490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 1.3766\nEpoch 491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1579 - val_loss: 1.3714\nEpoch 492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1565 - val_loss: 1.3665\nEpoch 493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1552 - val_loss: 1.3615\nEpoch 494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1539 - val_loss: 1.3565\nEpoch 495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1526 - val_loss: 1.3517\nEpoch 496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1513 - val_loss: 1.3466\nEpoch 497/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1500 - val_loss: 1.3417\nEpoch 498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1488 - val_loss: 1.3366\nEpoch 499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1475 - val_loss: 1.3318\nEpoch 500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1463 - val_loss: 1.3271\nEpoch 501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1451 - val_loss: 1.3222\nEpoch 502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1439 - val_loss: 1.3173\nEpoch 503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1427 - val_loss: 1.3124\nEpoch 504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1416 - val_loss: 1.3075\nEpoch 505/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1404 - val_loss: 1.3027\nEpoch 506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1392 - val_loss: 1.2979\nEpoch 507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1381 - val_loss: 1.2933\nEpoch 508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1370 - val_loss: 1.2886\nEpoch 509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1360 - val_loss: 1.2839\nEpoch 510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1349 - val_loss: 1.2792\nEpoch 511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1338 - val_loss: 1.2747\nEpoch 512/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1327 - val_loss: 1.2702\nEpoch 513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1317 - val_loss: 1.2654\nEpoch 514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1307 - val_loss: 1.2608\nEpoch 515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1297 - val_loss: 1.2564\nEpoch 516/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1287 - val_loss: 1.2521\nEpoch 517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1277 - val_loss: 1.2476\nEpoch 518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1267 - val_loss: 1.2432\nEpoch 519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1258 - val_loss: 1.2386\nEpoch 520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 1.2342\nEpoch 521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1238 - val_loss: 1.2300\nEpoch 522/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1229 - val_loss: 1.2255\nEpoch 523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1220 - val_loss: 1.2212\nEpoch 524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1211 - val_loss: 1.2168\nEpoch 525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1202 - val_loss: 1.2126\nEpoch 526/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1193 - val_loss: 1.2082\nEpoch 527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1185 - val_loss: 1.2039\nEpoch 528/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1176 - val_loss: 1.1997\nEpoch 529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1167 - val_loss: 1.1954\nEpoch 530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1159 - val_loss: 1.1911\nEpoch 531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1151 - val_loss: 1.1870\nEpoch 532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1142 - val_loss: 1.1831\nEpoch 533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1134 - val_loss: 1.1791\nEpoch 534/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1127 - val_loss: 1.1750\nEpoch 535/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1119 - val_loss: 1.1710\nEpoch 536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1111 - val_loss: 1.1669\nEpoch 537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1103 - val_loss: 1.1629\nEpoch 538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1096 - val_loss: 1.1590\nEpoch 539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1089 - val_loss: 1.1551\nEpoch 540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1081 - val_loss: 1.1512\nEpoch 541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1074 - val_loss: 1.1471\nEpoch 542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1067 - val_loss: 1.1431\nEpoch 543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1060 - val_loss: 1.1388\nEpoch 544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1053 - val_loss: 1.1347\nEpoch 545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1046 - val_loss: 1.1307\nEpoch 546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1039 - val_loss: 1.1267\nEpoch 547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1032 - val_loss: 1.1226\nEpoch 548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1026 - val_loss: 1.1187\nEpoch 549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1020 - val_loss: 1.1147\nEpoch 550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 1.1107\nEpoch 551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1006 - val_loss: 1.1068\nEpoch 552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1000 - val_loss: 1.1028\nEpoch 553/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0994 - val_loss: 1.0990\nEpoch 554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0988 - val_loss: 1.0952\nEpoch 555/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0982 - val_loss: 1.0914\nEpoch 556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0976 - val_loss: 1.0878\nEpoch 557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 1.0838\nEpoch 558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 1.0799\nEpoch 559/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0958 - val_loss: 1.0761\nEpoch 560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0953 - val_loss: 1.0723\nEpoch 561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0947 - val_loss: 1.0685\nEpoch 562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0942 - val_loss: 1.0649\nEpoch 563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0936 - val_loss: 1.0613\nEpoch 564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0931 - val_loss: 1.0577\nEpoch 565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0926 - val_loss: 1.0539\nEpoch 566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0921 - val_loss: 1.0501\nEpoch 567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0915 - val_loss: 1.0466\nEpoch 568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0910 - val_loss: 1.0428\nEpoch 569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0905 - val_loss: 1.0394\nEpoch 570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 1.0356\nEpoch 571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0895 - val_loss: 1.0320\nEpoch 572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0891 - val_loss: 1.0284\nEpoch 573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0886 - val_loss: 1.0249\nEpoch 574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0881 - val_loss: 1.0212\nEpoch 575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0877 - val_loss: 1.0176\nEpoch 576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0872 - val_loss: 1.0141\nEpoch 577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0867 - val_loss: 1.0107\nEpoch 578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0863 - val_loss: 1.0072\nEpoch 579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0859 - val_loss: 1.0037\nEpoch 580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0854 - val_loss: 1.0002\nEpoch 581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0850 - val_loss: 0.9969\nEpoch 582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0846 - val_loss: 0.9933\nEpoch 583/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0842 - val_loss: 0.9898\nEpoch 584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0838 - val_loss: 0.9865\nEpoch 585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0834 - val_loss: 0.9831\nEpoch 586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0830 - val_loss: 0.9796\nEpoch 587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0826 - val_loss: 0.9761\nEpoch 588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0822 - val_loss: 0.9726\nEpoch 589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0818 - val_loss: 0.9689\nEpoch 590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0814 - val_loss: 0.9655\nEpoch 591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0811 - val_loss: 0.9621\nEpoch 592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0807 - val_loss: 0.9589\nEpoch 593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0803 - val_loss: 0.9555\nEpoch 594/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0800 - val_loss: 0.9522\nEpoch 595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0796 - val_loss: 0.9488\nEpoch 596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0792 - val_loss: 0.9454\nEpoch 597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0789 - val_loss: 0.9419\nEpoch 598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0786 - val_loss: 0.9388\nEpoch 599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 0.9355\nEpoch 600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0779 - val_loss: 0.9324\nEpoch 601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0776 - val_loss: 0.9292\nEpoch 602/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0773 - val_loss: 0.9261\nEpoch 603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0770 - val_loss: 0.9230\nEpoch 604/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0767 - val_loss: 0.9196\nEpoch 605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 0.9165\nEpoch 606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0760 - val_loss: 0.9135\nEpoch 607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0758 - val_loss: 0.9102\nEpoch 608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0755 - val_loss: 0.9070\nEpoch 609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0752 - val_loss: 0.9039\nEpoch 610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0749 - val_loss: 0.9008\nEpoch 611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0746 - val_loss: 0.8976\nEpoch 612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0743 - val_loss: 0.8945\nEpoch 613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0740 - val_loss: 0.8914\nEpoch 614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0737 - val_loss: 0.8883\nEpoch 615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0735 - val_loss: 0.8852\nEpoch 616/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0732 - val_loss: 0.8820\nEpoch 617/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0729 - val_loss: 0.8789\nEpoch 618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0727 - val_loss: 0.8759\nEpoch 619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0724 - val_loss: 0.8729\nEpoch 620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0722 - val_loss: 0.8700\nEpoch 621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.8669\nEpoch 622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0717 - val_loss: 0.8641\nEpoch 623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.8610\nEpoch 624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0712 - val_loss: 0.8578\nEpoch 625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0709 - val_loss: 0.8547\nEpoch 626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0707 - val_loss: 0.8518\nEpoch 627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0705 - val_loss: 0.8489\nEpoch 628/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0702 - val_loss: 0.8459\nEpoch 629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.8430\nEpoch 630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0698 - val_loss: 0.8400\nEpoch 631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0696 - val_loss: 0.8370\nEpoch 632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0693 - val_loss: 0.8339\nEpoch 633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0691 - val_loss: 0.8309\nEpoch 634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0689 - val_loss: 0.8279\nEpoch 635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0687 - val_loss: 0.8249\nEpoch 636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0685 - val_loss: 0.8221\nEpoch 637/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.8193\nEpoch 638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8163\nEpoch 639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0679 - val_loss: 0.8137\nEpoch 640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0677 - val_loss: 0.8107\nEpoch 641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0675 - val_loss: 0.8078\nEpoch 642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0673 - val_loss: 0.8048\nEpoch 643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0671 - val_loss: 0.8015\nEpoch 644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0669 - val_loss: 0.7986\nEpoch 645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0667 - val_loss: 0.7956\nEpoch 646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0665 - val_loss: 0.7927\nEpoch 647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0663 - val_loss: 0.7898\nEpoch 648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0662 - val_loss: 0.7870\nEpoch 649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0660 - val_loss: 0.7843\nEpoch 650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.7816\nEpoch 651/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.7787\nEpoch 652/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0654 - val_loss: 0.7759\nEpoch 653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.7731\nEpoch 654/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0651 - val_loss: 0.7703\nEpoch 655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0649 - val_loss: 0.7676\nEpoch 656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0647 - val_loss: 0.7649\nEpoch 657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0646 - val_loss: 0.7621\nEpoch 658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0644 - val_loss: 0.7593\nEpoch 659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0642 - val_loss: 0.7565\nEpoch 660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.7537\nEpoch 661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7511\nEpoch 662/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0638 - val_loss: 0.7482\nEpoch 663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.7454\nEpoch 664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0635 - val_loss: 0.7426\nEpoch 665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0633 - val_loss: 0.7400\nEpoch 666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0631 - val_loss: 0.7373\nEpoch 667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0630 - val_loss: 0.7345\nEpoch 668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0628 - val_loss: 0.7320\nEpoch 669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.7292\nEpoch 670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0625 - val_loss: 0.7266\nEpoch 671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0624 - val_loss: 0.7240\nEpoch 672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0623 - val_loss: 0.7214\nEpoch 673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0621 - val_loss: 0.7188\nEpoch 674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0620 - val_loss: 0.7162\nEpoch 675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.7136\nEpoch 676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0617 - val_loss: 0.7108\nEpoch 677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0615 - val_loss: 0.7081\nEpoch 678/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0614 - val_loss: 0.7053\nEpoch 679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0613 - val_loss: 0.7027\nEpoch 680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0611 - val_loss: 0.7002\nEpoch 681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.6976\nEpoch 682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.6952\nEpoch 683/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0607 - val_loss: 0.6927\nEpoch 684/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0606 - val_loss: 0.6903\nEpoch 685/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0605 - val_loss: 0.6877\nEpoch 686/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0603 - val_loss: 0.6851\nEpoch 687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 0.6827\nEpoch 688/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0601 - val_loss: 0.6801\nEpoch 689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.6775\nEpoch 690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0598 - val_loss: 0.6750\nEpoch 691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.6725\nEpoch 692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.6700\nEpoch 693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0595 - val_loss: 0.6674\nEpoch 694/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0593 - val_loss: 0.6648\nEpoch 695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.6624\nEpoch 696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.6600\nEpoch 697/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6576\nEpoch 698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0589 - val_loss: 0.6551\nEpoch 699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.6525\nEpoch 700/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.6500\nEpoch 701/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.6474\nEpoch 702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 0.6449\nEpoch 703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.6424\nEpoch 704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0582 - val_loss: 0.6398\nEpoch 705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0581 - val_loss: 0.6376\nEpoch 706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0580 - val_loss: 0.6352\nEpoch 707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.6328\nEpoch 708/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.6303\nEpoch 709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0576 - val_loss: 0.6278\nEpoch 710/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0575 - val_loss: 0.6253\nEpoch 711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0574 - val_loss: 0.6231\nEpoch 712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6207\nEpoch 713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0572 - val_loss: 0.6183\nEpoch 714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0571 - val_loss: 0.6157\nEpoch 715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0570 - val_loss: 0.6132\nEpoch 716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.6107\nEpoch 717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0568 - val_loss: 0.6083\nEpoch 718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0566 - val_loss: 0.6059\nEpoch 719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0566 - val_loss: 0.6035\nEpoch 720/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0564 - val_loss: 0.6012\nEpoch 721/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0563 - val_loss: 0.5989\nEpoch 722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0562 - val_loss: 0.5964\nEpoch 723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0561 - val_loss: 0.5941\nEpoch 724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0560 - val_loss: 0.5915\nEpoch 725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0559 - val_loss: 0.5893\nEpoch 726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0558 - val_loss: 0.5869\nEpoch 727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5844\nEpoch 728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0556 - val_loss: 0.5821\nEpoch 729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.5799\nEpoch 730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0554 - val_loss: 0.5777\nEpoch 731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0553 - val_loss: 0.5753\nEpoch 732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0552 - val_loss: 0.5728\nEpoch 733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0551 - val_loss: 0.5701\nEpoch 734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.5676\nEpoch 735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0549 - val_loss: 0.5653\nEpoch 736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0548 - val_loss: 0.5630\nEpoch 737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0547 - val_loss: 0.5606\nEpoch 738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0546 - val_loss: 0.5584\nEpoch 739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0545 - val_loss: 0.5560\nEpoch 740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0544 - val_loss: 0.5537\nEpoch 741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0543 - val_loss: 0.5514\nEpoch 742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5491\nEpoch 743/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0541 - val_loss: 0.5468\nEpoch 744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5446\nEpoch 745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5421\nEpoch 746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0539 - val_loss: 0.5399\nEpoch 747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0538 - val_loss: 0.5374\nEpoch 748/2000\n6/6 [==============================] - 0s 6ms/step - loss: 0.0537 - val_loss: 0.5350\nEpoch 749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0536 - val_loss: 0.5327\nEpoch 750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.5305\nEpoch 751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0534 - val_loss: 0.5284\nEpoch 752/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0533 - val_loss: 0.5262\nEpoch 753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0532 - val_loss: 0.5242\nEpoch 754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.5220\nEpoch 755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0530 - val_loss: 0.5198\nEpoch 756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5177\nEpoch 757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5156\nEpoch 758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0528 - val_loss: 0.5135\nEpoch 759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.5112\nEpoch 760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5089\nEpoch 761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0525 - val_loss: 0.5067\nEpoch 762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5046\nEpoch 763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0523 - val_loss: 0.5024\nEpoch 764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.5002\nEpoch 765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.4980\nEpoch 766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0520 - val_loss: 0.4958\nEpoch 767/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0520 - val_loss: 0.4936\nEpoch 768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.4915\nEpoch 769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0518 - val_loss: 0.4895\nEpoch 770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4875\nEpoch 771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0516 - val_loss: 0.4855\nEpoch 772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.4833\nEpoch 773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4813\nEpoch 774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4791\nEpoch 775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0513 - val_loss: 0.4772\nEpoch 776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4752\nEpoch 777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0511 - val_loss: 0.4731\nEpoch 778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.4709\nEpoch 779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4689\nEpoch 780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4670\nEpoch 781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0508 - val_loss: 0.4652\nEpoch 782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0507 - val_loss: 0.4631\nEpoch 783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.4610\nEpoch 784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0505 - val_loss: 0.4586\nEpoch 785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4565\nEpoch 786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4545\nEpoch 787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.4524\nEpoch 788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0502 - val_loss: 0.4504\nEpoch 789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4484\nEpoch 790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4464\nEpoch 791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4442\nEpoch 792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.4421\nEpoch 793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.4401\nEpoch 794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0497 - val_loss: 0.4380\nEpoch 795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0496 - val_loss: 0.4359\nEpoch 796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0495 - val_loss: 0.4339\nEpoch 797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0495 - val_loss: 0.4318\nEpoch 798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0494 - val_loss: 0.4299\nEpoch 799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.4280\nEpoch 800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4262\nEpoch 801/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.4242\nEpoch 802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.4221\nEpoch 803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0490 - val_loss: 0.4202\nEpoch 804/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0489 - val_loss: 0.4183\nEpoch 805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4164\nEpoch 806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4144\nEpoch 807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0487 - val_loss: 0.4125\nEpoch 808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4106\nEpoch 809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.4085\nEpoch 810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4066\nEpoch 811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4047\nEpoch 812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0483 - val_loss: 0.4027\nEpoch 813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.4008\nEpoch 814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.3989\nEpoch 815/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.3970\nEpoch 816/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0480 - val_loss: 0.3949\nEpoch 817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3930\nEpoch 818/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0478 - val_loss: 0.3912\nEpoch 819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0478 - val_loss: 0.3894\nEpoch 820/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0477 - val_loss: 0.3874\nEpoch 821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3857\nEpoch 822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3841\nEpoch 823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3824\nEpoch 824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.3807\nEpoch 825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0473 - val_loss: 0.3787\nEpoch 826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3770\nEpoch 827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3750\nEpoch 828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3731\nEpoch 829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3715\nEpoch 830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3696\nEpoch 831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3677\nEpoch 832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0468 - val_loss: 0.3660\nEpoch 833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.3641\nEpoch 834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3624\nEpoch 835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3604\nEpoch 836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.3585\nEpoch 837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3564\nEpoch 838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3546\nEpoch 839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3528\nEpoch 840/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0462 - val_loss: 0.3510\nEpoch 841/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0461 - val_loss: 0.3492\nEpoch 842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0461 - val_loss: 0.3473\nEpoch 843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3457\nEpoch 844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3439\nEpoch 845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3422\nEpoch 846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0458 - val_loss: 0.3405\nEpoch 847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3388\nEpoch 848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3372\nEpoch 849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.3356\nEpoch 850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0455 - val_loss: 0.3339\nEpoch 851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0455 - val_loss: 0.3322\nEpoch 852/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0454 - val_loss: 0.3303\nEpoch 853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3286\nEpoch 854/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3270\nEpoch 855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3253\nEpoch 856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3237\nEpoch 857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3220\nEpoch 858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.3204\nEpoch 859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3187\nEpoch 860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3169\nEpoch 861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.3150\nEpoch 862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.3133\nEpoch 863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.3115\nEpoch 864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3098\nEpoch 865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3080\nEpoch 866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3063\nEpoch 867/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0444 - val_loss: 0.3048\nEpoch 868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.3032\nEpoch 869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.3017\nEpoch 870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.3002\nEpoch 871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2985\nEpoch 872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2970\nEpoch 873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2955\nEpoch 874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2940\nEpoch 875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.2925\nEpoch 876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2908\nEpoch 877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2892\nEpoch 878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.2877\nEpoch 879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 0.2861\nEpoch 880/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0436 - val_loss: 0.2846\nEpoch 881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2829\nEpoch 882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2812\nEpoch 883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2797\nEpoch 884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.2782\nEpoch 885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.2769\nEpoch 886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2753\nEpoch 887/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0432 - val_loss: 0.2739\nEpoch 888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2724\nEpoch 889/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2710\nEpoch 890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2694\nEpoch 891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2679\nEpoch 892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2663\nEpoch 893/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0428 - val_loss: 0.2647\nEpoch 894/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2633\nEpoch 895/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2617\nEpoch 896/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0426 - val_loss: 0.2602\nEpoch 897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2587\nEpoch 898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 0.2571\nEpoch 899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2558\nEpoch 900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2544\nEpoch 901/2000\n6/6 [==============================] - 0s 5ms/step - loss: 0.0423 - val_loss: 0.2529\nEpoch 902/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0423 - val_loss: 0.2514\nEpoch 903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2499\nEpoch 904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2484\nEpoch 905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2470\nEpoch 906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2456\nEpoch 907/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0420 - val_loss: 0.2441\nEpoch 908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2425\nEpoch 909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2410\nEpoch 910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2395\nEpoch 911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2381\nEpoch 912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2366\nEpoch 913/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2355\nEpoch 914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2341\nEpoch 915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2328\nEpoch 916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2314\nEpoch 917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2301\nEpoch 918/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0414 - val_loss: 0.2288\nEpoch 919/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0413 - val_loss: 0.2274\nEpoch 920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2262\nEpoch 921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2251\nEpoch 922/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0412 - val_loss: 0.2237\nEpoch 923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2223\nEpoch 924/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0411 - val_loss: 0.2210\nEpoch 925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2197\nEpoch 926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2184\nEpoch 927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2171\nEpoch 928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2157\nEpoch 929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2144\nEpoch 930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2131\nEpoch 931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2119\nEpoch 932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2106\nEpoch 933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2094\nEpoch 934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2080\nEpoch 935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2067\nEpoch 936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2056\nEpoch 937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2044\nEpoch 938/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0404 - val_loss: 0.2032\nEpoch 939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2021\nEpoch 940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2008\nEpoch 941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1997\nEpoch 942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1985\nEpoch 943/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0401 - val_loss: 0.1976\nEpoch 944/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0401 - val_loss: 0.1964\nEpoch 945/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0400 - val_loss: 0.1952\nEpoch 946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1941\nEpoch 947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1928\nEpoch 948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1916\nEpoch 949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1903\nEpoch 950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1892\nEpoch 951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1882\nEpoch 952/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0397 - val_loss: 0.1870\nEpoch 953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1859\nEpoch 954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1847\nEpoch 955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1835\nEpoch 956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1824\nEpoch 957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1814\nEpoch 958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1803\nEpoch 959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1794\nEpoch 960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1783\nEpoch 961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1771\nEpoch 962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1760\nEpoch 963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1749\nEpoch 964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1738\nEpoch 965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1726\nEpoch 966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1716\nEpoch 967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1705\nEpoch 968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1694\nEpoch 969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1685\nEpoch 970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1674\nEpoch 971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1662\nEpoch 972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1650\nEpoch 973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1639\nEpoch 974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1628\nEpoch 975/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1618\nEpoch 976/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0387 - val_loss: 0.1609\nEpoch 977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1600\nEpoch 978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1590\nEpoch 979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1579\nEpoch 980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1569\nEpoch 981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1560\nEpoch 982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1552\nEpoch 983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1543\nEpoch 984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1533\nEpoch 985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1523\nEpoch 986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1514\nEpoch 987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1503\nEpoch 988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1493\nEpoch 989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1484\nEpoch 990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1475\nEpoch 991/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0381 - val_loss: 0.1465\nEpoch 992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1457\nEpoch 993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1449\nEpoch 994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1439\nEpoch 995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1430\nEpoch 996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1420\nEpoch 997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1411\nEpoch 998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1401\nEpoch 999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1392\nEpoch 1000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1382\nEpoch 1001/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0378 - val_loss: 0.1374\nEpoch 1002/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1364\nEpoch 1003/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1357\nEpoch 1004/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1349\nEpoch 1005/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1341\nEpoch 1006/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1333\nEpoch 1007/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1325\nEpoch 1008/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1318\nEpoch 1009/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1311\nEpoch 1010/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1301\nEpoch 1011/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1293\nEpoch 1012/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1285\nEpoch 1013/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1277\nEpoch 1014/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1269\nEpoch 1015/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1262\nEpoch 1016/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1254\nEpoch 1017/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0373 - val_loss: 0.1246\nEpoch 1018/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1239\nEpoch 1019/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1231\nEpoch 1020/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1223\nEpoch 1021/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1216\nEpoch 1022/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1209\nEpoch 1023/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1202\nEpoch 1024/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1194\nEpoch 1025/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0371 - val_loss: 0.1187\nEpoch 1026/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1179\nEpoch 1027/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0370 - val_loss: 0.1170\nEpoch 1028/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1162\nEpoch 1029/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1155\nEpoch 1030/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1148\nEpoch 1031/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0369 - val_loss: 0.1141\nEpoch 1032/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0369 - val_loss: 0.1135\nEpoch 1033/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0368 - val_loss: 0.1128\nEpoch 1034/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1120\nEpoch 1035/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1114\nEpoch 1036/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1107\nEpoch 1037/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1102\nEpoch 1038/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1096\nEpoch 1039/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1090\nEpoch 1040/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1084\nEpoch 1041/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0366 - val_loss: 0.1077\nEpoch 1042/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1071\nEpoch 1043/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1065\nEpoch 1044/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1059\nEpoch 1045/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1052\nEpoch 1046/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0365 - val_loss: 0.1045\nEpoch 1047/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1039\nEpoch 1048/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1031\nEpoch 1049/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1025\nEpoch 1050/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1018\nEpoch 1051/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1013\nEpoch 1052/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1006\nEpoch 1053/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0999\nEpoch 1054/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0992\nEpoch 1055/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0985\nEpoch 1056/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0980\nEpoch 1057/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0362 - val_loss: 0.0974\nEpoch 1058/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0968\nEpoch 1059/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0962\nEpoch 1060/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0957\nEpoch 1061/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0950\nEpoch 1062/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0945\nEpoch 1063/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0939\nEpoch 1064/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0933\nEpoch 1065/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0926\nEpoch 1066/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0921\nEpoch 1067/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0916\nEpoch 1068/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0911\nEpoch 1069/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0905\nEpoch 1070/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0900\nEpoch 1071/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0895\nEpoch 1072/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0890\nEpoch 1073/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0885\nEpoch 1074/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0880\nEpoch 1075/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0876\nEpoch 1076/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0871\nEpoch 1077/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0865\nEpoch 1078/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0860\nEpoch 1079/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0857\nEpoch 1080/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.0852\nEpoch 1081/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0846\nEpoch 1082/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0842\nEpoch 1083/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0836\nEpoch 1084/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0830\nEpoch 1085/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0825\nEpoch 1086/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0821\nEpoch 1087/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0816\nEpoch 1088/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0811\nEpoch 1089/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0806\nEpoch 1090/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0801\nEpoch 1091/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0796\nEpoch 1092/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0792\nEpoch 1093/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0787\nEpoch 1094/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0782\nEpoch 1095/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0778\nEpoch 1096/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0774\nEpoch 1097/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0770\nEpoch 1098/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0766\nEpoch 1099/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0762\nEpoch 1100/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0759\nEpoch 1101/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0755\nEpoch 1102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0751\nEpoch 1103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0747\nEpoch 1104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0743\nEpoch 1105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0739\nEpoch 1106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0735\nEpoch 1107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0731\nEpoch 1108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0727\nEpoch 1109/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0724\nEpoch 1110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0721\nEpoch 1111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0717\nEpoch 1112/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0714\nEpoch 1113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0710\nEpoch 1114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0706\nEpoch 1115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0702\nEpoch 1116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0698\nEpoch 1117/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0352 - val_loss: 0.0694\nEpoch 1118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0690\nEpoch 1119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0686\nEpoch 1120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0683\nEpoch 1121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0679\nEpoch 1122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0675\nEpoch 1123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0673\nEpoch 1124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0669\nEpoch 1125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0664\nEpoch 1126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0660\nEpoch 1127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0656\nEpoch 1128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0652\nEpoch 1129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0649\nEpoch 1130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0646\nEpoch 1131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0643\nEpoch 1132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0640\nEpoch 1133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0637\nEpoch 1134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0634\nEpoch 1135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0631\nEpoch 1136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0628\nEpoch 1137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0626\nEpoch 1138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0623\nEpoch 1139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0620\nEpoch 1140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0617\nEpoch 1141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0615\nEpoch 1142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0613\nEpoch 1143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0610\nEpoch 1144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0607\nEpoch 1145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0603\nEpoch 1146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0601\nEpoch 1147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0598\nEpoch 1148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0596\nEpoch 1149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0594\nEpoch 1150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0591\nEpoch 1151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0588\nEpoch 1152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0585\nEpoch 1153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0582\nEpoch 1154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0581\nEpoch 1155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0579\nEpoch 1156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0576\nEpoch 1157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0574\nEpoch 1158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0572\nEpoch 1159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0570\nEpoch 1160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0568\nEpoch 1161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0566\nEpoch 1162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0564\nEpoch 1163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0563\nEpoch 1164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0560\nEpoch 1165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0558\nEpoch 1166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0556\nEpoch 1167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0553\nEpoch 1168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0551\nEpoch 1169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0549\nEpoch 1170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0548\nEpoch 1171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0546\nEpoch 1172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0544\nEpoch 1173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0542\nEpoch 1174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0540\nEpoch 1175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0538\nEpoch 1176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0536\nEpoch 1177/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0534\nEpoch 1178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0533\nEpoch 1179/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0531\nEpoch 1180/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0530\nEpoch 1181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0529\nEpoch 1182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0527\nEpoch 1183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0525\nEpoch 1184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0524\nEpoch 1185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0522\nEpoch 1186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0520\nEpoch 1187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0519\nEpoch 1188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0517\nEpoch 1189/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0516\nEpoch 1190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0514\nEpoch 1191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0512\nEpoch 1192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0510\nEpoch 1193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0508\nEpoch 1194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0508\nEpoch 1195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0506\nEpoch 1196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0504\nEpoch 1197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0502\nEpoch 1198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0501\nEpoch 1199/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0499\nEpoch 1200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0497\nEpoch 1201/2000\n6/6 [==============================] - 0s 5ms/step - loss: 0.0345 - val_loss: 0.0496\nEpoch 1202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0494\nEpoch 1203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0492\nEpoch 1204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0491\nEpoch 1205/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0490\nEpoch 1206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0489\nEpoch 1207/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0487\nEpoch 1208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0486\nEpoch 1209/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0483\nEpoch 1210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0482\nEpoch 1211/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0481\nEpoch 1212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0479\nEpoch 1213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0478\nEpoch 1214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0477\nEpoch 1215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0476\nEpoch 1216/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0475\nEpoch 1217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473\nEpoch 1218/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0472\nEpoch 1219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0471\nEpoch 1220/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0470\nEpoch 1221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0469\nEpoch 1222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0468\nEpoch 1223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0467\nEpoch 1224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0466\nEpoch 1225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0465\nEpoch 1226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0464\nEpoch 1227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463\nEpoch 1228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462\nEpoch 1229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0461\nEpoch 1230/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0460\nEpoch 1231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0459\nEpoch 1232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458\nEpoch 1233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456\nEpoch 1234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0455\nEpoch 1235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0453\nEpoch 1238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452\nEpoch 1239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0451\nEpoch 1240/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0450\nEpoch 1241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0449\nEpoch 1242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448\nEpoch 1243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0447\nEpoch 1244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446\nEpoch 1245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446\nEpoch 1246/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0444\nEpoch 1247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444\nEpoch 1248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443\nEpoch 1249/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443\nEpoch 1250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442\nEpoch 1251/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0441\nEpoch 1252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440\nEpoch 1253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439\nEpoch 1254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438\nEpoch 1255/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0437\nEpoch 1256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0436\nEpoch 1257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435\nEpoch 1258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433\nEpoch 1260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432\nEpoch 1261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0429\nEpoch 1266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1273/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424\nEpoch 1275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424\nEpoch 1276/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0420\nEpoch 1282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419\nEpoch 1283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419\nEpoch 1284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419\nEpoch 1285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419\nEpoch 1286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418\nEpoch 1287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418\nEpoch 1288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0417\nEpoch 1290/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416\nEpoch 1291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416\nEpoch 1292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1293/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413\nEpoch 1297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1332/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1336/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1338/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1339/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1355/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1360/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1363/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1370/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1380/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1400/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1401/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1405/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1408/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1409/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1413/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1424/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1429/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1438/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1458/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1459/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1461/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1463/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1473/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1474/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1479/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1484/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1485/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1486/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1497/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1500/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1505/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1507/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1512/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1513/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1514/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1521/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1522/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1527/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1528/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1532/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1538/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1551/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1578/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1593/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1594/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1605/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1611/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1612/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1617/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1626/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1630/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1631/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1637/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1641/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1645/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1647/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1651/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1659/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1663/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1684/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1685/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1687/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1694/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1697/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1701/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1711/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1715/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1720/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1721/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1726/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1735/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1739/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1758/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1770/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1771/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1773/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1776/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1783/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1786/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1799/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1800/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1801/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1815/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1838/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1840/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1842/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1845/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1850/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1852/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1854/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1857/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1880/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1887/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1889/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1901/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1907/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1913/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1919/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1922/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1933/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1938/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1945/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1952/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1953/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1975/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1980/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1989/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1995/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1998/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 2000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\n\n\n<keras.callbacks.History at 0x7f708335a740>\n\n\n\nnet.weights\n\n[<tf.Variable 'dense_9/kernel:0' shape=(5, 1) dtype=float32, numpy=\n array([[ 2.9966218e+00],\n        [ 1.0097879e+00],\n        [-1.4235497e-02],\n        [ 3.8510602e-04],\n        [ 4.8625717e-01]], dtype=float32)>,\n <tf.Variable 'dense_9/bias:0' shape=(1,) dtype=float32, numpy=array([-2.0080342], dtype=float32)>]\n\n\n\nnet.summary()\n\nModel: \"sequential_9\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_9 (Dense)             (None, 1)                 6         \n                                                                 \n=================================================================\nTotal params: 6\nTrainable params: 6\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n#\n#%tensorboard --logdir logs --host 0.0.0.0\n\n\n\n텐서보드: 사용자지정그림 에폭별로 시각화 (1)\n- 100에폭마다 적합결과를 시각화 하고 싶다 + 가중치와 같이!! - https://www.tensorflow.org/guide/keras/custom_callback\n- 일단 “100에폭마다 가중치적합과정 시각화 + 최종적합곡선 시각화” 까지 구현\n\n#collapse_output\n!rm -rf logs\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\ncb1= tf.keras.callbacks.TensorBoard(update_freq='epoch',histogram_freq=100)\nnet.fit(X,y,epochs=2000, batch_size=100, validation_split=0.45,callbacks=cb1)\n\nEpoch 1/2000\n6/6 [==============================] - 0s 11ms/step - loss: 6.6990 - val_loss: 14.1016\nEpoch 2/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.6442 - val_loss: 14.0394\nEpoch 3/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5913 - val_loss: 13.9775\nEpoch 4/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.5361 - val_loss: 13.9172\nEpoch 5/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4823 - val_loss: 13.8573\nEpoch 6/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.4314 - val_loss: 13.7958\nEpoch 7/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3769 - val_loss: 13.7367\nEpoch 8/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.3262 - val_loss: 13.6770\nEpoch 9/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2734 - val_loss: 13.6171\nEpoch 10/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.2212 - val_loss: 13.5591\nEpoch 11/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1701 - val_loss: 13.5023\nEpoch 12/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.1201 - val_loss: 13.4448\nEpoch 13/2000\n6/6 [==============================] - 0s 3ms/step - loss: 6.0705 - val_loss: 13.3874\nEpoch 14/2000\n6/6 [==============================] - 0s 4ms/step - loss: 6.0197 - val_loss: 13.3311\nEpoch 15/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9705 - val_loss: 13.2742\nEpoch 16/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.9195 - val_loss: 13.2184\nEpoch 17/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8713 - val_loss: 13.1627\nEpoch 18/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.8223 - val_loss: 13.1072\nEpoch 19/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7729 - val_loss: 13.0523\nEpoch 20/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.7259 - val_loss: 12.9976\nEpoch 21/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6771 - val_loss: 12.9449\nEpoch 22/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.6299 - val_loss: 12.8918\nEpoch 23/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5827 - val_loss: 12.8385\nEpoch 24/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.5371 - val_loss: 12.7853\nEpoch 25/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.4892 - val_loss: 12.7333\nEpoch 26/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.4440 - val_loss: 12.6799\nEpoch 27/2000\n6/6 [==============================] - 0s 4ms/step - loss: 5.3974 - val_loss: 12.6281\nEpoch 28/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3519 - val_loss: 12.5757\nEpoch 29/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.3069 - val_loss: 12.5253\nEpoch 30/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2628 - val_loss: 12.4748\nEpoch 31/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.2181 - val_loss: 12.4244\nEpoch 32/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1746 - val_loss: 12.3737\nEpoch 33/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.1311 - val_loss: 12.3234\nEpoch 34/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0871 - val_loss: 12.2756\nEpoch 35/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0448 - val_loss: 12.2277\nEpoch 36/2000\n6/6 [==============================] - 0s 3ms/step - loss: 5.0017 - val_loss: 12.1806\nEpoch 37/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.9595 - val_loss: 12.1327\nEpoch 38/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.9173 - val_loss: 12.0851\nEpoch 39/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8764 - val_loss: 12.0371\nEpoch 40/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.8342 - val_loss: 11.9906\nEpoch 41/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7936 - val_loss: 11.9438\nEpoch 42/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.7533 - val_loss: 11.8970\nEpoch 43/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.7121 - val_loss: 11.8512\nEpoch 44/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6723 - val_loss: 11.8053\nEpoch 45/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.6318 - val_loss: 11.7611\nEpoch 46/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5923 - val_loss: 11.7176\nEpoch 47/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5532 - val_loss: 11.6732\nEpoch 48/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.5130 - val_loss: 11.6284\nEpoch 49/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4748 - val_loss: 11.5833\nEpoch 50/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.4350 - val_loss: 11.5400\nEpoch 51/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3965 - val_loss: 11.4966\nEpoch 52/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3589 - val_loss: 11.4541\nEpoch 53/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.3203 - val_loss: 11.4124\nEpoch 54/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2825 - val_loss: 11.3700\nEpoch 55/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2451 - val_loss: 11.3271\nEpoch 56/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.2078 - val_loss: 11.2843\nEpoch 57/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1703 - val_loss: 11.2416\nEpoch 58/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.1330 - val_loss: 11.1995\nEpoch 59/2000\n6/6 [==============================] - 0s 4ms/step - loss: 4.0959 - val_loss: 11.1581\nEpoch 60/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0600 - val_loss: 11.1171\nEpoch 61/2000\n6/6 [==============================] - 0s 3ms/step - loss: 4.0237 - val_loss: 11.0773\nEpoch 62/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9879 - val_loss: 11.0384\nEpoch 63/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9532 - val_loss: 10.9997\nEpoch 64/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.9196 - val_loss: 10.9601\nEpoch 65/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.8845 - val_loss: 10.9219\nEpoch 66/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8504 - val_loss: 10.8832\nEpoch 67/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.8163 - val_loss: 10.8449\nEpoch 68/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.7821 - val_loss: 10.8072\nEpoch 69/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.7485 - val_loss: 10.7692\nEpoch 70/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.7154 - val_loss: 10.7316\nEpoch 71/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6823 - val_loss: 10.6955\nEpoch 72/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6498 - val_loss: 10.6591\nEpoch 73/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.6170 - val_loss: 10.6233\nEpoch 74/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5846 - val_loss: 10.5880\nEpoch 75/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5533 - val_loss: 10.5517\nEpoch 76/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.5209 - val_loss: 10.5146\nEpoch 77/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4886 - val_loss: 10.4796\nEpoch 78/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4579 - val_loss: 10.4437\nEpoch 79/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.4264 - val_loss: 10.4078\nEpoch 80/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3956 - val_loss: 10.3729\nEpoch 81/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3637 - val_loss: 10.3384\nEpoch 82/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.3335 - val_loss: 10.3034\nEpoch 83/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.3032 - val_loss: 10.2686\nEpoch 84/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2735 - val_loss: 10.2348\nEpoch 85/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2434 - val_loss: 10.2016\nEpoch 86/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.2150 - val_loss: 10.1684\nEpoch 87/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1850 - val_loss: 10.1359\nEpoch 88/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.1563 - val_loss: 10.1026\nEpoch 89/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.1270 - val_loss: 10.0714\nEpoch 90/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0990 - val_loss: 10.0392\nEpoch 91/2000\n6/6 [==============================] - 0s 4ms/step - loss: 3.0712 - val_loss: 10.0072\nEpoch 92/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0426 - val_loss: 9.9763\nEpoch 93/2000\n6/6 [==============================] - 0s 3ms/step - loss: 3.0153 - val_loss: 9.9454\nEpoch 94/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9878 - val_loss: 9.9147\nEpoch 95/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9603 - val_loss: 9.8845\nEpoch 96/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9336 - val_loss: 9.8549\nEpoch 97/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.9068 - val_loss: 9.8259\nEpoch 98/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8804 - val_loss: 9.7963\nEpoch 99/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8538 - val_loss: 9.7673\nEpoch 100/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.8276 - val_loss: 9.7382\nEpoch 101/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.8017 - val_loss: 9.7100\nEpoch 102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7755 - val_loss: 9.6811\nEpoch 103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7499 - val_loss: 9.6516\nEpoch 104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7244 - val_loss: 9.6223\nEpoch 105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6994 - val_loss: 9.5934\nEpoch 106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6744 - val_loss: 9.5652\nEpoch 107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6495 - val_loss: 9.5379\nEpoch 108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6246 - val_loss: 9.5108\nEpoch 109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6008 - val_loss: 9.4831\nEpoch 110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5761 - val_loss: 9.4555\nEpoch 111/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.5520 - val_loss: 9.4283\nEpoch 112/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.5287 - val_loss: 9.4010\nEpoch 113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5052 - val_loss: 9.3744\nEpoch 114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4813 - val_loss: 9.3473\nEpoch 115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4583 - val_loss: 9.3206\nEpoch 116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4353 - val_loss: 9.2952\nEpoch 117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4123 - val_loss: 9.2694\nEpoch 118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3897 - val_loss: 9.2431\nEpoch 119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3671 - val_loss: 9.2181\nEpoch 120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3451 - val_loss: 9.1919\nEpoch 121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3226 - val_loss: 9.1674\nEpoch 122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3013 - val_loss: 9.1414\nEpoch 123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2790 - val_loss: 9.1171\nEpoch 124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2575 - val_loss: 9.0930\nEpoch 125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2361 - val_loss: 9.0694\nEpoch 126/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.2154 - val_loss: 9.0448\nEpoch 127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1942 - val_loss: 9.0199\nEpoch 128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1733 - val_loss: 8.9960\nEpoch 129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1526 - val_loss: 8.9720\nEpoch 130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1320 - val_loss: 8.9488\nEpoch 131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1119 - val_loss: 8.9255\nEpoch 132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0918 - val_loss: 8.9022\nEpoch 133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0718 - val_loss: 8.8786\nEpoch 134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0517 - val_loss: 8.8560\nEpoch 135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0321 - val_loss: 8.8331\nEpoch 136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0127 - val_loss: 8.8099\nEpoch 137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9936 - val_loss: 8.7871\nEpoch 138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9746 - val_loss: 8.7652\nEpoch 139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9557 - val_loss: 8.7429\nEpoch 140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9368 - val_loss: 8.7217\nEpoch 141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9192 - val_loss: 8.7001\nEpoch 142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9003 - val_loss: 8.6779\nEpoch 143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8826 - val_loss: 8.6553\nEpoch 144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8645 - val_loss: 8.6343\nEpoch 145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8470 - val_loss: 8.6119\nEpoch 146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8295 - val_loss: 8.5907\nEpoch 147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8117 - val_loss: 8.5700\nEpoch 148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7949 - val_loss: 8.5491\nEpoch 149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7779 - val_loss: 8.5288\nEpoch 150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7607 - val_loss: 8.5087\nEpoch 151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7440 - val_loss: 8.4884\nEpoch 152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7277 - val_loss: 8.4686\nEpoch 153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7107 - val_loss: 8.4488\nEpoch 154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6944 - val_loss: 8.4297\nEpoch 155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6783 - val_loss: 8.4099\nEpoch 156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6626 - val_loss: 8.3904\nEpoch 157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6466 - val_loss: 8.3712\nEpoch 158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6305 - val_loss: 8.3510\nEpoch 159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6148 - val_loss: 8.3309\nEpoch 160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5992 - val_loss: 8.3106\nEpoch 161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5839 - val_loss: 8.2924\nEpoch 162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5687 - val_loss: 8.2737\nEpoch 163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5537 - val_loss: 8.2558\nEpoch 164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5393 - val_loss: 8.2368\nEpoch 165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5241 - val_loss: 8.2189\nEpoch 166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5098 - val_loss: 8.2008\nEpoch 167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4951 - val_loss: 8.1831\nEpoch 168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4808 - val_loss: 8.1653\nEpoch 169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4669 - val_loss: 8.1461\nEpoch 170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4523 - val_loss: 8.1289\nEpoch 171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4385 - val_loss: 8.1115\nEpoch 172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4247 - val_loss: 8.0941\nEpoch 173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4108 - val_loss: 8.0772\nEpoch 174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3972 - val_loss: 8.0592\nEpoch 175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3837 - val_loss: 8.0412\nEpoch 176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3700 - val_loss: 8.0239\nEpoch 177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3571 - val_loss: 8.0066\nEpoch 178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3440 - val_loss: 7.9895\nEpoch 179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3308 - val_loss: 7.9722\nEpoch 180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3181 - val_loss: 7.9550\nEpoch 181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3055 - val_loss: 7.9387\nEpoch 182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2926 - val_loss: 7.9229\nEpoch 183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2805 - val_loss: 7.9067\nEpoch 184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2681 - val_loss: 7.8900\nEpoch 185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2556 - val_loss: 7.8737\nEpoch 186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2437 - val_loss: 7.8576\nEpoch 187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2318 - val_loss: 7.8403\nEpoch 188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2196 - val_loss: 7.8245\nEpoch 189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2079 - val_loss: 7.8084\nEpoch 190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1965 - val_loss: 7.7924\nEpoch 191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1850 - val_loss: 7.7777\nEpoch 192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1735 - val_loss: 7.7618\nEpoch 193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1625 - val_loss: 7.7459\nEpoch 194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1512 - val_loss: 7.7310\nEpoch 195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1403 - val_loss: 7.7166\nEpoch 196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1294 - val_loss: 7.7019\nEpoch 197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1186 - val_loss: 7.6871\nEpoch 198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1078 - val_loss: 7.6719\nEpoch 199/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0973 - val_loss: 7.6572\nEpoch 200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0866 - val_loss: 7.6426\nEpoch 201/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.0762 - val_loss: 7.6278\nEpoch 202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0657 - val_loss: 7.6135\nEpoch 203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0558 - val_loss: 7.5977\nEpoch 204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0456 - val_loss: 7.5829\nEpoch 205/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.0357 - val_loss: 7.5681\nEpoch 206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0256 - val_loss: 7.5545\nEpoch 207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0160 - val_loss: 7.5397\nEpoch 208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0064 - val_loss: 7.5247\nEpoch 209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9968 - val_loss: 7.5098\nEpoch 210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9873 - val_loss: 7.4949\nEpoch 211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9781 - val_loss: 7.4804\nEpoch 212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9685 - val_loss: 7.4660\nEpoch 213/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9593 - val_loss: 7.4515\nEpoch 214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9503 - val_loss: 7.4373\nEpoch 215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9412 - val_loss: 7.4229\nEpoch 216/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9323 - val_loss: 7.4090\nEpoch 217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9237 - val_loss: 7.3950\nEpoch 218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9149 - val_loss: 7.3820\nEpoch 219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9065 - val_loss: 7.3686\nEpoch 220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8979 - val_loss: 7.3558\nEpoch 221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8896 - val_loss: 7.3422\nEpoch 222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8814 - val_loss: 7.3290\nEpoch 223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8731 - val_loss: 7.3155\nEpoch 224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8654 - val_loss: 7.3014\nEpoch 225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8572 - val_loss: 7.2881\nEpoch 226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8492 - val_loss: 7.2753\nEpoch 227/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8416 - val_loss: 7.2624\nEpoch 228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8338 - val_loss: 7.2492\nEpoch 229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8262 - val_loss: 7.2358\nEpoch 230/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8185 - val_loss: 7.2236\nEpoch 231/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8110 - val_loss: 7.2112\nEpoch 232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8036 - val_loss: 7.1979\nEpoch 233/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.7962 - val_loss: 7.1849\nEpoch 234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7890 - val_loss: 7.1720\nEpoch 235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7817 - val_loss: 7.1592\nEpoch 236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7746 - val_loss: 7.1468\nEpoch 237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7674 - val_loss: 7.1337\nEpoch 238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7605 - val_loss: 7.1196\nEpoch 239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7536 - val_loss: 7.1064\nEpoch 240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7467 - val_loss: 7.0940\nEpoch 241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7399 - val_loss: 7.0820\nEpoch 242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7333 - val_loss: 7.0690\nEpoch 243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7267 - val_loss: 7.0568\nEpoch 244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7202 - val_loss: 7.0443\nEpoch 245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7139 - val_loss: 7.0311\nEpoch 246/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7075 - val_loss: 7.0185\nEpoch 247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7012 - val_loss: 7.0054\nEpoch 248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6948 - val_loss: 6.9938\nEpoch 249/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6887 - val_loss: 6.9809\nEpoch 250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6827 - val_loss: 6.9690\nEpoch 251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6768 - val_loss: 6.9565\nEpoch 252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6709 - val_loss: 6.9443\nEpoch 253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6650 - val_loss: 6.9322\nEpoch 254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6590 - val_loss: 6.9206\nEpoch 255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6533 - val_loss: 6.9084\nEpoch 256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6477 - val_loss: 6.8959\nEpoch 257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6422 - val_loss: 6.8833\nEpoch 258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6366 - val_loss: 6.8716\nEpoch 259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6312 - val_loss: 6.8591\nEpoch 260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6258 - val_loss: 6.8477\nEpoch 261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6206 - val_loss: 6.8359\nEpoch 262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6153 - val_loss: 6.8237\nEpoch 263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6101 - val_loss: 6.8123\nEpoch 264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6050 - val_loss: 6.8006\nEpoch 265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6000 - val_loss: 6.7892\nEpoch 266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5949 - val_loss: 6.7776\nEpoch 267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5900 - val_loss: 6.7652\nEpoch 268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5849 - val_loss: 6.7529\nEpoch 269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5801 - val_loss: 6.7406\nEpoch 270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5753 - val_loss: 6.7281\nEpoch 271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5705 - val_loss: 6.7161\nEpoch 272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5658 - val_loss: 6.7042\nEpoch 273/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5612 - val_loss: 6.6921\nEpoch 274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5567 - val_loss: 6.6803\nEpoch 275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5521 - val_loss: 6.6690\nEpoch 276/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5478 - val_loss: 6.6569\nEpoch 277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5433 - val_loss: 6.6454\nEpoch 278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5391 - val_loss: 6.6337\nEpoch 279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5346 - val_loss: 6.6222\nEpoch 280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5306 - val_loss: 6.6105\nEpoch 281/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5264 - val_loss: 6.5989\nEpoch 282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5223 - val_loss: 6.5878\nEpoch 283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5183 - val_loss: 6.5764\nEpoch 284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5143 - val_loss: 6.5650\nEpoch 285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5103 - val_loss: 6.5537\nEpoch 286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5065 - val_loss: 6.5421\nEpoch 287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5025 - val_loss: 6.5305\nEpoch 288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4988 - val_loss: 6.5192\nEpoch 289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4951 - val_loss: 6.5072\nEpoch 290/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4914 - val_loss: 6.4958\nEpoch 291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4876 - val_loss: 6.4849\nEpoch 292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4840 - val_loss: 6.4732\nEpoch 293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4805 - val_loss: 6.4623\nEpoch 294/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4768 - val_loss: 6.4509\nEpoch 295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4733 - val_loss: 6.4391\nEpoch 296/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4698 - val_loss: 6.4276\nEpoch 297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4664 - val_loss: 6.4161\nEpoch 298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4630 - val_loss: 6.4040\nEpoch 299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4595 - val_loss: 6.3927\nEpoch 300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4562 - val_loss: 6.3814\nEpoch 301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4529 - val_loss: 6.3698\nEpoch 302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4497 - val_loss: 6.3581\nEpoch 303/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4463 - val_loss: 6.3466\nEpoch 304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4432 - val_loss: 6.3346\nEpoch 305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4402 - val_loss: 6.3230\nEpoch 306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4370 - val_loss: 6.3121\nEpoch 307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4340 - val_loss: 6.3007\nEpoch 308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4310 - val_loss: 6.2892\nEpoch 309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4280 - val_loss: 6.2780\nEpoch 310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4251 - val_loss: 6.2666\nEpoch 311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4222 - val_loss: 6.2554\nEpoch 312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4194 - val_loss: 6.2441\nEpoch 313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4165 - val_loss: 6.2326\nEpoch 314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4137 - val_loss: 6.2205\nEpoch 315/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4110 - val_loss: 6.2087\nEpoch 316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4082 - val_loss: 6.1973\nEpoch 317/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4056 - val_loss: 6.1859\nEpoch 318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4030 - val_loss: 6.1745\nEpoch 319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4004 - val_loss: 6.1624\nEpoch 320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3977 - val_loss: 6.1507\nEpoch 321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3952 - val_loss: 6.1391\nEpoch 322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3927 - val_loss: 6.1269\nEpoch 323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3902 - val_loss: 6.1151\nEpoch 324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3878 - val_loss: 6.1037\nEpoch 325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3854 - val_loss: 6.0923\nEpoch 326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3829 - val_loss: 6.0804\nEpoch 327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3805 - val_loss: 6.0692\nEpoch 328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3782 - val_loss: 6.0571\nEpoch 329/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3759 - val_loss: 6.0447\nEpoch 330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3735 - val_loss: 6.0329\nEpoch 331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3713 - val_loss: 6.0203\nEpoch 332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3690 - val_loss: 6.0089\nEpoch 333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3668 - val_loss: 5.9969\nEpoch 334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3645 - val_loss: 5.9852\nEpoch 335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3625 - val_loss: 5.9734\nEpoch 336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3603 - val_loss: 5.9614\nEpoch 337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3581 - val_loss: 5.9501\nEpoch 338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3561 - val_loss: 5.9386\nEpoch 339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3541 - val_loss: 5.9270\nEpoch 340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3521 - val_loss: 5.9156\nEpoch 341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3501 - val_loss: 5.9042\nEpoch 342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3481 - val_loss: 5.8924\nEpoch 343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3463 - val_loss: 5.8808\nEpoch 344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3442 - val_loss: 5.8688\nEpoch 345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3424 - val_loss: 5.8569\nEpoch 346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3405 - val_loss: 5.8452\nEpoch 347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3386 - val_loss: 5.8337\nEpoch 348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3369 - val_loss: 5.8213\nEpoch 349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3350 - val_loss: 5.8097\nEpoch 350/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3332 - val_loss: 5.7979\nEpoch 351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3315 - val_loss: 5.7865\nEpoch 352/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3297 - val_loss: 5.7747\nEpoch 353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3280 - val_loss: 5.7631\nEpoch 354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3263 - val_loss: 5.7515\nEpoch 355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3246 - val_loss: 5.7400\nEpoch 356/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3230 - val_loss: 5.7283\nEpoch 357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3213 - val_loss: 5.7165\nEpoch 358/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3197 - val_loss: 5.7047\nEpoch 359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3181 - val_loss: 5.6931\nEpoch 360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3165 - val_loss: 5.6812\nEpoch 361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3150 - val_loss: 5.6690\nEpoch 362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3134 - val_loss: 5.6579\nEpoch 363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3118 - val_loss: 5.6461\nEpoch 364/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3104 - val_loss: 5.6340\nEpoch 365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3089 - val_loss: 5.6223\nEpoch 366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3074 - val_loss: 5.6100\nEpoch 367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3060 - val_loss: 5.5979\nEpoch 368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3045 - val_loss: 5.5858\nEpoch 369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3030 - val_loss: 5.5742\nEpoch 370/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3016 - val_loss: 5.5622\nEpoch 371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3002 - val_loss: 5.5502\nEpoch 372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2989 - val_loss: 5.5378\nEpoch 373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2975 - val_loss: 5.5254\nEpoch 374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2961 - val_loss: 5.5131\nEpoch 375/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2948 - val_loss: 5.5012\nEpoch 376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2936 - val_loss: 5.4891\nEpoch 377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2923 - val_loss: 5.4766\nEpoch 378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2910 - val_loss: 5.4642\nEpoch 379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2897 - val_loss: 5.4516\nEpoch 380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2885 - val_loss: 5.4399\nEpoch 381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2872 - val_loss: 5.4283\nEpoch 382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2860 - val_loss: 5.4164\nEpoch 383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2848 - val_loss: 5.4039\nEpoch 384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2836 - val_loss: 5.3920\nEpoch 385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2824 - val_loss: 5.3798\nEpoch 386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2812 - val_loss: 5.3679\nEpoch 387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2801 - val_loss: 5.3552\nEpoch 388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2790 - val_loss: 5.3430\nEpoch 389/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2778 - val_loss: 5.3307\nEpoch 390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2767 - val_loss: 5.3186\nEpoch 391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2756 - val_loss: 5.3061\nEpoch 392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2745 - val_loss: 5.2937\nEpoch 393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2734 - val_loss: 5.2816\nEpoch 394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2724 - val_loss: 5.2698\nEpoch 395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2713 - val_loss: 5.2575\nEpoch 396/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2702 - val_loss: 5.2452\nEpoch 397/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.2692 - val_loss: 5.2331\nEpoch 398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2681 - val_loss: 5.2213\nEpoch 399/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2671 - val_loss: 5.2088\nEpoch 400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2661 - val_loss: 5.1963\nEpoch 401/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2651 - val_loss: 5.1842\nEpoch 402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2641 - val_loss: 5.1719\nEpoch 403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2631 - val_loss: 5.1593\nEpoch 404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2620 - val_loss: 5.1472\nEpoch 405/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2611 - val_loss: 5.1346\nEpoch 406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2601 - val_loss: 5.1224\nEpoch 407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2592 - val_loss: 5.1101\nEpoch 408/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2583 - val_loss: 5.0975\nEpoch 409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2573 - val_loss: 5.0852\nEpoch 410/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2564 - val_loss: 5.0724\nEpoch 411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2555 - val_loss: 5.0598\nEpoch 412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2546 - val_loss: 5.0475\nEpoch 413/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2536 - val_loss: 5.0351\nEpoch 414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2528 - val_loss: 5.0226\nEpoch 415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2519 - val_loss: 5.0102\nEpoch 416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2510 - val_loss: 4.9976\nEpoch 417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2501 - val_loss: 4.9857\nEpoch 418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2492 - val_loss: 4.9732\nEpoch 419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2483 - val_loss: 4.9606\nEpoch 420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2475 - val_loss: 4.9481\nEpoch 421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2466 - val_loss: 4.9357\nEpoch 422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2458 - val_loss: 4.9231\nEpoch 423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2450 - val_loss: 4.9108\nEpoch 424/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2442 - val_loss: 4.8982\nEpoch 425/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.2433 - val_loss: 4.8859\nEpoch 426/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.2425 - val_loss: 4.8733\nEpoch 427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2417 - val_loss: 4.8607\nEpoch 428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2409 - val_loss: 4.8482\nEpoch 429/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2402 - val_loss: 4.8354\nEpoch 430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2393 - val_loss: 4.8233\nEpoch 431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2386 - val_loss: 4.8107\nEpoch 432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2378 - val_loss: 4.7986\nEpoch 433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2371 - val_loss: 4.7858\nEpoch 434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2363 - val_loss: 4.7728\nEpoch 435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2355 - val_loss: 4.7603\nEpoch 436/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2347 - val_loss: 4.7470\nEpoch 437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2340 - val_loss: 4.7336\nEpoch 438/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2331 - val_loss: 4.7212\nEpoch 439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2324 - val_loss: 4.7081\nEpoch 440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2317 - val_loss: 4.6952\nEpoch 441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2309 - val_loss: 4.6824\nEpoch 442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2301 - val_loss: 4.6697\nEpoch 443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2294 - val_loss: 4.6565\nEpoch 444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2286 - val_loss: 4.6438\nEpoch 445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2280 - val_loss: 4.6312\nEpoch 446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2272 - val_loss: 4.6187\nEpoch 447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2265 - val_loss: 4.6062\nEpoch 448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2258 - val_loss: 4.5933\nEpoch 449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2251 - val_loss: 4.5809\nEpoch 450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2244 - val_loss: 4.5682\nEpoch 451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2237 - val_loss: 4.5557\nEpoch 452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2230 - val_loss: 4.5432\nEpoch 453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2223 - val_loss: 4.5304\nEpoch 454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2216 - val_loss: 4.5172\nEpoch 455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2209 - val_loss: 4.5045\nEpoch 456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2203 - val_loss: 4.4913\nEpoch 457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2196 - val_loss: 4.4780\nEpoch 458/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2189 - val_loss: 4.4653\nEpoch 459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2182 - val_loss: 4.4529\nEpoch 460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2176 - val_loss: 4.4401\nEpoch 461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2169 - val_loss: 4.4275\nEpoch 462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2163 - val_loss: 4.4144\nEpoch 463/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2156 - val_loss: 4.4018\nEpoch 464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2150 - val_loss: 4.3893\nEpoch 465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2143 - val_loss: 4.3762\nEpoch 466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2137 - val_loss: 4.3634\nEpoch 467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2131 - val_loss: 4.3504\nEpoch 468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2125 - val_loss: 4.3378\nEpoch 469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2118 - val_loss: 4.3251\nEpoch 470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2112 - val_loss: 4.3126\nEpoch 471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2106 - val_loss: 4.2997\nEpoch 472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2100 - val_loss: 4.2867\nEpoch 473/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2093 - val_loss: 4.2739\nEpoch 474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2086 - val_loss: 4.2609\nEpoch 475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2080 - val_loss: 4.2479\nEpoch 476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2074 - val_loss: 4.2348\nEpoch 477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2068 - val_loss: 4.2221\nEpoch 478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2062 - val_loss: 4.2093\nEpoch 479/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2056 - val_loss: 4.1964\nEpoch 480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2050 - val_loss: 4.1831\nEpoch 481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2044 - val_loss: 4.1703\nEpoch 482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2038 - val_loss: 4.1576\nEpoch 483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2031 - val_loss: 4.1450\nEpoch 484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2025 - val_loss: 4.1318\nEpoch 485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2019 - val_loss: 4.1189\nEpoch 486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2013 - val_loss: 4.1055\nEpoch 487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2007 - val_loss: 4.0927\nEpoch 488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2001 - val_loss: 4.0799\nEpoch 489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1995 - val_loss: 4.0670\nEpoch 490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1989 - val_loss: 4.0543\nEpoch 491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1983 - val_loss: 4.0412\nEpoch 492/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1977 - val_loss: 4.0288\nEpoch 493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1971 - val_loss: 4.0160\nEpoch 494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1965 - val_loss: 4.0029\nEpoch 495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1960 - val_loss: 3.9893\nEpoch 496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1954 - val_loss: 3.9760\nEpoch 497/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1948 - val_loss: 3.9628\nEpoch 498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1942 - val_loss: 3.9498\nEpoch 499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1936 - val_loss: 3.9373\nEpoch 500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1931 - val_loss: 3.9246\nEpoch 501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1925 - val_loss: 3.9113\nEpoch 502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1919 - val_loss: 3.8987\nEpoch 503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1914 - val_loss: 3.8862\nEpoch 504/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1907 - val_loss: 3.8739\nEpoch 505/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1901 - val_loss: 3.8608\nEpoch 506/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1896 - val_loss: 3.8478\nEpoch 507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1889 - val_loss: 3.8350\nEpoch 508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1884 - val_loss: 3.8218\nEpoch 509/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1878 - val_loss: 3.8091\nEpoch 510/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1872 - val_loss: 3.7960\nEpoch 511/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1867 - val_loss: 3.7827\nEpoch 512/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1861 - val_loss: 3.7695\nEpoch 513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1855 - val_loss: 3.7564\nEpoch 514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1849 - val_loss: 3.7437\nEpoch 515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1843 - val_loss: 3.7308\nEpoch 516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1837 - val_loss: 3.7178\nEpoch 517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1832 - val_loss: 3.7048\nEpoch 518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1826 - val_loss: 3.6917\nEpoch 519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1821 - val_loss: 3.6781\nEpoch 520/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1815 - val_loss: 3.6648\nEpoch 521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1809 - val_loss: 3.6515\nEpoch 522/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1804 - val_loss: 3.6382\nEpoch 523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1799 - val_loss: 3.6252\nEpoch 524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1793 - val_loss: 3.6128\nEpoch 525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1787 - val_loss: 3.5998\nEpoch 526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1781 - val_loss: 3.5868\nEpoch 527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1776 - val_loss: 3.5735\nEpoch 528/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1770 - val_loss: 3.5601\nEpoch 529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1765 - val_loss: 3.5467\nEpoch 530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1759 - val_loss: 3.5337\nEpoch 531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1753 - val_loss: 3.5207\nEpoch 532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1748 - val_loss: 3.5074\nEpoch 533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1743 - val_loss: 3.4943\nEpoch 534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1737 - val_loss: 3.4814\nEpoch 535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1731 - val_loss: 3.4686\nEpoch 536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1726 - val_loss: 3.4557\nEpoch 537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1721 - val_loss: 3.4433\nEpoch 538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1715 - val_loss: 3.4307\nEpoch 539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1710 - val_loss: 3.4174\nEpoch 540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1704 - val_loss: 3.4044\nEpoch 541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1699 - val_loss: 3.3917\nEpoch 542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1693 - val_loss: 3.3786\nEpoch 543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1688 - val_loss: 3.3658\nEpoch 544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1683 - val_loss: 3.3522\nEpoch 545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1677 - val_loss: 3.3393\nEpoch 546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1672 - val_loss: 3.3264\nEpoch 547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1667 - val_loss: 3.3137\nEpoch 548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1661 - val_loss: 3.3012\nEpoch 549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1656 - val_loss: 3.2885\nEpoch 550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1651 - val_loss: 3.2761\nEpoch 551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1646 - val_loss: 3.2636\nEpoch 552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1641 - val_loss: 3.2506\nEpoch 553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1635 - val_loss: 3.2386\nEpoch 554/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1630 - val_loss: 3.2257\nEpoch 555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1625 - val_loss: 3.2130\nEpoch 556/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1619 - val_loss: 3.2007\nEpoch 557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1614 - val_loss: 3.1881\nEpoch 558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1609 - val_loss: 3.1755\nEpoch 559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1604 - val_loss: 3.1625\nEpoch 560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1598 - val_loss: 3.1501\nEpoch 561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 3.1372\nEpoch 562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1588 - val_loss: 3.1248\nEpoch 563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1582 - val_loss: 3.1119\nEpoch 564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1577 - val_loss: 3.0991\nEpoch 565/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1572 - val_loss: 3.0867\nEpoch 566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1567 - val_loss: 3.0744\nEpoch 567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1562 - val_loss: 3.0622\nEpoch 568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1557 - val_loss: 3.0497\nEpoch 569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1552 - val_loss: 3.0377\nEpoch 570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1547 - val_loss: 3.0254\nEpoch 571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1541 - val_loss: 3.0136\nEpoch 572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1536 - val_loss: 3.0013\nEpoch 573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1531 - val_loss: 2.9889\nEpoch 574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1526 - val_loss: 2.9764\nEpoch 575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1521 - val_loss: 2.9640\nEpoch 576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1516 - val_loss: 2.9517\nEpoch 577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1511 - val_loss: 2.9392\nEpoch 578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1505 - val_loss: 2.9271\nEpoch 579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1500 - val_loss: 2.9147\nEpoch 580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1495 - val_loss: 2.9022\nEpoch 581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1490 - val_loss: 2.8904\nEpoch 582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1485 - val_loss: 2.8783\nEpoch 583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1480 - val_loss: 2.8657\nEpoch 584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1475 - val_loss: 2.8539\nEpoch 585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1470 - val_loss: 2.8414\nEpoch 586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1465 - val_loss: 2.8292\nEpoch 587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1460 - val_loss: 2.8170\nEpoch 588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1455 - val_loss: 2.8047\nEpoch 589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1451 - val_loss: 2.7925\nEpoch 590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1445 - val_loss: 2.7802\nEpoch 591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1440 - val_loss: 2.7680\nEpoch 592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1435 - val_loss: 2.7559\nEpoch 593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1430 - val_loss: 2.7437\nEpoch 594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1426 - val_loss: 2.7318\nEpoch 595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1420 - val_loss: 2.7202\nEpoch 596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1415 - val_loss: 2.7084\nEpoch 597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1411 - val_loss: 2.6963\nEpoch 598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1405 - val_loss: 2.6843\nEpoch 599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1400 - val_loss: 2.6728\nEpoch 600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1396 - val_loss: 2.6609\nEpoch 601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1391 - val_loss: 2.6491\nEpoch 602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1386 - val_loss: 2.6370\nEpoch 603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1381 - val_loss: 2.6249\nEpoch 604/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1376 - val_loss: 2.6129\nEpoch 605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1371 - val_loss: 2.6011\nEpoch 606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1366 - val_loss: 2.5889\nEpoch 607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1361 - val_loss: 2.5768\nEpoch 608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1356 - val_loss: 2.5650\nEpoch 609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1351 - val_loss: 2.5529\nEpoch 610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1347 - val_loss: 2.5414\nEpoch 611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1342 - val_loss: 2.5297\nEpoch 612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1337 - val_loss: 2.5178\nEpoch 613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1332 - val_loss: 2.5065\nEpoch 614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1327 - val_loss: 2.4948\nEpoch 615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1323 - val_loss: 2.4828\nEpoch 616/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1318 - val_loss: 2.4708\nEpoch 617/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1313 - val_loss: 2.4591\nEpoch 618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1308 - val_loss: 2.4475\nEpoch 619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1304 - val_loss: 2.4358\nEpoch 620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1299 - val_loss: 2.4243\nEpoch 621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1294 - val_loss: 2.4127\nEpoch 622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1290 - val_loss: 2.4011\nEpoch 623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1285 - val_loss: 2.3899\nEpoch 624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1280 - val_loss: 2.3783\nEpoch 625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1275 - val_loss: 2.3666\nEpoch 626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1271 - val_loss: 2.3553\nEpoch 627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1266 - val_loss: 2.3438\nEpoch 628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1262 - val_loss: 2.3323\nEpoch 629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1258 - val_loss: 2.3208\nEpoch 630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 2.3097\nEpoch 631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 2.2984\nEpoch 632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 2.2874\nEpoch 633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1239 - val_loss: 2.2767\nEpoch 634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 2.2654\nEpoch 635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1230 - val_loss: 2.2545\nEpoch 636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1225 - val_loss: 2.2435\nEpoch 637/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.1221 - val_loss: 2.2321\nEpoch 638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1217 - val_loss: 2.2217\nEpoch 639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1213 - val_loss: 2.2106\nEpoch 640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1208 - val_loss: 2.2000\nEpoch 641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1204 - val_loss: 2.1890\nEpoch 642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1199 - val_loss: 2.1783\nEpoch 643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1194 - val_loss: 2.1677\nEpoch 644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1190 - val_loss: 2.1568\nEpoch 645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1186 - val_loss: 2.1462\nEpoch 646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1182 - val_loss: 2.1353\nEpoch 647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1177 - val_loss: 2.1245\nEpoch 648/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1173 - val_loss: 2.1141\nEpoch 649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1168 - val_loss: 2.1036\nEpoch 650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1164 - val_loss: 2.0928\nEpoch 651/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1160 - val_loss: 2.0823\nEpoch 652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1155 - val_loss: 2.0714\nEpoch 653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1151 - val_loss: 2.0609\nEpoch 654/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1147 - val_loss: 2.0502\nEpoch 655/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1143 - val_loss: 2.0392\nEpoch 656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1138 - val_loss: 2.0286\nEpoch 657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1134 - val_loss: 2.0182\nEpoch 658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1130 - val_loss: 2.0077\nEpoch 659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1125 - val_loss: 1.9972\nEpoch 660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1121 - val_loss: 1.9869\nEpoch 661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1117 - val_loss: 1.9767\nEpoch 662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1113 - val_loss: 1.9659\nEpoch 663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1109 - val_loss: 1.9554\nEpoch 664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1104 - val_loss: 1.9449\nEpoch 665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1100 - val_loss: 1.9350\nEpoch 666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1096 - val_loss: 1.9250\nEpoch 667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1091 - val_loss: 1.9149\nEpoch 668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1087 - val_loss: 1.9043\nEpoch 669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 1.8937\nEpoch 670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1079 - val_loss: 1.8833\nEpoch 671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1075 - val_loss: 1.8730\nEpoch 672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1071 - val_loss: 1.8626\nEpoch 673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1066 - val_loss: 1.8525\nEpoch 674/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1062 - val_loss: 1.8425\nEpoch 675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1058 - val_loss: 1.8325\nEpoch 676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1054 - val_loss: 1.8222\nEpoch 677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1050 - val_loss: 1.8121\nEpoch 678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1045 - val_loss: 1.8023\nEpoch 679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1042 - val_loss: 1.7921\nEpoch 680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1038 - val_loss: 1.7819\nEpoch 681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1033 - val_loss: 1.7721\nEpoch 682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1029 - val_loss: 1.7620\nEpoch 683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1025 - val_loss: 1.7520\nEpoch 684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1021 - val_loss: 1.7421\nEpoch 685/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1018 - val_loss: 1.7322\nEpoch 686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 1.7224\nEpoch 687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1010 - val_loss: 1.7127\nEpoch 688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1006 - val_loss: 1.7031\nEpoch 689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1002 - val_loss: 1.6937\nEpoch 690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0998 - val_loss: 1.6841\nEpoch 691/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0994 - val_loss: 1.6741\nEpoch 692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0990 - val_loss: 1.6640\nEpoch 693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0986 - val_loss: 1.6543\nEpoch 694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0982 - val_loss: 1.6454\nEpoch 695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0979 - val_loss: 1.6361\nEpoch 696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0975 - val_loss: 1.6266\nEpoch 697/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0971 - val_loss: 1.6170\nEpoch 698/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 1.6073\nEpoch 699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0963 - val_loss: 1.5979\nEpoch 700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0960 - val_loss: 1.5886\nEpoch 701/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0956 - val_loss: 1.5796\nEpoch 702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0952 - val_loss: 1.5706\nEpoch 703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0949 - val_loss: 1.5613\nEpoch 704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0945 - val_loss: 1.5523\nEpoch 705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0941 - val_loss: 1.5429\nEpoch 706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0938 - val_loss: 1.5336\nEpoch 707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0934 - val_loss: 1.5247\nEpoch 708/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0930 - val_loss: 1.5156\nEpoch 709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0926 - val_loss: 1.5068\nEpoch 710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0923 - val_loss: 1.4980\nEpoch 711/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0919 - val_loss: 1.4888\nEpoch 712/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0915 - val_loss: 1.4796\nEpoch 713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0912 - val_loss: 1.4702\nEpoch 714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0908 - val_loss: 1.4612\nEpoch 715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0904 - val_loss: 1.4523\nEpoch 716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 1.4433\nEpoch 717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0897 - val_loss: 1.4345\nEpoch 718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0894 - val_loss: 1.4258\nEpoch 719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0890 - val_loss: 1.4173\nEpoch 720/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0886 - val_loss: 1.4085\nEpoch 721/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0883 - val_loss: 1.3997\nEpoch 722/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0879 - val_loss: 1.3913\nEpoch 723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0876 - val_loss: 1.3827\nEpoch 724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0873 - val_loss: 1.3742\nEpoch 725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0869 - val_loss: 1.3656\nEpoch 726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0866 - val_loss: 1.3571\nEpoch 727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0862 - val_loss: 1.3485\nEpoch 728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0859 - val_loss: 1.3402\nEpoch 729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0856 - val_loss: 1.3317\nEpoch 730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0852 - val_loss: 1.3236\nEpoch 731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0849 - val_loss: 1.3152\nEpoch 732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0845 - val_loss: 1.3069\nEpoch 733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0842 - val_loss: 1.2986\nEpoch 734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0839 - val_loss: 1.2901\nEpoch 735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0835 - val_loss: 1.2821\nEpoch 736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0832 - val_loss: 1.2735\nEpoch 737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0828 - val_loss: 1.2655\nEpoch 738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0825 - val_loss: 1.2574\nEpoch 739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0822 - val_loss: 1.2491\nEpoch 740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0819 - val_loss: 1.2410\nEpoch 741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0815 - val_loss: 1.2326\nEpoch 742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0812 - val_loss: 1.2245\nEpoch 743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0809 - val_loss: 1.2162\nEpoch 744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0806 - val_loss: 1.2082\nEpoch 745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0802 - val_loss: 1.2005\nEpoch 746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0799 - val_loss: 1.1927\nEpoch 747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0796 - val_loss: 1.1848\nEpoch 748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0793 - val_loss: 1.1771\nEpoch 749/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0790 - val_loss: 1.1692\nEpoch 750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0787 - val_loss: 1.1613\nEpoch 751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0784 - val_loss: 1.1534\nEpoch 752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0780 - val_loss: 1.1456\nEpoch 753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0778 - val_loss: 1.1380\nEpoch 754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0774 - val_loss: 1.1301\nEpoch 755/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0771 - val_loss: 1.1222\nEpoch 756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0768 - val_loss: 1.1146\nEpoch 757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0765 - val_loss: 1.1073\nEpoch 758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0762 - val_loss: 1.0998\nEpoch 759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0759 - val_loss: 1.0928\nEpoch 760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0756 - val_loss: 1.0854\nEpoch 761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0754 - val_loss: 1.0781\nEpoch 762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 1.0707\nEpoch 763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 1.0630\nEpoch 764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0745 - val_loss: 1.0560\nEpoch 765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0742 - val_loss: 1.0486\nEpoch 766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0739 - val_loss: 1.0412\nEpoch 767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0736 - val_loss: 1.0341\nEpoch 768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 1.0270\nEpoch 769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0730 - val_loss: 1.0200\nEpoch 770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0727 - val_loss: 1.0131\nEpoch 771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0725 - val_loss: 1.0066\nEpoch 772/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0722 - val_loss: 0.9998\nEpoch 773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.9929\nEpoch 774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0716 - val_loss: 0.9858\nEpoch 775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0713 - val_loss: 0.9790\nEpoch 776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0711 - val_loss: 0.9719\nEpoch 777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0708 - val_loss: 0.9653\nEpoch 778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0705 - val_loss: 0.9583\nEpoch 779/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0702 - val_loss: 0.9515\nEpoch 780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.9449\nEpoch 781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.9380\nEpoch 782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0694 - val_loss: 0.9312\nEpoch 783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0692 - val_loss: 0.9246\nEpoch 784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0689 - val_loss: 0.9182\nEpoch 785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0686 - val_loss: 0.9118\nEpoch 786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0684 - val_loss: 0.9054\nEpoch 787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8986\nEpoch 788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0679 - val_loss: 0.8915\nEpoch 789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0676 - val_loss: 0.8849\nEpoch 790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0673 - val_loss: 0.8786\nEpoch 791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0671 - val_loss: 0.8724\nEpoch 792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.8663\nEpoch 793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0666 - val_loss: 0.8602\nEpoch 794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0663 - val_loss: 0.8539\nEpoch 795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0661 - val_loss: 0.8475\nEpoch 796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.8416\nEpoch 797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.8354\nEpoch 798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.8295\nEpoch 799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.8233\nEpoch 800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0648 - val_loss: 0.8172\nEpoch 801/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0646 - val_loss: 0.8115\nEpoch 802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0644 - val_loss: 0.8055\nEpoch 803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.7994\nEpoch 804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7937\nEpoch 805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.7877\nEpoch 806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0634 - val_loss: 0.7819\nEpoch 807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.7761\nEpoch 808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0629 - val_loss: 0.7702\nEpoch 809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.7640\nEpoch 810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0625 - val_loss: 0.7584\nEpoch 811/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0622 - val_loss: 0.7529\nEpoch 812/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0620 - val_loss: 0.7469\nEpoch 813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.7411\nEpoch 814/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0615 - val_loss: 0.7350\nEpoch 815/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0613 - val_loss: 0.7292\nEpoch 816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.7238\nEpoch 817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.7180\nEpoch 818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0606 - val_loss: 0.7123\nEpoch 819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0604 - val_loss: 0.7069\nEpoch 820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0601 - val_loss: 0.7016\nEpoch 821/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0599 - val_loss: 0.6959\nEpoch 822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.6902\nEpoch 823/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0595 - val_loss: 0.6845\nEpoch 824/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.6790\nEpoch 825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6735\nEpoch 826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.6681\nEpoch 827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0586 - val_loss: 0.6627\nEpoch 828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 0.6575\nEpoch 829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0582 - val_loss: 0.6524\nEpoch 830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0580 - val_loss: 0.6472\nEpoch 831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.6421\nEpoch 832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0576 - val_loss: 0.6370\nEpoch 833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6318\nEpoch 834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0572 - val_loss: 0.6265\nEpoch 835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.6211\nEpoch 836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0567 - val_loss: 0.6162\nEpoch 837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0565 - val_loss: 0.6111\nEpoch 838/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0563 - val_loss: 0.6061\nEpoch 839/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0561 - val_loss: 0.6013\nEpoch 840/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0559 - val_loss: 0.5965\nEpoch 841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5917\nEpoch 842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0556 - val_loss: 0.5869\nEpoch 843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0554 - val_loss: 0.5819\nEpoch 844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0552 - val_loss: 0.5771\nEpoch 845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.5722\nEpoch 846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0548 - val_loss: 0.5672\nEpoch 847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0546 - val_loss: 0.5627\nEpoch 848/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0544 - val_loss: 0.5580\nEpoch 849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5534\nEpoch 850/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0540 - val_loss: 0.5487\nEpoch 851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0539 - val_loss: 0.5441\nEpoch 852/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0537 - val_loss: 0.5395\nEpoch 853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.5350\nEpoch 854/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0533 - val_loss: 0.5304\nEpoch 855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.5259\nEpoch 856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5214\nEpoch 857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0528 - val_loss: 0.5170\nEpoch 858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5125\nEpoch 859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5082\nEpoch 860/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0522 - val_loss: 0.5039\nEpoch 861/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0521 - val_loss: 0.4995\nEpoch 862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.4952\nEpoch 863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4909\nEpoch 864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.4865\nEpoch 865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4823\nEpoch 866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4780\nEpoch 867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.4738\nEpoch 868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4696\nEpoch 869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0507 - val_loss: 0.4655\nEpoch 870/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0505 - val_loss: 0.4615\nEpoch 871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4575\nEpoch 872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0502 - val_loss: 0.4533\nEpoch 873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4495\nEpoch 874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.4459\nEpoch 875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.4420\nEpoch 876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0496 - val_loss: 0.4379\nEpoch 877/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0494 - val_loss: 0.4340\nEpoch 878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.4303\nEpoch 879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4264\nEpoch 880/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0490 - val_loss: 0.4226\nEpoch 881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0489 - val_loss: 0.4190\nEpoch 882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0487 - val_loss: 0.4154\nEpoch 883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4118\nEpoch 884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4082\nEpoch 885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0483 - val_loss: 0.4049\nEpoch 886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.4012\nEpoch 887/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.3976\nEpoch 888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3939\nEpoch 889/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0477 - val_loss: 0.3903\nEpoch 890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3869\nEpoch 891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.3831\nEpoch 892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0473 - val_loss: 0.3796\nEpoch 893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3762\nEpoch 894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3729\nEpoch 895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3695\nEpoch 896/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0467 - val_loss: 0.3662\nEpoch 897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3629\nEpoch 898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.3596\nEpoch 899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3563\nEpoch 900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0462 - val_loss: 0.3530\nEpoch 901/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0461 - val_loss: 0.3499\nEpoch 902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3468\nEpoch 903/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0458 - val_loss: 0.3435\nEpoch 904/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0457 - val_loss: 0.3403\nEpoch 905/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0456 - val_loss: 0.3371\nEpoch 906/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0455 - val_loss: 0.3340\nEpoch 907/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3310\nEpoch 908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3280\nEpoch 909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3251\nEpoch 910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.3219\nEpoch 911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3189\nEpoch 912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.3159\nEpoch 913/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3130\nEpoch 914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3100\nEpoch 915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.3072\nEpoch 916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.3044\nEpoch 917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.3015\nEpoch 918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2987\nEpoch 919/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0440 - val_loss: 0.2960\nEpoch 920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.2933\nEpoch 921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2905\nEpoch 922/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0437 - val_loss: 0.2878\nEpoch 923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2853\nEpoch 924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2827\nEpoch 925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.2801\nEpoch 926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2777\nEpoch 927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2752\nEpoch 928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2727\nEpoch 929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2703\nEpoch 930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.2679\nEpoch 931/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2655\nEpoch 932/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2633\nEpoch 933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2607\nEpoch 934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 0.2584\nEpoch 935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2559\nEpoch 936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.2534\nEpoch 937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2510\nEpoch 938/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2486\nEpoch 939/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0420 - val_loss: 0.2461\nEpoch 940/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0419 - val_loss: 0.2437\nEpoch 941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2413\nEpoch 942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2390\nEpoch 943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2367\nEpoch 944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2345\nEpoch 945/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2322\nEpoch 946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2300\nEpoch 947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2276\nEpoch 948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2254\nEpoch 949/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0411 - val_loss: 0.2232\nEpoch 950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2211\nEpoch 951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2190\nEpoch 952/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2170\nEpoch 953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2151\nEpoch 954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2131\nEpoch 955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2111\nEpoch 956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2091\nEpoch 957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2071\nEpoch 958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2051\nEpoch 959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2030\nEpoch 960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.2010\nEpoch 961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1989\nEpoch 962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1972\nEpoch 963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1953\nEpoch 964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1932\nEpoch 965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1913\nEpoch 966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1894\nEpoch 967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1875\nEpoch 968/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0396 - val_loss: 0.1856\nEpoch 969/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0396 - val_loss: 0.1839\nEpoch 970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1821\nEpoch 971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1802\nEpoch 972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1784\nEpoch 973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1767\nEpoch 974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1751\nEpoch 975/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1734\nEpoch 976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1719\nEpoch 977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1703\nEpoch 978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1687\nEpoch 979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1670\nEpoch 980/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0388 - val_loss: 0.1656\nEpoch 981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1639\nEpoch 982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1624\nEpoch 983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1608\nEpoch 984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1592\nEpoch 985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1577\nEpoch 986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1563\nEpoch 987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1548\nEpoch 988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1533\nEpoch 989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1518\nEpoch 990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1504\nEpoch 991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1488\nEpoch 992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1475\nEpoch 993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1462\nEpoch 994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1448\nEpoch 995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1434\nEpoch 996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1421\nEpoch 997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1407\nEpoch 998/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 0.1393\nEpoch 999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1380\nEpoch 1000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1367\nEpoch 1001/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1355\nEpoch 1002/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1343\nEpoch 1003/2000\n6/6 [==============================] - ETA: 0s - loss: 0.032 - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1331\nEpoch 1004/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1320\nEpoch 1005/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1308\nEpoch 1006/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1296\nEpoch 1007/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1284\nEpoch 1008/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1272\nEpoch 1009/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1261\nEpoch 1010/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1250\nEpoch 1011/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1239\nEpoch 1012/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1229\nEpoch 1013/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1218\nEpoch 1014/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1207\nEpoch 1015/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1196\nEpoch 1016/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1185\nEpoch 1017/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1174\nEpoch 1018/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1163\nEpoch 1019/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1153\nEpoch 1020/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1143\nEpoch 1021/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1133\nEpoch 1022/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1123\nEpoch 1023/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1114\nEpoch 1024/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0367 - val_loss: 0.1104\nEpoch 1025/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1093\nEpoch 1026/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1082\nEpoch 1027/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1073\nEpoch 1028/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1063\nEpoch 1029/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1054\nEpoch 1030/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1044\nEpoch 1031/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0365 - val_loss: 0.1036\nEpoch 1032/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1027\nEpoch 1033/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1019\nEpoch 1034/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1009\nEpoch 1035/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.1001\nEpoch 1036/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0992\nEpoch 1037/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0983\nEpoch 1038/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0975\nEpoch 1039/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0965\nEpoch 1040/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0956\nEpoch 1041/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0948\nEpoch 1042/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0940\nEpoch 1043/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0932\nEpoch 1044/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0925\nEpoch 1045/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0917\nEpoch 1046/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0910\nEpoch 1047/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0360 - val_loss: 0.0902\nEpoch 1048/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0894\nEpoch 1049/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0886\nEpoch 1050/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0878\nEpoch 1051/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0870\nEpoch 1052/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0863\nEpoch 1053/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0857\nEpoch 1054/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0851\nEpoch 1055/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0844\nEpoch 1056/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0838\nEpoch 1057/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0831\nEpoch 1058/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0825\nEpoch 1059/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0819\nEpoch 1060/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0813\nEpoch 1061/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0807\nEpoch 1062/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0801\nEpoch 1063/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0795\nEpoch 1064/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0789\nEpoch 1065/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0783\nEpoch 1066/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0777\nEpoch 1067/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0771\nEpoch 1068/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0765\nEpoch 1069/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0759\nEpoch 1070/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0753\nEpoch 1071/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0748\nEpoch 1072/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0742\nEpoch 1073/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0736\nEpoch 1074/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0730\nEpoch 1075/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0724\nEpoch 1076/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0719\nEpoch 1077/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0714\nEpoch 1078/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0710\nEpoch 1079/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0705\nEpoch 1080/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0700\nEpoch 1081/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0695\nEpoch 1082/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0690\nEpoch 1083/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.0685\nEpoch 1084/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.0680\nEpoch 1085/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0675\nEpoch 1086/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0671\nEpoch 1087/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0667\nEpoch 1088/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0662\nEpoch 1089/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0659\nEpoch 1090/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 0.0655\nEpoch 1091/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0651\nEpoch 1092/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0646\nEpoch 1093/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0642\nEpoch 1094/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0637\nEpoch 1095/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0633\nEpoch 1096/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0629\nEpoch 1097/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0625\nEpoch 1098/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0622\nEpoch 1099/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0618\nEpoch 1100/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0614\nEpoch 1101/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0611\nEpoch 1102/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 0.0607\nEpoch 1103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0604\nEpoch 1104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0601\nEpoch 1105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0598\nEpoch 1106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0595\nEpoch 1107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0591\nEpoch 1108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0587\nEpoch 1109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0585\nEpoch 1110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0582\nEpoch 1111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0579\nEpoch 1112/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0575\nEpoch 1113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0572\nEpoch 1114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0570\nEpoch 1115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0567\nEpoch 1116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0563\nEpoch 1117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0560\nEpoch 1118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0557\nEpoch 1119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0555\nEpoch 1120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0551\nEpoch 1121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0548\nEpoch 1122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0545\nEpoch 1123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0542\nEpoch 1124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0539\nEpoch 1125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0538\nEpoch 1126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0536\nEpoch 1127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0533\nEpoch 1128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0531\nEpoch 1129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0528\nEpoch 1130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0526\nEpoch 1131/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0524\nEpoch 1132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0522\nEpoch 1133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0520\nEpoch 1134/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0518\nEpoch 1135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0515\nEpoch 1136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0513\nEpoch 1137/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0511\nEpoch 1138/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0509\nEpoch 1139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0507\nEpoch 1140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0505\nEpoch 1141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0503\nEpoch 1142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0502\nEpoch 1143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0500\nEpoch 1144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0498\nEpoch 1145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0496\nEpoch 1146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0494\nEpoch 1147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0492\nEpoch 1148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0490\nEpoch 1149/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0488\nEpoch 1150/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0486\nEpoch 1151/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0484\nEpoch 1152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0482\nEpoch 1153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0481\nEpoch 1154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0480\nEpoch 1155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0478\nEpoch 1156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0476\nEpoch 1157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0474\nEpoch 1158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0472\nEpoch 1159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0471\nEpoch 1160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0470\nEpoch 1161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0468\nEpoch 1162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0467\nEpoch 1163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0465\nEpoch 1164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463\nEpoch 1165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462\nEpoch 1166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0461\nEpoch 1167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0460\nEpoch 1168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0459\nEpoch 1169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458\nEpoch 1170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456\nEpoch 1171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0455\nEpoch 1172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1173/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0453\nEpoch 1174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452\nEpoch 1175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0451\nEpoch 1176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0450\nEpoch 1177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0449\nEpoch 1178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448\nEpoch 1179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0447\nEpoch 1180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446\nEpoch 1181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444\nEpoch 1182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443\nEpoch 1183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442\nEpoch 1184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441\nEpoch 1185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440\nEpoch 1186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439\nEpoch 1187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438\nEpoch 1188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0436\nEpoch 1189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435\nEpoch 1190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435\nEpoch 1191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433\nEpoch 1193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432\nEpoch 1194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1197/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0429\nEpoch 1198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1199/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1201/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424\nEpoch 1205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420\nEpoch 1211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419\nEpoch 1212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419\nEpoch 1213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419\nEpoch 1214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0418\nEpoch 1215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418\nEpoch 1216/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416\nEpoch 1219/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413\nEpoch 1224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1234/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1246/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1249/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1250/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1257/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1259/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1273/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1276/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1277/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1286/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1289/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1290/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1292/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1312/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1314/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1323/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1324/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1325/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1347/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1365/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1370/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1371/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1379/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1396/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1401/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1408/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1409/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1411/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1413/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1419/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1424/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1429/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1438/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1445/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1446/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1451/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1458/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1463/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1469/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1473/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1475/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1479/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1488/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1497/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1505/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1506/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1509/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1512/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1517/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1522/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1524/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1528/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1538/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1542/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1552/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1567/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1580/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1587/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1591/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1593/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1598/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1610/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1617/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1634/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1637/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1651/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1653/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1659/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1665/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1667/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1682/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1685/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1697/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1701/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1712/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1713/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1720/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1721/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1739/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1748/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1750/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1751/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1782/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1788/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1790/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1801/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1812/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1815/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1836/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1840/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1842/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1847/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1851/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1852/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1854/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1872/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1878/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1879/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1880/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1882/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1887/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1889/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1891/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1901/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1905/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1907/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1911/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1913/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1919/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1922/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1923/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1926/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1938/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1945/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1950/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1952/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1953/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1958/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1962/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1966/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1967/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1975/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1988/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 2000/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\n\n\n<keras.callbacks.History at 0x7f708353a650>\n\n\n\nfig, ax = plt.subplots() \nax.plot(y,'.',alpha=0.2)\nax.plot(net(X),'--')\nwith tf.summary.create_file_writer(logdir).as_default():\n    tf.summary.image(\"적합결과시각화\", plot_to_image(fig), step=0)\n\n\n# \n#%tensorboard --logdir logs --host 0.0.0.0 \n\n- 아래의 코드를 100에폭마다 실행하고 싶다.\nfig, ax = plt.subplots() \nax.plot(y,'.',alpha=0.2)\nax.plot(net(X),'--')\nwith tf.summary.create_file_writer(logdir).as_default():\n    tf.summary.image(\"적합결과시각화\", plot_to_image(fig), step=0)\n- 일단 net.fit직전까지의 코드를 구현\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(loss='mse',optimizer='adam')\n\n- 사용자정의 콜백클래스를 만듬\n\nclass PlotYhat(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self,epoch,logs): # 입력은 무조건 self, epoch, logs를 써야합니다 --> 이 함수안에 에폭이 끝날때마다 할 동작을 정의한다. \n        if epoch % 100 ==0: \n            fig, ax = plt.subplots() \n            ax.plot(y,'.',alpha=0.2)\n            ax.plot(net(X),'--')\n            with tf.summary.create_file_writer(logdir).as_default():\n                tf.summary.image(\"적합결과시각화\"+str(epoch), plot_to_image(fig), step=0)\n\n- 내가 만든 클래스에서 cb2를 생성\n\n#collapse_output\n!rm -rf logs\ncb1= tf.keras.callbacks.TensorBoard(update_freq='epoch',histogram_freq=100)\ncb2= PlotYhat() \nnet.fit(X,y,epochs=2000, batch_size=100, validation_split=0.45,callbacks=[cb1,cb2])\n\nEpoch 1/2000\n1/6 [====>.........................] - ETA: 0s - loss: 2.9239WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0007s vs `on_train_batch_end` time: 0.0019s). Check your callbacks.\n6/6 [==============================] - 0s 6ms/step - loss: 2.8708 - val_loss: 9.1608\nEpoch 2/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8444 - val_loss: 9.1250\nEpoch 3/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.8186 - val_loss: 9.0901\nEpoch 4/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7930 - val_loss: 9.0545\nEpoch 5/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7673 - val_loss: 9.0188\nEpoch 6/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7417 - val_loss: 8.9828\nEpoch 7/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.7169 - val_loss: 8.9476\nEpoch 8/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6913 - val_loss: 8.9134\nEpoch 9/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.6667 - val_loss: 8.8786\nEpoch 10/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6420 - val_loss: 8.8450\nEpoch 11/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.6175 - val_loss: 8.8107\nEpoch 12/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5932 - val_loss: 8.7766\nEpoch 13/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5690 - val_loss: 8.7433\nEpoch 14/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.5453 - val_loss: 8.7103\nEpoch 15/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.5214 - val_loss: 8.6774\nEpoch 16/2000\n6/6 [==============================] - 0s 4ms/step - loss: 2.4980 - val_loss: 8.6453\nEpoch 17/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4749 - val_loss: 8.6120\nEpoch 18/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4520 - val_loss: 8.5786\nEpoch 19/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4293 - val_loss: 8.5454\nEpoch 20/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.4065 - val_loss: 8.5130\nEpoch 21/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3840 - val_loss: 8.4813\nEpoch 22/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3618 - val_loss: 8.4501\nEpoch 23/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3398 - val_loss: 8.4176\nEpoch 24/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.3182 - val_loss: 8.3857\nEpoch 25/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2960 - val_loss: 8.3532\nEpoch 26/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2748 - val_loss: 8.3213\nEpoch 27/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2534 - val_loss: 8.2902\nEpoch 28/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2321 - val_loss: 8.2592\nEpoch 29/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.2113 - val_loss: 8.2272\nEpoch 30/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1904 - val_loss: 8.1962\nEpoch 31/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1696 - val_loss: 8.1648\nEpoch 32/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1493 - val_loss: 8.1346\nEpoch 33/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1288 - val_loss: 8.1042\nEpoch 34/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.1088 - val_loss: 8.0739\nEpoch 35/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0887 - val_loss: 8.0442\nEpoch 36/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0688 - val_loss: 8.0146\nEpoch 37/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0491 - val_loss: 7.9854\nEpoch 38/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0297 - val_loss: 7.9553\nEpoch 39/2000\n6/6 [==============================] - 0s 3ms/step - loss: 2.0102 - val_loss: 7.9261\nEpoch 40/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9913 - val_loss: 7.8968\nEpoch 41/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9720 - val_loss: 7.8684\nEpoch 42/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9533 - val_loss: 7.8398\nEpoch 43/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.9346 - val_loss: 7.8111\nEpoch 44/2000\n6/6 [==============================] - 0s 2ms/step - loss: 1.9162 - val_loss: 7.7827\nEpoch 45/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8976 - val_loss: 7.7536\nEpoch 46/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8796 - val_loss: 7.7262\nEpoch 47/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8613 - val_loss: 7.6986\nEpoch 48/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.8437 - val_loss: 7.6714\nEpoch 49/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.8257 - val_loss: 7.6441\nEpoch 50/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.8083 - val_loss: 7.6171\nEpoch 51/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.7911 - val_loss: 7.5898\nEpoch 52/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7739 - val_loss: 7.5620\nEpoch 53/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7568 - val_loss: 7.5342\nEpoch 54/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7399 - val_loss: 7.5075\nEpoch 55/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7231 - val_loss: 7.4809\nEpoch 56/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.7066 - val_loss: 7.4545\nEpoch 57/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6898 - val_loss: 7.4279\nEpoch 58/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6734 - val_loss: 7.4019\nEpoch 59/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6573 - val_loss: 7.3760\nEpoch 60/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6410 - val_loss: 7.3502\nEpoch 61/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6251 - val_loss: 7.3247\nEpoch 62/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.6095 - val_loss: 7.2996\nEpoch 63/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5937 - val_loss: 7.2744\nEpoch 64/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.5782 - val_loss: 7.2490\nEpoch 65/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5626 - val_loss: 7.2237\nEpoch 66/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5476 - val_loss: 7.1979\nEpoch 67/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5324 - val_loss: 7.1726\nEpoch 68/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.5175 - val_loss: 7.1478\nEpoch 69/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.5027 - val_loss: 7.1222\nEpoch 70/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4878 - val_loss: 7.0974\nEpoch 71/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4732 - val_loss: 7.0722\nEpoch 72/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4588 - val_loss: 7.0463\nEpoch 73/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4445 - val_loss: 7.0214\nEpoch 74/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4301 - val_loss: 6.9963\nEpoch 75/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.4162 - val_loss: 6.9712\nEpoch 76/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.4024 - val_loss: 6.9467\nEpoch 77/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3885 - val_loss: 6.9229\nEpoch 78/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3749 - val_loss: 6.8992\nEpoch 79/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3612 - val_loss: 6.8755\nEpoch 80/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3479 - val_loss: 6.8522\nEpoch 81/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3345 - val_loss: 6.8282\nEpoch 82/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3213 - val_loss: 6.8048\nEpoch 83/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.3083 - val_loss: 6.7817\nEpoch 84/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2955 - val_loss: 6.7583\nEpoch 85/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2826 - val_loss: 6.7351\nEpoch 86/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2700 - val_loss: 6.7123\nEpoch 87/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2574 - val_loss: 6.6893\nEpoch 88/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2449 - val_loss: 6.6665\nEpoch 89/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2327 - val_loss: 6.6433\nEpoch 90/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2205 - val_loss: 6.6207\nEpoch 91/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.2085 - val_loss: 6.5983\nEpoch 92/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1964 - val_loss: 6.5757\nEpoch 93/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1844 - val_loss: 6.5534\nEpoch 94/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1727 - val_loss: 6.5304\nEpoch 95/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1610 - val_loss: 6.5080\nEpoch 96/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1494 - val_loss: 6.4857\nEpoch 97/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1381 - val_loss: 6.4647\nEpoch 98/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1267 - val_loss: 6.4424\nEpoch 99/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.1155 - val_loss: 6.4206\nEpoch 100/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.1044 - val_loss: 6.3984\nEpoch 101/2000\n6/6 [==============================] - 0s 4ms/step - loss: 1.0934 - val_loss: 6.3769\nEpoch 102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0824 - val_loss: 6.3556\nEpoch 103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0716 - val_loss: 6.3349\nEpoch 104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0609 - val_loss: 6.3136\nEpoch 105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0506 - val_loss: 6.2930\nEpoch 106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0399 - val_loss: 6.2721\nEpoch 107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0296 - val_loss: 6.2521\nEpoch 108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0194 - val_loss: 6.2312\nEpoch 109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 1.0093 - val_loss: 6.2111\nEpoch 110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9992 - val_loss: 6.1916\nEpoch 111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9893 - val_loss: 6.1708\nEpoch 112/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.9792 - val_loss: 6.1501\nEpoch 113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9696 - val_loss: 6.1293\nEpoch 114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9600 - val_loss: 6.1098\nEpoch 115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9503 - val_loss: 6.0898\nEpoch 116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9407 - val_loss: 6.0703\nEpoch 117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9313 - val_loss: 6.0498\nEpoch 118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9219 - val_loss: 6.0300\nEpoch 119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9129 - val_loss: 6.0099\nEpoch 120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.9036 - val_loss: 5.9900\nEpoch 121/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8947 - val_loss: 5.9708\nEpoch 122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8857 - val_loss: 5.9519\nEpoch 123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8769 - val_loss: 5.9320\nEpoch 124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8682 - val_loss: 5.9135\nEpoch 125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8595 - val_loss: 5.8947\nEpoch 126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8508 - val_loss: 5.8760\nEpoch 127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8425 - val_loss: 5.8576\nEpoch 128/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.8340 - val_loss: 5.8392\nEpoch 129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8256 - val_loss: 5.8206\nEpoch 130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8175 - val_loss: 5.8018\nEpoch 131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8094 - val_loss: 5.7832\nEpoch 132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.8012 - val_loss: 5.7644\nEpoch 133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7932 - val_loss: 5.7460\nEpoch 134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7853 - val_loss: 5.7276\nEpoch 135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7774 - val_loss: 5.7094\nEpoch 136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7698 - val_loss: 5.6909\nEpoch 137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7620 - val_loss: 5.6725\nEpoch 138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7544 - val_loss: 5.6538\nEpoch 139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7470 - val_loss: 5.6356\nEpoch 140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7397 - val_loss: 5.6178\nEpoch 141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7322 - val_loss: 5.6006\nEpoch 142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7251 - val_loss: 5.5828\nEpoch 143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7178 - val_loss: 5.5649\nEpoch 144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7107 - val_loss: 5.5471\nEpoch 145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.7036 - val_loss: 5.5300\nEpoch 146/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.6967 - val_loss: 5.5130\nEpoch 147/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.6900 - val_loss: 5.4949\nEpoch 148/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.6831 - val_loss: 5.4773\nEpoch 149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6765 - val_loss: 5.4599\nEpoch 150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6698 - val_loss: 5.4428\nEpoch 151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6633 - val_loss: 5.4258\nEpoch 152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6568 - val_loss: 5.4088\nEpoch 153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6504 - val_loss: 5.3916\nEpoch 154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6440 - val_loss: 5.3746\nEpoch 155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6376 - val_loss: 5.3580\nEpoch 156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6313 - val_loss: 5.3409\nEpoch 157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6252 - val_loss: 5.3243\nEpoch 158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6191 - val_loss: 5.3078\nEpoch 159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6132 - val_loss: 5.2917\nEpoch 160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6071 - val_loss: 5.2757\nEpoch 161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.6012 - val_loss: 5.2593\nEpoch 162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5954 - val_loss: 5.2427\nEpoch 163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5896 - val_loss: 5.2265\nEpoch 164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5839 - val_loss: 5.2100\nEpoch 165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5783 - val_loss: 5.1937\nEpoch 166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5726 - val_loss: 5.1774\nEpoch 167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5672 - val_loss: 5.1611\nEpoch 168/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.5616 - val_loss: 5.1447\nEpoch 169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5562 - val_loss: 5.1285\nEpoch 170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5508 - val_loss: 5.1128\nEpoch 171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5455 - val_loss: 5.0969\nEpoch 172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5403 - val_loss: 5.0809\nEpoch 173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5351 - val_loss: 5.0651\nEpoch 174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5299 - val_loss: 5.0495\nEpoch 175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5248 - val_loss: 5.0335\nEpoch 176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5199 - val_loss: 5.0180\nEpoch 177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5150 - val_loss: 5.0023\nEpoch 178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5100 - val_loss: 4.9867\nEpoch 179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5052 - val_loss: 4.9713\nEpoch 180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.5005 - val_loss: 4.9561\nEpoch 181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4957 - val_loss: 4.9413\nEpoch 182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4911 - val_loss: 4.9263\nEpoch 183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4864 - val_loss: 4.9110\nEpoch 184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4818 - val_loss: 4.8960\nEpoch 185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4773 - val_loss: 4.8807\nEpoch 186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4727 - val_loss: 4.8652\nEpoch 187/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4684 - val_loss: 4.8500\nEpoch 188/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4640 - val_loss: 4.8352\nEpoch 189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4597 - val_loss: 4.8199\nEpoch 190/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4554 - val_loss: 4.8049\nEpoch 191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4511 - val_loss: 4.7902\nEpoch 192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4470 - val_loss: 4.7757\nEpoch 193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4430 - val_loss: 4.7610\nEpoch 194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4388 - val_loss: 4.7460\nEpoch 195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4348 - val_loss: 4.7315\nEpoch 196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4308 - val_loss: 4.7174\nEpoch 197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4270 - val_loss: 4.7021\nEpoch 198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4231 - val_loss: 4.6869\nEpoch 199/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4192 - val_loss: 4.6721\nEpoch 200/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4154 - val_loss: 4.6581\nEpoch 201/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.4117 - val_loss: 4.6438\nEpoch 202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4080 - val_loss: 4.6298\nEpoch 203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4042 - val_loss: 4.6157\nEpoch 204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.4007 - val_loss: 4.6012\nEpoch 205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3971 - val_loss: 4.5873\nEpoch 206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3935 - val_loss: 4.5730\nEpoch 207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3900 - val_loss: 4.5583\nEpoch 208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3865 - val_loss: 4.5440\nEpoch 209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3830 - val_loss: 4.5298\nEpoch 210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3796 - val_loss: 4.5153\nEpoch 211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3763 - val_loss: 4.5010\nEpoch 212/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3730 - val_loss: 4.4872\nEpoch 213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3697 - val_loss: 4.4732\nEpoch 214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3665 - val_loss: 4.4590\nEpoch 215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3633 - val_loss: 4.4456\nEpoch 216/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3602 - val_loss: 4.4320\nEpoch 217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3571 - val_loss: 4.4182\nEpoch 218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3540 - val_loss: 4.4044\nEpoch 219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3510 - val_loss: 4.3908\nEpoch 220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3480 - val_loss: 4.3772\nEpoch 221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3451 - val_loss: 4.3643\nEpoch 222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3422 - val_loss: 4.3514\nEpoch 223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3392 - val_loss: 4.3379\nEpoch 224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3364 - val_loss: 4.3246\nEpoch 225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3336 - val_loss: 4.3112\nEpoch 226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3309 - val_loss: 4.2979\nEpoch 227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3281 - val_loss: 4.2849\nEpoch 228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3255 - val_loss: 4.2719\nEpoch 229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3228 - val_loss: 4.2590\nEpoch 230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3202 - val_loss: 4.2463\nEpoch 231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3176 - val_loss: 4.2340\nEpoch 232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3150 - val_loss: 4.2210\nEpoch 233/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3125 - val_loss: 4.2083\nEpoch 234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3100 - val_loss: 4.1954\nEpoch 235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3075 - val_loss: 4.1827\nEpoch 236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3051 - val_loss: 4.1695\nEpoch 237/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.3026 - val_loss: 4.1566\nEpoch 238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.3002 - val_loss: 4.1435\nEpoch 239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2978 - val_loss: 4.1301\nEpoch 240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2955 - val_loss: 4.1168\nEpoch 241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2931 - val_loss: 4.1040\nEpoch 242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2908 - val_loss: 4.0912\nEpoch 243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2886 - val_loss: 4.0783\nEpoch 244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2864 - val_loss: 4.0650\nEpoch 245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2842 - val_loss: 4.0519\nEpoch 246/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2820 - val_loss: 4.0394\nEpoch 247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2799 - val_loss: 4.0260\nEpoch 248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2778 - val_loss: 4.0132\nEpoch 249/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2756 - val_loss: 4.0004\nEpoch 250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2736 - val_loss: 3.9875\nEpoch 251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2716 - val_loss: 3.9748\nEpoch 252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2695 - val_loss: 3.9623\nEpoch 253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2675 - val_loss: 3.9500\nEpoch 254/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2656 - val_loss: 3.9379\nEpoch 255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2637 - val_loss: 3.9257\nEpoch 256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2618 - val_loss: 3.9131\nEpoch 257/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2598 - val_loss: 3.9008\nEpoch 258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2581 - val_loss: 3.8884\nEpoch 259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2562 - val_loss: 3.8763\nEpoch 260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2544 - val_loss: 3.8639\nEpoch 261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2526 - val_loss: 3.8518\nEpoch 262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2508 - val_loss: 3.8400\nEpoch 263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2491 - val_loss: 3.8280\nEpoch 264/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2474 - val_loss: 3.8163\nEpoch 265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2457 - val_loss: 3.8043\nEpoch 266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2440 - val_loss: 3.7922\nEpoch 267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2424 - val_loss: 3.7796\nEpoch 268/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2407 - val_loss: 3.7679\nEpoch 269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2391 - val_loss: 3.7561\nEpoch 270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2375 - val_loss: 3.7440\nEpoch 271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2358 - val_loss: 3.7320\nEpoch 272/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2342 - val_loss: 3.7200\nEpoch 273/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2327 - val_loss: 3.7085\nEpoch 274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2311 - val_loss: 3.6968\nEpoch 275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2296 - val_loss: 3.6854\nEpoch 276/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2282 - val_loss: 3.6738\nEpoch 277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2267 - val_loss: 3.6621\nEpoch 278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2252 - val_loss: 3.6506\nEpoch 279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2238 - val_loss: 3.6389\nEpoch 280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2225 - val_loss: 3.6272\nEpoch 281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2210 - val_loss: 3.6159\nEpoch 282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2197 - val_loss: 3.6041\nEpoch 283/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2184 - val_loss: 3.5925\nEpoch 284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2170 - val_loss: 3.5810\nEpoch 285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2157 - val_loss: 3.5693\nEpoch 286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2144 - val_loss: 3.5573\nEpoch 287/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2132 - val_loss: 3.5455\nEpoch 288/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2118 - val_loss: 3.5341\nEpoch 289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2106 - val_loss: 3.5220\nEpoch 290/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2093 - val_loss: 3.5100\nEpoch 291/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.2080 - val_loss: 3.4983\nEpoch 292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2068 - val_loss: 3.4868\nEpoch 293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2056 - val_loss: 3.4754\nEpoch 294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2044 - val_loss: 3.4642\nEpoch 295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2033 - val_loss: 3.4525\nEpoch 296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2021 - val_loss: 3.4414\nEpoch 297/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.2010 - val_loss: 3.4307\nEpoch 298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1998 - val_loss: 3.4204\nEpoch 299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1987 - val_loss: 3.4090\nEpoch 300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1976 - val_loss: 3.3979\nEpoch 301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1966 - val_loss: 3.3865\nEpoch 302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1955 - val_loss: 3.3759\nEpoch 303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1944 - val_loss: 3.3648\nEpoch 304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1934 - val_loss: 3.3537\nEpoch 305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1923 - val_loss: 3.3431\nEpoch 306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1913 - val_loss: 3.3316\nEpoch 307/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1903 - val_loss: 3.3203\nEpoch 308/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1893 - val_loss: 3.3087\nEpoch 309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1882 - val_loss: 3.2972\nEpoch 310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1873 - val_loss: 3.2862\nEpoch 311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1864 - val_loss: 3.2753\nEpoch 312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1854 - val_loss: 3.2644\nEpoch 313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1844 - val_loss: 3.2537\nEpoch 314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1836 - val_loss: 3.2427\nEpoch 315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1826 - val_loss: 3.2318\nEpoch 316/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1817 - val_loss: 3.2210\nEpoch 317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1808 - val_loss: 3.2101\nEpoch 318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1799 - val_loss: 3.1994\nEpoch 319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1790 - val_loss: 3.1886\nEpoch 320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1781 - val_loss: 3.1771\nEpoch 321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1772 - val_loss: 3.1665\nEpoch 322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1764 - val_loss: 3.1559\nEpoch 323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1756 - val_loss: 3.1453\nEpoch 324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1748 - val_loss: 3.1339\nEpoch 325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1739 - val_loss: 3.1228\nEpoch 326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1731 - val_loss: 3.1123\nEpoch 327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1723 - val_loss: 3.1017\nEpoch 328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1716 - val_loss: 3.0913\nEpoch 329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1708 - val_loss: 3.0808\nEpoch 330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1700 - val_loss: 3.0705\nEpoch 331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1692 - val_loss: 3.0602\nEpoch 332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1685 - val_loss: 3.0498\nEpoch 333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1677 - val_loss: 3.0388\nEpoch 334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1670 - val_loss: 3.0285\nEpoch 335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1662 - val_loss: 3.0177\nEpoch 336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1655 - val_loss: 3.0074\nEpoch 337/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1648 - val_loss: 2.9975\nEpoch 338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1641 - val_loss: 2.9873\nEpoch 339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1634 - val_loss: 2.9772\nEpoch 340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1627 - val_loss: 2.9667\nEpoch 341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1620 - val_loss: 2.9566\nEpoch 342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1613 - val_loss: 2.9468\nEpoch 343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1607 - val_loss: 2.9365\nEpoch 344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1599 - val_loss: 2.9265\nEpoch 345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 2.9164\nEpoch 346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1586 - val_loss: 2.9060\nEpoch 347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1580 - val_loss: 2.8955\nEpoch 348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1573 - val_loss: 2.8854\nEpoch 349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1567 - val_loss: 2.8753\nEpoch 350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1560 - val_loss: 2.8653\nEpoch 351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1554 - val_loss: 2.8548\nEpoch 352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1548 - val_loss: 2.8449\nEpoch 353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1542 - val_loss: 2.8345\nEpoch 354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1536 - val_loss: 2.8244\nEpoch 355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1530 - val_loss: 2.8139\nEpoch 356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1524 - val_loss: 2.8030\nEpoch 357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1517 - val_loss: 2.7928\nEpoch 358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1512 - val_loss: 2.7827\nEpoch 359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1505 - val_loss: 2.7725\nEpoch 360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1500 - val_loss: 2.7618\nEpoch 361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1494 - val_loss: 2.7521\nEpoch 362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1488 - val_loss: 2.7423\nEpoch 363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1483 - val_loss: 2.7323\nEpoch 364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1477 - val_loss: 2.7222\nEpoch 365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1471 - val_loss: 2.7124\nEpoch 366/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1466 - val_loss: 2.7025\nEpoch 367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1460 - val_loss: 2.6925\nEpoch 368/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1455 - val_loss: 2.6823\nEpoch 369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1449 - val_loss: 2.6724\nEpoch 370/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1444 - val_loss: 2.6625\nEpoch 371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1438 - val_loss: 2.6529\nEpoch 372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1433 - val_loss: 2.6425\nEpoch 373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1428 - val_loss: 2.6325\nEpoch 374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1422 - val_loss: 2.6229\nEpoch 375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1417 - val_loss: 2.6133\nEpoch 376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1412 - val_loss: 2.6035\nEpoch 377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1407 - val_loss: 2.5939\nEpoch 378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1402 - val_loss: 2.5842\nEpoch 379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1397 - val_loss: 2.5744\nEpoch 380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1392 - val_loss: 2.5642\nEpoch 381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1387 - val_loss: 2.5543\nEpoch 382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1382 - val_loss: 2.5446\nEpoch 383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1378 - val_loss: 2.5352\nEpoch 384/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1372 - val_loss: 2.5256\nEpoch 385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1368 - val_loss: 2.5152\nEpoch 386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1363 - val_loss: 2.5051\nEpoch 387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1358 - val_loss: 2.4956\nEpoch 388/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1353 - val_loss: 2.4856\nEpoch 389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1348 - val_loss: 2.4758\nEpoch 390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1344 - val_loss: 2.4662\nEpoch 391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1339 - val_loss: 2.4572\nEpoch 392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1335 - val_loss: 2.4477\nEpoch 393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1330 - val_loss: 2.4390\nEpoch 394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1325 - val_loss: 2.4299\nEpoch 395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1321 - val_loss: 2.4209\nEpoch 396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1316 - val_loss: 2.4115\nEpoch 397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1312 - val_loss: 2.4028\nEpoch 398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1307 - val_loss: 2.3940\nEpoch 399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1303 - val_loss: 2.3844\nEpoch 400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1299 - val_loss: 2.3751\nEpoch 401/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1295 - val_loss: 2.3659\nEpoch 402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1290 - val_loss: 2.3568\nEpoch 403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1286 - val_loss: 2.3472\nEpoch 404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1282 - val_loss: 2.3378\nEpoch 405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1277 - val_loss: 2.3288\nEpoch 406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1273 - val_loss: 2.3193\nEpoch 407/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1269 - val_loss: 2.3105\nEpoch 408/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1265 - val_loss: 2.3011\nEpoch 409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1261 - val_loss: 2.2917\nEpoch 410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1256 - val_loss: 2.2824\nEpoch 411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1252 - val_loss: 2.2731\nEpoch 412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 2.2642\nEpoch 413/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 2.2553\nEpoch 414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1240 - val_loss: 2.2461\nEpoch 415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 2.2373\nEpoch 416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1232 - val_loss: 2.2284\nEpoch 417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1228 - val_loss: 2.2194\nEpoch 418/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1224 - val_loss: 2.2105\nEpoch 419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1220 - val_loss: 2.2017\nEpoch 420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1216 - val_loss: 2.1927\nEpoch 421/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1212 - val_loss: 2.1841\nEpoch 422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1208 - val_loss: 2.1749\nEpoch 423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1205 - val_loss: 2.1657\nEpoch 424/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1200 - val_loss: 2.1567\nEpoch 425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1196 - val_loss: 2.1476\nEpoch 426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1192 - val_loss: 2.1388\nEpoch 427/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1189 - val_loss: 2.1294\nEpoch 428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1185 - val_loss: 2.1209\nEpoch 429/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1181 - val_loss: 2.1120\nEpoch 430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1177 - val_loss: 2.1027\nEpoch 431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1173 - val_loss: 2.0937\nEpoch 432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1170 - val_loss: 2.0851\nEpoch 433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1166 - val_loss: 2.0769\nEpoch 434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1162 - val_loss: 2.0680\nEpoch 435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1158 - val_loss: 2.0597\nEpoch 436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1154 - val_loss: 2.0511\nEpoch 437/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1151 - val_loss: 2.0421\nEpoch 438/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1147 - val_loss: 2.0333\nEpoch 439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1143 - val_loss: 2.0251\nEpoch 440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1140 - val_loss: 2.0163\nEpoch 441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1136 - val_loss: 2.0074\nEpoch 442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1133 - val_loss: 1.9985\nEpoch 443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1129 - val_loss: 1.9900\nEpoch 444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1125 - val_loss: 1.9811\nEpoch 445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1121 - val_loss: 1.9727\nEpoch 446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1118 - val_loss: 1.9641\nEpoch 447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1114 - val_loss: 1.9554\nEpoch 448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1111 - val_loss: 1.9468\nEpoch 449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1107 - val_loss: 1.9385\nEpoch 450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1104 - val_loss: 1.9299\nEpoch 451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1100 - val_loss: 1.9216\nEpoch 452/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1097 - val_loss: 1.9130\nEpoch 453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1093 - val_loss: 1.9047\nEpoch 454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1090 - val_loss: 1.8963\nEpoch 455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1086 - val_loss: 1.8880\nEpoch 456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 1.8797\nEpoch 457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1079 - val_loss: 1.8712\nEpoch 458/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1076 - val_loss: 1.8626\nEpoch 459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1072 - val_loss: 1.8541\nEpoch 460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1068 - val_loss: 1.8456\nEpoch 461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 1.8371\nEpoch 462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1062 - val_loss: 1.8291\nEpoch 463/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1058 - val_loss: 1.8207\nEpoch 464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1055 - val_loss: 1.8123\nEpoch 465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1051 - val_loss: 1.8044\nEpoch 466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1048 - val_loss: 1.7963\nEpoch 467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1044 - val_loss: 1.7885\nEpoch 468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1041 - val_loss: 1.7803\nEpoch 469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1038 - val_loss: 1.7722\nEpoch 470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1034 - val_loss: 1.7646\nEpoch 471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1031 - val_loss: 1.7565\nEpoch 472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 1.7484\nEpoch 473/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1025 - val_loss: 1.7402\nEpoch 474/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1021 - val_loss: 1.7330\nEpoch 475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1018 - val_loss: 1.7250\nEpoch 476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1015 - val_loss: 1.7168\nEpoch 477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1012 - val_loss: 1.7089\nEpoch 478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1008 - val_loss: 1.7008\nEpoch 479/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.1005 - val_loss: 1.6927\nEpoch 480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.1002 - val_loss: 1.6846\nEpoch 481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0999 - val_loss: 1.6768\nEpoch 482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0996 - val_loss: 1.6686\nEpoch 483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0992 - val_loss: 1.6610\nEpoch 484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0989 - val_loss: 1.6530\nEpoch 485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0986 - val_loss: 1.6448\nEpoch 486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0983 - val_loss: 1.6373\nEpoch 487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0980 - val_loss: 1.6292\nEpoch 488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0976 - val_loss: 1.6219\nEpoch 489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0973 - val_loss: 1.6139\nEpoch 490/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 1.6062\nEpoch 491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0967 - val_loss: 1.5984\nEpoch 492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 1.5904\nEpoch 493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0961 - val_loss: 1.5827\nEpoch 494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0957 - val_loss: 1.5750\nEpoch 495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0954 - val_loss: 1.5672\nEpoch 496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0951 - val_loss: 1.5593\nEpoch 497/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0948 - val_loss: 1.5515\nEpoch 498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0945 - val_loss: 1.5438\nEpoch 499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0942 - val_loss: 1.5357\nEpoch 500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0938 - val_loss: 1.5281\nEpoch 501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0935 - val_loss: 1.5207\nEpoch 502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0932 - val_loss: 1.5133\nEpoch 503/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0929 - val_loss: 1.5063\nEpoch 504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0926 - val_loss: 1.4992\nEpoch 505/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0923 - val_loss: 1.4920\nEpoch 506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0920 - val_loss: 1.4839\nEpoch 507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0917 - val_loss: 1.4763\nEpoch 508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0914 - val_loss: 1.4686\nEpoch 509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0911 - val_loss: 1.4608\nEpoch 510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0908 - val_loss: 1.4534\nEpoch 511/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0905 - val_loss: 1.4456\nEpoch 512/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0902 - val_loss: 1.4380\nEpoch 513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0899 - val_loss: 1.4307\nEpoch 514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0896 - val_loss: 1.4231\nEpoch 515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0893 - val_loss: 1.4158\nEpoch 516/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0890 - val_loss: 1.4086\nEpoch 517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0887 - val_loss: 1.4011\nEpoch 518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0884 - val_loss: 1.3936\nEpoch 519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0881 - val_loss: 1.3862\nEpoch 520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0878 - val_loss: 1.3786\nEpoch 521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0875 - val_loss: 1.3714\nEpoch 522/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0872 - val_loss: 1.3644\nEpoch 523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0869 - val_loss: 1.3574\nEpoch 524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0866 - val_loss: 1.3505\nEpoch 525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0864 - val_loss: 1.3437\nEpoch 526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0861 - val_loss: 1.3364\nEpoch 527/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0858 - val_loss: 1.3296\nEpoch 528/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0855 - val_loss: 1.3227\nEpoch 529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0852 - val_loss: 1.3158\nEpoch 530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0849 - val_loss: 1.3094\nEpoch 531/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0847 - val_loss: 1.3025\nEpoch 532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0844 - val_loss: 1.2952\nEpoch 533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0841 - val_loss: 1.2883\nEpoch 534/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0838 - val_loss: 1.2809\nEpoch 535/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0835 - val_loss: 1.2734\nEpoch 536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0832 - val_loss: 1.2665\nEpoch 537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0830 - val_loss: 1.2597\nEpoch 538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0827 - val_loss: 1.2527\nEpoch 539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0824 - val_loss: 1.2453\nEpoch 540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0821 - val_loss: 1.2380\nEpoch 541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0818 - val_loss: 1.2310\nEpoch 542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0815 - val_loss: 1.2237\nEpoch 543/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0812 - val_loss: 1.2171\nEpoch 544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0810 - val_loss: 1.2103\nEpoch 545/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0807 - val_loss: 1.2032\nEpoch 546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0804 - val_loss: 1.1967\nEpoch 547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0801 - val_loss: 1.1904\nEpoch 548/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0798 - val_loss: 1.1842\nEpoch 549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0796 - val_loss: 1.1775\nEpoch 550/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0793 - val_loss: 1.1708\nEpoch 551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0790 - val_loss: 1.1644\nEpoch 552/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0788 - val_loss: 1.1575\nEpoch 553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0785 - val_loss: 1.1506\nEpoch 554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 1.1441\nEpoch 555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0779 - val_loss: 1.1371\nEpoch 556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0777 - val_loss: 1.1302\nEpoch 557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0774 - val_loss: 1.1237\nEpoch 558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0771 - val_loss: 1.1172\nEpoch 559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0769 - val_loss: 1.1104\nEpoch 560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0766 - val_loss: 1.1040\nEpoch 561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 1.0975\nEpoch 562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0761 - val_loss: 1.0910\nEpoch 563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0758 - val_loss: 1.0850\nEpoch 564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0756 - val_loss: 1.0784\nEpoch 565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0753 - val_loss: 1.0720\nEpoch 566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 1.0654\nEpoch 567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 1.0591\nEpoch 568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0746 - val_loss: 1.0525\nEpoch 569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0743 - val_loss: 1.0462\nEpoch 570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0741 - val_loss: 1.0400\nEpoch 571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0738 - val_loss: 1.0336\nEpoch 572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0736 - val_loss: 1.0274\nEpoch 573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 1.0215\nEpoch 574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0731 - val_loss: 1.0155\nEpoch 575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0728 - val_loss: 1.0095\nEpoch 576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0726 - val_loss: 1.0033\nEpoch 577/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0723 - val_loss: 0.9969\nEpoch 578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0721 - val_loss: 0.9911\nEpoch 579/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.9855\nEpoch 580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0716 - val_loss: 0.9800\nEpoch 581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.9744\nEpoch 582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0711 - val_loss: 0.9686\nEpoch 583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0709 - val_loss: 0.9626\nEpoch 584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0707 - val_loss: 0.9567\nEpoch 585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0704 - val_loss: 0.9504\nEpoch 586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0702 - val_loss: 0.9447\nEpoch 587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0699 - val_loss: 0.9390\nEpoch 588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.9330\nEpoch 589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0695 - val_loss: 0.9273\nEpoch 590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0692 - val_loss: 0.9217\nEpoch 591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0690 - val_loss: 0.9159\nEpoch 592/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0688 - val_loss: 0.9097\nEpoch 593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0685 - val_loss: 0.9039\nEpoch 594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.8983\nEpoch 595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8930\nEpoch 596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0678 - val_loss: 0.8872\nEpoch 597/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0676 - val_loss: 0.8821\nEpoch 598/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0674 - val_loss: 0.8764\nEpoch 599/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0672 - val_loss: 0.8709\nEpoch 600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0669 - val_loss: 0.8655\nEpoch 601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0667 - val_loss: 0.8599\nEpoch 602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0665 - val_loss: 0.8545\nEpoch 603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0662 - val_loss: 0.8489\nEpoch 604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0660 - val_loss: 0.8432\nEpoch 605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.8379\nEpoch 606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.8323\nEpoch 607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0654 - val_loss: 0.8270\nEpoch 608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.8218\nEpoch 609/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0649 - val_loss: 0.8166\nEpoch 610/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0647 - val_loss: 0.8109\nEpoch 611/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0645 - val_loss: 0.8056\nEpoch 612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0643 - val_loss: 0.8000\nEpoch 613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.7947\nEpoch 614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7896\nEpoch 615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0637 - val_loss: 0.7842\nEpoch 616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0635 - val_loss: 0.7790\nEpoch 617/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.7744\nEpoch 618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0630 - val_loss: 0.7692\nEpoch 619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0628 - val_loss: 0.7644\nEpoch 620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0626 - val_loss: 0.7597\nEpoch 621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0624 - val_loss: 0.7546\nEpoch 622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0622 - val_loss: 0.7497\nEpoch 623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0620 - val_loss: 0.7450\nEpoch 624/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0618 - val_loss: 0.7403\nEpoch 625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0616 - val_loss: 0.7351\nEpoch 626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0614 - val_loss: 0.7301\nEpoch 627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0612 - val_loss: 0.7254\nEpoch 628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.7203\nEpoch 629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0608 - val_loss: 0.7157\nEpoch 630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.7109\nEpoch 631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.7062\nEpoch 632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.7016\nEpoch 633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0601 - val_loss: 0.6969\nEpoch 634/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0599 - val_loss: 0.6921\nEpoch 635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.6874\nEpoch 636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0595 - val_loss: 0.6825\nEpoch 637/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0593 - val_loss: 0.6778\nEpoch 638/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.6729\nEpoch 639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6679\nEpoch 640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.6632\nEpoch 641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0586 - val_loss: 0.6585\nEpoch 642/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 0.6537\nEpoch 643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0582 - val_loss: 0.6489\nEpoch 644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0580 - val_loss: 0.6440\nEpoch 645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.6392\nEpoch 646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0576 - val_loss: 0.6343\nEpoch 647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0574 - val_loss: 0.6300\nEpoch 648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6258\nEpoch 649/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0571 - val_loss: 0.6210\nEpoch 650/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0569 - val_loss: 0.6163\nEpoch 651/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0567 - val_loss: 0.6116\nEpoch 652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0565 - val_loss: 0.6071\nEpoch 653/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0564 - val_loss: 0.6025\nEpoch 654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0562 - val_loss: 0.5979\nEpoch 655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0560 - val_loss: 0.5934\nEpoch 656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0558 - val_loss: 0.5893\nEpoch 657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5852\nEpoch 658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.5812\nEpoch 659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0553 - val_loss: 0.5772\nEpoch 660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0552 - val_loss: 0.5731\nEpoch 661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.5690\nEpoch 662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0548 - val_loss: 0.5651\nEpoch 663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0547 - val_loss: 0.5608\nEpoch 664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0545 - val_loss: 0.5567\nEpoch 665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0543 - val_loss: 0.5525\nEpoch 666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5482\nEpoch 667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5440\nEpoch 668/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0538 - val_loss: 0.5399\nEpoch 669/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0537 - val_loss: 0.5359\nEpoch 670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.5321\nEpoch 671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0533 - val_loss: 0.5282\nEpoch 672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0532 - val_loss: 0.5240\nEpoch 673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0530 - val_loss: 0.5203\nEpoch 674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5162\nEpoch 675/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.5126\nEpoch 676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5090\nEpoch 677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5054\nEpoch 678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0523 - val_loss: 0.5018\nEpoch 679/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.4978\nEpoch 680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.4943\nEpoch 681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0518 - val_loss: 0.4907\nEpoch 682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4869\nEpoch 683/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.4836\nEpoch 684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0513 - val_loss: 0.4795\nEpoch 685/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4756\nEpoch 686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.4716\nEpoch 687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4680\nEpoch 688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0507 - val_loss: 0.4640\nEpoch 689/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.4605\nEpoch 690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4566\nEpoch 691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.4532\nEpoch 692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4497\nEpoch 693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4460\nEpoch 694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.4424\nEpoch 695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0497 - val_loss: 0.4388\nEpoch 696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0496 - val_loss: 0.4354\nEpoch 697/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0494 - val_loss: 0.4322\nEpoch 698/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.4287\nEpoch 699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4255\nEpoch 700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0490 - val_loss: 0.4220\nEpoch 701/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0489 - val_loss: 0.4185\nEpoch 702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4151\nEpoch 703/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4117\nEpoch 704/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.4084\nEpoch 705/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4050\nEpoch 706/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.4019\nEpoch 707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.3990\nEpoch 708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.3958\nEpoch 709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3925\nEpoch 710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0477 - val_loss: 0.3891\nEpoch 711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3859\nEpoch 712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3827\nEpoch 713/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.3799\nEpoch 714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3766\nEpoch 715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3737\nEpoch 716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3704\nEpoch 717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3674\nEpoch 718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.3644\nEpoch 719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3615\nEpoch 720/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.3587\nEpoch 721/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3559\nEpoch 722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3527\nEpoch 723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0462 - val_loss: 0.3499\nEpoch 724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3468\nEpoch 725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3439\nEpoch 726/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0458 - val_loss: 0.3409\nEpoch 727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3379\nEpoch 728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.3352\nEpoch 729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0455 - val_loss: 0.3323\nEpoch 730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0454 - val_loss: 0.3293\nEpoch 731/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3264\nEpoch 732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3236\nEpoch 733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.3209\nEpoch 734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3183\nEpoch 735/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0448 - val_loss: 0.3156\nEpoch 736/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0447 - val_loss: 0.3129\nEpoch 737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3105\nEpoch 738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3079\nEpoch 739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.3053\nEpoch 740/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0443 - val_loss: 0.3026\nEpoch 741/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0442 - val_loss: 0.3001\nEpoch 742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2975\nEpoch 743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2950\nEpoch 744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.2923\nEpoch 745/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2897\nEpoch 746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.2873\nEpoch 747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 0.2851\nEpoch 748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2827\nEpoch 749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2804\nEpoch 750/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0433 - val_loss: 0.2780\nEpoch 751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2757\nEpoch 752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2732\nEpoch 753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2710\nEpoch 754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2687\nEpoch 755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.2662\nEpoch 756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0427 - val_loss: 0.2639\nEpoch 757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2617\nEpoch 758/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0425 - val_loss: 0.2595\nEpoch 759/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 0.2574\nEpoch 760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2552\nEpoch 761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.2530\nEpoch 762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2508\nEpoch 763/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2487\nEpoch 764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2465\nEpoch 765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2444\nEpoch 766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2422\nEpoch 767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2400\nEpoch 768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2380\nEpoch 769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2356\nEpoch 770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2336\nEpoch 771/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0415 - val_loss: 0.2313\nEpoch 772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2293\nEpoch 773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2273\nEpoch 774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2255\nEpoch 775/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0411 - val_loss: 0.2236\nEpoch 776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2217\nEpoch 777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2200\nEpoch 778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2178\nEpoch 779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2158\nEpoch 780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2139\nEpoch 781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2120\nEpoch 782/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2101\nEpoch 783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2082\nEpoch 784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2063\nEpoch 785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2045\nEpoch 786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2026\nEpoch 787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2007\nEpoch 788/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0402 - val_loss: 0.1989\nEpoch 789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1971\nEpoch 790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1955\nEpoch 791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1938\nEpoch 792/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1920\nEpoch 793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1903\nEpoch 794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1883\nEpoch 795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1866\nEpoch 796/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1848\nEpoch 797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1831\nEpoch 798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1816\nEpoch 799/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1799\nEpoch 800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1782\nEpoch 801/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0393 - val_loss: 0.1765\nEpoch 802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1750\nEpoch 803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1736\nEpoch 804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1721\nEpoch 805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1708\nEpoch 806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1693\nEpoch 807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1678\nEpoch 808/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1663\nEpoch 809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1648\nEpoch 810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1632\nEpoch 811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1617\nEpoch 812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1603\nEpoch 813/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1589\nEpoch 814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1575\nEpoch 815/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.1562\nEpoch 816/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1547\nEpoch 817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1534\nEpoch 818/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1521\nEpoch 819/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1507\nEpoch 820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1493\nEpoch 821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1479\nEpoch 822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1466\nEpoch 823/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1453\nEpoch 824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1441\nEpoch 825/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0380 - val_loss: 0.1429\nEpoch 826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1416\nEpoch 827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1404\nEpoch 828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1392\nEpoch 829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1378\nEpoch 830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1365\nEpoch 831/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 0.1353\nEpoch 832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1342\nEpoch 833/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1330\nEpoch 834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1319\nEpoch 835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1309\nEpoch 836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1297\nEpoch 837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1287\nEpoch 838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1276\nEpoch 839/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.1265\nEpoch 840/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1253\nEpoch 841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1241\nEpoch 842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1229\nEpoch 843/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1218\nEpoch 844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1209\nEpoch 845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1198\nEpoch 846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1189\nEpoch 847/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1178\nEpoch 848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1168\nEpoch 849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1159\nEpoch 850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1149\nEpoch 851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1139\nEpoch 852/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1129\nEpoch 853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1120\nEpoch 854/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1111\nEpoch 855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1102\nEpoch 856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1093\nEpoch 857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1083\nEpoch 858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1074\nEpoch 859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1066\nEpoch 860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1057\nEpoch 861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1047\nEpoch 862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1038\nEpoch 863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1029\nEpoch 864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1019\nEpoch 865/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0364 - val_loss: 0.1009\nEpoch 866/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.1000\nEpoch 867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0992\nEpoch 868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0984\nEpoch 869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0976\nEpoch 870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0968\nEpoch 871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0962\nEpoch 872/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0362 - val_loss: 0.0954\nEpoch 873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0946\nEpoch 874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0939\nEpoch 875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0931\nEpoch 876/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0924\nEpoch 877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0916\nEpoch 878/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0908\nEpoch 879/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0902\nEpoch 880/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0895\nEpoch 881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0889\nEpoch 882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0881\nEpoch 883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0875\nEpoch 884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0868\nEpoch 885/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0861\nEpoch 886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0853\nEpoch 887/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0846\nEpoch 888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0839\nEpoch 889/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0833\nEpoch 890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0827\nEpoch 891/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0357 - val_loss: 0.0821\nEpoch 892/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0815\nEpoch 893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0810\nEpoch 894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0804\nEpoch 895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0799\nEpoch 896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0794\nEpoch 897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0789\nEpoch 898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0783\nEpoch 899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0778\nEpoch 900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0773\nEpoch 901/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0768\nEpoch 902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0762\nEpoch 903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0756\nEpoch 904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0751\nEpoch 905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0746\nEpoch 906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0740\nEpoch 907/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0735\nEpoch 908/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0731\nEpoch 909/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0726\nEpoch 910/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0723\nEpoch 911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0719\nEpoch 912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0714\nEpoch 913/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0352 - val_loss: 0.0709\nEpoch 914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0703\nEpoch 915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0699\nEpoch 916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0694\nEpoch 917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0690\nEpoch 918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0684\nEpoch 919/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0680\nEpoch 920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0675\nEpoch 921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0671\nEpoch 922/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0666\nEpoch 923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0663\nEpoch 924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0659\nEpoch 925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0654\nEpoch 926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0649\nEpoch 927/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0644\nEpoch 928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0639\nEpoch 929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0634\nEpoch 930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0631\nEpoch 931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0628\nEpoch 932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0624\nEpoch 933/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0622\nEpoch 934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0618\nEpoch 935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0614\nEpoch 936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0611\nEpoch 937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0608\nEpoch 938/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 0.0606\nEpoch 939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0603\nEpoch 940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0599\nEpoch 941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0597\nEpoch 942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0594\nEpoch 943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0591\nEpoch 944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0588\nEpoch 945/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.0584\nEpoch 946/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0580\nEpoch 947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0577\nEpoch 948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0575\nEpoch 949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0571\nEpoch 950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0569\nEpoch 951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0565\nEpoch 952/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0563\nEpoch 953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0560\nEpoch 954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0557\nEpoch 955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0554\nEpoch 956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0552\nEpoch 957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0549\nEpoch 958/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0347 - val_loss: 0.0547\nEpoch 959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0544\nEpoch 960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0542\nEpoch 961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0540\nEpoch 962/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0537\nEpoch 963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0534\nEpoch 964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0532\nEpoch 965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0529\nEpoch 966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0527\nEpoch 967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0524\nEpoch 968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0522\nEpoch 969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0520\nEpoch 970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0519\nEpoch 971/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0517\nEpoch 972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0515\nEpoch 973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0513\nEpoch 974/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0511\nEpoch 975/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0509\nEpoch 976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0507\nEpoch 977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0505\nEpoch 978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0503\nEpoch 979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0501\nEpoch 980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0499\nEpoch 981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0497\nEpoch 982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0495\nEpoch 983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0494\nEpoch 984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0492\nEpoch 985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0490\nEpoch 986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0488\nEpoch 987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0486\nEpoch 988/2000\n6/6 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.0484\nEpoch 989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0482\nEpoch 990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0481\nEpoch 991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0479\nEpoch 992/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0478\nEpoch 993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0476\nEpoch 994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0475\nEpoch 995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473\nEpoch 996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0472\nEpoch 997/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0470\nEpoch 998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0469\nEpoch 999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0467\nEpoch 1000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0466\nEpoch 1001/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0464\nEpoch 1002/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463\nEpoch 1003/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462\nEpoch 1004/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0461\nEpoch 1005/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0459\nEpoch 1006/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458\nEpoch 1007/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458\nEpoch 1008/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0457\nEpoch 1009/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0457\nEpoch 1010/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456\nEpoch 1011/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0455\nEpoch 1012/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1013/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454\nEpoch 1014/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0452\nEpoch 1015/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452\nEpoch 1016/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0450\nEpoch 1017/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448\nEpoch 1018/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446\nEpoch 1019/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0445\nEpoch 1020/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444\nEpoch 1021/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443\nEpoch 1022/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442\nEpoch 1023/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441\nEpoch 1024/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441\nEpoch 1025/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440\nEpoch 1026/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439\nEpoch 1027/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438\nEpoch 1028/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0437\nEpoch 1029/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0436\nEpoch 1030/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1031/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434\nEpoch 1032/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433\nEpoch 1033/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432\nEpoch 1034/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1035/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431\nEpoch 1036/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0430\nEpoch 1037/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0429\nEpoch 1038/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428\nEpoch 1039/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1040/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427\nEpoch 1041/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426\nEpoch 1042/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425\nEpoch 1043/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424\nEpoch 1044/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1045/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423\nEpoch 1046/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422\nEpoch 1047/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1048/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421\nEpoch 1049/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420\nEpoch 1050/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419\nEpoch 1051/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0418\nEpoch 1052/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0418\nEpoch 1053/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0417\nEpoch 1054/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1055/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417\nEpoch 1056/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0416\nEpoch 1057/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416\nEpoch 1058/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416\nEpoch 1059/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1060/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415\nEpoch 1061/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1062/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414\nEpoch 1063/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413\nEpoch 1064/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1065/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412\nEpoch 1066/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411\nEpoch 1067/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1068/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410\nEpoch 1069/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1070/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1071/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1072/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409\nEpoch 1073/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1074/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408\nEpoch 1075/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1076/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1077/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1078/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0407\nEpoch 1079/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1080/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1081/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406\nEpoch 1082/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1083/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1084/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405\nEpoch 1085/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1086/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1087/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404\nEpoch 1088/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1089/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1090/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1091/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1092/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403\nEpoch 1093/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1094/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1095/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402\nEpoch 1096/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1097/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1098/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1099/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1100/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1101/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0401\nEpoch 1102/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1103/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1104/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400\nEpoch 1105/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1106/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1107/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1108/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399\nEpoch 1109/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1110/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1111/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1112/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1113/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1114/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398\nEpoch 1115/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1116/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1117/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397\nEpoch 1118/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1119/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1120/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1121/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396\nEpoch 1122/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1123/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1124/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1125/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1126/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1127/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1128/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1129/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1130/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1131/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1132/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1133/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395\nEpoch 1134/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1135/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1136/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1137/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1138/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1139/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1140/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1141/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1142/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1143/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394\nEpoch 1144/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1145/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1146/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1147/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1148/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1149/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1150/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1151/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1152/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1153/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1154/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1155/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393\nEpoch 1156/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1157/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1158/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1159/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1160/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1161/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1162/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392\nEpoch 1163/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1164/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1165/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1166/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1167/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1168/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1169/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1170/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1171/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1172/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1173/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1174/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391\nEpoch 1175/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1176/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1177/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1178/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1179/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1180/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1181/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1182/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1183/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1184/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1185/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1186/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1187/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1188/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1189/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1190/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1191/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1192/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1193/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1194/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1195/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1196/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1197/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1198/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1199/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1200/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390\nEpoch 1201/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1202/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1203/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1204/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1205/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1206/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1207/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1208/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1209/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1210/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1211/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1212/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1213/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1214/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1215/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1216/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1217/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1218/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1219/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1220/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389\nEpoch 1221/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1222/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1223/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1224/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1225/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1226/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1227/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1228/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1229/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1230/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1231/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1232/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1233/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1234/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1235/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1236/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1237/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1238/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1239/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1240/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1241/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1242/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1243/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1244/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1245/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1246/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1247/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1248/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1249/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1250/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1251/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1252/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1253/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1254/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1255/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1256/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1257/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1258/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388\nEpoch 1259/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1260/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1261/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1262/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1263/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1264/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1265/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1266/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1267/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1268/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1269/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1270/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1271/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1272/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1273/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1274/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1275/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1276/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1277/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1278/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1279/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1280/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1281/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1282/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1283/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1284/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1285/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1286/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1287/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1288/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1289/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1290/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1291/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1292/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1293/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1294/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1295/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1296/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1297/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1298/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1299/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1300/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1301/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1302/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1303/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1304/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1305/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1306/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1307/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1308/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1309/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1310/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1311/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1312/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1313/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1314/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1315/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1316/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1317/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1318/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1319/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1320/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1321/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1322/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1323/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1324/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1325/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1326/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1327/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1328/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1329/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1330/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1331/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1332/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1333/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1334/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1335/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1336/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1337/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1338/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1339/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1340/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1341/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1342/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1343/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1344/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1345/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1346/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1347/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1348/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1349/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1350/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1351/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1352/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1353/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1354/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1355/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1356/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1357/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1358/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1359/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1360/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1361/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1362/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1363/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1364/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1365/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1366/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1367/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1368/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1369/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1370/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1371/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1372/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1373/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1374/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1375/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1376/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1377/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1378/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1379/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1380/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1381/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1382/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1383/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1384/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1385/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1386/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1387/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1388/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1389/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1390/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1391/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1392/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1393/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1394/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1395/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1396/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1397/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1398/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1399/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1400/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1401/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1402/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1403/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1404/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1405/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1406/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1407/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1408/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1409/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1410/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1411/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1412/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1413/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1414/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1415/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1416/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1417/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1418/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1419/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1420/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1421/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1422/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1423/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1424/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1425/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1426/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1427/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1428/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1429/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1430/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1431/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1432/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1433/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1434/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1435/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1436/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1437/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1438/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1439/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1440/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1441/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1442/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1443/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1444/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1445/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1446/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1447/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1448/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1449/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1450/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1451/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1452/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1453/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1454/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1455/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1456/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1457/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1458/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1459/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1460/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1461/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1462/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1463/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1464/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1465/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1466/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1467/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1468/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1469/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1470/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1471/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1472/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1473/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1474/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1475/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1476/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1477/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1478/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1479/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1480/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1481/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1482/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1483/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1484/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1485/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1486/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1487/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1488/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1489/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1490/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1491/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1492/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1493/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1494/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1495/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1496/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1497/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1498/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1499/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1500/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1501/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1502/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1503/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1504/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1505/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1506/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1507/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1508/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1509/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1510/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1511/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1512/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1513/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1514/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1515/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1516/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1517/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1518/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1519/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1520/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1521/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1522/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1523/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1524/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1525/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1526/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1527/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1528/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1529/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1530/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1531/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1532/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1533/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1534/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1535/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1536/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1537/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1538/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1539/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1540/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1541/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1542/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1543/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1544/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1545/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1546/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1547/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1548/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1549/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1550/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1551/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1552/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1553/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1554/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1555/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1556/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1557/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1558/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1559/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1560/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1561/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1562/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1563/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1564/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1565/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1566/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1567/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1568/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1569/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1570/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1571/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1572/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1573/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1574/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1575/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1576/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1577/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1578/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1579/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1580/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1581/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1582/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1583/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1584/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1585/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1586/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1587/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1588/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1589/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1590/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1591/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1592/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1593/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1594/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1595/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1596/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1597/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1598/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1599/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1600/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1601/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1602/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1603/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1604/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1605/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1606/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1607/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1608/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1609/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1610/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1611/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1612/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1613/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1614/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1615/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1616/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1617/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1618/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1619/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1620/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1621/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1622/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1623/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1624/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1625/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1626/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1627/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1628/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1629/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1630/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1631/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1632/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1633/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1634/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1635/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1636/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1637/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1638/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1639/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1640/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1641/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1642/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1643/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1644/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1645/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1646/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1647/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1648/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1649/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1650/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1651/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1652/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1653/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1654/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1655/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1656/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1657/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1658/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1659/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1660/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1661/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1662/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1663/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1664/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1665/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1666/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1667/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1668/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1669/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1670/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1671/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1672/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1673/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1674/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1675/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1676/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1677/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1678/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1679/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1680/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1681/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1682/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1683/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1684/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1685/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1686/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1687/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1688/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1689/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1690/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1691/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1692/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1693/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1694/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1695/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1696/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1697/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1698/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1699/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1700/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1701/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1702/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1703/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1704/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1705/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1706/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1707/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1708/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1709/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1710/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1711/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1712/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1713/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1714/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1715/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1716/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1717/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1718/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1719/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1720/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1721/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1722/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1723/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1724/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1725/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1726/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1727/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1728/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1729/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1730/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1731/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1732/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1733/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1734/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1735/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1736/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1737/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1738/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1739/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1740/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1741/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1742/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1743/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1744/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1745/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1746/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1747/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1748/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1749/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1750/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1751/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1752/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1753/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1754/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1755/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1756/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1757/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1758/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1759/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1760/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1761/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1762/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1763/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1764/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1765/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1766/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1767/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1768/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1769/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1770/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1771/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1772/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1773/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1774/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1775/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1776/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1777/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1778/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1779/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1780/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1781/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1782/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1783/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1784/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1785/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1786/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1787/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1788/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1789/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1790/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1791/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1792/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1793/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1794/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1795/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1796/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1797/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1798/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1799/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1800/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1801/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1802/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1803/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1804/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1805/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1806/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1807/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1808/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1809/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1810/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1811/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1812/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1813/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1814/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1815/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1816/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1817/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1818/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1819/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1820/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1821/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1822/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1823/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1824/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1825/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1826/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1827/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1828/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1829/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1830/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1831/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1832/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1833/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1834/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1835/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1836/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1837/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1838/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1839/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1840/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1841/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1842/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1843/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1844/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1845/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1846/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1847/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1848/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1849/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1850/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1851/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1852/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1853/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1854/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1855/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1856/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1857/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1858/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1859/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1860/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1861/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387\nEpoch 1862/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1863/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1864/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1865/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1866/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1867/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1868/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1869/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1870/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1871/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1872/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1873/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1874/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1875/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1876/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1877/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1878/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1879/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1880/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1881/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1882/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1883/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1884/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1885/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1886/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1887/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1888/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1889/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1890/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1891/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1892/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1893/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1894/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1895/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1896/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1897/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1898/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1899/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1900/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1901/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1902/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1903/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1904/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1905/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1906/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1907/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1908/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1909/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1910/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1911/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1912/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1913/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1914/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1915/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1916/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1917/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1918/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1919/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1920/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1921/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1922/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1923/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1924/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1925/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1926/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1927/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1928/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1929/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1930/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1931/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1932/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1933/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1934/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1935/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1936/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1937/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1938/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1939/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1940/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1941/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1942/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1943/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1944/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1945/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1946/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1947/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1948/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1949/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1950/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1951/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1952/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1953/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1954/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1955/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1956/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1957/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1958/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1959/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1960/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1961/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1962/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1963/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1964/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1965/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1966/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1967/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1968/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1969/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1970/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1971/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1972/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1973/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385\nEpoch 1974/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1975/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1976/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1977/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1978/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1979/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1980/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1981/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1982/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1983/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1984/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1985/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1986/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1987/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1988/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1989/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1990/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1991/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1992/2000\n6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1993/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1994/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1995/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1996/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1997/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1998/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 1999/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\nEpoch 2000/2000\n6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386\n\n\n<keras.callbacks.History at 0x7f67c81a26b0>\n\n\n\n# \n#%tensorboard --logdir logs --host 0.0.0.0 \n\n\n\n텐서보드: 사용자지정그림 에폭별로 시각화 (2)\n- 중간층의 출력결과를 시각화하고 싶다.\n4. Piecewise-linear regression (15점)\n아래의 모형을 고려하자.\nmodel: \\(y_i=\\begin{cases} x_i +0.3\\epsilon_i & x\\leq 0 \\\\ 3.5x_i +0.3\\epsilon_i & x>0 \\end{cases}\\)\n아래는 위의 모형에서 생성한 샘플이다.\n\n## data \nnp.random.seed(43052)\nN=100\nx= np.linspace(-1,1,N).reshape(N,1)\ny= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x<0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1)\n\n(풀이)\n\ntf.random.set_seed(43055) \nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(2))\nnet.add(tf.keras.layers.Activation(tf.nn.relu))\nnet.add(tf.keras.layers.Dense(1))\nnet.compile(optimizer='sgd',loss='mse')\nnet.fit(x,y,epochs=1,batch_size=100)\n\n1/1 [==============================] - 0s 79ms/step - loss: 2.1414\n\n\n<keras.callbacks.History at 0x7f68849e9420>\n\n\n\nl1,a1,l2 =net.layers\n\n\nfig, (ax1,ax2,ax3) = plt.subplots(1,3) \nfig.set_figwidth(9)\nax1.plot(x,y,'.',alpha=0.2); ax1.plot(x,l1(x),'--'); \nax2.plot(x,y,'.',alpha=0.2); ax2.plot(x,a1(l1(x)),'--'); \nax3.plot(x,y,'.',alpha=0.2); ax3.plot(x,l2(a1(l1(x))),'--r');\n\n\n\n\n\n이런 그림이 100에폭마다 그려졌으면 좋겠다.\n\n- 새로운 클래스를 만들자.\n\nclass PlotMidlayer(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self,epoch,logs): # 입력은 무조건 self, epoch, logs를 써야합니다 --> 이 함수안에 에폭이 끝날때마다 할 동작을 정의한다. \n        if epoch % 100 ==0: \n            fig, (ax1,ax2,ax3) = plt.subplots(1,3) \n            fig.set_figwidth(9)\n            ax1.plot(x,y,'.',alpha=0.2); ax1.plot(x,l1(x),'--'); \n            ax2.plot(x,y,'.',alpha=0.2); ax2.plot(x,a1(l1(x)),'--'); \n            ax3.plot(x,y,'.',alpha=0.2); ax3.plot(x,l2(a1(l1(x))),'--r');            \n            with tf.summary.create_file_writer(logdir).as_default():\n                tf.summary.image(\"적합결과시각화\"+str(epoch), plot_to_image(fig), step=0)\n\n\n!rm -rf logs\ncb1= tf.keras.callbacks.TensorBoard(update_freq='epoch',histogram_freq=100)\ncb2= PlotMidlayer() \nnet.fit(x,y,epochs=1000, batch_size=100,verbose=0 ,callbacks=[cb1,cb2])\n\n<keras.callbacks.History at 0x7f6ef421c280>\n\n\n\n# \n#%tensorboard --logdir logs --host 0.0.0.0"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-06-13-기말고사 문제풀이.html#imports",
    "href": "post/Bigdata Analysis/2022-06-13-기말고사 문제풀이.html#imports",
    "title": "14. Final term",
    "section": "imports",
    "text": "imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow.experimental.numpy as tnp \n\n\ntnp.experimental_enable_numpy_behavior()\n\n\n%load_ext tensorboard\n\n\n%tensorflow_version 2.x\nimport tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n  raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))\n\nFound GPU at: /device:GPU:0\n\n\n\nimport graphviz\ndef gv(s): return graphviz.Source('digraph G{ rankdir=\"LR\"'+ s + ';}')"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-06-13-기말고사 문제풀이.html#fashion_mnist-dnn-30점",
    "href": "post/Bigdata Analysis/2022-06-13-기말고사 문제풀이.html#fashion_mnist-dnn-30점",
    "title": "14. Final term",
    "section": "1. Fashion_mnist, DNN (30점)",
    "text": "1. Fashion_mnist, DNN (30점)\n\n#collapse\ngv('''\nsplines=line\nsubgraph cluster_1{\n    style=filled;\n    color=lightgrey;\n    \"x1\"\n    \"x2\"\n    \"..\"\n    \"x784\"\n    label = \"Layer 0\"\n}\nsubgraph cluster_2{\n    style=filled;\n    color=lightgrey;\n    \"x1\" -> \"node1\"\n    \"x2\" -> \"node1\"\n    \"..\" -> \"node1\"\n    \"x784\" -> \"node1\"\n    \n    \"x1\" -> \"node2\"\n    \"x2\" -> \"node2\"\n    \"..\" -> \"node2\"\n    \"x784\" -> \"node2\"\n    \n    \"x1\" -> \"...\"\n    \"x2\" -> \"...\"\n    \"..\" -> \"...\"\n    \"x784\" -> \"...\"\n\n    \"x1\" -> \"node20\"\n    \"x2\" -> \"node20\"\n    \"..\" -> \"node20\"\n    \"x784\" -> \"node20\"\n\n\n    label = \"Layer 1: relu\"\n}\nsubgraph cluster_3{\n    style=filled;\n    color=lightgrey;\n    \"node1\" -> \"node1 \"\n    \"node2\" -> \"node1 \"\n    \"...\" -> \"node1 \"\n    \"node20\" -> \"node1 \"\n    \n    \"node1\" -> \"node2 \"\n    \"node2\" -> \"node2 \"\n    \"...\" -> \"node2 \"\n    \"node20\" -> \"node2 \"\n    \n    \"node1\" -> \"... \"\n    \"node2\" -> \"... \"\n    \"...\" -> \"... \"\n    \"node20\" -> \"... \"\n\n    \"node1\" -> \"node30 \"\n    \"node2\" -> \"node30 \"\n    \"...\" -> \"node30 \"\n    \"node20\" -> \"node30 \"\n\n\n    label = \"Layer 2: relu\"\n}\nsubgraph cluster_4{\n    style=filled;\n    color=lightgrey;\n\n    \"node1 \" -> \"y10\"\n    \"node2 \" -> \"y10\"\n    \"... \" -> \"y10\"\n    \"node30 \" -> \"y10\"\n    \n    \"node1 \" -> \"y1\"\n    \"node2 \" -> \"y1\"\n    \"... \" -> \"y1\"\n    \"node30 \" -> \"y1\"\n    \n    \"node1 \" -> \".\"\n    \"node2 \" -> \".\"\n    \"... \" -> \".\"\n    \"node30 \" -> \".\"\n    \n    label = \"Layer 3: softmax\"\n}\n''')\n\n\n\n\n(1) tf.keras.datasets.fashion_mnist.load_data()을 이용하여 fashion_mnist 자료를 불러온 뒤 아래의 네트워크를 이용하여 적합하라. (6점)\n\n평가지표로 accuracy를 이용할 것\nepoch은 10으로 설정할 것\noptimizer는 adam을 이용할 것\n\n\nSolution\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n\n\nX= tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n\nX.shape,XX.shape,y.shape, yy.shape\n\n(TensorShape([60000, 28, 28, 1]),\n TensorShape([10000, 28, 28, 1]),\n (60000, 10),\n (10000, 10))\n\n\n\nwith tf.device(\"/device:GPU:0\") :  \n    tf.random.set_seed(202150256)\n\n    net1 = tf.keras.Sequential()\n    net1.add(tf.keras.layers.Flatten())\n    net1.add(tf.keras.layers.Dense(20,activation=\"relu\"))\n    net1.add(tf.keras.layers.Dense(30,activation=\"relu\"))\n    net1.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n    net1.compile(optimizer=\"adam\",loss=tf.losses.categorical_crossentropy,metrics=[\"accuracy\"]) \n    net1.fit(X,y,epochs=10,verbose=1)\n\nEpoch 1/10\n1875/1875 [==============================] - 7s 2ms/step - loss: 2.1564 - accuracy: 0.3564\nEpoch 2/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.3186 - accuracy: 0.4421\nEpoch 3/10\n1875/1875 [==============================] - 5s 3ms/step - loss: 1.2579 - accuracy: 0.4616\nEpoch 4/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.0641 - accuracy: 0.5364\nEpoch 5/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.9958 - accuracy: 0.5610\nEpoch 6/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.9831 - accuracy: 0.5718\nEpoch 7/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.9711 - accuracy: 0.5751\nEpoch 8/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.9348 - accuracy: 0.5914\nEpoch 9/10\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.9211 - accuracy: 0.5998\nEpoch 10/10\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.9046 - accuracy: 0.6149\n\n\n(2) (1)에서 적합된 네트워크를 이용하여 test data의 accuracy를 구하라. (6점)\n\n\nSolution\n\nresult1 = net1.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 2ms/step - loss: 0.9063 - accuracy: 0.6474\n\n\n(3) train set에서 20%의 자료를 validation 으로 분리하여 50에폭동안 학습하라. 텐서보드를 이용하여 train accuracy와 validation accuracy를 시각화 하고 결과를 해석하라. 오버피팅이라고 볼 수 있는가? (6점)\n\n\nSolution\n\ntf.random.set_seed(202150256)\n!rm -rf logs\nwith tf.device(\"/device:GPU:0\") :\n    net2 = tf.keras.Sequential()\n    net2.add(tf.keras.layers.Flatten())\n    net2.add(tf.keras.layers.Dense(20,activation=\"relu\"))\n    net2.add(tf.keras.layers.Dense(30,activation=\"relu\"))\n    net2.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n    net2.compile(optimizer=\"adam\",loss=tf.losses.categorical_crossentropy,metrics=[\"accuracy\"]) \n    cb1 = tf.keras.callbacks.TensorBoard()\n    net2.fit(X,y,epochs=50,validation_split=0.2,callbacks=cb1,verbose=0)\n\n\n%tensorboard --logdir logs\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n- 오버피팅이 아니다. accuracy와 loss를 살펴본 결과 에포크가 증가할 수록 train data와 validation data에 대한 accuracy, loss 비슷하며 심지어 validation data 더 뛰어난 성능을 보이는 구간도 존재한다.\n\n!kill 483\n\n(4) (3)에서 적합된 네트워크를 이용하여 test data의 accuracy를 구하라. (2)의 결과와 비교하라. (6점)\n\n\nSolution\n\nresult2 = net2.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.7737\n\n\n\nx = [\"DNN\",\"DNN with val,epoch=50\"]\nacc = [result1[1],result2[1]]\n\nplt.bar(x[0],acc[0],width=0.6)\nplt.bar(x[1],acc[1],width=0.6)\nplt.ylim([0.4,0.8])\nplt.title(\"accuracy\")\n\nText(0.5, 1.0, 'accuracy')\n\n\n\n\n\n\nepoch 수와 validation data를 추가하여 학습한 모델이 test데이터에 대한 정확도가 더 높았다.\n\n(5) 조기종료기능을 이용하여 (3)의 네트워크를 다시 학습하라. 학습결과를 텐서보드를 이용하여 시각화 하라. (6점) - patience=3 으로 설정할 것\n\n\nSolution\n\ntf.random.set_seed(202150256)\n!rm -rf logs\nwith tf.device(\"/device:GPU:0\") :\n    net3 = tf.keras.Sequential()\n    net3.add(tf.keras.layers.Flatten())\n    net3.add(tf.keras.layers.Dense(20,activation=\"relu\"))\n    net3.add(tf.keras.layers.Dense(30,activation=\"relu\"))\n    net3.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n    net3.compile(optimizer=\"adam\",loss=tf.losses.categorical_crossentropy,metrics=[\"accuracy\"]) \n    cb1 = tf.keras.callbacks.TensorBoard()\n    cb2 = tf.keras.callbacks.EarlyStopping(patience=3)\n    net3.fit(X,y,epochs=50,validation_split=0.2,callbacks=[cb1,cb2],verbose=0)\n\n\n%tensorboard --logdir logs\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n!kill 617"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-06-13-기말고사 문제풀이.html#fashion_mnist-cnn-30점",
    "href": "post/Bigdata Analysis/2022-06-13-기말고사 문제풀이.html#fashion_mnist-cnn-30점",
    "title": "14. Final term",
    "section": "2. Fashion_mnist, CNN (30점)",
    "text": "2. Fashion_mnist, CNN (30점)\n(1) tf.keras.datasets.fashion_mnist.load_data()을 이용하여 fashion_mnist 자료를 불러온 뒤 아래의 네트워크를 이용하여 적합하라. (10점)\n\n이때 n1=6, n2=16, n3=120 으로 설정한다, 드랍아웃비율은 20%로 설정한다.\nnet.summary()를 출력하여 설계결과를 확인하라.\n\n\n\nSolution\n\n!rm -rf logs\nwith tf.device(\"/device:GPU:0\") :\n    net4 = tf.keras.Sequential()\n    net4.add(tf.keras.layers.Conv2D(6,(5,5)))\n    net4.add(tf.keras.layers.MaxPool2D())\n    net4.add(tf.keras.layers.Conv2D(16,(5,5)))\n    net4.add(tf.keras.layers.MaxPool2D())\n    net4.add(tf.keras.layers.Flatten())\n    net4.add(tf.keras.layers.Dense(120,activation=\"relu\"))\n    net4.add(tf.keras.layers.Dropout(0.2))\n    net4.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n    net4.compile(optimizer=\"adam\",loss=tf.losses.categorical_crossentropy,metrics=\"accuracy\")\n    net4.fit(X,y,epochs=1,verbose=0) \n\n\nnet4.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (32, 24, 24, 6)           156       \n                                                                 \n max_pooling2d (MaxPooling2D  (32, 12, 12, 6)          0         \n )                                                               \n                                                                 \n conv2d_1 (Conv2D)           (32, 8, 8, 16)            2416      \n                                                                 \n max_pooling2d_1 (MaxPooling  (32, 4, 4, 16)           0         \n 2D)                                                             \n                                                                 \n flatten_3 (Flatten)         (32, 256)                 0         \n                                                                 \n dense_9 (Dense)             (32, 120)                 30840     \n                                                                 \n dropout (Dropout)           (32, 120)                 0         \n                                                                 \n dense_10 (Dense)            (32, 10)                  1210      \n                                                                 \n=================================================================\nTotal params: 34,622\nTrainable params: 34,622\nNon-trainable params: 0\n_________________________________________________________________\n\n\n(2) n1=(6,64,128), n2=(16,256)에 대하여 test set의 loss가 최소화되는 조합을 찾아라. 결과를 텐서보드로 시각화하는 코드를 작성하라. (20점) - epoc은 3회로 한정한다. - validation_split은 0.2로 설정한다.\n\n\nSolution\n\nfrom tensorboard.plugins.hparams import api as hp\n\n\n!rm -rf logs\nN1= [6,64,128]\nN2= [16,256]\nfor n1 in N1 :\n    for n2 in N2 : \n      logdir =  \"logs/gc_{}_{}\".format(n1,n2) \n      with tf.summary.create_file_writer(logdir).as_default() :\n           with tf.device(\"/device:GPU:0\") :\n              net5 = tf.keras.Sequential()\n              net5.add(tf.keras.layers.Conv2D(n1,(5,5)))\n              net5.add(tf.keras.layers.MaxPool2D())\n              net5.add(tf.keras.layers.Conv2D(n2,(5,5)))\n              net5.add(tf.keras.layers.MaxPool2D())\n              net5.add(tf.keras.layers.Flatten())\n              net5.add(tf.keras.layers.Dense(120,activation=\"relu\"))\n              net5.add(tf.keras.layers.Dropout(0.2))\n              net5.add(tf.keras.layers.Dense(10,activation=\"softmax\"))\n              net5.compile(optimizer=\"adam\",loss=tf.losses.categorical_crossentropy,metrics=\"accuracy\")\n              cb3 = hp.KerasCallback(logdir,{\"n1\":n1,\"n2\":n2})\n              net5.fit(X,y,epochs=3, validation_split=0.2,callbacks=cb3,verbose=0,batch_size=200)\n              _result = net5.evaluate(XX,yy)\n              tf.summary.scalar('loss', _result[0], step=1)\n\n313/313 [==============================] - 1s 3ms/step - loss: 0.5943 - accuracy: 0.7761\n313/313 [==============================] - 1s 3ms/step - loss: 0.3916 - accuracy: 0.8671\n313/313 [==============================] - 1s 3ms/step - loss: 0.4765 - accuracy: 0.8311\n313/313 [==============================] - 1s 3ms/step - loss: 0.4310 - accuracy: 0.8517\n313/313 [==============================] - 1s 3ms/step - loss: 0.5023 - accuracy: 0.8182\n313/313 [==============================] - 1s 3ms/step - loss: 0.7565 - accuracy: 0.7443\n\n\n\n%tensorboard --logdir logs\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\n\\(n_1 = 6, n_2 = 256\\)인 경우가 가장 test loss가 작았다."
  },
  {
    "objectID": "post/Bigdata Analysis/2022-06-13-기말고사 문제풀이.html#cifar10-30점",
    "href": "post/Bigdata Analysis/2022-06-13-기말고사 문제풀이.html#cifar10-30점",
    "title": "14. Final term",
    "section": "3. CIFAR10 (30점)",
    "text": "3. CIFAR10 (30점)\ntf.keras.datasets.cifar10.load_data()을 이용하여 CIFAR10을 불러온 뒤 적당한 네트워크를 사용하여 적합하라.\n\n결과를 텐서보드로 시각화할 필요는 없다.\n자유롭게 모형을 설계하여 적합하라.\ntest set의 accuracy가 70%이상인 경우만 정답으로 인정한다.\n\n\nSolution\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170500096/170498071 [==============================] - 11s 0us/step\n170508288/170498071 [==============================] - 11s 0us/step\n\n\n\nX = tf.constant(x_train.reshape(-1,32,32,3),dtype=tf.float64)\ny = tf.keras.utils.to_categorical(y_train)\nXX = tf.constant(x_test.reshape(-1,32,32,3),dtype=tf.float64)\nyy = tf.keras.utils.to_categorical(y_test)\n\n\ntf.random.set_seed(202150256)\n!rm -rf logs\nwith tf.device(\"/device:GPU:0\") :\n    net6 = tf.keras.Sequential()\n    net6.add(tf.keras.layers.Conv2D(128, (2, 2), activation='relu'))\n    net6.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    net6.add(tf.keras.layers.Conv2D(128, (2, 2), activation='relu'))\n    net6.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    net6.add(tf.keras.layers.Conv2D(256, (2, 2), activation='relu'))\n    net6.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    net6.add(tf.keras.layers.Conv2D(256, (2, 2), activation='relu'))\n    net6.add(tf.keras.layers.Flatten())\n    net6.add(tf.keras.layers.Dense(512, activation='relu'))\n    net6.add(tf.keras.layers.Dropout(0.5))\n    net6.add(tf.keras.layers.Dense(10, activation='softmax'))\n    net6.compile(optimizer=\"adam\",loss=tf.losses.categorical_crossentropy,metrics=\"accuracy\")\n    net6.fit(X,y,epochs=10,verbose=0,batch_size=200)\n\n\nresult6 = net6.evaluate(XX,yy)\n\n313/313 [==============================] - 1s 4ms/step - loss: 0.8870 - accuracy: 0.7130"
  },
  {
    "objectID": "post/Bigdata Analysis/2022-06-13-기말고사 문제풀이.html#다음을-읽고-물음에-답하라.-10점",
    "href": "post/Bigdata Analysis/2022-06-13-기말고사 문제풀이.html#다음을-읽고-물음에-답하라.-10점",
    "title": "14. Final term",
    "section": "4. 다음을 읽고 물음에 답하라. (10점)",
    "text": "4. 다음을 읽고 물음에 답하라. (10점)\n(1) (1,128,128,3)의 shape을 가진 텐서가 tf.keras.layers.Conv2D(5,(2,2))으로 만들어진 커널을 통과할시 나오는 shape은?\n\nSolution\n(1,127,127,5)\n(2) (1,24,24,16)의 shape을 가진 텐서가 tf.keras.layers.Flatten()을 통과할때 나오는 텐서의 shape은?\n\n\nSolution\n(1,9216)\n(3)-(5)\n아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i\\]\n여기에서 \\(t=(t_1,\\dots,t_{1000})=\\) np.linspace(0,5,1000) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.linspace(0,5,1000)\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.1)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2, 3, 1, 0, 0, 0.5 이다)\n(3) 모형에 대한 설명 중 옳은 것을 모두 골라라.\n(하영) 이 모형의 경우 MSEloss를 최소화하는 \\(\\hat{\\beta}_0,\\dots,\\hat{\\beta}_5\\)를 구하는것은 최대우도함수를 최대화하는 \\(\\hat{\\beta}_0,\\dots,\\hat{\\beta}_5\\)를 구하는 것과 같다.\n(재인) 하영의 말이 옳은 이유는 오차항이 정규분포를 따른다는 가정이 있기 때문이다.\n(서연) 이 모형에서 적절한 학습률이 선택되더라도 경사하강법을 이용하면 MSEloss를 최소화하는 \\(\\hat{\\beta}_0,\\dots,\\hat{\\beta}_5\\)를 종종 구할 수 없는 문제가 생긴다. 왜냐하면 손실함수가 convex하지 않아서 local minimum에 빠질 위험이 있기 때문이다.\n(규빈) 만약에 경사하강법 대신 확률적 경사하강법을 쓴다면 local minimum을 언제나 탈출 할 수 있다. 따라서 서연이 언급한 문제점은 생기지 않는다.\n\n\nSolution\n하영,재인,서연\n(4) 다음은 아래 모형을 학습한 결과이다. 옳게 해석한 것을 모두 고르시오.\n\ny = y.reshape(1000,1)\nx1 = np.cos(t) \nx2 = np.cos(2*t)\nx3 = np.cos(3*t)\nx4 = np.cos(4*t)\nx5 = np.cos(5*t)\nX = tf.stack([x1,x2,x3,x4,x5],axis=1)\n\n\nnet = tf.keras.Sequential()\nnet.add(tf.keras.layers.Dense(1)) \nnet.compile(loss='mse',optimizer='adam')\nnet.fit(X,y,epochs=500,batch_size=100, validation_split=0.45,verbose=0) \n\n\nplt.plot(y,'.',alpha=0.1)\nplt.plot(net(X),'--')\n\n(재인) 처음 550개의 데이터만 학습하고 이후의 450개의 데이터는 학습하지 않고 validation으로 이용하였다.\n(서연) validation에서의 적합결과가 좋지 않다.\n(규빈) validation의 적합결과가 좋지 않기 때문에 오버피팅을 의심할 수 있다. 따라서 만약에 네트워크에 드랍아웃층을 추가한다면 오버피팅을 방지하는 효과가 있어 validation의 loss가 줄어들 것이다.\n(하영) 이 모형의 경우 더 많은 epoch으로 학습한다면 train loss와 validation loss를 둘 다 줄일 수 있다.\n\n\nSolution\n재인. 서연, 하영\n(5) 다음을 잘 읽고 참 거짓을 판별하라. - Convolution은 선형변환이다. - CNN을 이용하면 언제나 손실함수를 MSEloss로 선택해야 한다. - CNN은 adam optimizer를 통해서만 최적화할 수 있다. - 이미지자료는 CNN을 이용하여서만 분석할 수 있으며 DNN으로는 분석불가능하다. - CNN은 칼라이미지일 경우에만 적용가능하다.\n\n\nSolution\nO, X, X, X, X"
  },
  {
    "objectID": "post/ML with Pytorch/2022-01-27-Intro.html",
    "href": "post/ML with Pytorch/2022-01-27-Intro.html",
    "title": "0. Intro",
    "section": "",
    "text": "from fastai.vision.all import *\n\n\n\n\n옆에 폴더에 보면 저장된 데이터들을 확인해볼 수 있다!\n안보이는 파일들은 리눅스처럼 숨겨진 파일들을 의미한다.\n\n\nuntar_data(URLs.PETS)/'images'\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:11<00:00]\n    \n    \n\n\nPath('/root/.fastai/data/oxford-iiit-pet/images')\n\n\n\n위의 결과를 보면 해당 경로에 이미지를 저장한 것이다!\n즉, 위의 명령은 URL에서 데이터를 불러오고 해당경로에 데이터를 저장\n\n\nURLs.PETS\n\n'https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz'\n\n\n\n해당 경로를 복사하고 붙여 넣으면 자동으로 다운되는 것이 있는데 untar_data은 압축파일을 해제 후 images 폴더에 저장된다.\n경로로 가보면 압축이 해제되고 이미지 파일이 저장된 것을 볼 수 있으나 열어볼 수는 없음…..\n보고싶다면 아래와 같은 method를 이용하자\n\n\nPILImage.create('/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg')\n\n\n\n\n\nPILImage.create('/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_100.jpg')\n\n\n\n\n\n위 같은 과정이 그렇게 편하진 않은 것 같다.\n\n- 조금 더 쉬운 방법 * 파일들 이름의 리스트로 저장한다면?\n\npath = Path('/root/.fastai/data/oxford-iiit-pet/images')\n\n\nfiles = get_image_files(path)\n\n\nfiles[0]\n\nPath('/root/.fastai/data/oxford-iiit-pet/images/pomeranian_9.jpg')\n\n\n\nPILImage.create('/root/.fastai/data/oxford-iiit-pet/images/saint_bernard_134.jpg')\n\n\n\n\n\nPILImage.create(files[0])\n\n\n\n\n\nprint(files[2])\nPILImage.create(files[2])\n\n/root/.fastai/data/oxford-iiit-pet/images/shiba_inu_187.jpg\n\n\n\n\n\n- 해당 데이터는 앞글자가 대문자면 고양이, 소문자면 강아지이다.\n- 특1 : 앞글자가 대문자면 고양이, 소문자면 강아지 \\(\\to\\) 창의적인 방법임\n- 특2 : 이미지크기가 서로 다르다…\n- 특징 1을 이용해서 한번 강아지와 고양이를 분류해보자\n\n\n\n\ndef label_func(fname) :\n    if fname[0].isupper() :\n      return \"cat\"\n    else : \n      return \"dog\"\n\n\nlabel_func(\"Egyptian_Mau_120.jpg\")\n\n'cat'\n\n\n\nlabel_func(\"egyptian_Mau_120.jpg\")\n\n'dog'\n\n\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(224))\n\n\npath 경로에서 files에 해당하는 파일들을 불러와서 \\(X\\)를 만들고 정의한 함수 label_func를 적용해 item_tfms에 정의된 방식으로 \\(X\\)를 변환하여 저장한다.\n\n\ndls.show_batch(max_n=8) ## 8개의 그림만보자\n\n\n\n\n\n\n- 1차 목표 : 이미지 파일을 받으면 -> 개인지 고양이인지 판단하는 모형을 만들자\n- 2차 목표 : 그 모형에 새로운 이미지 파일을 전달하여 이미지를 분류할 것이다.\n- cnn_learner 라는 함수를 이용해서 1차 목표와 2차목표를 달성할 object를 만들것임.\n\nclsfr = cnn_learner(dls,resnet34, metrics = error_rate)\n\n/usr/local/lib/python3.8/dist-packages/fastai/vision/learner.py:288: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n- clsfr 필요한 정보 : 우리가 넣어줘야하는 것들이 대부분\n\n모델정보 : 어떠한 분류기를 사용할 것인가\n데이터\n평가기준표 : 채점을 할지표\n\n- clsfr에 필요한 동작 : 이미 구현이 되어있는 것들을 사용\n\n학습\n결과\n예측\n\n- cpu를 써서 학습을 하니 너무 오래 걸리니 런타임 유형을 gpu로 바꿔서 하자\n\nclsfr.fine_tune(1) # 한번 학습\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.149759\n      0.025045\n      0.006766\n      00:16\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.049579\n      0.015034\n      0.004060\n      00:10\n    \n  \n\n\n\n\n\n- 첫 번째 원소는 고양이일 확률, 두 번째 원소는 강아지일 확률 - TensorBase : 고양이면 1. 강아지면 0\n\nclsfr.predict(files[0])\n\n\n\n\n\n\n\n\n('dog', TensorBase(1), TensorBase([1.0673e-07, 1.0000e+00]))\n\n\n\nclsfr.predict(files[7])\n\n\n\n\n\n\n\n\n('cat', TensorBase(0), TensorBase([9.9998e-01, 1.7946e-05]))\n\n\n- 랜덤으로 예측결과를 살펴보자\n\nclsfr.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\n\n- 오답을 분석하는 오브젝트를 만들고 그 안에 학습한 clsfr을 집어넣자.\n\ninterpreter = Interpretation.from_learner(clsfr) \n\n\n\n\n\n\n\n\n- 오답중 loss가 큰 상위 9개만 살펴보자\n\ninterpreter.plot_top_losses(9)\n\n\n\n\n\n\n\n\n\n\n\n- 근데 모형이 과적합되어서 학습한거면?? \\(\\to\\) 약간 가슴깊이 의구심이 든다.\n\n\n\n\n\ntest_file = get_image_files('/content')\n\n\ntest_file\n\n(#10) [Path('/content/pexels-fabian-köhler-14652203.jpg'),Path('/content/pexels-brett-sayles-15075137.jpg'),Path('/content/pexels-pixabay-57416.jpg'),Path('/content/pexels-vadim-b-127028.jpg'),Path('/content/pexels-peng-louis-1643457.jpg'),Path('/content/pexels-poodles-doodles-1458926.jpg'),Path('/content/pexels-pixabay-416160.jpg'),Path('/content/pexels-pixabay-45201.jpg'),Path('/content/pexels-poodles-doodles-1458914.jpg'),Path('/content/pexels-batitay-japheth-14308789.jpg')]\n\n\n\nPILImage.create(test_file[0])\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\ntest_file\n\n(#10) [Path('/content/pexels-fabian-köhler-14652203.jpg'),Path('/content/pexels-brett-sayles-15075137.jpg'),Path('/content/pexels-pixabay-57416.jpg'),Path('/content/pexels-vadim-b-127028.jpg'),Path('/content/pexels-peng-louis-1643457.jpg'),Path('/content/pexels-poodles-doodles-1458926.jpg'),Path('/content/pexels-pixabay-416160.jpg'),Path('/content/pexels-pixabay-45201.jpg'),Path('/content/pexels-poodles-doodles-1458914.jpg'),Path('/content/pexels-batitay-japheth-14308789.jpg')]\n\n\n\nPILImage.create(test_file[6])\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\nclsfr.predict(test_file[0])\n\n\n\n\n\n\n\n\n('dog', TensorBase(1), TensorBase([5.4642e-05, 9.9995e-01]))\n\n\n\nclsfr.predict(test_file[6])\n\n\n\n\n\n\n\n\n('cat', TensorBase(0), TensorBase([9.9993e-01, 6.6712e-05]))\n\n\n- 뭐 어느정도는? test 데이터에 대해서도 잘 분류를 하는것 같다!"
  },
  {
    "objectID": "post/R for Data Scienece/데이터 전처리/결측치(1205).html",
    "href": "post/R for Data Scienece/데이터 전처리/결측치(1205).html",
    "title": "01. 결측치 처리",
    "section": "",
    "text": "is.na(x) : 벡터에서 결측가 있는 경우 True를 반환\ncomplete.cases(x) : 해당 데이터프레임에서 어떤 객체가 가지는 변수 중에 한 개라도 NA가 있을 경우 False 를 뱉음"
  },
  {
    "objectID": "post/R for Data Scienece/데이터 전처리/결측치(1205).html#r실습",
    "href": "post/R for Data Scienece/데이터 전처리/결측치(1205).html#r실습",
    "title": "01. 결측치 처리",
    "section": "R실습",
    "text": "R실습\n\nOzone 변수에 존재하는 na의 개수 산출\n\n\nsum(is.na(airquality$Ozone))\n\n37\n\n\n\ntable(is.na(airquality$Ozone))\n\n\nFALSE  TRUE \n  116    37 \n\n\n\napply함수를 이용하여 각 변수의 na값이 몇 개가 있는지 확인\n\n\n#summary(airquality)\napply(airquality,2,function(x) sum(is.na(x)))\n\nOzone37Solar.R7Wind0Temp0Month0Day0\n\n\n\ncomplete.case 함수를 이용하여 airquality 데이터에서 na값이 하나라도 존재하는 행들을 air_na 변수에 저장하고, na값을 하나도 가지지 않는 행들을 air_com 변수에 저장하기.\n\n\nair_na <- airquality[!complete.cases(airquality),]\n\n\nhead(air_na)\n\n\n\nA data.frame: 6 × 6\n\n    OzoneSolar.RWindTempMonthDay\n    <int><int><dbl><int><int><int>\n\n\n    5NA NA14.3565 5\n    628 NA14.9665 6\n    10NA194 8.669510\n    11 7 NA 6.974511\n    25NA 6616.657525\n    26NA26614.958526\n\n\n\n\n\nair_com <- airquality[complete.cases(airquality),]\n\n\nhead(air_com)\n\n\n\nA data.frame: 6 × 6\n\n    OzoneSolar.RWindTempMonthDay\n    <int><int><dbl><int><int><int>\n\n\n    141190 7.46751\n    236118 8.07252\n    31214912.67453\n    41831311.56254\n    723299 8.66557\n    819 9913.85958"
  },
  {
    "objectID": "post/R for Data Scienece/데이터 전처리/결측치(1205).html#r-실습",
    "href": "post/R for Data Scienece/데이터 전처리/결측치(1205).html#r-실습",
    "title": "01. 결측치 처리",
    "section": "R 실습",
    "text": "R 실습\n\nairquality의 Ozone 변수 값이 존재하지 않는 경우, 해당 변수를 평균값으로 대체하자.\n\n\nairquality$Ozone[is.na(airquality$Ozone)] <- mean(airquality$Ozone, na.rm=T)\n\n\nsum(is.na(airquality$Ozone))\n\n0\n\n\n\nDMwR 패키지\n\nlibrary(DMwR2)\n\n\nair_before <- airquality\n\n\nair_after <- centralImputation(airquality) ## 결측치를 중앙값으로 대치\n\n\nna_indx <- which(!complete.cases(airquality)) ## Na인덱스를 추출\n\n\nhead(air_before[na_indx,])\n\n\n\nA data.frame: 6 × 6\n\n    OzoneSolar.RWindTempMonthDay\n    <dbl><int><dbl><int><int><int>\n\n\n    542.12931NA14.3565 5\n    628.00000NA14.9665 6\n    11 7.00000NA 6.974511\n    2742.12931NA 8.057527\n    9678.00000NA 6.9868 4\n    9735.00000NA 7.4858 5\n\n\n\n\n\nhead(air_after)\n\n\n\nA data.frame: 6 × 6\n\n    OzoneSolar.RWindTempMonthDay\n    <dbl><dbl><dbl><int><int><int>\n\n\n    141.00000190 7.46751\n    236.00000118 8.07252\n    312.0000014912.67453\n    418.0000031311.56254\n    542.1293120514.35655\n    628.0000020514.96656\n\n\n\n\n\n아래의 값을 살펴본 결과 결측치가 중앙값으로 잘 대치 되었음을 확인하였다.\n\n\nmedian(airquality$Solar.R,na.rm=T)\n\n205\n\n\n\nk최근접 이웃 알고리즘을 이용하여 na값을 대치해보기\n\n\napply(knnImputation(air_before,k=3),2,function(x) sum(is.na(x)))\n\nOzone0Solar.R0Wind0Temp0Month0Day0"
  },
  {
    "objectID": "post/R for Data Scienece/데이터 전처리/날짜 데이터 전처리.html",
    "href": "post/R for Data Scienece/데이터 전처리/날짜 데이터 전처리.html",
    "title": "03. 날짜 데이터",
    "section": "",
    "text": "today <- Sys.Date()\ntoday\n\n2021-12-05\n\n\n\nclass(today)\n\n'Date'\n\n\n\ntime <- Sys.time()\ntime\n\n[1] \"2021-12-05 14:46:38 KST\"\n\n\n\nclass(time)\n\n\n'POSIXct''POSIXt'\n\n\n\n각각의 형식을 보면 생성된 객체들의 특성을 파악할 수 있다."
  },
  {
    "objectID": "post/R for Data Scienece/데이터 전처리/날짜 데이터 전처리.html#현재-날자로부터-100일-후의-날짜-구하기",
    "href": "post/R for Data Scienece/데이터 전처리/날짜 데이터 전처리.html#현재-날자로부터-100일-후의-날짜-구하기",
    "title": "03. 날짜 데이터",
    "section": "현재 날자로부터 100일 후의 날짜 구하기",
    "text": "현재 날자로부터 100일 후의 날짜 구하기\n\nSys.Date() + 100\n\n2022-03-15"
  },
  {
    "objectID": "post/R for Data Scienece/데이터 전처리/날짜 데이터 전처리.html#로-표현된-데이터로부터-365일-후-의-날짜-구하기",
    "href": "post/R for Data Scienece/데이터 전처리/날짜 데이터 전처리.html#로-표현된-데이터로부터-365일-후-의-날짜-구하기",
    "title": "03. 날짜 데이터",
    "section": "“2020-01-01”로 표현된 데이터로부터 365일 후 의 날짜 구하기",
    "text": "“2020-01-01”로 표현된 데이터로부터 365일 후 의 날짜 구하기\n\nas.Date(\"2020-01-01\",format=\"%Y-%m-%d\") + 365\n\n2020-12-31"
  },
  {
    "objectID": "post/R for Data Scienece/데이터 전처리/날짜 데이터 전처리.html#과-2025-01-01-사이의-일-수-구하기",
    "href": "post/R for Data Scienece/데이터 전처리/날짜 데이터 전처리.html#과-2025-01-01-사이의-일-수-구하기",
    "title": "03. 날짜 데이터",
    "section": "“1990-01-01”과 “2025-01-01” 사이의 일 수 구하기",
    "text": "“1990-01-01”과 “2025-01-01” 사이의 일 수 구하기\n\nas.Date(\"2025-01-01\")-as.Date(\"1990-01-01\")\n\nTime difference of 12784 days"
  },
  {
    "objectID": "post/R for Data Scienece/데이터 전처리/날짜 데이터 전처리.html#구한-일수만-알고싶다면",
    "href": "post/R for Data Scienece/데이터 전처리/날짜 데이터 전처리.html#구한-일수만-알고싶다면",
    "title": "03. 날짜 데이터",
    "section": "구한 일수만 알고싶다면?",
    "text": "구한 일수만 알고싶다면?\n\nas.numeric(as.Date(\"2025-01-01\")-as.Date(\"1990-01-01\"))\n\n12784\n\n\n\ndifftime(\"2025-01-01\",\"1990-01-01\")\n\nTime difference of 12784 days"
  },
  {
    "objectID": "post/R for Data Scienece/데이터 전처리/날짜 데이터 전처리.html#시간-차이를-구할-경우",
    "href": "post/R for Data Scienece/데이터 전처리/날짜 데이터 전처리.html#시간-차이를-구할-경우",
    "title": "03. 날짜 데이터",
    "section": "시간 차이를 구할 경우?",
    "text": "시간 차이를 구할 경우?\n\nas.difftime(\"09:40:00\")-as.difftime(\"18:30:00\")\n\nTime difference of -8.833333 hours"
  },
  {
    "objectID": "post/R for Data Scienece/데이터 전처리/표준화와 정규화(1205).html",
    "href": "post/R for Data Scienece/데이터 전처리/표준화와 정규화(1205).html",
    "title": "02. 표준화와 정규화",
    "section": "",
    "text": "각 개체들이 평균을 기준으로 얼마나 떨어져 있는지를 나타내는 값으로 변환하는 과정\nZ-Score 표준화는 각 요소의 값에서 평균을 뺀 후 표준편차로 나누어 수행\n변환 후 데이터의 평균은 0, 표준편차는 1의 값을 갖게 된다."
  },
  {
    "objectID": "post/R for Data Scienece/데이터 전처리/표준화와 정규화(1205).html#r실습",
    "href": "post/R for Data Scienece/데이터 전처리/표준화와 정규화(1205).html#r실습",
    "title": "02. 표준화와 정규화",
    "section": "R실습",
    "text": "R실습\n\nscale\n\ncenter : TRUE이면 데이터에서 해당벡터의 평균을 뺌\nscale :\n\ncenter = T, scale = T 이면 데이터를 해당 벡터의 표준편차로 나눔\ncenter = F, scale = T 이면 데이터를 해당 벡터의 제곱평균제곱근으로 나눔\nscale = F 이면 데이터를 어떤 값으로도 나누지 않음\n\n\nmtcars 데이터의 mpg, hp 변수로만 이루어진 데이터프레임을 생성하고 각 변수를 표준화한 새로운 변수를 추가해보자.\n\n\nlibrary(tidyverse)\n\n\ntest <- mtcars %>%  select(mpg,hp)\n\n\ntest %>% transmute_at(vars(mpg,hp),scale) %>% head()\n##scale(test$mpg, center=T,scale=T) 안되면 걍이런식으로 노가다 붙이자\n\n\n\nA data.frame: 6 × 2\n\n    mpghp\n    <dbl[,1]><dbl[,1]>\n\n\n    Mazda RX4 0.1508848-0.5350928\n    Mazda RX4 Wag 0.1508848-0.5350928\n    Datsun 710 0.4495434-0.7830405\n    Hornet 4 Drive 0.2172534-0.5350928\n    Hornet Sportabout-0.2307345 0.4129422\n    Valiant-0.3302874-0.6080186"
  },
  {
    "objectID": "post/R for Data Scienece/데이터 전처리/표준화와 정규화(1205).html#r실습-1",
    "href": "post/R for Data Scienece/데이터 전처리/표준화와 정규화(1205).html#r실습-1",
    "title": "02. 표준화와 정규화",
    "section": "R실습",
    "text": "R실습\n\n사용자 정의함수 생성\n\n\nnormal <- function(x)\n    {\n    return ((x- min(x)) / (max(x)-min(x)))\n}\n\n\niris2 <- iris %>% transmute_at(vars(-Species),normal)\n\n\nhead(iris); head(iris2)\n\n\n\nA data.frame: 6 × 5\n\n    Sepal.LengthSepal.WidthPetal.LengthPetal.WidthSpecies\n    <dbl><dbl><dbl><dbl><fct>\n\n\n    15.13.51.40.2setosa\n    24.93.01.40.2setosa\n    34.73.21.30.2setosa\n    44.63.11.50.2setosa\n    55.03.61.40.2setosa\n    65.43.91.70.4setosa\n\n\n\n\n\n\nA data.frame: 6 × 4\n\n    Sepal.LengthSepal.WidthPetal.LengthPetal.Width\n    <dbl><dbl><dbl><dbl>\n\n\n    10.222222220.62500000.067796610.04166667\n    20.166666670.41666670.067796610.04166667\n    30.111111110.50000000.050847460.04166667\n    40.083333330.45833330.084745760.04166667\n    50.194444440.66666670.067796610.04166667\n    60.305555560.79166670.118644070.12500000"
  },
  {
    "objectID": "post/R for Data Scienece/비정형데이터마이닝/텍스트마이닝.html",
    "href": "post/R for Data Scienece/비정형데이터마이닝/텍스트마이닝.html",
    "title": "01. text mining",
    "section": "",
    "text": "텍스트 마이닝을 수행하기 전에 tm 패키지를 활용해 Corpus를 만들고 생성된 Corpus를 전처리 하고 분석에 활용하여야 한다.\nVectorSource 함수를 사용하고 문서로 완성된 데이터를 VCorpus 함수를 이용하여 Corpus로 만든다.\n\n\nlibrary(tm)\n\n\ncrude 데이터는 tm 패키지 내에 내장된 Corpus데이터 이다.\ninspect 하수를 이용해 첫 번째 문서의 정보를 확인한다.\n\n\ninspect(crude[1])\n\n<<VCorpus>>\nMetadata:  corpus specific: 0, document level (indexed): 0\nContent:  documents: 1\n\n$`reut-00001.xml`\n<<PlainTextDocument>>\nMetadata:  15\nContent:  chars: 527\n\n\n\n\n\n\ntm_map(x, tolower) : 소문자로 만들기\ntm_map(x, stemDocument) : 어근만 남기기\ntm_map(x, stripWhitespace) : 공백 제거\ntm_map(x, removePunctuation) : 문장부호 제거\ntm_map(x, removeNumbers) : 숫자 제거\ntm_map(x, removeWords, “words”) : 특정 단어 제거\ntm_map(x, removeWords,stopwords (“english)) : 불용어 제거\ntm_map(x, PlainTextDocument) : TextDocument로 변환\n\n\nsetwd(\"C:\\\\Users\\\\rkdcj\\\\OneDrive\\\\바탕 화면\\\\PART 06 실습용 데이터\")\n\n\nlibrary(tm)\n\n\n데이터 로드\n\n\nnews <- readLines(\"키워드_뉴스.txt\")\n\n\nnews.corpus <- VCorpus(VectorSource(news))\n\n\nnews.corpus\n\n<<VCorpus>>\nMetadata:  corpus specific: 0, document level (indexed): 0\nContent:  documents: 10\n\n\n\n전처리 함수 생성\n\n\nclean_txt <- function(txt) {\n        txt <- tm_map(txt, removeNumbers) ## 숫자제거\n        txt <- tm_map(txt, removePunctuation) ## 문장부호 제거\n        txt <- tm_map(txt, stripWhitespace)  ## 공백제거\n        \n        return(txt)\n}\n\n\nclean.news <- clean_txt(news.corpus)\n\n\nnews.corpus[[1]]$content\n\n'동아대학교(총장 한석정)가 \\'수요자 데이터기반 스마트헬스케어 서비스\\'분야 ‘4차 산업혁명 혁신선도대학으로 최종선정됐습니다. 동아대가 혁신선도대학으로 펼치게 될 ‘수요자 데이터기반 스마트헬스케어 서비스’ 산업은 리빙데이터(운동·영양·약물)와 메디컬데이터(생체계측·진료기록)를 종합 분석, 다양한 헬스케어 서비스를 제공하는 것입니다. 동아대는 건강과학대학과 의료원, 재활요양병원 등 경쟁력 있는 인프라를 바탕으로 신뢰도 높은 정밀 분석을 실시, ‘헬스케어 기획 전문가’와 ‘헬스케어 데이터분석 전문가’ 등 수요자 맞춤형 헬스케어 서비스 분야를 선도하는 전문 인재를 키워나갈 계획입니다. ‘스마트헬스케어 융합전공’을 신설, 경영정보학과를 중심으로 한 빅데이터 분석, 식품영양학과·의약생명공학과·건강관리학과 중심의 헬스케어 등 학문 간 경계는 교육혁신도 이뤄나갈 방침입니다. '\n\n\n\nclean.news[[1]]$content\n\n'동아대학교총장 한석정가 수요자 데이터기반 스마트헬스케어 서비스분야 ‘차 산업혁명 혁신선도대학으로 최종선정됐습니다 동아대가 혁신선도대학으로 펼치게 될 ‘수요자 데이터기반 스마트헬스케어 서비스’ 산업은 리빙데이터운동·영양·약물와 메디컬데이터생체계측·진료기록를 종합 분석 다양한 헬스케어 서비스를 제공하는 것입니다 동아대는 건강과학대학과 의료원 재활요양병원 등 경쟁력 있는 인프라를 바탕으로 신뢰도 높은 정밀 분석을 실시 ‘헬스케어 기획 전문가’와 ‘헬스케어 데이터분석 전문가’ 등 수요자 맞춤형 헬스케어 서비스 분야를 선도하는 전문 인재를 키워나갈 계획입니다 ‘스마트헬스케어 융합전공’을 신설 경영정보학과를 중심으로 한 빅데이터 분석 식품영양학과·의약생명공학과·건강관리학과 중심의 헬스케어 등 학문 간 경계는 교육혁신도 이뤄나갈 방침입니다 '\n\n\n\n확인 결과 ” ’, \\(\\cdot\\) “와 같은 부호는 제거 되지않음\n아래와 같은 함수를 사용하여 제거해준다.\n\n\ntxt<- gsub(\"[[:punct:]]\",\"\",clean.news)"
  },
  {
    "objectID": "post/R for Data Scienece/비정형데이터마이닝/텍스트마이닝.html#news-data-not-use-corpus",
    "href": "post/R for Data Scienece/비정형데이터마이닝/텍스트마이닝.html#news-data-not-use-corpus",
    "title": "01. text mining",
    "section": "news data not use Corpus",
    "text": "news data not use Corpus\n\nclean_txt2 <- function(txt) {\n        txt <- removeNumbers(txt) ## 숫자제거\n        txt <- removePunctuation(txt) ## 문장부호 제거\n        txt <- stripWhitespace(txt) ## 공백제거\n        txt <- gsub(\"[^[:alnum:]]\",\" \",txt) ## 영어와 숫자를 제거\n        return(txt)\n}\n\n\nclean.news <- clean_txt2(news)\n\n\n전처리 결과 푸드테크, 스타트업 등과 같은 복합명사가 분리되어 출력되는 것을 확인할 수 있었다.\n따라서 복합명사를 명사로 인식할 수 있도록 사전에 등록하고 다시 분석 결과를 확인하면 복합명사도 하나의 명사로 추가된 것으로 확인할 수 있다.\n\n\nhead(extractNoun(clean.news[5]))\n\n\n'푸드''테크''스타트''업''빅데이터''기반'\n\n\n\nbuildDictionary(ext_dic= \"sejong\",\n                user_dic=data.frame(c(read.table(\"food.txt\"))))\n\n370965 words dictionary was built.\n\n\n\nread.table(\"food.txt\")\n\n\n\nA data.frame: 4 × 2\n\n    V1V2\n    <chr><chr>\n\n\n    푸드테크    ncn\n    스타트업    ncn\n    빅데이터    ncn\n    우아한형제들ncn\n\n\n\n\n\nhead(extractNoun(clean.news[5]))\n\n\n'푸드테크''스타트업''들이''빅데이터''기반''시스템'\n\n\n\n위와 같이 복합명사도 하나의 명사로 추출된 것을 확인할 수 있다."
  },
  {
    "objectID": "post/R for Data Scienece/비정형데이터마이닝/텍스트마이닝.html#simplepos22를-활용해-형용사-추출하기",
    "href": "post/R for Data Scienece/비정형데이터마이닝/텍스트마이닝.html#simplepos22를-활용해-형용사-추출하기",
    "title": "01. text mining",
    "section": "SimplePos22를 활용해 형용사 추출하기",
    "text": "SimplePos22를 활용해 형용사 추출하기\n\nlibrary(stringr) ## R에서 문자열을 처리할 수 있는 패키지\n\n\ndoc1 <- paste(SimplePos22(clean.news[[2]]))\n\n\nhead(doc1)\n\n\n'첨단/NC''정보통신기술/NC+에/JC''AI/F''등/NB+이/JC''더하/PA+어/EC+지/PX+면서/EC''무기체계/NC+가/JC'\n\n\n\n형용사의 품사는 PA 이므로 아래와 같은 함수를 이용하여 추출\n\n\ndoc2 <- str_match(doc1,\"([가-힣]+)/PA\") \n\n\nhead(doc2)\n\n\n\nA matrix: 6 × 2 of type chr\n\n    NA     NA  \n    NA     NA  \n    NA     NA  \n    NA     NA  \n    더하/PA더하\n    NA     NA  \n\n\n\n\n\ndoc3 <- doc2 %>% na.omit()\ndoc3[,2]\n\n\n'더하''빠르''새롭''빠르''빠르''이러하''걸맞'"
  },
  {
    "objectID": "post/R for Data Scienece/비정형데이터마이닝/텍스트마이닝.html#stemming",
    "href": "post/R for Data Scienece/비정형데이터마이닝/텍스트마이닝.html#stemming",
    "title": "01. text mining",
    "section": "Stemming",
    "text": "Stemming\n\n어간추출은 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 할 수 있다.\n\n즉, 공통 어간을 가지는 단어를 묶는 작업을 Stemming이라고 한다.\n\nR에서는 tm패키지에서 stemDocument 함수를 통해 공통으로 들어가지 않은 부분을 제외하고 stemCompletion 함수를 통해 stemming된 단어와 완성을 위한 dictionary를 함께 넣으면 기본적인 어휘로 완성해주는 역할을 한다.\n\n\ntest <- stemDocument(c(\"analyze\",\"analyzed\",\"analyzing\"))\ntest\n\n\n'analyz''analyz''analyz'\n\n\n\n아래에서 중요한것은 stemCompletion 함수를 사용할 때에는 단어의 완성을 위해 반드시 dictionary가 필요하다.\n\n\nstemCompletion(test,dictionary =c(\"analyze\",\"analyzed\",\"analyzing\") )\n\nanalyz'analyze'analyz'analyze'analyz'analyze'"
  },
  {
    "objectID": "post/R for Data Scienece/비정형데이터마이닝/텍스트마이닝.html#tdm",
    "href": "post/R for Data Scienece/비정형데이터마이닝/텍스트마이닝.html#tdm",
    "title": "01. text mining",
    "section": "TDM",
    "text": "TDM\n\nR에서 tm패키지의 TermDocumentMatric을 사용하여 문서-단어 빈도행렬을 만들 수 있다. 단 객체는 Corpus 객체를 이용하여야함.\n\n\nVC.news <- VCorpus(VectorSource(clean.news))\n\n\ninspect(TermDocumentMatrix(VC.news))\n\n<<TermDocumentMatrix (terms: 1011, documents: 10)>>\nNon-/sparse entries: 1144/8966\nSparsity           : 89%\nMaximal term length: 14\nWeighting          : term frequency (tf)\nSample             :\n            Docs\nTerms        1 10 2 3 4 5 6 7 8 9\n  관계자는   0  1 0 1 0 3 0 1 0 0\n  데이터     0  4 3 6 8 3 7 1 4 5\n  밝혔다     0  1 0 1 2 0 1 0 2 0\n  분석을     1  0 0 1 2 0 2 0 0 0\n  브라이틱스 0  0 0 0 9 0 0 0 0 0\n  빅데이터   1  1 3 1 2 5 0 2 1 4\n  빅데이터를 0  2 0 0 0 5 0 0 0 1\n  서비스를   1  2 1 0 0 2 2 0 0 0\n  전문가     2  0 1 0 1 0 1 0 2 0\n  활용한     0  0 2 0 1 0 2 0 2 1\n\n\n\n총 10개의 기사에서 11의 단어가 추출된 것을 확인하였다.\n\n\n명사만 추출하여 TDM을 만들기\n\n사용자 정의 함수 생성\n\n\nwords <- function(doc) {\n    doc <- as.character(doc)\n    extractNoun(doc)\n}\n\n\n명사만 추출한 TDM 생성\n\n\nTDM.news2 <- TermDocumentMatrix(VC.news, control = list(tokenize = words))\n\n\ninspect(TDM.news2)\n\n<<TermDocumentMatrix (terms: 289, documents: 10)>>\nNon-/sparse entries: 360/2530\nSparsity           : 88%\nMaximal term length: 14\nWeighting          : term frequency (tf)\nSample             :\n            Docs\nTerms        1 10 2 3 4  5  6 7 8 9\n  경쟁력     1  0 0 2 0  1  1 0 1 1\n  경진대회   0  0 0 0 1  0  0 2 0 5\n  관계자     0  1 0 1 0  3  0 1 0 0\n  광양제철소 0  0 0 0 0  0  0 0 0 6\n  데이터     3  6 3 7 8  7 10 3 5 5\n  브라이틱스 0  0 0 0 9  0  0 0 0 0\n  빅데이터   1  4 4 1 3 16  0 2 1 6\n  서비스     3  3 3 0 0  7  3 1 0 0\n  시스템     0  2 0 2 1  3  0 0 0 0\n  전문가     2  1 1 4 1  1  1 1 2 1\n\n\n\n총 289개의 단어가 추출되었다.\nTDM으로 나타난 단어들의 빈도 체크\n\n\ntdm2 <- as.matrix(TDM.news2)\ntdm3 <- rowSums(tdm2)\ntdm4 <- tdm3[order(tdm3,decreasing = T)]\n\n\ntdm4[1:10]\n\n데이터57빅데이터38서비스20전문가15브라이틱스9경진대회8시스템8경쟁력7관계자6광양제철소6\n\n\n\n단어 사전을 정의하여 특정 단어들에 대해서만 분석 결과를 확인해보자.\n\n\nmydict <- names(tdm4)[1:10]\n\n\nmy.news <- TermDocumentMatrix(VC.news, control = list(tokenize=words,dictionary=mydict))\n\n\nmy.news\n\n<<TermDocumentMatrix (terms: 10, documents: 10)>>\nNon-/sparse entries: 54/46\nSparsity           : 46%\nMaximal term length: 5\nWeighting          : term frequency (tf)\n\n\n\n상위 10개 단어에 대해서만 추출하엿다.\n\n\ninspect(my.news)\n\n<<TermDocumentMatrix (terms: 10, documents: 10)>>\nNon-/sparse entries: 54/46\nSparsity           : 46%\nMaximal term length: 5\nWeighting          : term frequency (tf)\nSample             :\n            Docs\nTerms        1 10 2 3 4  5  6 7 8 9\n  경쟁력     1  0 0 2 0  1  1 0 1 1\n  경진대회   0  0 0 0 1  0  0 2 0 5\n  관계자     0  1 0 1 0  3  0 1 0 0\n  광양제철소 0  0 0 0 0  0  0 0 0 6\n  데이터     3  6 3 7 8  7 10 3 5 5\n  브라이틱스 0  0 0 0 9  0  0 0 0 0\n  빅데이터   1  4 4 1 3 16  0 2 1 6\n  서비스     3  3 3 0 0  7  3 1 0 0\n  시스템     0  2 0 2 1  3  0 0 0 0\n  전문가     2  1 1 4 1  1  1 1 2 1"
  },
  {
    "objectID": "post/R for Data Scienece/정형데이터마이닝/Supervised Learning.html",
    "href": "post/R for Data Scienece/정형데이터마이닝/Supervised Learning.html",
    "title": "01. Supervised Learning",
    "section": "",
    "text": "절차 : 목적 설정 -> 데이터 준비 -> 가공 -> 기법적용 -> 검증\n세부적인 절차는 아래와 같다.\n\n\n데이터를 로드 한 후 데이터의 형(type), 기초통계량 확인\n훈련용 데이터와 테스트 데이터를 분류\n모형 적합 후 모형확인 -> 유의하지 않은 변수가 많을 경우 step 함수를 이용하여 적절히 변수선택법을 사용\n모형 적합 후 검증 : ROC curve, AUC, 정확도, 특이도, 민감도 등을 확인"
  },
  {
    "objectID": "post/R for Data Scienece/정형데이터마이닝/Supervised Learning.html#r을-이용한-이항-로지스틱",
    "href": "post/R for Data Scienece/정형데이터마이닝/Supervised Learning.html#r을-이용한-이항-로지스틱",
    "title": "01. Supervised Learning",
    "section": "1-1. R을 이용한 이항 로지스틱",
    "text": "1-1. R을 이용한 이항 로지스틱\n\nsetwd(\"C:\\\\Users\\\\lee\\\\Desktop\\\\PART 05 실습용 데이터\")\ncredit_data <- read_csv(\"credit_final.csv\")\n#glimpse(credit_data) ## credit.rating의 경우 factor 변수이므로 변환\ncredit_data$credit.rating <- as_factor(credit_data$credit.rating)\n\nRows: 1000 Columns: 21\n\n-- Column specification ------------------------------------------------------------------------------------------------\nDelimiter: \",\"\ndbl (21): credit.rating, account.balance, credit.duration.months, previous.c...\n\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n# summary(credit_data)\n## 데이터 분할 각각의 데이터를 7:3으로 나눔\ntrain <- credit_data %>% sample_frac(0.7)\ntest <- credit_data %>% setdiff(train)\n\n\n## 로지스틱 회귀모형 적합\nlogit_fit <-glm(credit.rating~., data =train, \n                family = \"binomial\") ## 이진분류이므로 \"binomial\"이라고 기입\n\n# summary(logit_fit)\n\nstep.logit <- step(glm(credit.rating~., data =train, \n    family = \"binomial\"),direction = 'both')\n\n# summary(step.logit) ## 총 12개의 변수가 채택됨\n# length(step.logit$coefficients)\n\n\npred <- predict(step.logit, test %>% select(-1),type=\"response\") %>% as_tibble()\npred <- pred %>% mutate(hat_y = as.factor(ifelse(value>0.5,1,0)))\n\nStart:  AIC=695.13\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + guarantor + residence.duration + \n    current.assets + age + other.credits + apartment.type + bank.credits + \n    occupation + dependents + telephone + foreign.worker\n\n                                 Df Deviance    AIC\n- residence.duration              1   653.14 693.14\n- dependents                      1   653.18 693.18\n- occupation                      1   653.19 693.19\n- age                             1   653.21 693.21\n- guarantor                       1   653.34 693.34\n- bank.credits                    1   653.76 693.76\n- foreign.worker                  1   654.15 694.15\n<none>                                653.13 695.13\n- employment.duration             1   655.13 695.13\n- other.credits                   1   655.68 695.68\n- apartment.type                  1   656.90 696.90\n- credit.amount                   1   656.97 696.97\n- telephone                       1   658.59 698.59\n- credit.duration.months          1   658.67 698.67\n- marital.status                  1   659.91 699.91\n- previous.credit.payment.status  1   661.76 701.76\n- installment.rate                1   662.17 702.17\n- savings                         1   663.97 703.97\n- current.assets                  1   665.97 705.97\n- credit.purpose                  1   666.41 706.41\n- account.balance                 1   710.79 750.79\n\nStep:  AIC=693.14\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + guarantor + current.assets + \n    age + other.credits + apartment.type + bank.credits + occupation + \n    dependents + telephone + foreign.worker\n\n                                 Df Deviance    AIC\n- dependents                      1   653.20 691.20\n- occupation                      1   653.20 691.20\n- age                             1   653.25 691.25\n- guarantor                       1   653.35 691.35\n- bank.credits                    1   653.77 691.77\n- foreign.worker                  1   654.18 692.18\n<none>                                653.14 693.14\n- employment.duration             1   655.28 693.28\n- other.credits                   1   655.70 693.70\n- apartment.type                  1   656.90 694.90\n- credit.amount                   1   656.99 694.99\n+ residence.duration              1   653.13 695.13\n- telephone                       1   658.64 696.64\n- credit.duration.months          1   658.71 696.71\n- marital.status                  1   659.93 697.93\n- previous.credit.payment.status  1   661.81 699.81\n- installment.rate                1   662.18 700.18\n- savings                         1   664.13 702.13\n- current.assets                  1   666.17 704.17\n- credit.purpose                  1   666.51 704.51\n- account.balance                 1   711.31 749.31\n\nStep:  AIC=691.2\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + guarantor + current.assets + \n    age + other.credits + apartment.type + bank.credits + occupation + \n    telephone + foreign.worker\n\n                                 Df Deviance    AIC\n- occupation                      1   653.27 689.27\n- age                             1   653.29 689.29\n- guarantor                       1   653.41 689.41\n- bank.credits                    1   653.84 689.84\n- foreign.worker                  1   654.22 690.22\n<none>                                653.20 691.20\n- employment.duration             1   655.31 691.31\n- other.credits                   1   655.78 691.78\n- apartment.type                  1   656.91 692.91\n- credit.amount                   1   657.03 693.03\n+ dependents                      1   653.14 693.14\n+ residence.duration              1   653.18 693.18\n- telephone                       1   658.71 694.71\n- credit.duration.months          1   658.75 694.75\n- marital.status                  1   659.95 695.95\n- previous.credit.payment.status  1   661.92 697.92\n- installment.rate                1   662.21 698.21\n- savings                         1   664.16 700.16\n- current.assets                  1   666.24 702.24\n- credit.purpose                  1   666.62 702.62\n- account.balance                 1   711.60 747.60\n\nStep:  AIC=689.27\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + guarantor + current.assets + \n    age + other.credits + apartment.type + bank.credits + telephone + \n    foreign.worker\n\n                                 Df Deviance    AIC\n- age                             1   653.35 687.35\n- guarantor                       1   653.48 687.48\n- bank.credits                    1   653.93 687.93\n- foreign.worker                  1   654.28 688.28\n<none>                                653.27 689.27\n- employment.duration             1   655.52 689.52\n- other.credits                   1   655.88 689.88\n- apartment.type                  1   656.97 690.97\n- credit.amount                   1   657.05 691.05\n+ occupation                      1   653.20 691.20\n+ dependents                      1   653.20 691.20\n+ residence.duration              1   653.25 691.25\n- credit.duration.months          1   658.90 692.90\n- telephone                       1   659.76 693.76\n- marital.status                  1   659.97 693.97\n- previous.credit.payment.status  1   661.95 695.95\n- installment.rate                1   662.22 696.22\n- savings                         1   664.17 698.17\n- current.assets                  1   666.30 700.30\n- credit.purpose                  1   666.99 700.99\n- account.balance                 1   711.99 745.99\n\nStep:  AIC=687.35\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + guarantor + current.assets + \n    other.credits + apartment.type + bank.credits + telephone + \n    foreign.worker\n\n                                 Df Deviance    AIC\n- guarantor                       1   653.57 685.57\n- bank.credits                    1   653.97 685.97\n- foreign.worker                  1   654.39 686.39\n<none>                                653.35 687.35\n- other.credits                   1   655.96 687.96\n- employment.duration             1   656.02 688.02\n- credit.amount                   1   657.10 689.10\n+ age                             1   653.27 689.27\n+ occupation                      1   653.29 689.29\n+ dependents                      1   653.30 689.30\n+ residence.duration              1   653.32 689.32\n- apartment.type                  1   657.85 689.85\n- credit.duration.months          1   659.25 691.25\n- marital.status                  1   659.98 691.98\n- telephone                       1   660.07 692.07\n- previous.credit.payment.status  1   662.05 694.05\n- installment.rate                1   662.25 694.25\n- savings                         1   664.35 696.35\n- current.assets                  1   666.35 698.35\n- credit.purpose                  1   666.99 698.99\n- account.balance                 1   712.02 744.02\n\nStep:  AIC=685.57\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + current.assets + other.credits + \n    apartment.type + bank.credits + telephone + foreign.worker\n\n                                 Df Deviance    AIC\n- bank.credits                    1   654.17 684.17\n- foreign.worker                  1   654.67 684.67\n<none>                                653.57 685.57\n- other.credits                   1   656.21 686.21\n- employment.duration             1   656.25 686.25\n- credit.amount                   1   657.25 687.25\n+ guarantor                       1   653.35 687.35\n+ age                             1   653.48 687.48\n+ occupation                      1   653.52 687.52\n+ dependents                      1   653.52 687.52\n+ residence.duration              1   653.55 687.55\n- apartment.type                  1   658.04 688.04\n- credit.duration.months          1   659.38 689.38\n- telephone                       1   660.23 690.23\n- marital.status                  1   660.32 690.32\n- previous.credit.payment.status  1   662.15 692.15\n- installment.rate                1   662.49 692.49\n- savings                         1   664.37 694.37\n- credit.purpose                  1   667.27 697.27\n- current.assets                  1   667.27 697.27\n- account.balance                 1   712.07 742.07\n\nStep:  AIC=684.17\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + current.assets + other.credits + \n    apartment.type + telephone + foreign.worker\n\n                                 Df Deviance    AIC\n- foreign.worker                  1   655.39 683.39\n<none>                                654.17 684.17\n- employment.duration             1   656.69 684.69\n- other.credits                   1   657.30 685.30\n+ bank.credits                    1   653.57 685.57\n+ guarantor                       1   653.97 685.97\n- credit.amount                   1   658.07 686.07\n+ occupation                      1   654.09 686.09\n+ dependents                      1   654.10 686.10\n+ age                             1   654.12 686.12\n+ residence.duration              1   654.16 686.16\n- apartment.type                  1   658.67 686.67\n- credit.duration.months          1   659.92 687.92\n- telephone                       1   660.69 688.69\n- marital.status                  1   660.79 688.79\n- previous.credit.payment.status  1   662.55 690.55\n- installment.rate                1   662.92 690.92\n- savings                         1   665.50 693.50\n- current.assets                  1   667.58 695.58\n- credit.purpose                  1   668.17 696.17\n- account.balance                 1   712.69 740.69\n\nStep:  AIC=683.39\ncredit.rating ~ account.balance + credit.duration.months + previous.credit.payment.status + \n    credit.purpose + credit.amount + savings + employment.duration + \n    installment.rate + marital.status + current.assets + other.credits + \n    apartment.type + telephone\n\n                                 Df Deviance    AIC\n<none>                                655.39 683.39\n- employment.duration             1   657.93 683.93\n+ foreign.worker                  1   654.17 684.17\n- other.credits                   1   658.53 684.53\n+ bank.credits                    1   654.67 684.67\n- credit.amount                   1   658.86 684.86\n+ guarantor                       1   655.13 685.13\n+ occupation                      1   655.31 685.31\n+ age                             1   655.32 685.32\n+ dependents                      1   655.33 685.33\n+ residence.duration              1   655.36 685.36\n- apartment.type                  1   659.85 685.85\n- telephone                       1   661.73 687.73\n- credit.duration.months          1   661.88 687.88\n- marital.status                  1   662.40 688.40\n- previous.credit.payment.status  1   664.08 690.08\n- installment.rate                1   664.36 690.36\n- savings                         1   666.61 692.61\n- credit.purpose                  1   668.81 694.81\n- current.assets                  1   669.65 695.65\n- account.balance                 1   713.23 739.23\n\n\n\nconfusionMatrix(data=pred$hat_y,reference = test$credit.rating,positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  40  23\n         1  56 181\n                                         \n               Accuracy : 0.7367         \n                 95% CI : (0.683, 0.7856)\n    No Information Rate : 0.68           \n    P-Value [Acc > NIR] : 0.0192801      \n                                         \n                  Kappa : 0.3343         \n                                         \n Mcnemar's Test P-Value : 0.0003179      \n                                         \n            Sensitivity : 0.8873         \n            Specificity : 0.4167         \n         Pos Pred Value : 0.7637         \n         Neg Pred Value : 0.6349         \n             Prevalence : 0.6800         \n         Detection Rate : 0.6033         \n   Detection Prevalence : 0.7900         \n      Balanced Accuracy : 0.6520         \n                                         \n       'Positive' Class : 1              \n                                         \n\n\n\noptions(repr.plot.res=150,repr.plot.width=8,repr.plot.height=5)\npred.roc <- prediction(as.numeric(pred$hat_y),as.numeric(test$credit.rating))\nplot(performance(pred.roc,\"tpr\",\"fpr\"),lwd=2)\nabline(a=0,b=1,lty=\"dashed\",col=\"red\",lwd=2)\nperformance(pred.roc,\"auc\")@y.values\n\n\n    0.651960784313725\n\n\n\n\n\n\n\n위를 살펴보면 기존의 로지스틱회귀 모형을 적합했을 때 유의미하지 않은 변수가 다수 보여 단계적 변수 선택법을 적용하여 모델을 적합시켰다.\n정확도는 약 73%, 민감도와 특이도는 각각 85%, 44%로 측정되었으며, AUC값은 0.6484로 산출되었다. 그닥 훌륭한 모델이라고 보기 어렵다."
  },
  {
    "objectID": "post/R for Data Scienece/정형데이터마이닝/Supervised Learning.html#다항로지스틱",
    "href": "post/R for Data Scienece/정형데이터마이닝/Supervised Learning.html#다항로지스틱",
    "title": "01. Supervised Learning",
    "section": "1-2. 다항로지스틱",
    "text": "1-2. 다항로지스틱\n\n##iris 데이터를 이용한 다항 로지스틱 회귀분석\ntrain.iris <- iris %>% sample_frac(0.7)\ntest.iris <- iris %>% setdiff(train.iris)\n\nmul.iris <- multinom(Species~.,train.iris)\nsummary(mul.iris)\n\n# weights:  18 (10 variable)\ninitial  value 115.354290 \niter  10 value 11.452176\niter  20 value 3.153238\niter  30 value 2.649388\niter  40 value 2.341641\niter  50 value 2.256572\niter  60 value 1.995702\niter  70 value 1.245143\niter  80 value 0.904025\niter  90 value 0.621338\niter 100 value 0.603201\nfinal  value 0.603201 \nstopped after 100 iterations\n\n\nCall:\nmultinom(formula = Species ~ ., data = train.iris)\n\nCoefficients:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    51.48231    -30.82089   -21.44772     85.88038    24.74862\nvirginica    -96.62251    -68.87111   -61.97239    164.13765    94.06323\n\nStd. Errors:\n           (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width\nversicolor    121.5106     132.8265    97.66829     74.74537    36.83325\nvirginica     118.3141     144.8161   106.79819     64.21134    33.88914\n\nResidual Deviance: 1.206402 \nAIC: 21.2064 \n\n\n\npred.mul <- predict(mul.iris,test.iris %>% select(-5)) \nconfusionMatrix(pred.mul,test.iris$Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         13          0         0\n  versicolor      0         16         0\n  virginica       0          1        15\n\nOverall Statistics\n                                          \n               Accuracy : 0.9778          \n                 95% CI : (0.8823, 0.9994)\n    No Information Rate : 0.3778          \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.9665          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.9412           1.0000\nSpecificity                 1.0000            1.0000           0.9667\nPos Pred Value              1.0000            1.0000           0.9375\nNeg Pred Value              1.0000            0.9655           1.0000\nPrevalence                  0.2889            0.3778           0.3333\nDetection Rate              0.2889            0.3556           0.3333\nDetection Prevalence        0.2889            0.3556           0.3556\nBalanced Accuracy           1.0000            0.9706           0.9833"
  },
  {
    "objectID": "post/R for Data Scienece/정형데이터마이닝/Supervised Learning.html#binary-classification",
    "href": "post/R for Data Scienece/정형데이터마이닝/Supervised Learning.html#binary-classification",
    "title": "01. Supervised Learning",
    "section": "2-1. binary classification",
    "text": "2-1. binary classification\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\n\ndt.model <- rpart(credit.rating~.,method=\"class\", ## 회귀가 아닌 분류이므로 \"class\" 기입\n                                   data = train, \n                                   control = rpart.control(maxdepth=5,\n                                                           minsplit=15)) ## 의사결정나무의 최대 깊이는 5, 노드에서의 최소 관측치는 15개이 상\n\n\noptions(repr.plot.res=200)\nrpart.plot(dt.model,extra=2) ## extra 인자를 이용해 각각 범주의 분류된 개수를 파악, extra=3 은 오분류된 개수를 보여주\n\n\n\n\n\n총 700개의 관측치 중 496개의 관측치를 1로 분류하였으며, account.balance <3 인 377개의 노드 중 210개가 1로 분류되었음을 의미한다.\ncptable을 확인하여 xerror가 가장 낮은 split 개수를 택한 후 가지치기를 적용하자.\nxerror는해당 CP에서 교차검증오류를 나타내며 CP는 복잡성을 나타낸다.\n\n\ndt.model$cptable\n\n\n\nA matrix: 4 × 5 of type dbl\n\n    CPnsplitrel errorxerrorxstd\n\n\n    10.0526960801.00000001.00000000.05893547\n    20.0245098040.78921570.92647060.05757874\n    30.0147058850.76470590.88725490.05678633\n    40.0100000070.73529410.88725490.05678633\n\n\n\n\n\n확인 결과 xerror가 가장 낮은 분할 횟수는 7이며, 앞선 모형의 그래프를 봤을 때 모델이 분할을 7번까지 한다고 할 수 있다.\n해당 위치에서의 CP 값을 전달받자\n\n\nopt <- which.min(dt.model$cptable[,\"xerror\"])\ncp  <- dt.model$cptable[opt,\"CP\"]\n\n\nplotcp를 그려본결과 8번째 분할에서 xerror가 가장 낮지만 과적합을 막기위해 cptable에서 산출한 cp값을 이용하여 가지치기를 하자.\n\n\nplotcp(dt.model)\n\n\n\n\n\n가지치기 수행\n\n\noptions(repr.plot.res=200)\nrpart.plot(dt.model,extra=2) ## extra 인자를 이용해 각각 범주의 분류된 개수를 파악, extra=3 은 오분류된 개수를 보여준다\n\n\n\n\n\nprune.c <- prune(dt.model,cp=cp)\nrpart.plot(prune.c,extra=2)\n\n\n\n\n\n기존의 모델보다 복잡성이 줄어든 것으로 볼 수 있다.\n예측을 통하여 우리가 생성한 모델의 타당성을 평가하자.\n\n\npred.dt <- predict(dt.model, test %>% select(-1),type=\"class\")\n\n\nconfusionMatrix(data=pred.dt,reference= test$credit.rating)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  37  19\n         1  59 185\n                                          \n               Accuracy : 0.74            \n                 95% CI : (0.6865, 0.7887)\n    No Information Rate : 0.68            \n    P-Value [Acc > NIR] : 0.014           \n                                          \n                  Kappa : 0.3285          \n                                          \n Mcnemar's Test P-Value : 1.006e-05       \n                                          \n            Sensitivity : 0.3854          \n            Specificity : 0.9069          \n         Pos Pred Value : 0.6607          \n         Neg Pred Value : 0.7582          \n             Prevalence : 0.3200          \n         Detection Rate : 0.1233          \n   Detection Prevalence : 0.1867          \n      Balanced Accuracy : 0.6461          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n정분류율은 0.73이며 특이도, 민감도는 각각 0.8578, 0.4270으로 산출되었다.\n\n\npred.dt.roc <- prediction(as.numeric(pred.dt),as.numeric(test$credit.rating))\n\n\nplot(performance(pred.dt.roc,\"tpr\",\"fpr\"),lwd=2)\nabline(a=0,b=1,lty=2,col=\"red\",lwd=2)\n\n\n\n\n\nperformance(pred.dt.roc,\"auc\")@y.values\n\n\n    0.646139705882353\n\n\n\n\n산출된 AUC값은 0.642로 좋은 모델은 아니다."
  },
  {
    "objectID": "post/R for Data Scienece/정형데이터마이닝/Supervised Learning.html#multiple-classification",
    "href": "post/R for Data Scienece/정형데이터마이닝/Supervised Learning.html#multiple-classification",
    "title": "01. Supervised Learning",
    "section": "2-2. multiple classification",
    "text": "2-2. multiple classification\n\ndt.model2 <- rpart(Species ~. ,data=train.iris)\n\n\nrpart.plot(dt.model2,extra=2)\n\n\n\n\n\n총 105개의 데이터에서 39개의 데이터가 virginica로 분류되었고, Petal.Length < 2.5인 34개의 데이터는 모두 setosa로 분류되었다.\n\n\npred.dt2 <- predict(dt.model2, test.iris %>% select(-5),type=\"class\")\n\n\nconfusionMatrix(pred.dt2,test.iris$Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         13          0         0\n  versicolor      0         16         3\n  virginica       0          1        12\n\nOverall Statistics\n                                          \n               Accuracy : 0.9111          \n                 95% CI : (0.7878, 0.9752)\n    No Information Rate : 0.3778          \n    P-Value [Acc > NIR] : 1.099e-13       \n                                          \n                  Kappa : 0.8655          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            0.9412           0.8000\nSpecificity                 1.0000            0.8929           0.9667\nPos Pred Value              1.0000            0.8421           0.9231\nNeg Pred Value              1.0000            0.9615           0.9062\nPrevalence                  0.2889            0.3778           0.3333\nDetection Rate              0.2889            0.3556           0.2667\nDetection Prevalence        0.2889            0.4222           0.2889\nBalanced Accuracy           1.0000            0.9170           0.8833\n\n\n\n정확도의 0.9556, 각각의 class에서 특이도와 민감도를 확인해보니 적절한 분류가 이루어졌음을 알 수 있다."
  },
  {
    "objectID": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html",
    "href": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html",
    "title": "02. unsupervised learning",
    "section": "",
    "text": "개체간 유사성에 따라 집단을 분류하고, 군집 내 유사성과 군집 간 상이성을 규명하는 다변량 분석 기법이다.\n군집분석에서는 관측 데이터 간 유사성이나 근접성을 측정해 어느 군집으로 묶을 수 있는지 판단해야 한다.\n\n\n\n\n\n\n유클리디안거리 (euclidean) : 가장 널리 사용되나 통계적 개념이 내포되어 있지 않아 변수들의 산포 정도가 전혀 감안되어 있지 않음.\n표준화 거리(Standardized) : 해당변수의 표준편차로 척도 변환 후 유클리드안 거리를 계산하는 방법. 분산 차이로 인한 왜곡을 피할 수 있음.\n마할라노비스 거리(Mahalanobis) : 통계적 개념이 포함된 거리로 두 벡터 사이의 거리를 표본공분산으로 나눠주어야 한다. 그룹에 대한 사전 지식 없이는 표본공분산 S를 계산할 수 없으므로 사용하기 곤란하다.\n체비셰프 거리 (Chebyshev) : \\(d(x,y) = max_{i} |x_i-y_i|\\)\n멘하탄 거리 (Manhattan) : 유클리디안과 비슷 -> \\(d(x,y) = \\sum |x_i-y_i|\\)\n캔버라 거리(Canberra), 민코우스키(Minkowski) 거리 등이 있음\n\n\n\n\n\n자카드 거리\n\n\\[ 1-  J(A,B) = \\frac {|A\\cup B|- |A\\cap B|}{|A\\cup B|}\\]\n\n코사인 유사도 : 두 개체의 백터 내적의 코사인 값을 이용하여 측정된 벡터간의 유사한 정도\n\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html#종류",
    "href": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html#종류",
    "title": "02. unsupervised learning",
    "section": "1-1. 종류",
    "text": "1-1. 종류\n\n최단연결법 (single linkage, nearest neighbor)\n\nnxn 거리 행렬에서 가장 가까운 데이터를 묶어서 군집을 형성한다.\n\n최장연결법 (complete linkage) : 데이터 간 거리를 계산할 때 최장거리를 거리로 계산하여 거리행렬을 수정하는 방법\n평균연결법 (average linkage)\n와드연결법(Ward linkage) : 군집 내 편차들의 제곱합을 고려한 방법, 군집간 정보의 손실을 최소화 하기 위해 군집화를 진행 \\(\\to sse\\)를 고려한다고 생각"
  },
  {
    "objectID": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html#절차",
    "href": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html#절차",
    "title": "02. unsupervised learning",
    "section": "1-2. 절차",
    "text": "1-2. 절차\nstep 1. 거리행렬을 통해 가장 가까운 거리의 객체들 간의 관계를 규명하고 덴드로그램을 그린다.\nstep 2. 덴드로그램을 보고 군집의 개수를 변화해 가면서 적절한 군집 수를 선정한다.\nstep 3. 군집의 수는 분석 목적에 따라 선정할수 있지만 대부분 5개 이상의 군집은 잘 활용하지 않는다."
  },
  {
    "objectID": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html#r-실습",
    "href": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html#r-실습",
    "title": "02. unsupervised learning",
    "section": "1-3. R 실습",
    "text": "1-3. R 실습\n\ndist(data, method)\n\nmethod : 거리측정방법, euclidean, maximum, manhattan, canberra, binary, minkowski가 있음\n\nhclust (data, method)\n\ndata : dist 함수로 거리가 측정된 데이터\nmethod : 거리측정 밥법, single, complete, average, median, ward.D 가 있음\n\n\n\nUS <- USArrests\nhead(US)\n\n\n\nA data.frame: 6 × 4\n\n    MurderAssaultUrbanPopRape\n    <dbl><int><int><dbl>\n\n\n    Alabama13.22365821.2\n    Alaska10.02634844.5\n    Arizona 8.12948031.0\n    Arkansas 8.81905019.5\n    California 9.02769140.6\n    Colorado 7.92047838.7\n\n\n\n\n\nUS.dist <- dist(US, \"euclidean\")\n\n\ndist^2을 한 이유는 거리의 차이를 많이 두어 군집이 나뉘는 것을 쉽게 확인하기 위해서임.\n\n\n# hclust 함수를 이용하여 계층적 군집분석\noptions(repr.plot.res=200,repr.plot.hight=5,repr.plot.width=7)\nUS.single <- hclust(US.dist^2,method=\"single\")\nplot(US.single)\n\n\n\n\n\n# cutree함수를 이용하여 계층적 군집결과를 그릅 나누기\ngroup <- cutree(US.single,k=5) ## k는 군집의 수, h는 높이\ngroup\n\nAlabama1Alaska2Arizona1Arkansas3California1Colorado3Connecticut3Delaware1Florida4Georgia3Hawaii3Idaho3Illinois1Indiana3Iowa3Kansas3Kentucky3Louisiana1Maine3Maryland1Massachusetts3Michigan1Minnesota3Mississippi1Missouri3Montana3Nebraska3Nevada1New Hampshire3New Jersey3New Mexico1New York1North Carolina5North Dakota3Ohio3Oklahoma3Oregon3Pennsylvania3Rhode Island3South Carolina1South Dakota3Tennessee3Texas3Utah3Vermont3Virginia3Washington3West Virginia3Wisconsin3Wyoming3\n\n\n\n## 각각의 그룹을 사각형으로 그룹지어 나타나기\nplot(US.single)\nrect.hclust(US.single,k=5,border=\"red\")"
  },
  {
    "objectID": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html#측도",
    "href": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html#측도",
    "title": "02. unsupervised learning",
    "section": "4-1. 측도",
    "text": "4-1. 측도\n\\(\\divideontimes\\) tip:측도의 단어 뜻을 직관적으로 받아들여 해석하자.\n\n지지도 (Support) : 전체 거래 중 항목 A와 항목 B를 동시에 포함하는 거래의 비율로 정의한다.\n\n\\(\\quad P (A \\cup B)\\)\n\n신뢰도 (Confidence) : 항목 A를 포함한 거래중 항목 B도 같이 포함될 확률\n\n\\(\\begin {eqnarray*}\\quad P(B|A) = \\frac {P(A\\cap B)}{P(A)} \\end{eqnarray*}\\)\n\n향상도 (Lift) : A가 구매되지 않았을 때 품목 B의 구매확률에 비해 A가 구매됐을 때 품목 B의 구매확률의 증가비\n\n\\(\\quad \\begin {eqnarray*} \\frac {P(B|A)}{P(B)}\\end{eqnarray*}\\)"
  },
  {
    "objectID": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html#apriori-알고리즘",
    "href": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html#apriori-알고리즘",
    "title": "02. unsupervised learning",
    "section": "4-2. Apriori 알고리즘",
    "text": "4-2. Apriori 알고리즘\n\n빈발항목집합(frequent item set) : 최소 지지도 보다 큰 지지도 값을 갖는 품목의 집합\nApriori 알고리즘 : 모든 품목집합에 대한 지지도를 전부 계산하는 것이 아니라, 최소 지지도 이상의 빈발항목집합을 찾은 후 그것들에 대해서만 연관규칙을 계산하는 것.\n지지도가 낮은 후보 집합 생성 시 아이템의 개수가 많아지면 계산 복잡도가 증가한다는 문제점을 가지고 있다."
  },
  {
    "objectID": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html#r실습",
    "href": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html#r실습",
    "title": "02. unsupervised learning",
    "section": "4-3. R실습",
    "text": "4-3. R실습\n\nas함수를 이용하여 데이터프레임을 transactions 함수로 변형 후에 진행하여야 하나 예제 데이터는 이미 변형이 된 데이터이다.\n\n\nlibrary(arules)\n\n\ndata(Groceries)\ninspect(Groceries[1:3]) ##연관규칙 확인\n\n    items                \n[1] {citrus fruit,       \n     semi-finished bread,\n     margarine,          \n     ready soups}        \n[2] {tropical fruit,     \n     yogurt,             \n     coffee}             \n[3] {whole milk}         \n\n\n\nrules <- apriori(Groceries,\n                 parameter = list(support=0.01,\n                                  confidence = 0.3))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n        0.3    0.1    1 none FALSE            TRUE       5    0.01      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 98 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].\nsorting and recoding items ... [88 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 done [0.00s].\nwriting ... [125 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\n\n\n총 125개의 연관규칙이 생성되었다.\n\n\ninspect(sort(rules,by=c(\"confidence\"),decreasing = T)[1:5])\n\n    lhs                                 rhs                support   \n[1] {citrus fruit,root vegetables}   => {other vegetables} 0.01037112\n[2] {tropical fruit,root vegetables} => {other vegetables} 0.01230300\n[3] {curd,yogurt}                    => {whole milk}       0.01006609\n[4] {other vegetables,butter}        => {whole milk}       0.01148958\n[5] {tropical fruit,root vegetables} => {whole milk}       0.01199797\n    confidence coverage   lift     count\n[1] 0.5862069  0.01769192 3.029608 102  \n[2] 0.5845411  0.02104728 3.020999 121  \n[3] 0.5823529  0.01728521 2.279125  99  \n[4] 0.5736041  0.02003050 2.244885 113  \n[5] 0.5700483  0.02104728 2.230969 118  \n\n\n\n좌항에서 우항, 우항에서 좌항의 규칙이 겹치는 경우가 있으므로 중복규칙 제거함수 구현\n\n\nprune.dup.rules <- function(rules) {\n                    rule.subset.matrix <- is.subset(rules,rules,sparse=F)\n                    rule.subset.matrix[lower.tri(rule.subset.matrix, diag =T)] <- NA\n                    dup.rules <- colSums(rule.subset.matrix,na.rm =T) >=1\n                    pruned.rules <- rules[!dup.rules]\n                    return (pruned.rules)\n                    }\n\n\n만약 우변의 아이템 구매를 이끌 아이템을 찾고 싶다면?\n\n\nmetric.params <- list(supp=0.001,conf=0.5,minlen=2) ## minlen은 좌항과 우항을 합친 최소 물품수\nrules <- apriori(Groceries,\n                 parameter = metric.params,\n                 appearance = list(default = \"lhs\",rhs=\"soda\"),\n                 control = list(verbose=F))\n\n\napperance : 우측의 soda를 사기위해 좌항의 아이템을 찾는 것으로 설정\nverbose : apriori 함수 실행 결과를 나타낼지의 여부를 묻는 인자\n\n\nrules <- prune.dup.rules(rules)\n\n\nrules <- sort(rules,decreasing=T,by=\"confidence\")\n\n\ninspect(rules[1:5])\n\n    lhs                                     rhs    support     confidence\n[1] {coffee,misc. beverages}             => {soda} 0.001016777 0.7692308 \n[2] {sausage,bottled water,bottled beer} => {soda} 0.001118454 0.7333333 \n[3] {sausage,white bread,shopping bags}  => {soda} 0.001016777 0.6666667 \n[4] {rolls/buns,bottled water,chocolate} => {soda} 0.001321810 0.6500000 \n[5] {pastry,misc. beverages}             => {soda} 0.001220132 0.6315789 \n    coverage    lift     count\n[1] 0.001321810 4.411303 10   \n[2] 0.001525165 4.205442 11   \n[3] 0.001525165 3.823129 10   \n[4] 0.002033554 3.727551 13   \n[5] 0.001931876 3.621912 12   \n\n\n\n만약 좌변의 아이템 세트를 가지고 있을 때 물품을 찾고 싶다면?\n\n\nmetric.params <- list(supp=0.001,conf=0.3,minlen=2) ## minlen은 좌항과 우항을 합친 최소 물품수\nrules <- apriori(Groceries,\n                 parameter = metric.params,\n                 appearance = list(default = \"rhs\",lhs=c(\"yogurt\",\"sugar\")),\n                 control = list(verbose=F))\n\n\nrules <- prune.dup.rules(rules)\n\n\nrules <- sort(rules,decreasing=T, by=\"confidence\")\n\n\ninspect(rules,decreasing=T,by=\"confidence\")\n\n    lhs               rhs                  support     confidence coverage   \n[1] {sugar}        => {whole milk}         0.015048297 0.4444444  0.033858668\n[2] {yogurt}       => {whole milk}         0.056024403 0.4016035  0.139501779\n[3] {sugar}        => {other vegetables}   0.010777834 0.3183183  0.033858668\n[4] {yogurt}       => {other vegetables}   0.043416370 0.3112245  0.139501779\n[5] {yogurt,sugar} => {whipped/sour cream} 0.002135231 0.3088235  0.006914082\n[6] {yogurt,sugar} => {root vegetables}    0.002135231 0.3088235  0.006914082\n    lift     count\n[1] 1.739400 148  \n[2] 1.571735 551  \n[3] 1.645119 106  \n[4] 1.608457 427  \n[5] 4.308198  21  \n[6] 2.833283  21"
  },
  {
    "objectID": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html#extra.-som-자기조직화지도",
    "href": "post/R for Data Scienece/정형데이터마이닝/Unsupervised Learning(1204).html#extra.-som-자기조직화지도",
    "title": "02. unsupervised learning",
    "section": "Extra. SOM (자기조직화지도)",
    "text": "Extra. SOM (자기조직화지도)\n\nlibrary(kohonen) ## somgmgid를 위한함수\n\n\n\ntrain <- sample(1:150, 100) #무작위로 100개 추출 (학습데이터)\ntrain_Set <-  list( x = as.matrix(iris[train,-5]), Species = as.factor(iris[train,5])) #학습데이터 list형\ntest_Set <- list(x = as.matrix(iris[-train,-5]), Species = as.factor(iris[-train,5])) #테스트 데이터 list형\ngr <- somgrid(xdim = 3, ydim = 5, topo = \"hexagonal\") #grid 갯수 및 모양 설정\nss <- supersom(train_Set, gr, rlen = 200, alpha = c(0.05, 0.01)) #som 학습하기\n\n\ngrid : 경쟁층의 가충치 테이블을 5 x 4로 설정 topology 위상방법은 rectangular, hexagonal 두가지 중 하나를 설정한다.\nrlen : 전체 데이트 세트가 네트워크에 표시되는 횟수 (= 학습 횟수)\nalpha : 학습율, 변화의 양을 나타내는 두 개의 숫자의 벡터. 기본값은 rlen 업데이트에 대해 0.05에서 0.01로 선형적으로 감소하는 것이다.\n\n\nsummary(ss)\n\nSOM of size 3x5 with a hexagonal topology and a bubble neighbourhood function.\nThe number of data layers is 2.\nDistance measure(s) used: sumofsquares, tanimoto.\nTraining data included: 100 objects.\nMean distance to the closest unit in the map: 0.008.\n\n\n\n요약결과 가장 가까운 노드까지의 평균 거리가 0.008 이다.\n학습을 거듭하면서 뉴런과 학습 데이터의 거리가 짧아짐을 아래와 같이 확인할 수 있었다.\n\n\nplot(ss,type=\"change\")\n\n\n\n\n\n또한 학습된 som모델의 각 뉴런이 몇 개의 학습 데이터와 매핑이 되는 지를 확인 할 수 있었다.\n결과적으로 보았을 때 매핑이 더ㅣ지않은 뉴런이 존재하는 것으로 보아 생성한 som 모델의 신경망 크기가 크다는 것을 의미한다.\n\n\nplot(ss, type=\"count\")\n\n\n\n\n\n아래와 같이 통합거리매트릭스를 그려 각 뉴런의 이웃간 거리를 표현할 수 있었다. 값이 높을 수록 해당 뉴런은 이웃뉴련과 비유사하다고 할 수 있다.\n\n\nplot(ss, type=\"dist.neighbours\")\n\n\n\n\n\n또한 아래그림처럼 각 뉴런에서 Species의 속성들 가중치 기여율을 알 수 있다.\n\n\nplot(ss, type=\"codes\")\n\n\n\n\n\n\n\n\n학습된 모델을 통해 학습에 사용된 데이터 분류와 모델분류를 확인하기 위해 혼동행렬을 그려보면 아래와 같다.\n\n\nsom.prediction <- predict(ss, newdata = test_Set[1],type=\"class\")\n\n\nsom.prediction$predictions$Species\n\n\nsetosasetosasetosasetosasetosasetosasetosasetosasetosasetosasetosasetosa<NA>versicolorversicolorversicolorversicolorversicolorversicolorversicolor<NA>versicolorversicolorversicolorversicolorversicolorversicolorversicolorvirginicavirginicavirginicavirginicavirginicavirginicavirginicavirginicaversicolorvirginicavirginicavirginicavirginicavirginicavirginicavirginicavirginicavirginicavirginicavirginicavirginicavirginica\n\n\n    \n        Levels:\n    \n    \n    'setosa''versicolor''virginica'\n\n\n\n\nlibrary(caret)\n\n\nconfusionMatrix(som.prediction$predictions$Species,test_Set$Species)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa         12          0         0\n  versicolor      0         14         1\n  virginica       0          0        21\n\nOverall Statistics\n                                          \n               Accuracy : 0.9792          \n                 95% CI : (0.8893, 0.9995)\n    No Information Rate : 0.4583          \n    P-Value [Acc > NIR] : 3.148e-15       \n                                          \n                  Kappa : 0.9677          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                   1.00            1.0000           0.9545\nSpecificity                   1.00            0.9706           1.0000\nPos Pred Value                1.00            0.9333           1.0000\nNeg Pred Value                1.00            1.0000           0.9630\nPrevalence                    0.25            0.2917           0.4583\nDetection Rate                0.25            0.2917           0.4375\nDetection Prevalence          0.25            0.3125           0.4375\nBalanced Accuracy             1.00            0.9853           0.9773"
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/T-검정.html",
    "href": "post/R for Data Scienece/통계분석/T-검정.html",
    "title": "02. T-test",
    "section": "",
    "text": "개념 : 단일모집단에서 관심이 있는 연속형 벼수의 평균값을 특정기준값과 비교하고자 할 때 사용\n\n\\[ H_0 : \\mu_0 = \\mu_1 \\quad H_1 : not\\,\\, H_0\\]\n\n모집단이 정규성을 따른다고 가정하고 표본의 크기가 30보다 클 경우, 중심극한정리에 따라 정규분포를 따른다고 가정함.\n정규성을 만족할 경우 t-test, 그렇지 않은 경우 wilcox.test 함수를 이용하여 T-검정을 수행\n\n\n\n\\[H_0 : \\mu = 2.6 \\quad H_1 :  not \\, H_0\\]\n\nlibrary(MASS)\n\n\n str(cats) ## \n\n'data.frame':   144 obs. of  3 variables:\n $ Sex: Factor w/ 2 levels \"F\",\"M\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Bwt: num  2 2 2 2.1 2.1 2.1 2.1 2.1 2.1 2.1 ...\n $ Hwt: num  7 7.4 9.5 7.2 7.3 7.6 8.1 8.2 8.3 8.5 ...\n\n\n\n표본의 크기가 30보다 크므로 정규성을 만족한다고 가정\n\n\nt.test(cats$Bwt,mu=2.6,alternative= \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  cats$Bwt\nt = 3.0565, df = 143, p-value = 0.002673\nalternative hypothesis: true mean is not equal to 2.6\n95 percent confidence interval:\n 2.643669 2.803553\nsample estimates:\nmean of x \n 2.723611 \n\n\n\n검정통계량의 근거한 p-value 값을 보았을 때 귀무가설을 기각한다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/T-검정.html#r-실습-1",
    "href": "post/R for Data Scienece/통계분석/T-검정.html#r-실습-1",
    "title": "02. T-test",
    "section": "2-1. R 실습",
    "text": "2-1. R 실습\n\ndata <- data.frame(before = c(7,3,4,5,2,1,6,6,5,4),\n                  after = c(8,4,5,6,2,3,6,8,6,5))\n\n\nshapiro.test(c(data$before,data$after)) ## 정규성을 만족\n\n\n    Shapiro-Wilk normality test\n\ndata:  c(data$before, data$after)\nW = 0.95961, p-value = 0.5362\n\n\n\nt.test(data$before,data$after, paired=T,alternative=\"less\")\n\n\n    Paired t-test\n\ndata:  data$before and data$after\nt = -4.7434, df = 9, p-value = 0.0005269\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n       -Inf -0.6135459\nsample estimates:\nmean of the differences \n                     -1 \n\n\n\n결과적으로 영양제 복용 후 수면시간이 더 길었다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/T-검정.html#r-실습-2",
    "href": "post/R for Data Scienece/통계분석/T-검정.html#r-실습-2",
    "title": "02. T-test",
    "section": "3-1. R 실습",
    "text": "3-1. R 실습\n\nlibrary(MASS)\n\n\nstr(cats)\n\n'data.frame':   144 obs. of  3 variables:\n $ Sex: Factor w/ 2 levels \"F\",\"M\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Bwt: num  2 2 2 2.1 2.1 2.1 2.1 2.1 2.1 2.1 ...\n $ Hwt: num  7 7.4 9.5 7.2 7.3 7.6 8.1 8.2 8.3 8.5 ...\n\n\n\nvar.test(Bwt~Sex,data=cats)\n\n\n    F test to compare two variances\n\ndata:  Bwt by Sex\nF = 0.3435, num df = 46, denom df = 96, p-value = 0.0001157\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.2126277 0.5803475\nsample estimates:\nratio of variances \n         0.3435015 \n\n\n\n등분산 검정결과 성별에 따른 두집단은 등분산성을 만족하지 않음\n\n\nt.test(Bwt~Sex, data=cats, alternative=\"two.sided\",var.equal = F)\n\n\n    Welch Two Sample t-test\n\ndata:  Bwt by Sex\nt = -8.7095, df = 136.84, p-value = 8.831e-15\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n -0.6631268 -0.4177242\nsample estimates:\nmean in group F mean in group M \n       2.359574        2.900000 \n\n\n\n“고양이들의 성별에 따른 평균 몸무게에는 통계적으로 유의한 차이가 존재한다.” 고 결론을 내릴 수 있다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/고급회귀분석.html",
    "href": "post/R for Data Scienece/통계분석/고급회귀분석.html",
    "title": "07. regression analysis-2",
    "section": "",
    "text": "정규화 선형회귀는 선형회귀 계수에 대한 제약 조건을 추가하여 모델이 과도하게 최적화되는 현상을 막는 방법이다.\n즉, 불편성은 포기하되 모형의 해석력을 단순화하자는 것이 초점이다.\n\n\n\n\n교차타당법에 의한 선택이 일반적임\n또는 임의의 \\(\\lambda\\) grid를 선택하고 가능한 모든 값 중 가장 우수한 \\(\\lambda\\)를 선택"
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/고급회귀분석.html#r-실습",
    "href": "post/R for Data Scienece/통계분석/고급회귀분석.html#r-실습",
    "title": "07. regression analysis-2",
    "section": "R 실습",
    "text": "R 실습\n\nlibrary(tidyverse)\nlibrary(mosaicData)\nlibrary(glmnet) ## 리지 및 라쏘 회귀를 적합하기 위한 패키지\n\n\nglimpse(RailTrail)\n\nRows: 90\nColumns: 11\n$ hightemp   <int> 83, 73, 74, 95, 44, 69, 66, 66, 80, 79, 78, 65, 41, 59, 50,~\n$ lowtemp    <int> 50, 49, 52, 61, 52, 54, 39, 38, 55, 45, 55, 48, 49, 35, 35,~\n$ avgtemp    <dbl> 66.5, 61.0, 63.0, 78.0, 48.0, 61.5, 52.5, 52.0, 67.5, 62.0,~\n$ spring     <int> 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,~\n$ summer     <int> 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,~\n$ fall       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,~\n$ cloudcover <dbl> 7.6, 6.3, 7.5, 2.6, 10.0, 6.6, 2.4, 0.0, 3.8, 4.1, 8.5, 7.2~\n$ precip     <dbl> 0.00, 0.29, 0.32, 0.00, 0.14, 0.02, 0.00, 0.00, 0.00, 0.00,~\n$ volume     <int> 501, 419, 397, 385, 200, 375, 417, 629, 533, 547, 432, 418,~\n$ weekday    <lgl> TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, TR~\n$ dayType    <chr> \"weekday\", \"weekday\", \"weekday\", \"weekend\", \"weekday\", \"wee~\n\n\n\nmodel.matrix는 기본적으로 절편을 포함한 모형을 산정한다. 절편을 제외시킨 모형을 고려해보자\n\n\nx <- model.matrix(volume~.-1 ,RailTrail) \n\n\ny <- RailTrail$volume\n\n\n모형적합\n\n\nridge.fit <- cv.glmnet(x,y,alpha=0) ## 모형적합 alpha=0 은 릿지를 말함\n\n\n교차타당에러를 최소로하는 람다값을 구한결과 11.757로 산출되었다.\n\n\nbestlam <- ridge.fit$lambda.min\nlog(bestlam)\n\n2.65058474242334\n\n\n\ncoef(ridge.fit, s = \"lambda.min\")\n\n12 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)     97.879570\nhightemp         3.923302\nlowtemp         -1.148241\navgtemp          1.971339\nspring          10.236204\nsummer           3.316273\nfall           -27.057463\ncloudcover      -8.082976\nprecip         -90.969378\nweekdayFALSE    13.410258\nweekdayTRUE    -13.487794\ndayTypeweekend  13.616159\n\n\n\noptions(repr.plot.res=200,repr.plot.height=4,repr.plot.width=10)\n\n\nbestlam에서 MSE가 최소가 되는 지점을 확인\n\n\nplot(ridge.fit)\nabline(v=log(bestlam),lty=\"dashed\",lwd=2,col=\"blue\")\n\n\n\n\n\n산출된 best.lam값으로 모형을 다시 적합 후 MSE를 구해보자.\n\n\ntrain  <- sample(1:nrow(x),nrow(x)*0.7)\ntest  <- -train\ny.test <- y[test]\ny.train <- y[train]\n\n\nridge.fit <- glmnet(x[train,],y.train,alpha=0,lambda=bestlam,family=\"gaussian\")\n\n\nridge.pred <- predict(ridge.fit,s=bestlam,newx=x[test,]) ## type=\"coefficients\"로 하면 예측된 베타계수를 보여줌\n\n\nridge.coef <- predict(ridge.fit,s=bestlam,newx=x[test,],type=\"coefficients\") ## type=\"coefficients\"로 하면 예측된 베타계수를 보여줌\n\n\nridge.coef\n\n12 x 1 sparse Matrix of class \"dgCMatrix\"\n                        s1\n(Intercept)     64.9007304\nhightemp         3.9758591\nlowtemp         -0.7261573\navgtemp          2.0855905\nspring          20.3027908\nsummer         -13.1841751\nfall           -18.2167429\ncloudcover      -6.5169836\nprecip         -59.6275608\nweekdayFALSE    12.1230181\nweekdayTRUE    -12.6441270\ndayTypeweekend  12.3701010\n\n\n\nmean((ridge.pred - y.test)^2)\n\n11360.201935448\n\n\n\nmse가 만 단위가 넘게 나왔는데. 지나치게 모형을 단순화 하였다는 생각이 든다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/교차분석.html",
    "href": "post/R for Data Scienece/통계분석/교차분석.html",
    "title": "04. chi-square test",
    "section": "",
    "text": "개념 : 범주형 자료(명목/서열)인 두 변수 간의 관계를 알아보기 위해 실시하는 분석 기법\n적합성, 독립성, 동질성 검정에 사용되며, 카이제곱($ ^2$) 검정 통계량을 이용한다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/교차분석.html#r-실습",
    "href": "post/R for Data Scienece/통계분석/교차분석.html#r-실습",
    "title": "04. chi-square test",
    "section": "R 실습",
    "text": "R 실습\n$H_0 : $ 전체 응답자 중 왼손잡이의 비율이 20%, 오른손잡이의 비율이 80% 이다.\n\\(H_1 : not\\,\\,H_0\\)\n\nlibrary(MASS)\n\n\nstr(survey)\n\n'data.frame':   237 obs. of  12 variables:\n $ Sex   : Factor w/ 2 levels \"Female\",\"Male\": 1 2 2 2 2 1 2 1 2 2 ...\n $ Wr.Hnd: num  18.5 19.5 18 18.8 20 18 17.7 17 20 18.5 ...\n $ NW.Hnd: num  18 20.5 13.3 18.9 20 17.7 17.7 17.3 19.5 18.5 ...\n $ W.Hnd : Factor w/ 2 levels \"Left\",\"Right\": 2 1 2 2 2 2 2 2 2 2 ...\n $ Fold  : Factor w/ 3 levels \"L on R\",\"Neither\",..: 3 3 1 3 2 1 1 3 3 3 ...\n $ Pulse : int  92 104 87 NA 35 64 83 74 72 90 ...\n $ Clap  : Factor w/ 3 levels \"Left\",\"Neither\",..: 1 1 2 2 3 3 3 3 3 3 ...\n $ Exer  : Factor w/ 3 levels \"Freq\",\"None\",..: 3 2 2 2 3 3 1 1 3 3 ...\n $ Smoke : Factor w/ 4 levels \"Heavy\",\"Never\",..: 2 4 3 2 2 2 2 2 2 2 ...\n $ Height: num  173 178 NA 160 165 ...\n $ M.I   : Factor w/ 2 levels \"Imperial\",\"Metric\": 2 1 NA 2 2 1 1 2 2 2 ...\n $ Age   : num  18.2 17.6 16.9 20.3 23.7 ...\n\n\n\ndata <- table(survey$W.Hnd)\n\n\nchisq.test(data, p = c(0.2,0.8))\n\n\n    Chi-squared test for given probabilities\n\ndata:  data\nX-squared = 22.581, df = 1, p-value = 2.015e-06\n\n\n\n유의확률이 0.05보다 작으므로 “전체 응답자 중 왼손잡이의 20%, 오른손잡이의 80%이다.” 라는 귀무가설을 기각한다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/교차분석.html#r-실습-1",
    "href": "post/R for Data Scienece/통계분석/교차분석.html#r-실습-1",
    "title": "04. chi-square test",
    "section": "R 실습",
    "text": "R 실습\n$H_0 : $ 주로 사용하는 손(W.Hnd)와 운동 빈도(Exer)는 독립이다.\n\\(H_1: not \\,\\, H_0\\)\n\ndata <- table(survey$W.Hnd,survey$Exer)\nchisq.test(data)\n\nWarning message in chisq.test(data):\n\"Chi-squared approximation may be incorrect\"\n\n\n\n    Pearson's Chi-squared test\n\ndata:  data\nX-squared = 1.2065, df = 2, p-value = 0.547\n\n\n\n검정결과 주로 사용하는 손과 운동 빈도는 서로 독립이라고 말할 수 있다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/다차원척도법(1204).html",
    "href": "post/R for Data Scienece/통계분석/다차원척도법(1204).html",
    "title": "10. Multidimensional scaling",
    "section": "",
    "text": "1. 객체간 근접성(Proximity)을 시각화하는 통계기법.\n2. 군집분석과 같이 개체들을 대상으로 변수들을 측정한 후 개체들 사이의 유사성/비유사성을 측정하여 2차원 또는 3차원 공간상에 점으로 표현"
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/다차원척도법(1204).html#가.-계량적-mdsmetric-mds",
    "href": "post/R for Data Scienece/통계분석/다차원척도법(1204).html#가.-계량적-mdsmetric-mds",
    "title": "10. Multidimensional scaling",
    "section": "가. 계량적 MDS(Metric MDS)",
    "text": "가. 계량적 MDS(Metric MDS)\n\n데이터가 구간 또는 비율척도인 경우 활용\nN개의 케이스에 대해서 p개의 특성변수가 있는 경우, 각 개체들간의 유클리드 거리행렬을 계산하고 개체들간의 비유사성 \\(S\\)를 공간상에 표현한다.\n\n\nR 실습\n\nlibrary(MASS)\n\n\nMASS 패키지 안에 내장된 자료를 이용\n\n\nloc <- cmdscale(eurodist,k=2) ## 거리행렬을 다시 x,y좌표로 바꾸는 작업?\n\n\nx <- loc[,1]\ny <- -loc[,2]  ## y축은 북쪽 도시를 상단에 표시하기 위해 부호를 바꾼다.\n\n\noptions(repr.plot.res=200,repr.plot.height=5,repr.plot.width=10)\n\n\nplot(x,y,type=\"n\",asp=1,main=\"metric MDS\" )\ntext(x,y,rownames(loc),cex=0.7)\nabline(v=0,h=0,lty=2,lwd=0.5)\n\n\n\n\n\n즉 유사한 도시들은 가까이 밀집해있고 그렇지 않은 도시들은 멀리 떨어져있음을 확인할 수 있다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/다차원척도법(1204).html#나.-비계량적-mds-nonmetric-mds",
    "href": "post/R for Data Scienece/통계분석/다차원척도법(1204).html#나.-비계량적-mds-nonmetric-mds",
    "title": "10. Multidimensional scaling",
    "section": "나. 비계량적 MDS (Nonmetric MDS)",
    "text": "나. 비계량적 MDS (Nonmetric MDS)\n\n데이터가 순서척도 인 경우 활용한다.\n개체들간의 거리가 순서로 주어진 경우에는 순서척도를 거리의 속성과 같도록 변환\n그 후 거리를 생성하여 적용한다.\nR 에서는 isoMDS 함수를 이용한다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/다차원척도법(1204).html#r-실습-1",
    "href": "post/R for Data Scienece/통계분석/다차원척도법(1204).html#r-실습-1",
    "title": "10. Multidimensional scaling",
    "section": "R 실습",
    "text": "R 실습\n\n기존의 데이터프레임을 행렬구조로 변환해줌\n\n\nswiss.x <- as.matrix(swiss[,-1])\n\n\nswiss.dist <- dist(swiss.x)\n\n\nswiss.mds <- isoMDS(swiss.dist)\n\ninitial  value 2.979731 \niter   5 value 2.431486\niter  10 value 2.343353\nfinal  value 2.338839 \nconverged\n\n\n\nplot(swiss.mds$points,type=\"n\")\ntext(swiss.mds$points,labels=as.character(1:nrow(swiss.x)))"
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/데이터 샘플링.html",
    "href": "post/R for Data Scienece/통계분석/데이터 샘플링.html",
    "title": "01. sampling",
    "section": "",
    "text": "1. 단순 임의 추출\n\nn <- nrow(iris)\n\n\nidx <- sample(1:n,n*0.7,replace=F)\n\n\ntrain <- iris[idx,]\ntest <- iris[-idx]\n\n\n\n2. 층화 임의 추출\n\nmethod  1. srswor : 비복원 단순 임의 추출  2. srswr : 복원 단순 임의 추출  3. possion : 포아송 추출  4. systematic : 계통 추출 \n\n\nlibrary(sampling)\n\n\nsamples <- strata(iris,c(\"Species\"),size=c(20,15,15),method=\"srswor\")\n\n\niris_sample <- getdata(iris,samples)\n\n\n\n3. 계통 추출\n\nformula : ~ 우축에 나열한 이름에 따라 데이터가 그룹으로 묶임\nfrac = 0.1 : 추출할 샘플 비율 기본값은 10%\nreplace : 복원 추출 여부\ndata = parent.frame() : 추출할 데이터 프레임\nsystematic = F : 계통 추출(Systematic Sampling)을 사용할지 여부\n\n\nlibrary(doBy)\n\n\nhead(sampleBy(~ Species,frac = 0.3,data = iris, systematic =T))\n\n\n\nA data.frame: 6 × 5\n\n    Sepal.LengthSepal.WidthPetal.LengthPetal.WidthSpecies\n    <dbl><dbl><dbl><dbl><fct>\n\n\n    setosa.15.13.51.40.2setosa\n    setosa.44.63.11.50.2setosa\n    setosa.74.63.41.40.3setosa\n    setosa.115.43.71.50.2setosa\n    setosa.144.33.01.10.1setosa\n    setosa.175.43.91.30.4setosa"
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/분산분석(ANOVA).html",
    "href": "post/R for Data Scienece/통계분석/분산분석(ANOVA).html",
    "title": "03. ANOVA",
    "section": "",
    "text": "T-검정과 달리 두 개 이상의 다수 집단 간 평균을 비교하는 통계분석 방법\n반응값에 대한 하나의 범주형 변수의 영향을 알아보기 위해 사용한다.\n가정 : 각 집단의 측정치는 서로 독립적이며, 정규분포를 따른다. 또한 각 집단 측정치의 분산은 같다.\n등분산 검정은 : 정규성을 만족할 경우 Bartlett.test, 그렇지 않은 경우Levene.test(lawstat 패키지)를 사용\n정규성 가정이 깨졌다면? \\(\\to\\) kruskal-Wallis Rank Sum Test\n정규성 가정이 깨지지 않았다면? ->F통계량에 근거한 p-value 값으로 가설의 유의성을 검증한다.\n사후 검정 : 귀무가설이 기각된 경우 어떠한 집단들에 대해서 평균의 차이가 존재하는지를 알아보기 위해 실시하는 분석\n\n\n\n\nby( iris$Sepal.Width,iris$Species,shapiro.test)\n\niris$Species: setosa\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.97172, p-value = 0.2715\n\n------------------------------------------------------------ \niris$Species: versicolor\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.97413, p-value = 0.338\n\n------------------------------------------------------------ \niris$Species: virginica\n\n    Shapiro-Wilk normality test\n\ndata:  dd[x, ]\nW = 0.96739, p-value = 0.1809\n\n\n\n정규성 검정결과 세 가지 종 모두 정규성을 만족한다.\n\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\nbartlett.test(Sepal.Width~Species ,data= iris)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  Sepal.Width by Species\nBartlett's K-squared = 2.0911, df = 2, p-value = 0.3515\n\n\n\n검정결과 3가지 종의 꽃잎길이는 등분산성을 만족한다.\n정규성과 등분산성을 만족한다는 가정하에 일원배치 분산분석을 수행\n\n\nresult <- oneway.test(Sepal.Width~Species, data=iris,var.equal=T)\nresult\n\n\n    One-way analysis of means\n\ndata:  Sepal.Width and Species\nF = 49.16, num df = 2, denom df = 147, p-value < 2.2e-16\n\n\n\n검정결과 귀무가설을 기각 즉, 적어도 어느 하나의 종의 Speal.Width가 나머지 종들과 통계적으로 유의한 차이가 있다고 말할 수 있다.\n사후검정을 통해 확인\n\n\nlibrary(stats)\n\n\npairwise.t.test(iris$Sepal.Width,iris$Species,pool.sd=F,p.adj=\"bonf\") ## pool.sd=F는 합동공분산 사용여부 \n\n\n    Pairwise comparisons using t tests with non-pooled SD \n\ndata:  iris$Sepal.Width and iris$Species \n\n           setosa  versicolor\nversicolor 7.5e-15 -         \nvirginica  1.4e-08 0.0055    \n\nP value adjustment method: bonferroni \n\n\n\n$H_0 : $ 집단들 사이의 평균은 같다.\n$H_1 : $ 집단들 사이의 평균은 같지 않다.\n\\(p-value\\) 값을 살펴본 결과 모든 종들에 대해서 꽃받침 폭의 평균값은 통계적으로 유의한 차이가 있다는 것을 알 수 있다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/분산분석(ANOVA).html#r-실습-1",
    "href": "post/R for Data Scienece/통계분석/분산분석(ANOVA).html#r-실습-1",
    "title": "03. ANOVA",
    "section": "R 실습",
    "text": "R 실습\n\n실린더 개수와 변속기 종류에 따른 주행거리의 차이가 있는지 검정\n\n\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n\nmtcars$cyl <- as.factor(mtcars$cyl)\nmtcars$am <- as.factor(mtcars$am)\n\n\nlibrary(tidyverse)\n\n\n#mtcars %>% \n #       group_by(cyl,am) %>% \n  #          summarise(statistic = shapiro.test(mpg)$statistic,\n  #                    p.value = shapiro.test(mpg)$p.value) \n\n\n본래는 위와 같이 검정을 하여야 하나 샘플사이즈가 3보다 적은 그룹이 있어 계산 불가…\n\n\ncar_aov <- aov(mpg~cyl*am,mtcars)\n\n\nsummary(car_aov)\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncyl          2  824.8   412.4  44.852 3.73e-09 ***\nam           1   36.8    36.8   3.999   0.0561 .  \ncyl:am       2   25.4    12.7   1.383   0.2686    \nResiduals   26  239.1     9.2                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n검정결과 실린더개수(cyl) 에 따른 주행거리 차이만 통계적으로 유의하다는 결론을 내렸다.\n\n\noptions(repr.plot.res=200,repr.plot.height=5,repr.plot.width=10)\n\n\ninteraction.plot(mtcars$cyl,mtcars$am,mtcars$mpg,col=c(\"red\",\"blue\"),lwd=2)\n\n\n\n\n\n일반적으로 상호작용 그래프에서 두 선이 서로 교차하고 있을 시에는 \\(x\\) 축에 있는 독립변수와 그래프에서 시각화된 독립변수 간에는 상호작용이 존재한다고 해석할 수 있다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/상관분석.html",
    "href": "post/R for Data Scienece/통계분석/상관분석.html",
    "title": "05. cor-test",
    "section": "",
    "text": "$H_ 0 : $ 변수간에는 상관관계가 없다( 상관계수 \\(= 0\\))\n$H_1 : $ 변수간에는 상관관계가 있다.( 상관계수 \\(\\neq 0\\) )\n\n피어슨 상관계수\n\n두 연속형 자료가 모두 정규성을 따른다는 가정하에 선형적 상관관계를 측정\n\n스피어만 상관계수\n\n데이터가 정규성을 만족하지 않거나 순위 및 순서 형태로 주어지는 경우 사용\n피어슨 상관계수와 달리 비선형 관계의 연관성을 파악할 수 있다.\n비모수적 방법\n\n켄달의 순위상관계수\n\n\\(X_i\\)가 커짐에 따라 \\(Y_i\\)도 커질 경우 부합, 작아질 경우 비부합이라고 본다.\n전체 데이터에서 비부합쌍에 대한 부합쌍의 비율로 상관계수를 산출한다.\n순위상관계수가 -1 일 경우 비부합쌍의 비율이 100%, 0일 경우 두 변수 \\(X,Y\\)는 상관성이 없음을 의미한다.\n\n\n\\(\\divideontimes\\) 원래의 경우 독립이면 상관계수는 0이지만, 이것에 대한 역은 반드시 성립하지 않는다.\nhttps://techntalk.tistory.com/entry/%ED%86%B5%EA%B3%84%EC%A0%81%EC%9C%BC%EB%A1%9C-%EB%91%90-%EB%B3%80%EC%88%98%EC%9D%98-%EB%8F%85%EB%A6%BDindependence%EA%B3%BC-%EC%83%81%EA%B4%80%EA%B3%84%EC%88%98correlation%EC%99%80%EC%9D%98-%EA%B4%80%EA%B3%84\n\n\n\nlibrary(tidyverse)\n\n\ndata(\"airquality\")\n\n\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n\n\nair <- airquality %>% select(-c(Day,Month))\n\n\nair_cor <- cor(air,use=\"pairwise.complete.obs\",method=\"pearson\")\n\n\nuse\n\neverything : 결측값 존재 시 NA출력\nall.obs : 결측값 존재 시 오류 메시지 출력\ncomplete.obs : 변수별로 결측값을 제외하고 상관계수 계산\npairwise.complete.obs : 모든 변수 쌍에서 결측값이 없는 데이터들에 대해 상관계수 계산\n\n\n\nlibrary(corrplot)\n\n\ntestRes <- cor.mtest(air,method=\"pearson\")\n\n\noptions(repr.plot.res=200,repr.plot.height=5,repr.plot.width=10)\ncorrplot(air_cor,diag=F,type=\"upper\",p.mat=testRes$p,\n            method=\"circle\",number.cex=1.5,addCoef.col=\"black\")\n\n\n\n\n\n위 그래프를 해석하면 Wind와 Solar.R간에는 상관관계가 없다고 해석할 수 있다.\n\n\ncor.test(air$Wind,air$Solar.R) ##실제 검정 결과도 동일하다.\n\n\n    Pearson's product-moment correlation\n\ndata:  air$Wind and air$Solar.R\nt = -0.6826, df = 144, p-value = 0.496\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.2172359  0.1066406\nsample estimates:\n        cor \n-0.05679167"
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/시계열 분석.html",
    "href": "post/R for Data Scienece/통계분석/시계열 분석.html",
    "title": "11. Time-seris analysis",
    "section": "",
    "text": "정상 시계열과 비정상 시계열을 구분할 줄 알아야한다.\n정상성을 만족하지 못하는 시계열 자료는 다른 시기로 일반화 할 수 없기 때문에 정상 시계열로 전환 후에 모형을 적합해주어야 한다.\nARIMA 모형과 분해 시계열 분석을 할 수 있다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/시계열 분석.html#가.-분석-방법",
    "href": "post/R for Data Scienece/통계분석/시계열 분석.html#가.-분석-방법",
    "title": "11. Time-seris analysis",
    "section": "가. 분석 방법",
    "text": "가. 분석 방법\n\nA. 단기 예측 - 이동평균법\n\n과거로부터 현재까지의 시계열 자료를 대상으로 일정기간별 이동평균을 계산하고, 이들의 추세를 파악하여 다음 기간을 예측하는 방법\n즉 표본평균처럼 관측값 전부에 동일한 가중치를 주는 대신에 최근 \\(m\\) 개의 값들만을 이용하여 평균을 구하는 방법이다.\n예측보다는 원 시계열 자료에서 계절변동과 불규칙 변동을 제거하여 추세변동과 순환변동만 가진 시계열로 변환하는 방법으로도 사용됨\n\n이론적인 설명은 저건데 모형 적합시에는 \\(\\hat {Z}_t = \\hat {T}_t + \\hat {S}_t\\) 로 예측값을 추정함\n가법모형은 계절성분의 진폭이 시계열 수준에 상관없이 일정할 때 사용 \\(\\hat {Z}_t = \\hat {T}_t + \\hat {S}_t\\)\n승법모형은 계절성분의 진폭이 시계열 수준에 따라 달라질 때 사용 \\(\\hat {Z}_t = \\hat {T}_t \\times \\hat {S}_t\\)\n\n\\(n+1\\) 시점의 데이터를 \\(m\\)개의 데이터를 가지고 예측하는 경우는 다음과 같다.\n\n\\[F_{n+1} = \\frac {1}{m}(Z_n + Z_{n-1} + \\dots Z_{n-m+1}) =\\frac 1m \\sum_{t}^{n}Z_t, \\quad t=n-m+1\\]\n\nPractice with R\n\nlibrary(forecast) ## 예측값을 구하기 위한 패키지\nlibrary(TTR) ## 이동평균법을 이용하기 위한 패키지\nlibrary(lmtest) ## 더비-왓슨 검정 \nlibrary(tidyverse) ## 전처리 패키지\n\n\nsetwd(\"C:\\\\Users\\\\lee\\\\Desktop\\\\고급시계열 분석\\\\제5판_시계열분석_프로그램\\\\제5판 시계열분석 data\")\n\n\n데이터 호출 : 주기는 12로 월별 데이터임\n\n\nz <- scan(\"food.txt\")\nt <- 1:length(z)\nfood <- ts(z, start=c(1981,1), frequency=12) \n\n\n시도표 확인  1. 추세성분이 보인다.  2. 계절성분이 보인다.  3. 이분산성이 보인다. \n\n\noptions(repr.plot.res=200,repr.plot.height=6,repr.plot.width=10)\nplot.ts(food)\n\n\n\n\n\n이분산성을 제거하기 위해 로그변환을 시도\n\n\nlog_food <- log(food)\nplot.ts(log_food)\n\n\n\n\n\n\nstl\n\nstl function을 이용하여 모델 적합\n\n\nstl_fit1  <- stl(log_food,s.window=12) ## s.window=12 : 계절성분의 주기가 12이다.\n\n\nstl_fit1$time.series[1:10,]\n\n\n\nA matrix: 10 × 3 of type dbl\n\n    seasonaltrendremainder\n\n\n    -0.090355043.789108 0.09223150\n    -0.147931393.787232 0.04957932\n    -0.019896143.785355-0.03017285\n     0.038295803.783478-0.04901299\n     0.094538723.782254-0.04815173\n     0.043787293.781031-0.01593573\n     0.016322463.779807-0.01421511\n     0.054057673.779134-0.03770230\n     0.039065453.778461-0.04937347\n    -0.017797403.777788 0.02192421\n\n\n\n\n\n계절성분, 추세성분, 불규칙성분의 값들을 볼 수 있음.\n추세성분으로 추정한 경우 \\(\\hat {\\beta}_0 + \\hat{\\beta}_1z_t= 0.07 + 3.7z_t\\)로 앞서 구한 데이터와 동일(이건 수업시간에 한거에 나와있음)\n\n\nplot(stl_fit1)\n\n\n\n\n\n추정값을 구해보자\n\n\npred_stl <- stl_fit1$time.series[,1] + stl_fit1$time.series[,2] ## 추세성분 + 계절성분\n\nts.plot(log_food, pred_stl, col=1:2, lty=1:2, ylab=\"food\", xlab=\"time\",\n        main=\"원시계열과 분해법에 의한 추정값\",lwd=2)\nlegend(\"topleft\", lty=1:2, col=1:2, c(\"원시계열\", \"추정값\"))\n\n\n\n\n\nirregular <- log_food-pred_stl\nsum(irregular^2)/(length(log_food)-1) ## MSE 계산\n\n0.001245257904363\n\n\n\n\ndecompose\n\ndecompose function을 이용하여 모형 적합\n\n\ndec_fit  <- decompose(log_food)\n\n\ndec_fit$trend[1:10]\n\n\n<NA><NA><NA><NA><NA><NA>3.776909155269433.768171473872113.765649627472043.77219261288612\n\n\n\ndec_fit$seasonal[1:10]\n\n\n-0.0799020938844753-0.146043800620445-0.01274493183601310.04049677614460120.09068184389565770.03967866993284980.01048177141592280.0587619091303550.041341551797343-0.0179374241117825\n\n\n\ndec_fit$random[1:10] ## 불규칙성분\n\n\n<NA><NA><NA><NA><NA><NA>-0.00547660660422888-0.0314441938302674-0.03883854426093690.0276591313067893\n\n\n\nplot(dec_fit)\n\n\n\n\n\n추정치 구하기\n\n\npred_dec <-dec_fit$trend+dec_fit$seasonal\n\nts.plot(pred_stl, pred_dec, col=1:2, lty=1:2, ylab=\"food\", xlab=\"time\",\n        main=\"원시계열과 분해법에 의한 추정값\",lwd=2)\nlegend(\"topleft\", lty=1:2, col=1:2, c(\"원시계열\", \"추정값\"),lwd=2)\n\n\n\n\n\nstl vs decompose\n\n\nts.plot(pred_stl, pred_dec, col=1:2, lty=1:2, ylab=\"food\", xlab=\"time\",\n        main=\"stl vs. decompose\",lwd=2)\nlegend(\"topleft\", lty=1:2, col=1:2, c(\"stl\", \"decompose\"),lwd=2)\n\n\n\n\n\nstl과 달리 decompose는 승법모형을 지원해준다.\n\n\ndec_fit2 <- decompose(food, type = \"multiplicative\")\ndec_fit2$trend[1:10]\n\n\n<NA><NA><NA><NA><NA><NA>43.729166666666743.379166666666743.291666666666743.5875\n\n\n\ndec_fit2$seasonal[1:10]\n\n\n0.9219246356829520.8621213373421160.9844960475951441.038004198013531.091650489534581.037859336820561.008002905607491.059170985651181.042754035631620.980759172033513\n\n\n\ndec_fit2$random[1:10]\n\n\n<NA><NA><NA><NA><NA><NA>0.995936238493310.9685292269139350.9591835261281731.0269284401149\n\n\n\nplot(dec_fit2)\n\n\n\n\n\npred_dec2 <-dec_fit2$trend*dec_fit2$seasonal\n\nts.plot(food, pred_dec2, col=1:2, lty=1:2, ylab=\"food\", xlab=\"time\",\n        main=\"원시계열과 분해법에 의한 추정값\",lwd=2)\nlegend(\"topleft\", lty=1:2, col=1:2, c(\"원시계열\", \"추정값\"),lwd=2)\n\n\n\n\n\n가법모형 vs 승법모형\n\n\nts.plot(exp(pred_dec), pred_dec2, col=1:2, lty=1:2, ylab=\"food\", xlab=\"time\",\n        main=\"가법모형과 승법모형 추정값\")\nlegend(\"topleft\", lty=1:2, col=1:2, c(\"가법모형\", \"승법모형\"))\n\n\n\n\n\n승법모형과 가법모형 비교 결과 거의 동일한 모형임을 확인할 수 있다.\n\n\n\n\n\nB. 단기 예측 - 지수평활법\n\n이동평균법이 \\(m\\)개의 데이터의 평균을 이용하여(=동일한 가중치를 부여) \\(n+1\\) 시점의 예측값을 구했다면\n지수평활법은 최근 시점에 관측치에 더 많은 가중치를 부여하여 미래를 예측하는 방법이다.\n\n$F_{n+1} = Z_n + (1-) Z_{n-1} + (1-)^nZ_0 $\n\n\\(\\alpha=\\) 지수평활계수로 작을수록 과거시점에 가중치를 더 많이 주고, 클 수록 현시점에 가중치를 많이 준다.\n\n\\(\\alpha\\)는 예측오차를 비교하여 예측오차가 가장 작은값을 선택하는 것이 바람직함\n불규칙변동이 큰 시계열의 경우 작은 값을, 작을 경우 큰 값을 적용\n\n또한 이동평균법과 달리 모든 \\(m\\)개가 아닌 전체 데이터를 고려하여 예측값을 산출한다.\n추세성분과 계절성분이 없을 경우 \\(\\to\\) 단순지수평활법\n추세성분이 있고 계절성분이 없는 경우 \\(\\to\\) 이중지수평활법\n추세성분과 계절성이 모두 관측된 경우 \\(\\to\\) winters 계절지수 평활법\n\n\nPractice with R\n\n\n단순지수 평활법\n\nrain  <- scan(\"precip1.txt\",skip=1)\nrainseries <- ts(rain,start=c(1813))\nrainseries ## 1년 주기 데이터\n\nA Time Series:\n23.5626.0721.8631.2423.6523.8826.4122.6731.6923.8624.1132.4323.2622.572327.8825.3225.0827.7619.8224.7820.1224.3427.4219.4421.6327.4919.4331.1323.0925.8522.6522.7526.3617.729.8122.9319.2220.6335.3425.8918.6523.0622.2122.1818.7728.2132.2422.2727.5721.5916.9329.4831.626.2523.425.4221.3225.0233.8622.6718.8228.4426.1628.1734.0833.8230.2827.9227.1424.420.3526.6427.0119.2127.7423.8521.2328.1522.6119.827.9421.4723.5222.8617.6922.5423.2822.1720.8438.120.6522.9724.2623.0123.6726.7525.3624.7927.88\n\n\n\n시도료를 살펴본결과 추세와 계절성분이 관측되지 않아 단순지수평활법을 사용하겠음\n\n\nplot.ts(rainseries)\n\n\n\n\n\n[단순지수평활법] HoltWinters\n\n\nrainforecasts = HoltWinters(rainseries, beta=FALSE, gamma=FALSE) \nrainforecasts ## alpha=단순지수 평활법, beta = 추세성분 가중치 , gamma = 계절성분 가중치\n## alpha 값을 지정하지 않으면 최적의 alpha를 찾아줌\n\nHolt-Winters exponential smoothing without trend and without seasonal component.\n\nCall:\nHoltWinters(x = rainseries, beta = FALSE, gamma = FALSE)\n\nSmoothing parameters:\n alpha: 0.02412151\n beta : FALSE\n gamma: FALSE\n\nCoefficients:\n      [,1]\na 24.67819\n\n\n\n\\(\\alpha = 0.024\\dots\\) 로 산출\n\\(\\alpha\\)가 작다는 것은 과거값의 가중치를 더 많이 주는 것이므로 smoothing 효과가 더욱 커진다.\n\n\nhead(rainforecasts$fitted)\n\n\n\nA Time Series: 6 × 2\n\n    xhatlevel\n\n\n    181423.5600023.56000\n    181523.6205423.62054\n    181623.5780823.57808\n    181723.7629023.76290\n    181823.7601723.76017\n    181923.7630623.76306\n\n\n\n\n\nplot(rainforecasts) ## 빨간색은 smoothing 효과가 반영된 것이다.\n\n\n\n\n\n오차제곱합 계산\n\n\nrainforecasts$SSE\n\n1828.8548918638\n\n\n\n\\(\\alpha\\) 값을 변화해가며 비교\n\n\npar(mfrow=c(3,1))\nplot(HoltWinters(rainseries, alpha=0.3,\n                 beta=FALSE, gamma=FALSE), main=\"Alpha=0.3\")\n\nplot(HoltWinters(rainseries, alpha=0.7,\n                 beta=FALSE, gamma=FALSE), main=\"Alpha=0.7\")\n\nplot(HoltWinters(rainseries, alpha=1,\n                 beta=FALSE, gamma=FALSE), main=\"Alpha=1\") ## 1은 직전값만 보겠다는 뜻이다.\n\n\n\n\n\nalpha1=HoltWinters(rainseries, alpha=1, beta=FALSE, gamma=FALSE)\nalpha1$SSE\nalpha07=HoltWinters(rainseries, alpha=0.7, beta=FALSE, gamma=FALSE)\nalpha07$SSE\nalpha03=HoltWinters(rainseries, alpha=0.3, beta=FALSE, gamma=FALSE)\nalpha03$SSE\n\n3738.1784\n\n\n2849.29917342505\n\n\n2101.5603686547\n\n\n\n[단순지수평활] 초기값 변경 \\(\\to\\) 기존의 경우 관측값의 첫 번째 값을 사용\n\n\nrainforecasts35 = HoltWinters(rainseries, beta=FALSE, gamma=FALSE, l.start=35)\nrainforecasts35\n\nHolt-Winters exponential smoothing without trend and without seasonal component.\n\nCall:\nHoltWinters(x = rainseries, beta = FALSE, gamma = FALSE, l.start = 35)\n\nSmoothing parameters:\n alpha: 0.1955854\n beta : FALSE\n gamma: FALSE\n\nCoefficients:\n      [,1]\na 25.28555\n\n\n\n\\(\\alpha\\) 값이 앞에 초기값 설정 전 보다 작아짐\n스무딩 효과도 줄어들었다.\n\n\nrainforecasts35$SSE\n\n2209.42164806169\n\n\n\nplot(rainforecasts35)\n\n\n\n\n\nrainforecasts2 <- forecast(rainforecasts, h=5) ## h=예측할 시점, 즉 5개 시점을 예측해서 보겠다\nplot(rainforecasts2) ## 회색은 95%신뢰구간, 보라색은 80%신뢰구간\n\n\n\n\n\nZ <- scan(\"mindex.txt\")\n\n\nmindex  <- ts(Z,start=c(1986,1),frequency = 12) ## 1986년 1월부터 주기가 12인 데이터를 생성\n\n\nmindex\n\n\n\nA Time Series: 9 × 12\n\n    JanFebMarAprMayJunJulAugSepOctNovDec\n\n\n    1986 9.310.713.314.117.818.119.418.819.118.418.017.0\n    198719.520.119.415.715.616.114.916.014.618.318.223.0\n    198822.222.118.817.713.812.716.515.616.310.710.4 7.0\n    1989 4.7 4.5 4.0 6.0 6.2 5.7 4.4 4.2 5.0 5.8 6.4 4.9\n    1990 7.9 8.211.810.011.111.712.415.214.015.212.918.0\n    199114.412.7 8.311.511.911.610.3 8.511.612.314.511.1\n    199211.812.412.7 9.810.010.2 9.6 6.9 5.3 4.8 4.6 1.9\n    1993 3.8 4.7 7.7 7.0 7.2 7.8 8.611.410.711.811.316.0\n    199413.212.0 8.511.4                                \n\n\n\n\n\n시도표를 확인해본 결과 추세와 계절 성분이 없는 것을 확인하였다. 따라서 단순지수평활법을 적용\n\n\nplot.ts(mindex)\n\n\n\n\n\nmindexforecasts = HoltWinters(mindex, beta=FALSE, gamma=FALSE)\nmindexforecasts\n\nHolt-Winters exponential smoothing without trend and without seasonal component.\n\nCall:\nHoltWinters(x = mindex, beta = FALSE, gamma = FALSE)\n\nSmoothing parameters:\n alpha: 0.9036403\n beta : FALSE\n gamma: FALSE\n\nCoefficients:\n      [,1]\na 11.15433\n\n\n\npar(mfrow=c(3,1))\nplot(HoltWinters(mindex, alpha=0.3,\n                 beta=FALSE, gamma=FALSE), main=\"Alpha=0.3\")\n\nplot(HoltWinters(mindex, alpha=0.7,\n                 beta=FALSE, gamma=FALSE), main=\"Alpha=0.7\")\n\nplot(HoltWinters(mindex, alpha=1,\n                 beta=FALSE, gamma=FALSE), main=\"Alpha=1\")\n\n\n\n\n\nalpha1=HoltWinters(mindex, alpha=1, beta=FALSE, gamma=FALSE)\nalpha1$SSE\nalpha07=HoltWinters(mindex, alpha=0.7, beta=FALSE, gamma=FALSE)\nalpha07$SSE\nalpha03=HoltWinters(mindex, alpha=0.3, beta=FALSE, gamma=FALSE)\nalpha03$SSE\n\n443.23\n\n\n462.503411392955\n\n\n792.968599702168\n\n\n\n직접적으로 최적의 평활상수를 구해보자\n\n\nw <-c(seq(0.1,0.8,0.1), seq(0.81, 0.99, 0.01)) \nsse <- sapply(w, function(x) \n  return(sum(ses(mindex, alpha = x)$residuals^2)))\n\n\nw1 = w[-c(1:6)]  # xaxis from 0.7 to 1.0\nsse1 = sse[-c(1:6)]\nplot(w1,sse1, type=\"o\", xlab=\"weight\", ylab=\"sse\", pch=16,\n     main=\"1 시차 후 예측오차의 제곱합\")\n\n\n\n\n\nopt_w <- w[which.min(sse)]  # 최적 평활상수값\nopt_w\n\n0.9\n\n\n\nfit1 <- ses(mindex, alpha=w[which.min(sse)], h=6) ## ses 함수는 더 예측할 시점을 설정하여 모형을 적합할 수 있음\nfit1\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nMay 1994       11.14643 8.438328 13.85454 7.004743 15.28812\nJun 1994       11.14643 7.503050 14.78982 5.574359 16.71851\nJul 1994       11.14643 6.762982 15.52989 4.442522 17.85034\nAug 1994       11.14643 6.130952 16.16192 3.475916 18.81695\nSep 1994       11.14643 5.570103 16.72276 2.618171 19.67470\nOct 1994       11.14643 5.060723 17.23214 1.839142 20.45372\n\n\n\nplot(fit1, xlab=\"\", ylab=\"\", \n     main=paste0(\"중간재 출하지수와 단순지수평활값 alpha=\",opt_w), \n     lty=1,col=\"black\" )\nlines(fitted(fit1), col=\"red\", lty=2)\nlegend(\"topright\", legend=c(\"Mindex\", opt_w), \n       lty=1:2,col=c(\"black\",\"red\"))\n\n\n\n\n\n\n이중지수 평활법\n\n치맛단 길이 데이터를 사용\n\n\nskirts=scan(\"http://robjhyndman.com/tsdldata/roberts/skirts.dat\",\n            skip=5)\nskirtsseries <- ts(skirts,start=c(1866))\nskirtsseries # 주기가 1년인 데이터\n\nA Time Series:\n6086176256366576917287848168769499971027104710491018102110121018991962921871829822820802821819791746726661620588568542551541557556534528529523531\n\n\n\n시도표를 살펴본 결과 올라가다가 내려가는? 추세를 볼 수 있다.\n또한 추세가 일정한 것보다는 시간에 따라 추세가 변하는 것을 볼 수 있다\n따라서 일반적인 추세모형을 적합할 수 없으므로 이중지수평활법을 사용\n\n\nplot.ts(skirtsseries)\n\n\n\n\n[이중지수평활법] gamma =F\n\nskirtforecasts <- HoltWinters(skirtsseries, gamma=FALSE)\nskirtforecasts \n\nHolt-Winters exponential smoothing with trend and without seasonal component.\n\nCall:\nHoltWinters(x = skirtsseries, gamma = FALSE)\n\nSmoothing parameters:\n alpha: 0.8383481\n beta : 1\n gamma: FALSE\n\nCoefficients:\n        [,1]\na 529.308585\nb   5.690464\n\n\n\n\\(\\beta = 1, \\alpha = 0.83\\dots\\)로 최근값에 가중치가 더 많이 들어간 것으로 확인된다.\n\n\nplot(skirtforecasts)\n\n\n\n\n[이중지수평활법] 예측\n\nskirtsforecasts2=forecast(skirtforecasts, h=10)\nplot(skirtsforecasts2)\n\n\n\n\n\nz <- scan(\"stock.txt\")\nstock <- ts(z, start=c(1984,1), frequency=12)\nstock\n\n\n\nA Time Series: 8 × 12\n\n    JanFebMarAprMayJunJulAugSepOctNovDec\n\n\n    1984118.60129.84129.12133.23131.74133.53133.15135.08134.74132.21132.94138.34\n    1985139.98135.72134.30136.14133.17137.35135.77136.54136.34140.21145.26135.37\n    1986157.54168.83187.77206.02209.57239.30260.24268.28262.02243.64257.00237.30\n    1987289.09320.53365.85368.66383.97387.28454.00483.98481.52509.43477.53489.00\n    1988579.48644.36624.94643.46673.32710.98696.84693.39675.05713.39778.15884.34\n    1989884.23902.21965.89959.12936.94900.75886.35922.35951.58913.00898.67901.80\n    1990898.00867.18845.48785.50771.65766.70698.17637.42602.21682.79696.44712.46\n    1991647.53654.72670.55638.34630.19604.73646.25710.56684.34705.16668.07625.06\n\n\n\n\n\n레벨과 추세가 시간에 따라 변하는 것을 알 수 있다.\n\n\nplot.ts(stock, main='월별주가지수')\n\n\n\n\n\nfit4 = HoltWinters(stock, alpha=0.6, beta=0.6, gamma=FALSE) \nfit4\n\nHolt-Winters exponential smoothing with trend and without seasonal component.\n\nCall:\nHoltWinters(x = stock, alpha = 0.6, beta = 0.6, gamma = FALSE)\n\nSmoothing parameters:\n alpha: 0.6\n beta : 0.6\n gamma: FALSE\n\nCoefficients:\n       [,1]\na 650.78081\nb -26.14166\n\n\n\nplot(fit4)\n\n\n\n\n\n\\(\\alpha=1, \\beta = 0.109\\dots\\) 알파는 최근값, 베타는 과거값에 가중치를 더 많이 주었다.\n\n\nfit5 = HoltWinters(stock, gamma=FALSE) \nfit5\n\nHolt-Winters exponential smoothing with trend and without seasonal component.\n\nCall:\nHoltWinters(x = stock, gamma = FALSE)\n\nSmoothing parameters:\n alpha: 1\n beta : 0.1094451\n gamma: FALSE\n\nCoefficients:\n        [,1]\na 625.060000\nb  -7.097122\n\n\n\nfit5_2=forecast(fit5, h=10)\nplot(fit5_2)\n\n\n\n\n\n\n계절지수 평활법\n\nsouvenir=scan(\"http://robjhyndman.com/tsdldata/data/fancy.dat\")\nsouvenirtimeseries=ts(souvenir, frequency=12, start=c(1987,1))\n\nsouvenirtimeseries\n\n\n\nA Time Series: 7 × 12\n\n    JanFebMarAprMayJunJulAugSepOctNovDec\n\n\n    1987  1664.81  2397.53  2840.71  3547.29  3752.96  3714.74  4349.61  3566.34  5021.82  6423.48  7600.60 19756.21\n    1988  2499.81  5198.24  7225.14  4806.03  5900.88  4951.34  6179.12  4752.15  5496.43  5835.10 12600.08 28541.72\n    1989  4717.02  5702.63  9957.58  5304.78  6492.43  6630.80  7349.62  8176.62  8573.17  9690.50 15151.84 34061.01\n    1990  5921.10  5814.58 12421.25  6369.77  7609.12  7224.75  8121.22  7979.25  8093.06  8476.70 17914.66 30114.41\n    1991  4826.64  6470.23  9638.77  8821.17  8722.37 10209.48 11276.55 12552.22 11637.39 13606.89 21822.11 45060.69\n    1992  7615.03  9849.69 14558.40 11587.33  9332.56 13082.09 16732.78 19888.61 23933.38 25391.35 36024.80 80721.71\n    1993 10243.24 11266.88 21826.84 17357.33 15997.79 18601.53 26155.15 28586.52 30505.41 30821.33 46634.38104660.67\n\n\n\n\n\n시도표를 그려본 결과 추세성분과 계절성분이 관측되었고 이분산성도 관측되었음\n따라서 로그변환 후 계절지수평활법을 적용\n\n\nplot.ts(souvenirtimeseries) \n\n\n\n\n\nlogsouvenirtimeseries <- log(souvenirtimeseries)\nplot.ts(logsouvenirtimeseries)\n\n\n\n\n\nsouvenirforecasts <- HoltWinters(logsouvenirtimeseries)\nsouvenirforecasts\n\nHolt-Winters exponential smoothing with trend and additive seasonal component.\n\nCall:\nHoltWinters(x = logsouvenirtimeseries)\n\nSmoothing parameters:\n alpha: 0.413418\n beta : 0\n gamma: 0.9561275\n\nCoefficients:\n           [,1]\na   10.37661961\nb    0.02996319\ns1  -0.80952063\ns2  -0.60576477\ns3   0.01103238\ns4  -0.24160551\ns5  -0.35933517\ns6  -0.18076683\ns7   0.07788605\ns8   0.10147055\ns9   0.09649353\ns10  0.05197826\ns11  0.41793637\ns12  1.18088423\n\n\n\n\\(s_1,\\dots s_{12}\\)는 계절성분의 평활상수 이다.\n\n\nplot(souvenirforecasts)\n\n\n\n\n\nsouvenirforecasts2 <- forecast(souvenirforecasts, h=36)\nplot(souvenirforecasts2)\n\n\n\n\n\nz <- scan(\"koreapass.txt\")\npass <- ts(z, start=c(1981,1), frequency=12) \n\npass\n\n\n\nA Time Series: 9 × 12\n\n    JanFebMarAprMayJunJulAugSepOctNovDec\n\n\n    1981112696103070122800142496150064136128143033158223138626148761140539117693\n    1982133390117378136309149476157107147160153156164352139584165633147054129396\n    1983137638132128141178154545169127156056167749200228154371169199144594126590\n    1984132362131123146756160249175131169655178023193169171479186443172378150952\n    1985156240144008177073186693187296190206196132218560183651189003177325159672\n    1986163846158125193446193743205408210435219326249612205442234143216913185724\n    1987195346183669215524224547238201238304249401274307224528229684217576182898\n    1988199252193565230360242953274802271599293280328824236158305662312164267947\n    1989309654279943324963319020352602356546376214398079355931363955339887308085\n\n\n\n\n\nplot.ts(pass)\n\n\n\n\n\n가법모형 적용\n\n\nfit6 = HoltWinters(pass, seasonal=\"additive\") \nfit6\n\nHolt-Winters exponential smoothing with trend and additive seasonal component.\n\nCall:\nHoltWinters(x = pass, seasonal = \"additive\")\n\nSmoothing parameters:\n alpha: 0.4810767\n beta : 0.0383379\n gamma: 0.7345988\n\nCoefficients:\n          [,1]\na   347794.753\nb     3363.251\ns1  -12186.666\ns2  -33643.322\ns3    4855.643\ns4    5000.713\ns5   29085.909\ns6   22953.006\ns7   32200.195\ns8   49687.643\ns9  -11655.430\ns10  10218.813\ns11  -4226.391\ns12 -38683.394\n\n\n\n잔차 그림\n\n\nts.plot(resid(fit6), ylab=\"residual\", \n        main=\"가법모형의 예측오차\"); abline(h=0)\n\n\n\n\n\nfit6_2=forecast(fit6, h=12)\nplot(fit6_2)\n\n\n\n\n\n승법모형 적용\n\n\nfit7 =  HoltWinters(pass, seasonal=\"multiplicative\") \nfit7\n\nHolt-Winters exponential smoothing with trend and multiplicative seasonal component.\n\nCall:\nHoltWinters(x = pass, seasonal = \"multiplicative\")\n\nSmoothing parameters:\n alpha: 0.5623303\n beta : 0.03452066\n gamma: 0.3506508\n\nCoefficients:\n            [,1]\na   3.560417e+05\nb   3.517556e+03\ns1  9.199511e-01\ns2  8.489190e-01\ns3  9.796617e-01\ns4  1.016528e+00\ns5  1.095078e+00\ns6  1.060795e+00\ns7  1.089621e+00\ns8  1.178393e+00\ns9  9.706046e-01\ns10 1.065056e+00\ns11 9.983082e-01\ns12 8.552947e-01\n\n\n\nts.plot(resid(fit7), ylab=\"residual\", \n        main=\"승법모형의 예측오차\"); abline(h=0)\n\n\n\n\n\nfit6$SSE; fit7$SSE\n\n13764737658.0239\n\n\n12633778874.3409\n\n\n\nfit7_2=forecast(fit7, h=12)\nplot(fit7_2)"
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/시계열 분석.html#가.-자기회귀-모형-ar-모형-autoregressive-model",
    "href": "post/R for Data Scienece/통계분석/시계열 분석.html#가.-자기회귀-모형-ar-모형-autoregressive-model",
    "title": "11. Time-seris analysis",
    "section": "가. 자기회귀 모형 (AR 모형, Autoregressive Model)",
    "text": "가. 자기회귀 모형 (AR 모형, Autoregressive Model)\n\n\\(p\\) 시점 전의 자료가 현재 자료에 영향을 주는 모형\n\n\\[Z_t = \\phi_1 + Z_{t-1} + \\dots \\phi_pZ_{t-p} + \\alpha_t\\]\n\n\\(\\alpha_t \\sim WN(0,\\sigma^2)\\) 로 대표적인 정상시계열이다.\n자기상관함수(ACF)는 지수적으로 빠르게 감소, 부분자기상관함수(PACF)는 \\(p+1\\)시점에서 절단점을 가진다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/시계열 분석.html#나.-이동평균-모형-ma-모형-moving-average-model",
    "href": "post/R for Data Scienece/통계분석/시계열 분석.html#나.-이동평균-모형-ma-모형-moving-average-model",
    "title": "11. Time-seris analysis",
    "section": "나. 이동평균 모형 (MA 모형, Moving Average Model)",
    "text": "나. 이동평균 모형 (MA 모형, Moving Average Model)\n\\[ Z_t =\\alpha_t - \\theta_1\\alpha_{t-1}-\\theta_2\\alpha_{t-2}\\dots -\\theta_p \\alpha_{t-p}\\]\n\n유한개 개수의 백색잡음의 결합이므로 언제나 정상성을 만족한다.\n부분자기상관함수(PACF)는 지수적으로 빠르게 감소, 자기상관함수(ACF)는 \\(p+1\\)시점에서 절단점을 가진다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/시계열 분석.html#다.-자기회귀누적이동평균-모형-arima-pdq-모형-autoregressive-integrated-moving-average-model",
    "href": "post/R for Data Scienece/통계분석/시계열 분석.html#다.-자기회귀누적이동평균-모형-arima-pdq-모형-autoregressive-integrated-moving-average-model",
    "title": "11. Time-seris analysis",
    "section": "다. 자기회귀누적이동평균 모형 (ARIMA (p,d,q) 모형 , Autoregressive integrated moving average model)",
    "text": "다. 자기회귀누적이동평균 모형 (ARIMA (p,d,q) 모형 , Autoregressive integrated moving average model)\n\n\\(p \\to AR, \\, q\\to MA\\) 와 대응된다.\n\\(ARIMA\\) 모형은 비정상시계열 모형이다.\n\\(Z_t\\)가 비정상 시계열일 때 \\(d\\)번 차분한 시계열이 \\(ARMA(p,q)\\) 이면 \\(Z_t\\)는 \\(ARIMA(p,d,q)\\) 모형을 갖는다고 한다.\n\\(ARIMA(0,1,1)\\) 일 경우 1차분 후 \\(MA(1)\\)을 이용\n\\(ARIMA(1,1,0)\\) 일 경우 1차분 후 \\(AR(1)\\)을 이용"
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/시계열 분석.html#practice-with-r-2",
    "href": "post/R for Data Scienece/통계분석/시계열 분석.html#practice-with-r-2",
    "title": "11. Time-seris analysis",
    "section": "Practice with R",
    "text": "Practice with R\n\n데이터 로드 후 시도표\n딱히 이렇다할 추세나 계절 성분은 보이진 않는다.\n\n\nking <- scan(\"http://robjhyndman.com/tsdldata/misc/kings.dat\",skip=3)\nking.ts <- ts(king) ## 1년 주기로 데이터를 생성\nplot(king.ts)\n\n\n\n\n\nacf값과 pacf값을 확인\n\n\npar(mfrow=c(1,2))\nacf(king.ts); pacf(king.ts)\n\n\n\n\n\n또한 PACF 도표를 그려 확인한 결과 \\(lag\\, 2\\)에서 절단 점을 가지므로 \\(AR(1)\\) 모형을 1차적으로 고려해볼 수 있다.\n그러나 실제 ACF가 유의한지 아닌 지 검증을 수행하여 차분여부를 결정하자\n\n\nlibrary(fUnitRoots)\n\n\nadfTest(king.ts,lags=0, type=\"c\") ## type=\"c\" 는 적합할 모형에 상수항(절편)이 존재하는 경우\n                                  ## type= \"nc\"는 적합할 모형에 상수항이 0인 경우\n                                  ## type= \"ct\" 적합할 모형에 추세와 절편이 존재하는 경우\nadfTest(king.ts,lags=1, type=\"c\")\nadfTest(king.ts,lags=2, type=\"c\")\n\nWarning message in adfTest(king.ts, lags = 0, type = \"c\"):\n\"p-value smaller than printed p-value\"\n\n\n\nTitle:\n Augmented Dickey-Fuller Test\n\nTest Results:\n  PARAMETER:\n    Lag Order: 0\n  STATISTIC:\n    Dickey-Fuller: -4.0902\n  P VALUE:\n    0.01 \n\nDescription:\n Fri Nov 26 15:50:29 2021 by user: lee\n\n\n\nTitle:\n Augmented Dickey-Fuller Test\n\nTest Results:\n  PARAMETER:\n    Lag Order: 1\n  STATISTIC:\n    Dickey-Fuller: -3.0002\n  P VALUE:\n    0.04631 \n\nDescription:\n Fri Nov 26 15:50:29 2021 by user: lee\n\n\n\nTitle:\n Augmented Dickey-Fuller Test\n\nTest Results:\n  PARAMETER:\n    Lag Order: 2\n  STATISTIC:\n    Dickey-Fuller: -2.1483\n  P VALUE:\n    0.2665 \n\nDescription:\n Fri Nov 26 15:50:29 2021 by user: lee\n\n\n\n시차 0,1,2에서 검정 결과 차분을 1회할 경우 정상성을 만족할 것으로 보인다.\n검정결과만 보면 굳이 차분을 안해도 될 것 같긴하다.\n\n\npar(mfrow=c(1,2))\nking.ff1 <- diff(king.ts,differences=1)\nplot.ts(king.ts);plot.ts(king.ff1)\n\n\n\n\n\n시도표를 그려보았을 때는 기존보다 평균과 분산이 어느 정도 일정해짐을 볼 수 있었다.\n\n\npar(mfrow=c(1,2))\nacf(king.ff1); pacf(king.ff1)\n\n\n\n\n\nacf 그래프를 본 결과 lag 2부터 절단점을 가진다. \\(\\to MA(1)\\)\npacf 그래프를 살펴본 결과 lag 4부터 절단점을 가진다. \\(\\to AR(3)\\)\n모형의 해석력을 고려하여 \\(MA(1)\\)을 선택하는 것이 좋겠지만, auto.arima함수를 이용하여 적절한 모형을 선택\n\n\nlibrary(forecast)\n\n\nauto.arima(king) ##기존 데이터를 넣어 주어야함\n\nSeries: king \nARIMA(0,1,1) \n\nCoefficients:\n          ma1\n      -0.7218\ns.e.   0.1208\n\nsigma^2 estimated as 236.2:  log likelihood=-170.06\nAIC=344.13   AICc=344.44   BIC=347.56\n\n\n\n함수를 이용하여 산출된 모형은 \\(ARIMA(0,1,1)\\)이다.\n\n\\[ Z_t = \\varepsilon_t + 0.7218\\times \\varepsilon_{t-1}\\]\n\n따라서 \\(ARIMA(0,1,1)\\) 모형을 적합\n예측값을 살펴본 결과 43~52 번째 왕의 사망예측 나이는 67.75살로 추정된다.\n\n\nking.arima <- arima(king,order = c(0,1,1))\nking.forecasts <- forecast(king.arima)\nking.forecasts\nplot(king.forecasts)\n\n   Point Forecast    Lo 80    Hi 80    Lo 95     Hi 95\n43       67.75063 48.29647 87.20479 37.99806  97.50319\n44       67.75063 47.55748 87.94377 36.86788  98.63338\n45       67.75063 46.84460 88.65665 35.77762  99.72363\n46       67.75063 46.15524 89.34601 34.72333 100.77792\n47       67.75063 45.48722 90.01404 33.70168 101.79958\n48       67.75063 44.83866 90.66260 32.70979 102.79146\n49       67.75063 44.20796 91.29330 31.74523 103.75603\n50       67.75063 43.59372 91.90753 30.80583 104.69543\n51       67.75063 42.99472 92.50653 29.88974 105.61152\n52       67.75063 42.40988 93.09138 28.99529 106.50596"
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/요인분석(1205).html#참고",
    "href": "post/R for Data Scienece/통계분석/요인분석(1205).html#참고",
    "title": "09. Factor Analysis",
    "section": "참고",
    "text": "참고\n### 직각회전 : 회전된 인자들이 서로 상관되지 않도록 제약\n1. 베리맥스 (Variance is maximized) : 요인행렬을 변환할 때 행렬의 열을 기준으로 하여 더 큰 값은 크게, 작은 값은 더 작게 회전하는 길을 찾는다.\n2. 쿼티맥스 : 요인행렬의 행을 기준으로 분산을 극대화한다. 제 1요인만 과대 해석하고 기타 요인은 과소해석하는 문제가 있어, 단일요인 구조가 존재한다는 확신이 있을 때에나 한정적으로 사용한다.\n\n비직각회전(=사각회전) : 회전 결과에서 요인간의 상관이 0이 아님\n1. 회전된 요인간의 상관이 있다고 확신이 들 때 사용\n2.오블리민과 프로맥스가 있으나 R에서는 프로맥스를 제공\n\n\n요약\n\ndisplay_png(file = \"C:\\\\Users\\\\rkdcj\\\\OneDrive\\\\바탕 화면\\\\요인분석.png\")"
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/요인분석(1205).html#요인분석파트-주성분분석-함수써서-정리해야됨",
    "href": "post/R for Data Scienece/통계분석/요인분석(1205).html#요인분석파트-주성분분석-함수써서-정리해야됨",
    "title": "09. Factor Analysis",
    "section": "요인분석파트 주성분분석 함수써서 정리해야됨",
    "text": "요인분석파트 주성분분석 함수써서 정리해야됨"
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/주성분분석(1204).html",
    "href": "post/R for Data Scienece/통계분석/주성분분석(1204).html",
    "title": "08. PCA",
    "section": "",
    "text": "서로 상관성이 높은 변수들의 선형결합으로 만들어 기존의 상관성이 높은 변수들을 축소하는 기법이다.\n요인분석과 다르게 요인의 이름을 명명하지 않고 제1 주성분, 제2 주성분 등으로 표현된다.\n주성분분석의 결과에서 누적기여율 (cumulative proportion)이 85% 이상이면 주성분의 수로 결정할 수 있다.\n또한 scree plot을 그려 기울기가 급격히 줄어드는 전단계로 주성분의 수를 선택한다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/주성분분석(1204).html#princomp-vs-prcomp",
    "href": "post/R for Data Scienece/통계분석/주성분분석(1204).html#princomp-vs-prcomp",
    "title": "08. PCA",
    "section": "princomp vs prcomp",
    "text": "princomp vs prcomp\n\n둘의 차이는 princomp는 원데이터의 상관계수 또는 공분산행렬에 고유값 분해를\nprcomp는 원데이터에 특이값 분해를 적용하여 PCA를 수행한다는 것이다.\n\n\nlibrary(IRdisplay)\n\n\nsetwd(\"C:\\\\Users\\\\rkdcj\\\\OneDrive\\\\바탕 화면\")\n\n\ndisplay_png(file=\"pca.png\",width=800,height=700)\n\n\n\n\n\nsummary(princomp(USArrests,cor=T))\n\nImportance of components:\n                          Comp.1    Comp.2    Comp.3     Comp.4\nStandard deviation     1.5748783 0.9948694 0.5971291 0.41644938\nProportion of Variance 0.6200604 0.2474413 0.0891408 0.04335752\nCumulative Proportion  0.6200604 0.8675017 0.9566425 1.00000000\n\n\n\nsummary(prcomp(USArrests,scale=T))\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.5749 0.9949 0.59713 0.41645\nProportion of Variance 0.6201 0.2474 0.08914 0.04336\nCumulative Proportion  0.6201 0.8675 0.95664 1.00000"
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/주성분분석(1204).html#다시-돌아와서",
    "href": "post/R for Data Scienece/통계분석/주성분분석(1204).html#다시-돌아와서",
    "title": "08. PCA",
    "section": "다시 돌아와서",
    "text": "다시 돌아와서\n\nus.prin <- princomp(USArrests,cor=T)\nresult <- summary(us.prin)\nscreeplot(us.prin, npcs=4,type=\"lines\")\n\n\n\n\n\n누적기여율을 보았을 때 제 2주성분까지의 채택이 적절하다고 판단된다.\n또한 screeplot을 그려본결과 분산 감소 기울기가 급격히 줄어들기 전 시점인 2시점에서 제 2주성분을 채택하는 것이 적절하다고 판단된다.\n\n\nloadings(us.prin)\n\n\nLoadings:\n         Comp.1 Comp.2 Comp.3 Comp.4\nMurder    0.536  0.418  0.341  0.649\nAssault   0.583  0.188  0.268 -0.743\nUrbanPop  0.278 -0.873  0.378  0.134\nRape      0.543 -0.167 -0.818       \n\n               Comp.1 Comp.2 Comp.3 Comp.4\nSS loadings      1.00   1.00   1.00   1.00\nProportion Var   0.25   0.25   0.25   0.25\nCumulative Var   0.25   0.50   0.75   1.00\n\n\n\n\\(Comp1 = 0.536 \\times Muder + 0.583 \\times Assault + 0.278 \\times UrbanPop + 0.543 \\times Rape\\)\n제 1주성분은 범죄중에서도 muder, Assault와 관련있는 변수라고 볼 수 있다.\n아래는 각 주성분의 선형식을 통해 각 지역별로 얻은 결과이다.\n\n\nhead(us.prin$scores)\n\n\n\nA matrix: 6 × 4 of type dbl\n\n    Comp.1Comp.2Comp.3Comp.4\n\n\n    Alabama 0.9855659 1.1333924 0.44426879 0.156267145\n    Alaska 1.9501378 1.0732133-2.04000333-0.438583440\n    Arizona 1.7631635-0.7459568-0.05478082-0.834652924\n    Arkansas-0.1414203 1.1197968-0.11457369-0.182810896\n    California 2.5239801-1.5429340-0.59855680-0.341996478\n    Colorado 1.5145629-0.9875551-1.09500699 0.001464887\n\n\n\n\n\noptions(repr.plot.res=200, repr.plot.height=7,repr.plot.width=12)\n\n\nbiplot(us.prin,scale=0)\n\n\n\n\n\nbiplot은 원 변수와 주성분 간의 관계를 그래프로 표현한 것으로 그래프를 통해 각 주성분의 의미를 해석하고 각 개체들의 특성을 파악할 수 있다.\n화살표는 원 변수와 주성분의 상관계수를 의미하며, 주성분과 평행할수록 상관계수가 크므로 해당 주성분에 큰 영향을 끼친다.\n그리고 화살표가 같은 방향으로 인접해 있을수록 같은 주성분으로 생성될 수 도 있음을 알 수 있다.\n제 2주성분을 기준으로 Urbampop은 다른 변수들과 방향이 다르므로 상관관계가 낮다.\n제 1주성분을 기준으로 범죄와 관련된 3변수들은 같은 방향으로 인접해 있는 것을 확인할 수 있다.\n여기서 이상치인 도시는 Vermont, West Virginia 등은 변수 방향, 상관관계가 동떨어져 이상치로 판정될 수 있는 데이터이다.\n만약 이상치의 특성을 파악하라는 문제가 출제된다면, 위 결과에서 이상치라고 판단되는 값들 중 West Virginia는 범죄비율들과 도시인구비율이 적으므로 “범죄가 없는 시골” 이라고 해석하여 분석 결과를 본 미국 시민들이 그 도시로 몰릴 수도 있다고 판단할 수 있다."
  },
  {
    "objectID": "post/R for Data Scienece/통계분석/회귀분석.html",
    "href": "post/R for Data Scienece/통계분석/회귀분석.html",
    "title": "06. regression analysis",
    "section": "",
    "text": "변수들간의 인과관계를 밝히고 모형을 적합하여 관심 있는 변수를 예측하거나 추론하기 위해 사용하는 분석기법\n선형회귀분석의 가정\n\n오차의 등분산성\n오차의 독립성\n오차의 정규성 : Q-Q plot, Kolmogorov-Smirnov 검정, Shapiro-Wilk 검정을 확인하여 정규성을 확인한다.\n\n\n\n\n1. 모형 내의 개별 회귀계수에 대한 검정\n2. 모형에 설명력 \\(R^2\\)값을 통해 확인, 독립변수의 수가 많아지면 \\(adj-R^2\\) 값을 확인\n3. 회귀모형이 통계적으로 유의한가 확인\n4. 잔차 plot을 통해 모형의 진단\n\n\n\nCars93 데이터의 엔진크기(EngineSize)를 독립변수, 가격(Price)를 종속변수로 선정하여 단순 선형회귀분석을 실시한 후, 추정된 회귀모형에 대해 해석해보자.\n\nlibrary(MASS)\nlibrary(lmtest) ## 더비왓슨 테스트를 위함\nlibrary(tidyverse)\nselect <- dplyr::select\n\n\nfit1 <- lm(Price~EngineSize,data=Cars93)\nsummary(fit1)\n\n\nCall:\nlm(formula = Price ~ EngineSize, data = Cars93)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.684  -4.627  -1.795   2.592  39.429 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.6692     2.2390   2.085   0.0398 *  \nEngineSize    5.5629     0.7828   7.107 2.59e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.789 on 91 degrees of freedom\nMultiple R-squared:  0.3569,    Adjusted R-squared:  0.3499 \nF-statistic: 50.51 on 1 and 91 DF,  p-value: 2.588e-10\n\n\n\nfit1.1 <- lm(Price~ EngineSize +Horsepower +RPM + Width + Length + Weight,Cars93)\nsummary(fit1.1)\n\n\nCall:\nlm(formula = Price ~ EngineSize + Horsepower + RPM + Width + \n    Length + Weight, data = Cars93)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.659  -3.022  -0.144   2.376  27.955 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 77.117445  24.957836   3.090  0.00270 ** \nEngineSize   0.405792   1.699372   0.239  0.81184    \nHorsepower   0.141609   0.028745   4.926 4.02e-06 ***\nRPM         -0.001575   0.001933  -0.815  0.41751    \nWidth       -1.714985   0.386433  -4.438 2.68e-05 ***\nLength       0.152837   0.076182   2.006  0.04798 *  \nWeight       0.006586   0.002479   2.657  0.00939 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.45 on 86 degrees of freedom\nMultiple R-squared:  0.7024,    Adjusted R-squared:  0.6817 \nF-statistic: 33.83 on 6 and 86 DF,  p-value: < 2.2e-16\n\n\n\noptions(repr.plot.res=200,repr.plot.width=10,repr.plot.height=4)\n\n\nplot(Cars93$EngineSize,Cars93$Price,lwd=2)\nabline(a=coefficients(fit1)[2],b=coefficients(fit1)[1],col=\"red\",lwd=2)\n\n\n\n\n\npar(mfrow=c(1,2))\nplot(fit1,1); plot(fit1,2)\n\n\n\n\n\nshapiro.test(resid(fit1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(fit1)\nW = 0.85365, p-value = 3.886e-08\n\n\n\ndwtest(fit1,alternative=\"two.sided\")\n\n\n    Durbin-Watson test\n\ndata:  fit1\nDW = 1.1716, p-value = 2.236e-05\nalternative hypothesis: true autocorrelation is not 0\n\n\n1. 추정된 회귀계수는 모두 통계적으로 유의하다.\n2. 결정계수값과 수정된 결정계수 값이 각각 0.3569, 0.3499 로 산출되었다.\n3. F-통계량의 근거한 p-value값을 보아도 생성된 모델은 통계적으로 유의하다.\n4. 잔차 plot 을 그려본 결과 오차항의 정규성과 독립성 가정이 위배된 것 같다. * 실제로 test 결과 위배되었다는 결론이 통계적으로 유의미했다.\n5. 따라서 모형의 식별 단계로 돌아가 새로운 모형을 적합할 필요가 있어보인다.\n\ntest <- Cars93 %>% select(EngineSize)  %>% sample_n(5)\n\n\npredict(fit1,test,interval=\"none\") ##점추정\n\n117.4639509954058218.576539050536311.901010719755413.5698928024502513.0135987748851\n\n\n\npredict(fit1,test,interval=\"confidence\") # 회귀계수에 대한 신뢰구간을 고려한 구간\npredict(fit1,test,interval=\"prediction\") # 회귀계수에 대한 신뢰구간과 오차항을 고려한 구간\n\n\n\nA matrix: 5 × 3 of type dbl\n\n    fitlwrupr\n\n\n    117.4639515.76082419.16708\n    218.5765416.95120220.20188\n    311.90101 9.23710314.56492\n    413.5698911.26122015.87857\n    513.0136010.59070215.43650\n\n\n\n\n\n\nA matrix: 5 × 3 of type dbl\n\n    fitlwrupr\n\n\n    117.46395 1.89943633.02847\n    218.57654 3.02034434.13273\n    311.90101-3.79771227.59973\n    413.56989-2.07246929.21225\n    513.01360-2.64602928.67323\n\n\n\n\n\n\n\n\niris 데이터를 사용\nR에 lm함수는 범주형 변수를 자동으로 더미변수로 변환해줌\n\n\nfit2 <- lm(Petal.Length~.,data=iris)\nsummary(fit2) \n\n\nCall:\nlm(formula = Petal.Length ~ ., data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.78396 -0.15708  0.00193  0.14730  0.65418 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -1.11099    0.26987  -4.117 6.45e-05 ***\nSepal.Length       0.60801    0.05024  12.101  < 2e-16 ***\nSepal.Width       -0.18052    0.08036  -2.246   0.0262 *  \nPetal.Width        0.60222    0.12144   4.959 1.97e-06 ***\nSpeciesversicolor  1.46337    0.17345   8.437 3.14e-14 ***\nSpeciesvirginica   1.97422    0.24480   8.065 2.60e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2627 on 144 degrees of freedom\nMultiple R-squared:  0.9786,    Adjusted R-squared:  0.9778 \nF-statistic:  1317 on 5 and 144 DF,  p-value: < 2.2e-16\n\n\n\ndwtest(fit2,alternative=\"two.sided\")\n\n\n    Durbin-Watson test\n\ndata:  fit2\nDW = 1.6772, p-value = 0.03042\nalternative hypothesis: true autocorrelation is not 0\n\n\n\nshapiro.test(resid(fit2))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(fit2)\nW = 0.99389, p-value = 0.78\n\n\n\n\n\n\n\n\n1. 전진 선택법 (forward selection) : 절편만 있는 상수모형에서 시작하여 중요하다고 생각되는 설명변수부터 차례로 추가한다.\n2. 후진 제거법 (backward elimination) : 모든 독립변수를 포함한 모형에서 출발하여 종속변수에 가장 적은 영향을 주는 변수부터 하나씩 제거하면서 더 이상 제거할 변수가 없을 때의 모형을 선택한다.\n3. 단계적 방법 (stepwise method) : 전진선택법에 의해 변수를 추가하면서 새롭게 추가된 변수에 의해 기존 변수의 중요도가 약화되면 해당변수를 제거한다.\n\n\n\n\n모형의 복잡도에 따라 벌점을 주는 방식으로 \\(AIC, BIC\\) 값이 주로 사용된다.\n\n\n\n\n\nfit3 <- step(lm(Price~ EngineSize +Horsepower +RPM + Width + Length + Weight,Cars93),direction = \"both\")\nsummary(fit3)\n\nStart:  AIC=322.11\nPrice ~ EngineSize + Horsepower + RPM + Width + Length + Weight\n\n             Df Sum of Sq    RSS    AIC\n- EngineSize  1      1.69 2556.1 320.17\n- RPM         1     19.71 2574.1 320.82\n<none>                    2554.4 322.11\n- Length      1    119.55 2674.0 324.36\n- Weight      1    209.73 2764.2 327.45\n- Width       1    585.01 3139.4 339.29\n- Horsepower  1    720.84 3275.3 343.22\n\nStep:  AIC=320.17\nPrice ~ Horsepower + RPM + Width + Length + Weight\n\n             Df Sum of Sq    RSS    AIC\n- RPM         1     49.36 2605.5 319.95\n<none>                    2556.1 320.17\n+ EngineSize  1      1.69 2554.4 322.11\n- Length      1    140.92 2697.0 323.16\n- Weight      1    208.09 2764.2 325.45\n- Width       1    593.56 3149.7 337.59\n- Horsepower  1   1476.65 4032.8 360.57\n\nStep:  AIC=319.95\nPrice ~ Horsepower + Width + Length + Weight\n\n             Df Sum of Sq    RSS    AIC\n<none>                    2605.5 319.95\n+ RPM         1     49.36 2556.1 320.17\n+ EngineSize  1     31.34 2574.1 320.82\n- Length      1    132.02 2737.5 322.54\n- Weight      1    279.31 2884.8 327.42\n- Width       1    562.10 3167.6 336.12\n- Horsepower  1   1898.74 4504.2 368.86\n\n\n\nCall:\nlm(formula = Price ~ Horsepower + Width + Length + Weight, data = Cars93)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.956  -2.578  -0.182   2.114  28.448 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 53.005861  16.532269   3.206  0.00188 ** \nHorsepower   0.129653   0.016190   8.008 4.46e-12 ***\nWidth       -1.480623   0.339813  -4.357 3.56e-05 ***\nLength       0.152968   0.072440   2.112  0.03755 *  \nWeight       0.007339   0.002389   3.071  0.00283 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.441 on 88 degrees of freedom\nMultiple R-squared:  0.6965,    Adjusted R-squared:  0.6827 \nF-statistic: 50.48 on 4 and 88 DF,  p-value: < 2.2e-16\n\n\n\n\n\\[ C_p = \\frac{1}n (SSE+2p\\hat{\\sigma}^2)\\]\n\nCp 값이 p(변수의 개수)와 비슷한 경우 : bias가 작고 우수한 모델을 의미\nCp값이 p 보다 큰 경우 : bias가 크고 추가적인 변수가 필요한 모델을 의미\nCp값이 p 보다 작은 경우 : 분산의 증가폭보다 편향의 감소폭이 더 크며, 필요 없는 변수가 모델에 있다는 것을 의미\n일반적으로 Cp값이 작고, p+상수에 가까운 모형을 선택한다.\n\n\nlibrary(olsrr)\n\n\nols_mallows_cp(fit1, fit1.1)## fit1.1은 fullmodel이라고 생각\n\n96.8503933522773\n\n\n\nols_mallows_cp(fit3,fit1.1) ## fit1.1은 fullmodel이라고 생각\n\n4.71880013093038\n\n\n\nCp 통계량을 기준으로 보았을 때 \\(AIC\\)값과 단계적 선택법을 고려한 fit3가 fit1보다 적합한 모델이다."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html",
    "href": "post/etc/toeic speaking/part5.html",
    "title": "part 5",
    "section": "",
    "text": "- : 준비시간 45초, 답변시간 60초\n- part5 이론강의영상\n\n\n\n\n\nkeypoint : “문제에 대한 적절합 답변을 하는 것!”\n15초 이내로 문제 해석을 끝내는 것이 가장 좋음\n\n\n\n\n\n“파트 5 만능 문장, 파트3 만능 문장50가지” 를 이용하여 2문장 이상을 준비시간에 생각해내자\n파트 5 만능문장\n\n\n\n\n\n5글자 이상 되는 단어는 앞의 4-5글자만 쓰고, 동사 위주로만 필기하자.\n\n\n\n\n\n- 전반적인 순서 : 서론(결론) \\(\\to\\) 이유1 \\(\\to\\) 예시1\\(\\to\\) 이유2 \\(\\to\\) 예시2 \\(\\to\\) Therefore, I agree with the statement~\n- 할 수 있다면 만능문장과, 모범답안을 계속 모방해서 연습하자(고득점 전략!)\n\\(\\divideontimes\\) 동문서답금지 \\(\\to\\) 외운 답변을 문제에 적절하지 않게그대로 말하면 큰 실점이 된다.\n\n\n\n- 130 ~ 150 (IH) : 문제에 적합한 답변을 채점자가 알아 듣게만 하자 \\(\\to\\) 모범 답안의 70 ~ 80%의 퍼포먼스\n- 160이상 (AL 이상) : 파트5 만능 문장을 2문장 이상 다양하게 이용해서 말한다.\n- 파트 3의 만능 문장 50의 쉬운 문장들도 함께 이용한다.\n\n- 만능 문장을 적용해서 답변하는 연습 (30 set구비)\n\n130 + vs 160 : 구체성의 차이!\n\nA. IH 답변 : From my experience, I studied overseas\nB. AL 답변 : From my experience, I studied in Japan as an exchange student last year.\n\n\n\n\n\n\n\n\nThey can learn new things.\nThey can meet new people and expand their network.\nThey can have a lot of (new) experience and broaden their perspective.\nThey can’t make good decisions because they are not mature enough.\n\n\n\n\n\nThey will be distracted / if a teacher is not strict : strict(엄격한)\nThey can’t focus on their studies/work.\nThey can’t get good grades at school.\nThey will fall behind in class.\nThey can’t work efficiently.\n\n\n\n\n\nThey can save money.\nThe cost of living is too high.\nThey can’t make a living.\nI can get a high(er) salary.\nThe cost of N is too expensive.\nIt’s waste of money.\nThat’s good investment because it makes lives better.\n\n\n\n\n\nThey can focus better.\nThey will not be distracted by others.\nThey can set their own schedule.\nThey can have more freedom.\nThey feel more comfortable.\nIt’s fun and entertaining.\nThey can get information and share it with other people.\nIt feels more like a family.\n\n\n\n\n\nThey can get a lot of (useful information/latest information) on the Internet.\n\n기기나 인터넷 상에서 하는 것 앞에는 전치사 ‘on’ 을 쓴다.\n\nThey can communicate with their friends anytime anywhere on their smartphones.\nIt’s faster and convenient.\nThere is a lot of inaccurate information on the internet so it’s not reliable.\nit is very distracting for S so S can’t focus on their studies/work.\nI can get responses right away\nI can understand the feeling of the speaker more accurately.\n\n\n\n\n\nThey can make a friendly (work) atmosphere.\nThey can communicate with others better.\nThey can be good team players and make good relationships with others.\nThey can have a good reputation.\nThey can be very influential.\nThey can motivate others.\nEverything is always changing and there is a lot of competition.\nThey face a lot of challenges and difficulties.\nHe is able to handle a variety of situation due to his N.\nThey have a lot of experience/knowledge.\n\n\n\n\n\nEmployees can work more efficiently and productively.\nEmployees can be more satisfied with their jobs.\nIt can make a better work environment.\nThey might appear less professional.\nCustomers will fell satisfied and remain loyal.\nIt will attract more customers.\nThe business will be more successful.\nPeople frequently use N so it will be very effective.\n\n\n\n\n\nIt relieves their stress and they can relax.\nIt is good for their (Physical / mental) health.\nIt is not good for their health.\n\n\n\n\n\nIt is good for environment.\nPollution is a serious issue these days.\nIt can make a cleaner environment.\nWe will be able to protect the environment."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#서론",
    "href": "post/etc/toeic speaking/part5.html#서론",
    "title": "part 5",
    "section": "(1) 서론",
    "text": "(1) 서론\n- 서론 문장은 문제에 맞게 결론부터 말한다.\n- 유형별 빈출 서론문장\n1. 동의/비동의 : agree/disagree with the statement.\n\n2. 장/단점 : There are some advantages/disadvantages of N.\n\n3. 옵션 선택 : I think N is the most important.\n\n4. Do you think S + V : I think that S + V / I don't think S + V\n\n5. 선호도 질문 : I prefer A to B"
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#연결문장",
    "href": "post/etc/toeic speaking/part5.html#연결문장",
    "title": "part 5",
    "section": "(2) 연결문장",
    "text": "(2) 연결문장\n- Let me explain why I think this way."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#이유-1",
    "href": "post/etc/toeic speaking/part5.html#이유-1",
    "title": "part 5",
    "section": "(3) 이유 1",
    "text": "(3) 이유 1\n- 첫 번째 이유는 만능 문장을 이용해서 말하되, 문장에 맞게 적절하게 변형해서 이용한다.\n\nexample : Most of all, 첫번째 이유 S + V \\(\\to\\) 무엇보다도, 첫번째 이유"
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#예시-1",
    "href": "post/etc/toeic speaking/part5.html#예시-1",
    "title": "part 5",
    "section": "(4) 예시 1",
    "text": "(4) 예시 1\n- 첫 번째 예시에서는 경험 이야기를 하자.\nFrom my experienece, S + V\n- 경험 세부사항 말하는 법\n1) For me, it was very helpful because S + V\n\n2) It was great!, because \n\nex) For me, it was not helpful at all : 나에게 전혀 도움이 되지 않았다."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#이유-2",
    "href": "post/etc/toeic speaking/part5.html#이유-2",
    "title": "part 5",
    "section": "(5) 이유 2",
    "text": "(5) 이유 2\nOn top of that, 두 번째 이유 (if / When S + V) \\(\\to\\) 게다가 ~ 한다면, ~ 할 때 두 번째 이유다."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#예시-2",
    "href": "post/etc/toeic speaking/part5.html#예시-2",
    "title": "part 5",
    "section": "(6) 예시 2",
    "text": "(6) 예시 2\n- 뉴스기사와 전문가들의 견해를 연결하여 말하자.\nAccording to a recent news report,. the majority of 사람들 in korea said that S + V (if / when S + V)\n- 위 S + V 에는 주장을 쓰자 (문제와 내 노트 테이킹 한거를 다시 말하기!)"
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#결론",
    "href": "post/etc/toeic speaking/part5.html#결론",
    "title": "part 5",
    "section": "(7) 결론",
    "text": "(7) 결론\n- Therefore, 서론"
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#경험-교육",
    "href": "post/etc/toeic speaking/part5.html#경험-교육",
    "title": "part 5",
    "section": "(1) 경험 & 교육",
    "text": "(1) 경험 & 교육\n\nset2\n- 교재 154\nDo you agree or disagree with the statement :\nTeenagers Should get free admission to art galleries\n\nanswer\n- keyword: learn, expand, experience, broaden\n- 서론 : I agree with the statement\n- 연결문장 : Let me explain why I think this way.\n- 이유1\nMost of all, Teenagers can learn new things about the arts (만능문장1번)\n- 예시 1\nFrom my experienece, When I was a high school student, I got free admission to art gallery.(문제 질문을 그대로 인용)\nFor me, It was very helpful because I learned new things about the arts and art history (만능문장 1번 변형)\n- 이유 2\nOn top of that, they can have a lot of new experience and broaden their perspective.(만능문장 3번) when they visit art galleries.\n- 예시 2\nAccording to a recent news report, the majority of education experts in korea said that visiting art galleries(for free) is very beneficial for teenagers because they can experience many things and broaden their persfectives. (만능문장 3번 변형)\n- 결론\nTherefore, I think this way.\n\n\n\nset1\n- 교재 153 page\nWhat are advantages of learning how to play musical instruments for teenagers?\nUse specific reasons and examples to suppport your opinions.\n\nanswer\n- keyword: learn, expand, experience, broaden\n- 서론\nThere are some advangtages of learning how to play musical instruments for teenagers.(서론 만능 템플릿!)\n- 연결문장\nLet me explain why I think this way.\n- 이유 1\nMost of all, teenagers can learn new things such as music skills (만능문장 1번 변형)\n- 예시 1\nFrom my experience, When I was a highschool student, I took a piano lesson.\nFor me, It was very helpful because I learned new things such as piano skills and music skiils (이유 1에서 이용한 만능문장 그대로 사용)\n- 이유 2\nOn top of that, they can have a lot of new experience and broaden their perspective.(만능문장 3번)\n- 예시 2\nAccording to a recent news report, the majority of education experts in korea said that learnging how to play musical instruments is very beneficial for teenagers because they can have a lot of new experience and broaden their perspective (만능문장 3번)\n- 결론\ntherefore, I think this way\n\n\n\nset 24\n- 교재 187\nDo you agree or disagree with the following statement : Participating in an international internship is the best way to imporve one’s work performance\n\nanswer\n- keyword : hav/exper, broad/per, meet, expand/net, learn\n- 서론\nI agree with the statement\n- 연결문장\nLet me explain why I think this way.\n- 이유 1\nMost of all, if employees participate in an international internship, they can have a lot of experience and broaden their perspective (만능문장 3번 변형)\nAlso, they can meet new people and expand their business network overseas. (만능문장 2번 변형)\n- 이유 2\nOn top of that, employees can learn new things such as language skiis and culture.(만능문장 1번 변형)\n- 예시\nAccording to a recent news report, the majority of succesful CEOS in korea said that Participating in an international internship is very beneficial because they can improve theirs work performance.\nTherefore, I think this way."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#집중",
    "href": "post/etc/toeic speaking/part5.html#집중",
    "title": "part 5",
    "section": "(2) 집중",
    "text": "(2) 집중\n\nset 3\n- 교재 156\nDo you agree or disagree with the statments :\nElementary school studentes should not be allowed to have their own smartphones at school.\nuse specific reasons and examples to support tour opinions.\n\nanswer\n- keyword : distracted, focus/study, work/effi, can’t get grades\n- 서론 및 연결문장\nI agree with the statement.\nLet me explain why I think this way.\n-이유 1\nMost of all, If elementary school students are allowed to have their own smartphones at school (문제는 우리의 참고서!), they wiil be distracted and they can’t focus on their studies (만능문장 5번 6번)\n- 예시 1\nFrom my exeperience, When I was an elementary school student, I had my own smartphones.(문제를 꼼꼼히 보자)\nFor me , it was not helpful at all becasue I was distracted by my smartphones. So, I couldn’t focus on my studies. (주장을 한번 더 부각하고, 만능문장 6번 변형)\n- 이유 2\nOn top of that, according to a recent news report, the majority of education experts in korea said that elementary school students should not be allowed to have their smartphones at school because they can be distracted and can’t learn efficiently.(만능문장 5번 9번)\nTherefore, I agree with the statement.\n\\(\\divideontimes\\) 위 문제 같은 경우는 서론-이유1-예시1-예시2-결론 형태이다. 익숙해지면 나에게 편한 방식으로 뱉어보자"
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#돈",
    "href": "post/etc/toeic speaking/part5.html#돈",
    "title": "part 5",
    "section": "(3) 돈",
    "text": "(3) 돈\n\nset 4\n- 교재 158\nwhen you choose a place to work, which of the following is the most important?\n- a high salary\n- a flexible working schedule\n- more opportunities to work abroad\n\nanswer\n- keyword : save/money, cost/high, make/living, satisfied, work/effi\n- 서론\nI think a high salary is the most important.\nLet me explain I why think this way.\n- 이유 1\nMost of all, I can save money. (만능문장 10번)\nAs you know, the cost of living is too high these days. So, I can’t make a living without a high salary (만능문장 11, 12)\n- 이유 2\nOn top of that, I can be more satisfied with my job if I earn(get) a high salary.(만능문장 45)\n- 예시 2\nAccroding to a recent news report, the majority of offices workers in korea said that getting a high salary is very important (서론이용)\nbecause they can be more satisfied with their jobs and they can work more efficiently and productively when they get a high salary. (만능문장 45, 44)\ntherefore, I think a high salary is the most impoartant.\n\n\n\nset 5\n- 교재 159\nDo you agree with the following statement :\nLocal goverments should provide more financial support for making more green spaces such as parks.\nuse specific reasons and examples to support your opinons.\n\nanswer\n- keyword : investment, make/ppl’s lives, waste, good 4 env, protect\n- 서론\nI agree with the statement.\nLet me explan why I think this way.\n- 이유 1\nMost of all, making more green spaces such as parks is a good investment because it makes people’s lives better (만능문장 16) because It’s good for people’s health. so it’s not a waste of money. (만능문장 15, 53)\n- 이유 2\nOn top of that, making green spaces is good for the environment. (만능문장 55)\nPolloution is a serious issues these days,(만능문장 56). if the local goverments provide more financia(빠이낸셜) support for making green speaces, we will be able to protect the environment.(만능문장 58)\nTherefore, I agree with the statement."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#혼자-vs-같이",
    "href": "post/etc/toeic speaking/part5.html#혼자-vs-같이",
    "title": "part 5",
    "section": "(4) 혼자 vs 같이",
    "text": "(4) 혼자 vs 같이\n\nset 6\n-교재 161\nWhen you do projects, do you prefer to work with the people in your own department everytime or collaborate with employees of other departments? why?\nuse specific reasons and examples to supoort your opinions.\n\nanswer\n- keywords : get/inf, share/ppl, meet/ppl, expand/net\n- 서론\nWhen I do projects, I prefer to collaborate with employees of other departments.\nLet me explain why I think this way.\n- 이유 1\nMost of all, I can get information from employees from other departments and share it with other people. (만능문장 23)\n- 예시 1\nFrom my experience, I used to work at a company. At that times, I collaborated with employess from other departments. For me, It was very helpful because I was able to new information. (만능문장 25)\n- 예시 2\nOn top of that, I was able to meet new people from other departments and expand my network. (만능문장 2) (시간이 부족하면 생략가능)\nTherefore, When I do projects, I prefer to collaborate with employees of other departments.\n\n\n\nset 7\n- 교재 162\nwhat are some advantages of taking a class that provides one-on-one teaching rather than taking a big class for college student?\nuse specific reasons and examples to support your opinions.\n\nanswer\n- keywords : focus, not distract, set/schedule, feel/freedom\n- 서론\nthere are some advantages of taking a class that providing one-on-one teaching.\nLet me explain why I think this way.\n- 이유 1\nMost of all, Students can focus better and they will not be distracted by other students if a class provide one-on-one teaching. (만능문장 17, 20)\n- 예시 1\nFrom my experience, when I was college a student, I took a class that provided one-on-one teaching.\nFor me. it was very helpful because I was able to focus better and not be distracted by other students.(I feel more comfotable \\(\\to\\) 만능문장 21)\n- 이유 2\nOn top of that, students can set their own schedule(만능문장 19) if they take a class that provides one-on-one teaching. Then, They can have more freedom.\nTherefore, I think this way."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#기술",
    "href": "post/etc/toeic speaking/part5.html#기술",
    "title": "part 5",
    "section": "(5) 기술",
    "text": "(5) 기술\n\nset 8\n- 교재 165\nWhich of the following inventions do you think improved the quality of our lives more?\n\nsocial media, TV\n\n\nanswer\n- keywords : comm/any/ppl, get/inf\n- 서론\nI think social media improved the quality of our lives more.\nLet me explain why I think this way.\n- 이유 1, 예시 1\nMost of all, people can communicate with other people anytime anywhere on social media. (만능문장 26)\nFrom my experience, I use social media when I communicate with my friends.\nFor me, It is very helpful because i can communicate with my friends anytime anywhere, and It’s faster and more convenient.\n- 예시 2\nAccording to a recent news report, the majority of people in korea said that social media improved the qulity of our lives because they can get the latest information on social medial.\ntherefore, I think this way.\n\n\n\n\nset 10\n- 교재 167\nWhen having a meeting with clients at work, which one do you think is a more efficient way?\n\nhaving a video conference, - talking face-to-face\n\n\nanswer\n- keywords : under/feeling, builds, satisfied\n- 서론\nI think talking face-to-face is more efficient way.\nLet me explain why I think this way.\n- 이유 1\nMost of all, employees can understand the feeling of the clients more accurately. so it caues less misunderstanding. (만능문장 33)\n- 예시 1\nFrom my experience, I used to work at a company, and I had video conference with my clients.\nFor me, It was not helpful at all because we couldn’t talk face-to-face.\n- 이유 2\nOn top of that, talking face-to-face is more personal and builds a closer relationship with clients.\n- 예시 2\nAccording to a recent news report, the majoriy of successful CEOs in korea said that talking face-to-face with clients is very important because it builds a closer relationship with clients and clients will be more satisfied.\nTherefore, I think this way."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#특징",
    "href": "post/etc/toeic speaking/part5.html#특징",
    "title": "part 5",
    "section": "(6) 특징",
    "text": "(6) 특징\n\nset 11\n- 교재 170\nWhich of the following is the most important quality of a good business leader?\n\nKnowing how to deal with conflicts\nknowing how to listen to others\norganizational skills\n\n\nanswer\nI think knowing how to listen to others is the most important.\nlet me explain why I think this way.\nMost of all, leaders can communicate with other employees better when they know how to listen to others.\nFrom my experience, I used to work at a company, and my manager knew how to listen to others. It was great because he was able to commnuicate with other employees very well.\nHe was able to be a good team player and make good relationships wtih other employees.\nOn top of that, accroding to a recent news report. the majority of successful CEOs in korea said that knowing how to listen to others is very imortant for leaders because those leaders can make a friendly work atmosphere. (만능문장 34)\nTherefore, I think knowing how to listen to others is the most important.\n\n\n\nset 12\n- 교재 171\nDo you agree or disagree with the following statement : New employees should be creative in order to succeed in their careers.\n\nanswer\n- keywords : handle, every/chaining/competition, face/challenges/diff, deal with\nI agree with the statement.\nLet me explain why i think this way.\nMost of all, creative employees are able to handle a variety of situation due to their creativity. (만능문장42)\nEverything is always changing, and there is a lot of competition, and they face a lot of challenges and difficulties.(만능문장 40, 41)\nIf new employees are creative, they can handle a variety of situation (due to creativity. 생략가능)\nOn top of that, according to a recent news report, the majority of successful CEOs in korea said that creativity is very important for new employeees because they can deal with problems more effectively if they are creative.\nTherefore, I agree with the statement.\n\n\n\nset 13\n- 교재 172\nDo you think pop stars can be good role models for other people?\n\nanswer\n- keywords : hav/reputation, influe, motivate, bts\nI agree with the statement.\nLet me explain why I think this way.\nMost of all, pop stars can have a good reputation, and they can be very influential. So, they can motivate other people.(만능문장 37, 38, 39).\nFor example, the korean pop group, BTS has a good reputation and is very influential. BTS motivates a lot of teenagers.\nOn top of that, according to a recent news report, the majoriy of teenagers in korea said that pop stars are their good role models because pop stars motivate teenagers.\nTherfore, I agree with the statement."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#업무환경기업성공",
    "href": "post/etc/toeic speaking/part5.html#업무환경기업성공",
    "title": "part 5",
    "section": "(7) 업무환경/기업성공",
    "text": "(7) 업무환경/기업성공\n\nset 14\n- 교재 174\nDo you think telecommuting is better than working at an office?\n\nanswer\n- keywords : work/eff/prod, feel/comfor, make g env, satisfied\nI think telecommuting is better than working at an office.\nLet me explain why I think this way.\nMost of all. employees can work more efficiently and productively.(만능문장 44)\nFrom my experience. I used to work from home. For me, it was very helpful becasue I felt very comfortable at home. It made a better environment, so I worked more efficiently and productively (만능문장 44, 46 혼합).\nAlso, I was more satisfied with my job when I worked from home (만능문장 45).\nOn top of that, employees can be more satisfied with their jobs if they work from home.\nAccording to a rencent news report, the majority of office worker in korear said that they prefer to work from home,\ntherefore, I think telecommuing is better than working at an office.\n\n\n\nset 15\n- 교재 175\nWhat is the most important quality of start-up company should have in order to succeed in a market?\n\ngood location\nvariety of products\ngreat customer service\n\n\nanswer\n- keywords : satisfied, ramain loyal, attract\ni think a great customer service is the most important.\nLet me explain why I think this way.\nMost of all, customers will feel satisfied and remain loyal if a start-up company offers great customer service. (만능문장 48)\nFrom my experience, I visited a new restaurant in my town, and they offerd great customer service. I was very satisfied and became a regular customer.\nOn top of that, according to a recent news report, the majority of successful CEos in korea said that a great service is very important in order to succeed in a market because it can attract a lot of customers.\ntherfore, I think a great customer service is the most important."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#스트레스건강",
    "href": "post/etc/toeic speaking/part5.html#스트레스건강",
    "title": "part 5",
    "section": "(8) 스트레스/건강",
    "text": "(8) 스트레스/건강\n\nset 16\n- 교재 177\nWhich one do you think is the most important subject for high school students to learn?\n\nphysical education\nmath and science\narts and crafts\n\n\nanswer\n- keywords : reli, relax/str, be good 4 health\nI think Physical education is the most important.\nLet me explain why I think this way.\nMost of all, physical education relives their stress and they can relax. (만능문장 52)\nFrom my experience, When I was a high school student, I had a lot of stress.\nhowever, I took a physical education classes, and they relived my stresss, so I was able to relax. It was good for my mental health.\nOn top of that, it is good for their physical health. (만능문장 53)\nAccording to a recent news report, the majoriy of doctors in korea said that physical education is very beneficial for high school students because it is good for their health. (만능문장 54)\ntherefore, I think this way.\n\n\n\nset 17\n- 교재 178\nDo you agree or disagree with the following statement :\nVending machines should not be allowed in cafeterias(캐퍼티어리어스) of schools or workplaces.\n\nanswer\n- keywords : not good 4 health, develop unhealthy\nI agree with the statement.\nLet me explain why I think this way.\nMost of all, It is not good for people’s health.\nFrom my experience, when I was a high school student, There was a vending machine in my school cafeteria, and I used to buy sodas every day.\nFor me, it was not helpful at all. It developed unhealthy habits.\nOn top of that, accroding to recent news repot, the majority of doctors in korea said that vending machines should not be allowed in cafeterias because people can develop unhealthy habits.\ntherefore. I agree with the statement.\n\n\n\nset 18\nWhich of the following pastime activities would be the best way to relieve people’s stress\n\nplaying mobile game\nknitting\nwalking a dog\n\n\nanswer\n- keywords : fun/entertan, don’t get bored, relive/relax str\nI think playing mobile game is the best way to relieve people’s stress\nLet me explain why I think this way.\nMost of all. It’s more fun and entertaining more knitting and walking a dog\nFrom my experience, I like to play mobile game in my free time. it’s great because it’s fun and entertaining, and I don’t get bored.\nOn top of that, mobile games are very popular, so people love them.\nAccording to a recent news report, the majority of people in korea said that playing mobile games is the best way to relieve their stress. because it makes them happy and they can forget about their worries\ntherefore, I think this way"
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#환경",
    "href": "post/etc/toeic speaking/part5.html#환경",
    "title": "part 5",
    "section": "(9) 환경",
    "text": "(9) 환경\n\nset 19\n- 교재 181\nDo you think the goverment should make laws to have more people use public transportation?\n\nanswer\n- keywords : be good 4 env, pollu is serious, make cleaner env, protect\nI agree with the statement.\nLet me explain why I think this way.\nMost of all, it is good for environment.(만능문장 55)\nPollution is a serious issue these days. (만능문장 56)\nif the goverment makes laws to have more people use public transportation, it can make a cleaner environment. (만능문장 57)\nThen, we will able to protect environment.(만능문장 58)\nOn top of that, According to a recent news report, the majority of environmental experts in korea said that public transfortation is very important because it is good for the environment. (만능문장 55)\ntherfore, I think the goverment should make laws to have more people use public trasportation."
  },
  {
    "objectID": "post/etc/toeic speaking/part5.html#과거-현재-비교",
    "href": "post/etc/toeic speaking/part5.html#과거-현재-비교",
    "title": "part 5",
    "section": "(10) 과거 현재 비교",
    "text": "(10) 과거 현재 비교\n\nset 20\n- 교재 182 ~ 183\nDo you agree or disagree with the following statement :\nthese days, people spend more time using their cell phones than in the past.\n\nanswer\n- keywords : anytime~, spend\nI agree with the statement.\nLet me explain why I think this way.\nMost of all, thanks to technology, people can do everything anytime anywhere on their smartphones.\nSo, people spend more time using their smartphones than in the past.\nHowever, in the past, they didn’t have smartphones, so they were not able to do many things on their smartphones.\nSo, people spent less time using their cellphones than now.\ntherefore, I agree with the statement.\n\n\n\nset 21\n- 교재 184\nWhen you work at a company as a new employee, which one do you think is the best option.\n\na bonus\na long vacation\nflexible working hours\n\n\nanswer\n- keywords : relive, relax, work/effi/prod, satisfied\nWhen I work at a company as a new employee, I think a long vacation is the best option for me.\nLet me explain why I think this way.\nMost of all, it relieves my stress, and I can relax if I have a long vacation.\nFrom my experience, I used to work at a company, and the company offered a long vacation for employees.\nFor me, it was very helpful because I was able to relax and work more efficiently and productively after the vacation. I was able to be more satisfied with my job.\nOn top of that, employeess can learn new things during the vacation.\naccording to a recent news report, the majority of successful CEOs in korea said that a long vacation is very beneficial for employees because they can learn new thing and broaden their pespectives. (during the vacation)\ntherefore. I think this way.\n\n\n\nset 22\n- 교재 185\nDo you think high school students should have organizational skills to do well at school? why?\n\nanswer\n- keywords : save/time, get/grades, not distract, focus\nI think high school students should have organizational skiils to do well at school.\nlet me explain why I think this way.\nMost of all, they can save time if they have organizational skills.\nAs you know, there is a lot of competition at school.\nHowever, if they have organizational skiils and can save time, they can get good grades at school and will not fall behind in class.\nOn top of that, students can set their own schedule better if they have organizational skills.\nThen, they will not be distracted by other things and they can focus better on their studies.\ntherfore, I think this way."
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html",
    "href": "post/etc/toeic speaking/part4.html",
    "title": "part 4",
    "section": "",
    "text": "표 읽는시간 45초, 답변 준비시간 문항당 3초\n10 번을 제외하고 전부 1회 청취가능\n답변시간 : 8, 9번 \\(\\to\\) 15초, 10번 \\(\\to\\) 30초"
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#표-읽는-시간",
    "href": "post/etc/toeic speaking/part4.html#표-읽는-시간",
    "title": "part 4",
    "section": "(1) 표 읽는 시간",
    "text": "(1) 표 읽는 시간\n\n전반적인 내용\n중요한 부분(반복적 키워드)\n특이사항\n유형별 중요한 포인트"
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#고정된-템플릿",
    "href": "post/etc/toeic speaking/part4.html#고정된-템플릿",
    "title": "part 4",
    "section": "(2) 고정된 템플릿",
    "text": "(2) 고정된 템플릿\n\n동일한 표현을 썼더라도 문제에 대한 정확한 답변을 했을때 고득점을 받음!`"
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#듣기-잘-듣는-법",
    "href": "post/etc/toeic speaking/part4.html#듣기-잘-듣는-법",
    "title": "part 4",
    "section": "(3) 듣기 잘 듣는 법",
    "text": "(3) 듣기 잘 듣는 법\n- 파트 4는 유사한 패턴이 반족 출제 되므로 문제를 예측해서 키워드를 캐치하자!\n- 듣기 문제 예측방법\n\n표가 힌트다 \\(\\to\\) 표가 계속 화면에 떠있음\n의문사를 잘듣자 \\(\\to\\) 8번 문제의 경우 시간, 날짜, 장소, 금액 등 고정적으로 묻는 문제들이 출제됨\n키워드 듣기 \\(\\to\\) 9, 10번의 경우, 표에 나와있는 키워드를 언급을 한다면 그 부분이 정답이 되는 경우가 많음"
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#유형별-정리",
    "href": "post/etc/toeic speaking/part4.html#유형별-정리",
    "title": "part 4",
    "section": "(4) 유형별 정리",
    "text": "(4) 유형별 정리\n\n일정표\n개인 일정표\n이력서\n면접 일정표\n수업 시간표"
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#날짜-숫자-시간",
    "href": "post/etc/toeic speaking/part4.html#날짜-숫자-시간",
    "title": "part 4",
    "section": "(1) 날짜, 숫자, 시간",
    "text": "(1) 날짜, 숫자, 시간\n\nA. 서수 읽기\n- 날짜는 ’서수’로 읽는다. ‘숫자 + th’\n- 나머지는 교재 보고 읽기 연습\n\n\nB. 전치사 사용\n\n1. 시간\n- on + 날짜/요일\n- At + 시각\n- in + 월/계절\n- From A to B\n\n\n2. 장소\n- At : 특정 장소\n- in : 방안(실내장소), 도시, 나라\n- On : 층, 거리명, 교통편\n\n\n3. 기타\n\nby/with + 사람\n\nby : 그 사람에 의해서 진행되거나 이끌어 지는 경우\nwith : 그 사람이 진행 하거나 진행하지는 않지만, 그 사람과 함께 하는 경우\n\non/about + 주제 : 둘의 딱히 구별은 없음\nfrom + 소속\nthe, as + (a) 직책\n\nas는 이력서 유형에서 경력을 말할 때!"
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#일정표-템플릿",
    "href": "post/etc/toeic speaking/part4.html#일정표-템플릿",
    "title": "part 4",
    "section": "일정표 템플릿",
    "text": "일정표 템플릿\n\nQuestion 8\n- It will be held + 시간/장소 = it wiil take place + 시간/장소\n- It will start at 시간/장소 = It will begin at 시간/장소\n- It will finish at 시각\n\n\nQuestion 9\n- No, I’m afraid that you have the wrong information, Actually, + 맞는정보\n\n\nQuestion 10\n- There are ___ session. First, Next, Finally, there is 일정 on 주제 by 사람\n\n\nset 1\n- 교재 126\n\n- Q8. when and where is it scheduled to take place?\n- Answer\nIt will be held on Monday, November 2nd, at Washington Convention Center\n- Q8-1. On what date does the conference take place and what time does it start?\n- Answer\nIt will be held on Monday, November 2nd and it will start at 10 a.m\n- Q8-2\nHow much is the registration fee per person if I”m going with five other people?\n- Answer\nIt’s 15 dollars per person because you can get a group discount\n\n- Q9. I’m going with some other people. so will be a group of six people. Can we get a group discount? and, if so, how much is the discounted registration fee per person?\n- Answer\nYes, you can get a group discount, and it’s 15 dollars per person.\n- Q9-1. I heard that Murphy davis will give a presentation in the aftenoon is that right?\n- answer\nNo. I’m afraid that you have the wrong information. Actually Murphy Davis will give a presentation at 11 am.\n- Q9-2. I was told that there will be a seminar by jane Morris at 1 p.m. Can you confirm that?\n- answer\nNo, actually, there was supposed to be a seminar by Jane Morris, but it has been cancled.\n- Q9-3. I’m afraid I’ll have to leave after 4 p.m. Will I miss anything?\n- answer\nYes, from 4 to 5 p.m, there is a lecture on Customer Service Skills for Small Business Owners by Susan Foster\n\n- Q10. I’m looking forward to sessions that deal with customer service. Could you give me all the details of the sessions related to customer service?\n- answer\nSure. First, at 2 p.m. there is a workshop on how to provide great customer service by Ashley Parker. Next, at 4 p.m, there is a lecture on customer service skills for small business owners by Susan Poster\n- Q10-1. I want to know about the details of the sessions that deal with customer service. Can you tell me about them?\n- answer\nSure. First, at 2 p.m. there is a workshop on how to provide great customer service by Ashley Parker. Next, at 4 p.m, there is a lecture on customer service skills for small business owners by Susan Poster\n\n\nset 3\n- 교재 128\n\n- Q8. When is the first day of the orientation, and what is the first thing on the schedule?\n- answer\nSaturday, August 15th, is the first day of the orientaton. and the first thing on the schedule is an opening address at 10 a.m\n- Q9. I heard that there will be a company tour in the morning. Am I right?\n- answer\nNo. I’m afraid that you have the wrong information. actually, the company tour will be held at 1 p.m\n- Q10, Someone told me that I would find the lectures particularly helpful. Can you give me all the details of the lectures offered at the orientation?\n- answer\nSure, There are two lectures on Sunday. First, at 9 am. there is a lecture on how to succeed in a workplace by jennifer owen. Then, at noon, there is a lecture on Teamwork is Everything by Cindy Peterson"
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#수업시간표",
    "href": "post/etc/toeic speaking/part4.html#수업시간표",
    "title": "part 4",
    "section": "수업시간표",
    "text": "수업시간표\n\n템플릿1. 금액, 등록\n- You have to pay 금액 for 강좌\n- it’s 숫지 dollars.\n-You should register(=sign up) by 등록 마감일\n- If you V, it’s ~dollars\n- It’s free\n- It’s for 대상자\n\n\n템플릿 2. 수업 소개\n- There is a 과목명 class/course\n- 강사 will teach 과목명 class/coures\n- There is a class on 주제\n- 강사 will teach a class on 주제\n\n\nset 10\n- 교재 135\n\n- Q8. How much is the fee per course, and when is the deadline for registration?\n- answer\nIt’s 50 dollars per course, you have to register by December 2nd.\n-Q9. I heared that Elliot Ross will be teaching History of video Artwork. Can you confirm that?\n- answer\nNo. I’m afraid that you have wrong information. actually, Andrew Morgan will teach history of video Artworks.\n\n- Q10. I’m busy with my work these days. so I only have time on mondays and saturdays. Can you tell me all the details of the classes scheduled on mondays and saturdays?\n- answer\nSure. First, on mondays, there is a class on the importance of artworks appreciation(어프리쉬에이션) by Shellakeen from 1 to 2 pm. Next, on Saturdays. there is an oil painting for beginners by kate carter from 4 to 5 pm.\n\n\nSet 4\n- 가끔 출제되는 유형\n\n- Q8. What time does the first movie start, and what is the title of the movie?\n- answer\nIt will be held at 9 am. and the title of the movie is Seed of Evil\n- Q9. I’m a big fan of Sean Adams. and I want to watch his movie. A man in Love. I heard that the fee is 10 dollars per ticket. Is that right?\n- answer\nNo, actually, it’s a free. So you don’t have to pay for the ticket.\n- Q10. Could you tell me about all the screenings being held in the New york Theater?\n- answer\nThere are two films. First, at 11 am, there is Gloomy Fate. it’s 7 dollars per ticket. Next, at 3 p.m, there is Heaven. It’s 6 dollars per ticket."
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#개인일정표",
    "href": "post/etc/toeic speaking/part4.html#개인일정표",
    "title": "part 4",
    "section": "개인일정표",
    "text": "개인일정표\n\n템플릿 1. ~ 할것이다\n- You will depart from 출발지\n- You will arrive in 도착지\n- You will take 교통편\n- You will stay at 숙소\n- You will have lunch/ dinner/ a meeting.\n- You will give a sppech/ a lecture/ a presentation\n\n\n템플릿 2. 취소/ 연기 일정 변경\n-There was supposed to (비 서포스투) be 일정, but it has been canceled.\n- There was supposeed to be 일정, but it has been postponed.\n- There was supposed to be 일정 but it has been rescheduled to 바뀐날짜\n\n\nset 5\n\n- Q8. What time will the team meeting take place. and what we be discussing?\n- answer\nit will be held at 1 pm. and the topic will be reviewing safety issues\n-Q9. I’m planning to have a lunch appointment with my friend at noon. Would that be posssible?\n- answer\nNo, Actually it’s not possible, You have a lunch meeting with your clients, John Brown\n- Q9-1. As far as I know. I’m supposd to attend a staff meeting in the morning. Can you confirm that?\n- answer\nNo, actually, There is a staff meeting at 2 pm\n\n- Q10.I want to know about the things I have to do before noon. Can you tell me all the items scheduled before noon?\n- answer\nYes, First, at 10 am, there is a meeting with samuel Greene, sales director, next, at 11 am, there is a meeting with Shawna choi, regional(리지너) director.\n\n\nset 8\n\n- Q8. Which hotel wiil I be staying at, and what time do I arrive in Chicago on Wednesday, June 20th?\n- answer\nYou wiil stay at Glen Hotel, and You will arrive in Chicago at 10:30 am. on wednesday.\n- Q9. When I arrive in Chicao, do **I have to take a taxi to get to the hotel?\n- answer\nNo. You don’t have to, Actually, a rental car will be reserved at the airport on June 20th.\n\n- Q10. Can You tell me all the details of my schedule on Friday, June 22nd?\n- answer\nSure. First, you will depart from Chicago at 6 pm on Friday. You will take Los Angeles airlines. The flight number is 105,\nNext, You will arrive in San Antonio at 9:30 pm on Friday."
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#이력서",
    "href": "post/etc/toeic speaking/part4.html#이력서",
    "title": "part 4",
    "section": "이력서",
    "text": "이력서\n\n템플릿 1. 학력\n- He got a bachelor’s degree/master’s degree in 전공 from 학교 in 년도\n\n\n템플릿 2. 경력\n- From 시작일 to 종료일, He worked at 회사명 as 직책\n- From 시작일 up to now, He has worked at 회사명 as 직책\n\n\n템플릿 3. 기타 능력\n- I think he is qualified because ~\n- he is fluent in 언어명\n- He is certified(설디파이드) in 자격\n- he has a certificate(설티피킷).\n- he has experience in 분야\n\n\nset 9\n\n- Q8. What school did he get his master’s degree from, and what was his major?\n- answer\nhe got a master’s degree in computer enginering from the university of southern california in 2016.\n- Q9. I’d like to hire an applicant who understands software enginerring management. Do you think he is a qualified applicant?\n- answer\nYes. I think he is qualified because he is certified in software engineering management.\n\n- Q9-1. We are planning to work with Chinese companies next year and I want to hire an employee who can help us with that project. Is he qualified?\n- answer\nYes, I think he is qualified because he is fluent in Chinese.\n- Q10, we want to hire a highly experienced employee. What details can you tell me about his work experience?\n- answer\nYes, First, from 2017 to 2018, he woaked at Jefferson Software as an intern, Next, from 2018 until now, he has worked at STA Eletronics as Chief Engineer."
  },
  {
    "objectID": "post/etc/toeic speaking/part4.html#면접일정표",
    "href": "post/etc/toeic speaking/part4.html#면접일정표",
    "title": "part 4",
    "section": "면접일정표",
    "text": "면접일정표\n\n템플릿 1. 면접일정\n- You will interview 사람.\n- there is an interview with 사람 from 회사명 (Who is applying for 직책).\n- he has 5 years of experience.\n- he is applying for editor position.\n\n\nset 7\n\n- Q8. What date will the job interview be held, and where are they being held?\n- answer\nIt will be held on Monday, january 10th, and they will be held in metting room A\n- Q8-1. What time does the first inteview start and who am I interviewing?\n- answer\nThe first interview will start at 9 am. and you will interview Michael West from Smart Design.\n- Q9. I’m afraid that I have an urgent video conference with my client at 10 am. Would it be a problem?\n- answer\nFortuately(폴츄널리), there was supposed to be an interview with Aiden Thomas. but it has ben canceled. (So, it wouldn’t be a problem at all.)\n\n- Q9-1. I know that I’m supposed to have an interview with aiden thomas from creative minds. Am I right?\n- answer\nNo, actually, there was supposed to be an interview with Aiden Thomas. but it has ben canceled.\n- Q10. Can you give me all the details of applicants who are currently working at Smart Design?\n- answer\nThere are two applicants(어플리컨츠). First, at 9 am. there is an interview with Michael West. he is applying for web designer position. next, at 2 pm, there is an interview with Heater Stevenson. he is applying for Graphic Designer position.\n- Q10-1. Can you tell me about the details of the applicants who are applying for a web designer posution?\n- answer\nThere are two applicants. First, at 9 am. there is an interview with Michael West from Smart Design. Next, at 1 pm. there is an interview with Dylan Paley Desi from gn Master."
  }
]