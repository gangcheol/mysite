[
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n- 전북대학교 통계학과 학사(부전공: 컴퓨터공학) 졸업 | 2015. 03 ~ 2021. 02\n- 전북대학교 통계학과 석사 졸업예정 | 2021. 03 ~ 2023. 02"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n- 국민연금공단 빅데이터부 현장실습 | 2020. 03 ~ 2020. 06\n- 지역 문화산업 융복합 데이터 전문가 과정 | 과학기술정보통신부, 한국데이터산업진흥원 | 2021. 06 ~ 2021. 08\n- 에너지AI융합대학원 빅데이터 분석 특강 조교 | 전북대학교 AI에너지 융합대학원 |2021. 06 ~ 2021. 10\n- SPSS를 이용한 통계자료분석 특강 조교 | 전북대학교 통계학과 | 2022. 01 ~ 2022. 02\n- 데이터 준전문가 ADsP 특강 조교 | 전북대학교 통계학과 | 2022. 01 ~ 2022. 02"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About Me",
    "section": "Publications",
    "text": "Publications\n- 데이터 분석을 통한 지역별 고령친화도 시각화\n`-` 김영선, 강민구, 이강철  | 문화융복합아카이빙연구소 | 2021. 10 | 기록관리/보존 \n- 핵심어 추출 및 데이터 증강기법을 이용한 텍스트 분류 모델 성능 개선\n`-` 이강철, 안정용 | 한국자료분석학회 | 한국자료분석학회 | 2022. 10 | 통계학"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gang Cheol Portfolio",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 27, 2023\n\n\n1. 단순선형회귀\n\n\nGangcheol Lee\n\n\n\n\nJan 27, 2023\n\n\n0. Intro\n\n\nGANGCHEOL LEE\n\n\n\n\nJan 26, 2023\n\n\nQuarto test\n\n\nGANGCHEOL LEE\n\n\n\n\nMar 3, 2022\n\n\nbasic of linear regression\n\n\nGANGCHEOL LEE\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/R for the Data Scienece/2023-01-26-quarto test.html",
    "href": "post/R for the Data Scienece/2023-01-26-quarto test.html",
    "title": "Quarto test",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "post/R for the Data Scienece/2023-01-26-quarto test.html#데이터-구성-확인",
    "href": "post/R for the Data Scienece/2023-01-26-quarto test.html#데이터-구성-확인",
    "title": "Quarto test",
    "section": "2. 데이터 구성 확인",
    "text": "2. 데이터 구성 확인\n\nglimpse(iris)\n\nObservations: 150\nVariables: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4,...\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7,...\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5,...\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2,...\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa..."
  },
  {
    "objectID": "post/R for the Data Scienece/2023-01-26-quarto test.html#전처리-함수-확인",
    "href": "post/R for the Data Scienece/2023-01-26-quarto test.html#전처리-함수-확인",
    "title": "Quarto test",
    "section": "3. 전처리 함수 확인",
    "text": "3. 전처리 함수 확인\n\nglimpse(iris %>% filter(Sepal.Length <=5))\n\nObservations: 32\nVariables: 5\n$ Sepal.Length <dbl> 4.9, 4.7, 4.6, 5.0, 4.6, 5.0, 4.4, 4.9, 4.8, 4.8, 4.3,...\n$ Sepal.Width  <dbl> 3.0, 3.2, 3.1, 3.6, 3.4, 3.4, 2.9, 3.1, 3.4, 3.0, 3.0,...\n$ Petal.Length <dbl> 1.4, 1.3, 1.5, 1.4, 1.4, 1.5, 1.4, 1.5, 1.6, 1.4, 1.1,...\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.3, 0.2, 0.2, 0.1, 0.2, 0.1, 0.1,...\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa..."
  },
  {
    "objectID": "post/R for the Data Scienece/2023-01-26-quarto test.html#section",
    "href": "post/R for the Data Scienece/2023-01-26-quarto test.html#section",
    "title": "Quarto test",
    "section": "4",
    "text": "4"
  },
  {
    "objectID": "post/R for the Data Scienece/2023-01-26-quarto test.html#section-1",
    "href": "post/R for the Data Scienece/2023-01-26-quarto test.html#section-1",
    "title": "Quarto test",
    "section": "5",
    "text": "5"
  },
  {
    "objectID": "post/R for the Data Scienece/2023-01-26-quarto test.html#section-2",
    "href": "post/R for the Data Scienece/2023-01-26-quarto test.html#section-2",
    "title": "Quarto test",
    "section": "6",
    "text": "6"
  },
  {
    "objectID": "post/R for the Data Scienece/2022-03-03-선형회귀분석.html",
    "href": "post/R for the Data Scienece/2022-03-03-선형회귀분석.html",
    "title": "basic of linear regression",
    "section": "",
    "text": "변수들간의 인과관계를 밝히고 모형을 적합하여 관심 있는 변수를 예측하거나 추론하기 위해 사용하는 분석기법\n선형회귀분석의 가정\n\n오차의 등분산성\n오차의 독립성\n오차의 정규성 : Q-Q plot, Kolmogorov-Smirnov 검정, Shapiro-Wilk 검정을 확인하여 정규성을 확인한다.\n\n\n\n\n0. 회귀모형이 통계적으로 유의한가 확인\n1. 모형 내의 개별 회귀계수에 대한 검정\n2. 모형에 설명력 \\(R^2\\)값을 통해 확인, 독립변수의 수가 많아지면 \\(adj-R^2\\) 값을 확인\n3. 잔차 plot을 통해 모형의 진단\n4. 다중공선성의 확인 (10이상이면 다중공선성이 존재한다고 판단.) \\(\\to\\) car 패키지의 vif 함수 이용\n5. 잔차분석\n\n\n\nCars93 데이터의 엔진크기(EngineSize)를 독립변수, 가격(Price)를 종속변수로 선정하여 단순 선형회귀분석을 실시한 후, 추정된 회귀모형에 대해 해석해보자.\n\n#collapse-hide\nlibrary(MASS)\nlibrary(lmtest) ## 더비왓슨 테스트를 위함\nlibrary(tidyverse)\nselect <- dplyr::select\n\n\n#collapse-hide\nfit1 <- lm(Price~EngineSize,data=Cars93)\nsummary(fit1)\n\n\nCall:\nlm(formula = Price ~ EngineSize, data = Cars93)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.684  -4.627  -1.795   2.592  39.429 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.6692     2.2390   2.085   0.0398 *  \nEngineSize    5.5629     0.7828   7.107 2.59e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.789 on 91 degrees of freedom\nMultiple R-squared:  0.3569,    Adjusted R-squared:  0.3499 \nF-statistic: 50.51 on 1 and 91 DF,  p-value: 2.588e-10\n\n\n\n#collapse-hide\nplot(Cars93$EngineSize,Cars93$Price,lwd=2)\nabline(a=coefficients(fit1)[2],b=coefficients(fit1)[1],col=\"red\",lwd=2)\n\n\n\n\n\n#collapse-hide\npar(mfrow=c(1,2))\nplot(fit1,1); plot(fit1,2)\n\n\n\n\n\n#collapse-hide\nshapiro.test(resid(fit1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(fit1)\nW = 0.85365, p-value = 3.886e-08\n\n\n\n#collapse-hide\ndwtest(fit1,alternative=\"two.sided\")\n\n\n    Durbin-Watson test\n\ndata:  fit1\nDW = 1.1716, p-value = 2.236e-05\nalternative hypothesis: true autocorrelation is not 0\n\n\n1. 모형과 추정된 회귀계수는 모두 통계적으로 유의하다.\n2. 결정계수값과 수정된 결정계수 값이 각각 0.3569, 0.3499 로 산출되었다.\n3. F-통계량의 근거한 p-value값을 보아도 생성된 모델은 통계적으로 유의하다.\n4. 잔차 plot 을 그려본 결과 오차항의 정규성과 독립성 가정이 위배된 것 같다. * 실제로 test 결과 위배되었다는 결론이 통계적으로 유의미했다.\n5. 따라서 모형의 식별 단계로 돌아가 새로운 모형을 적합할 필요가 있어보인다.\n\n#collapse-hide\ntest <- Cars93 %>% select(EngineSize)  %>% sample_n(5)\n\n\n#collapse-hide\npredict(fit1,test,interval=\"none\") ##점추정\n\n123.0268912710567216.9076569678407324.1394793261868425.8083614088821523.5831852986217\n\n\n\n#collapse-hide\npredict(fit1,test,interval=\"confidence\") # 회귀계수에 대한 신뢰구간을 고려한 구간\npredict(fit1,test,interval=\"prediction\") # 회귀계수에 대한 신뢰구간과 오차항을 고려한 구간\n\n\n\nA matrix: 5 × 3 of type dbl\n\n    fitlwrupr\n\n\n    123.0268921.1453624.90842\n    216.9076615.1462318.66909\n    324.1394822.0783426.20061\n    425.8083623.4265328.19019\n    523.5831921.6159525.55043\n\n\n\n\n\n\nA matrix: 5 × 3 of type dbl\n\n    fitlwrupr\n\n\n    123.02689 7.44184638.61194\n    216.90766 1.33665432.47866\n    324.13948 8.53173239.74723\n    425.8083610.15503541.46169\n    523.58319 7.98756039.17881\n\n\n\n\n\n\n\n\niris 데이터를 사용\nR에 lm함수는 범주형 변수를 자동으로 더미변수로 변환해줌\n\n\n#collapse-hide\nfit2 <- lm(Petal.Length~.,data=iris)\nsummary(fit2) \n\n\nCall:\nlm(formula = Petal.Length ~ ., data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.78396 -0.15708  0.00193  0.14730  0.65418 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -1.11099    0.26987  -4.117 6.45e-05 ***\nSepal.Length       0.60801    0.05024  12.101  < 2e-16 ***\nSepal.Width       -0.18052    0.08036  -2.246   0.0262 *  \nPetal.Width        0.60222    0.12144   4.959 1.97e-06 ***\nSpeciesversicolor  1.46337    0.17345   8.437 3.14e-14 ***\nSpeciesvirginica   1.97422    0.24480   8.065 2.60e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2627 on 144 degrees of freedom\nMultiple R-squared:  0.9786,    Adjusted R-squared:  0.9778 \nF-statistic:  1317 on 5 and 144 DF,  p-value: < 2.2e-16\n\n\n\n#collapse-hide\ndwtest(fit2,alternative=\"two.sided\")\n\n\n    Durbin-Watson test\n\ndata:  fit2\nDW = 1.6772, p-value = 0.03042\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n#collapse-hide\nshapiro.test(resid(fit2))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(fit2)\nW = 0.99389, p-value = 0.78\n\n\n\n#collapse-hide\nlibrary(car)\nvif(fit2)\n\n\n\nA matrix: 4 × 3 of type dbl\n\n    GVIFDfGVIF^(1/(2*Df))\n\n\n    Sepal.Length 3.73670511.933056\n    Sepal.Width 2.64812711.627307\n    Petal.Width18.49697314.300811\n    Species28.55141622.311569\n\n\n\n\n\n\n\n\n\n\n1. 전진 선택법 (forward selection) : 절편만 있는 상수모형에서 시작하여 중요하다고 생각되는 설명변수부터 차례로 추가한다.\n2. 후진 제거법 (backward elimination) : 모든 독립변수를 포함한 모형에서 출발하여 종속변수에 가장 적은 영향을 주는 변수부터 하나씩 제거하면서 더 이상 제거할 변수가 없을 때의 모형을 선택한다.\n3. 단계적 방법 (stepwise method) : 전진선택법에 의해 변수를 추가하면서 새롭게 추가된 변수에 의해 기존 변수의 중요도가 약화되면 해당변수를 제거한다.\n\n\n\n\n모형의 복잡도에 따라 벌점을 주는 방식으로 \\(AIC, BIC\\) 값이 주로 사용된다.\n\n\n\n\n\n#collapse-hide\nfit3 <- step(lm(Price~ EngineSize +Horsepower +RPM + Width + Length + Weight,Cars93),direction = \"both\")\nsummary(fit3)\n\nStart:  AIC=322.11\nPrice ~ EngineSize + Horsepower + RPM + Width + Length + Weight\n\n             Df Sum of Sq    RSS    AIC\n- EngineSize  1      1.69 2556.1 320.17\n- RPM         1     19.71 2574.1 320.82\n<none>                    2554.4 322.11\n- Length      1    119.55 2674.0 324.36\n- Weight      1    209.73 2764.2 327.45\n- Width       1    585.01 3139.4 339.29\n- Horsepower  1    720.84 3275.3 343.22\n\nStep:  AIC=320.17\nPrice ~ Horsepower + RPM + Width + Length + Weight\n\n             Df Sum of Sq    RSS    AIC\n- RPM         1     49.36 2605.5 319.95\n<none>                    2556.1 320.17\n+ EngineSize  1      1.69 2554.4 322.11\n- Length      1    140.92 2697.0 323.16\n- Weight      1    208.09 2764.2 325.45\n- Width       1    593.56 3149.7 337.59\n- Horsepower  1   1476.65 4032.8 360.57\n\nStep:  AIC=319.95\nPrice ~ Horsepower + Width + Length + Weight\n\n             Df Sum of Sq    RSS    AIC\n<none>                    2605.5 319.95\n+ RPM         1     49.36 2556.1 320.17\n+ EngineSize  1     31.34 2574.1 320.82\n- Length      1    132.02 2737.5 322.54\n- Weight      1    279.31 2884.8 327.42\n- Width       1    562.10 3167.6 336.12\n- Horsepower  1   1898.74 4504.2 368.86\n\n\n\nCall:\nlm(formula = Price ~ Horsepower + Width + Length + Weight, data = Cars93)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.956  -2.578  -0.182   2.114  28.448 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 53.005861  16.532269   3.206  0.00188 ** \nHorsepower   0.129653   0.016190   8.008 4.46e-12 ***\nWidth       -1.480623   0.339813  -4.357 3.56e-05 ***\nLength       0.152968   0.072440   2.112  0.03755 *  \nWeight       0.007339   0.002389   3.071  0.00283 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.441 on 88 degrees of freedom\nMultiple R-squared:  0.6965,    Adjusted R-squared:  0.6827 \nF-statistic: 50.48 on 4 and 88 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "post/ML with pytorch/2022-01-27-Intro.html",
    "href": "post/ML with pytorch/2022-01-27-Intro.html",
    "title": "0. Intro",
    "section": "",
    "text": "from fastai.vision.all import *\n\n\n\n\n옆에 폴더에 보면 저장된 데이터들을 확인해볼 수 있다!\n안보이는 파일들은 리눅스처럼 숨겨진 파일들을 의미한다.\n\n\nuntar_data(URLs.PETS)/'images'\n\n\n\n\n\n\n    \n      \n      100.00% [811712512/811706944 00:11<00:00]\n    \n    \n\n\nPath('/root/.fastai/data/oxford-iiit-pet/images')\n\n\n\n위의 결과를 보면 해당 경로에 이미지를 저장한 것이다!\n즉, 위의 명령은 URL에서 데이터를 불러오고 해당경로에 데이터를 저장\n\n\nURLs.PETS\n\n'https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz'\n\n\n\n해당 경로를 복사하고 붙여 넣으면 자동으로 다운되는 것이 있는데 untar_data은 압축파일을 해제 후 images 폴더에 저장된다.\n경로로 가보면 압축이 해제되고 이미지 파일이 저장된 것을 볼 수 있으나 열어볼 수는 없음…..\n보고싶다면 아래와 같은 method를 이용하자\n\n\nPILImage.create('/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_1.jpg')\n\n\n\n\n\nPILImage.create('/root/.fastai/data/oxford-iiit-pet/images/Abyssinian_100.jpg')\n\n\n\n\n\n위 같은 과정이 그렇게 편하진 않은 것 같다.\n\n- 조금 더 쉬운 방법 * 파일들 이름의 리스트로 저장한다면?\n\npath = Path('/root/.fastai/data/oxford-iiit-pet/images')\n\n\nfiles = get_image_files(path)\n\n\nfiles[0]\n\nPath('/root/.fastai/data/oxford-iiit-pet/images/pomeranian_9.jpg')\n\n\n\nPILImage.create('/root/.fastai/data/oxford-iiit-pet/images/saint_bernard_134.jpg')\n\n\n\n\n\nPILImage.create(files[0])\n\n\n\n\n\nprint(files[2])\nPILImage.create(files[2])\n\n/root/.fastai/data/oxford-iiit-pet/images/shiba_inu_187.jpg\n\n\n\n\n\n- 해당 데이터는 앞글자가 대문자면 고양이, 소문자면 강아지이다.\n- 특1 : 앞글자가 대문자면 고양이, 소문자면 강아지 \\(\\to\\) 창의적인 방법임\n- 특2 : 이미지크기가 서로 다르다…\n- 특징 1을 이용해서 한번 강아지와 고양이를 분류해보자\n\n\n\n\ndef label_func(fname) :\n    if fname[0].isupper() :\n      return \"cat\"\n    else : \n      return \"dog\"\n\n\nlabel_func(\"Egyptian_Mau_120.jpg\")\n\n'cat'\n\n\n\nlabel_func(\"egyptian_Mau_120.jpg\")\n\n'dog'\n\n\n\ndls = ImageDataLoaders.from_name_func(path,files,label_func,item_tfms=Resize(224))\n\n\npath 경로에서 files에 해당하는 파일들을 불러와서 \\(X\\)를 만들고 정의한 함수 label_func를 적용해 item_tfms에 정의된 방식으로 \\(X\\)를 변환하여 저장한다.\n\n\ndls.show_batch(max_n=8) ## 8개의 그림만보자\n\n\n\n\n\n\n- 1차 목표 : 이미지 파일을 받으면 -> 개인지 고양이인지 판단하는 모형을 만들자\n- 2차 목표 : 그 모형에 새로운 이미지 파일을 전달하여 이미지를 분류할 것이다.\n- cnn_learner 라는 함수를 이용해서 1차 목표와 2차목표를 달성할 object를 만들것임.\n\nclsfr = cnn_learner(dls,resnet34, metrics = error_rate)\n\n/usr/local/lib/python3.8/dist-packages/fastai/vision/learner.py:288: UserWarning: `cnn_learner` has been renamed to `vision_learner` -- please update your code\n  warn(\"`cnn_learner` has been renamed to `vision_learner` -- please update your code\")\n/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n\n\n\n\n\n- clsfr 필요한 정보 : 우리가 넣어줘야하는 것들이 대부분\n\n모델정보 : 어떠한 분류기를 사용할 것인가\n데이터\n평가기준표 : 채점을 할지표\n\n- clsfr에 필요한 동작 : 이미 구현이 되어있는 것들을 사용\n\n학습\n결과\n예측\n\n- cpu를 써서 학습을 하니 너무 오래 걸리니 런타임 유형을 gpu로 바꿔서 하자\n\nclsfr.fine_tune(1) # 한번 학습\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.149759\n      0.025045\n      0.006766\n      00:16\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.049579\n      0.015034\n      0.004060\n      00:10\n    \n  \n\n\n\n\n\n- 첫 번째 원소는 고양이일 확률, 두 번째 원소는 강아지일 확률 - TensorBase : 고양이면 1. 강아지면 0\n\nclsfr.predict(files[0])\n\n\n\n\n\n\n\n\n('dog', TensorBase(1), TensorBase([1.0673e-07, 1.0000e+00]))\n\n\n\nclsfr.predict(files[7])\n\n\n\n\n\n\n\n\n('cat', TensorBase(0), TensorBase([9.9998e-01, 1.7946e-05]))\n\n\n- 랜덤으로 예측결과를 살펴보자\n\nclsfr.show_results()\n\n\n\n\n\n\n\n\n\n\n\n\n\n- 오답을 분석하는 오브젝트를 만들고 그 안에 학습한 clsfr을 집어넣자.\n\ninterpreter = Interpretation.from_learner(clsfr) \n\n\n\n\n\n\n\n\n- 오답중 loss가 큰 상위 9개만 살펴보자\n\ninterpreter.plot_top_losses(9)\n\n\n\n\n\n\n\n\n\n\n\n- 근데 모형이 과적합되어서 학습한거면?? \\(\\to\\) 약간 가슴깊이 의구심이 든다.\n\n\n\n\n\ntest_file = get_image_files('/content')\n\n\ntest_file\n\n(#10) [Path('/content/pexels-fabian-köhler-14652203.jpg'),Path('/content/pexels-brett-sayles-15075137.jpg'),Path('/content/pexels-pixabay-57416.jpg'),Path('/content/pexels-vadim-b-127028.jpg'),Path('/content/pexels-peng-louis-1643457.jpg'),Path('/content/pexels-poodles-doodles-1458926.jpg'),Path('/content/pexels-pixabay-416160.jpg'),Path('/content/pexels-pixabay-45201.jpg'),Path('/content/pexels-poodles-doodles-1458914.jpg'),Path('/content/pexels-batitay-japheth-14308789.jpg')]\n\n\n\nPILImage.create(test_file[0])\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\ntest_file\n\n(#10) [Path('/content/pexels-fabian-köhler-14652203.jpg'),Path('/content/pexels-brett-sayles-15075137.jpg'),Path('/content/pexels-pixabay-57416.jpg'),Path('/content/pexels-vadim-b-127028.jpg'),Path('/content/pexels-peng-louis-1643457.jpg'),Path('/content/pexels-poodles-doodles-1458926.jpg'),Path('/content/pexels-pixabay-416160.jpg'),Path('/content/pexels-pixabay-45201.jpg'),Path('/content/pexels-poodles-doodles-1458914.jpg'),Path('/content/pexels-batitay-japheth-14308789.jpg')]\n\n\n\nPILImage.create(test_file[6])\n\nOutput hidden; open in https://colab.research.google.com to view.\n\n\n\nclsfr.predict(test_file[0])\n\n\n\n\n\n\n\n\n('dog', TensorBase(1), TensorBase([5.4642e-05, 9.9995e-01]))\n\n\n\nclsfr.predict(test_file[6])\n\n\n\n\n\n\n\n\n('cat', TensorBase(0), TensorBase([9.9993e-01, 6.6712e-05]))\n\n\n- 뭐 어느정도는? test 데이터에 대해서도 잘 분류를 하는것 같다!"
  },
  {
    "objectID": "about.html#conctact",
    "href": "about.html#conctact",
    "title": "About Me",
    "section": "Conctact",
    "text": "Conctact\n- rkdcjf8232@gmail.com"
  },
  {
    "objectID": "post/BDA/2022-03-08-(1주차).html#모형의-매트릭스화",
    "href": "post/BDA/2022-03-08-(1주차).html#모형의-매트릭스화",
    "title": "1. 단순선형회귀",
    "section": "모형의 매트릭스화",
    "text": "모형의 매트릭스화\n\n모형을 행렬로 표현하면 변수가 여러개인 multiple linear regression 에서도 단순형태로 표현이 가능하다.\n\n우리의 모형은 아래와 같다.\n\\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad i=1,2,\\dots,10\\)\n풀어서 쓰면\n\\(\\begin{cases} y_1 = \\beta_0 +\\beta_1 x_1 + \\epsilon_1 \\\\ y_2 = \\beta_0 +\\beta_1 x_2 + \\epsilon_2 \\\\ \\dots \\\\ y_{10} = \\beta_0 +\\beta_1 x_{10} + \\epsilon_{10} \\end{cases}\\)\n아래와 같이 쓸 수 있다.\n$\n\\[\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\dots \\\\\ny_{10}\n\\end{bmatrix}\\]\n=\n\\[\\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\dots & \\dots \\\\\n1 & x_{10}\n\\end{bmatrix}\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix}\\]\n\n\\[\\begin{bmatrix}\n\\epsilon_1 \\\\\n\\epsilon_2 \\\\\n\\dots \\\\\n\\epsilon_{10}\n\\end{bmatrix}\\]\n$\n\n벡터와 매트릭스 형태로 정리하면\n\\({\\bf y} = {\\bf X} {\\boldsymbol \\beta} + \\boldsymbol{\\epsilon}\\)\n- 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다.\n\\(loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2\\)\n이것을 벡터표현으로 하면 아래와 같다.\n\\(loss=\\sum_{i=1}^{n}(y_i-\\beta_0-\\beta_1x_i)^2=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\)\n풀어보면\n\\(loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})={\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\)\n- 미분하는 과정의 매트릭스화\nloss를 최소화하는 \\({\\boldsymbol \\beta}\\)를 구해야하므로 loss를 \\({\\boldsymbol \\beta}\\)로 미분한식을 0이라고 놓고 풀면 된다.\n\\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} loss = \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\)\n$= 0 - {}^- {}^ + 2{}^ $\n따라서 \\(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}}loss=0\\)을 풀면 아래와 같다.\n$= ({}){-1}{}^ $\n- 공식도 매트릭스로 표현하면 : \\(\\left(\\boldsymbol{\\hat\\beta}= ({\\bf X}^\\top {\\bf X})^{-1}{\\bf X}^\\top {\\bf y}\\right) \\leftarrow\\) 외우자 이건..\n- 적용을 해보자.\n(X를 만드는 방법1)\n\nX=tf.transpose(tf.concat([[[1.0]*10],[x]],0)) # \nX \n\n<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[ 1. , 20.1],\n       [ 1. , 22.2],\n       [ 1. , 22.7],\n       [ 1. , 23.3],\n       [ 1. , 24.4],\n       [ 1. , 25.1],\n       [ 1. , 26.2],\n       [ 1. , 27.3],\n       [ 1. , 28.4],\n       [ 1. , 30.4]], dtype=float32)>\n\n\n(X를 만드는 방법2)\n\ntf.concat([[[1.0]*10],[x]],0)\n\n<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\narray([[ 1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ,  1. ],\n       [20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]],\n      dtype=float32)>\n\n\n\ntf.concat([[[1.0]*10],[x]],0).T\n\nAttributeError: \n        'EagerTensor' object has no attribute 'T'.\n        If you are looking for numpy-related methods, please run the following:\n        from tensorflow.python.ops.numpy_ops import np_config\n        np_config.enable_numpy_behavior()\n\n\n\n위 처럼 하면 error가 남\nerror를 읽어보면 numpy 스타일로 구성하고 싶을 경우 np_config.enable_numpy_behavior() 을 이용하라는 문구가 나옴\n\n\nfrom tensorflow.python.ops.numpy_ops import np_config\nnp_config.enable_numpy_behavior()\n\n\nX = tf.concat([[[1.0]*10],[x]],0).T\n\n오 이제된다.\n\nX\n\n<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[ 1. , 20.1],\n       [ 1. , 22.2],\n       [ 1. , 22.7],\n       [ 1. , 23.3],\n       [ 1. , 24.4],\n       [ 1. , 25.1],\n       [ 1. , 26.2],\n       [ 1. , 27.3],\n       [ 1. , 28.4],\n       [ 1. , 30.4]], dtype=float32)>\n\n\n\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([9.945015 , 2.2156935], dtype=float32)>\n\n\n결과를 보면 $ (_0,_1) = (9.94…,2.21…)$ 로 산출되었다.\n- 잘 구해진다.\n- 그런데..\n\nbeta0_estimated,beta1_estimated\n\n(<tf.Tensor: shape=(), dtype=float32, numpy=9.94458>,\n <tf.Tensor: shape=(), dtype=float32, numpy=2.2157042>)\n\n\n값이 좀 다르다..?\n- 같은 값입니다! 신경쓰지 마세요! 텐서플로우가 좀 대충계산합니다.\n- 실제로 조금 더 정확히 계산하기 위해서는 tensorflow 안에 내장된 numpy 를 사용한다.\n\nimport tensorflow.experimental.numpy as tnp \n\n\nx=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) \ny=10.2 + 2.2*x + epsilon \n\n\nbeta1_estimated = sum((x-sum(x)/10)*(y-sum(y)/10)) / sum((x-sum(x)/10)**2)\nbeta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 \n\n\nbeta0_estimated, beta1_estimated\n\n(<tf.Tensor: shape=(), dtype=float64, numpy=9.944573294798559>,\n <tf.Tensor: shape=(), dtype=float64, numpy=2.2157046054834106>)\n\n\n\nX=tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T\ntf.linalg.inv(X.T @ X) @ X.T @ y\n\n<tf.Tensor: shape=(2,), dtype=float64, numpy=array([9.94457329, 2.21570461])>"
  },
  {
    "objectID": "post/BDA/2022-03-08-(1주차).html#회귀계수-추정",
    "href": "post/BDA/2022-03-08-(1주차).html#회귀계수-추정",
    "title": "1. 단순선형회귀",
    "section": "회귀계수 추정",
    "text": "회귀계수 추정\n\\[Loss = \\sum (y-\\beta_0-\\beta_1 x)\\]\n\n\\(\\beta_0\\) 추정\n\n\\[L=Loss = \\sum (y-\\beta_0-\\beta_1 x)\\]\n\\[\\frac {d L}{d \\beta_0} = -2\\sum(y-\\beta_0-\\beta_1x) = 0\\]\n\\[\\therefore\\,\\,\\hat \\beta_0 = \\bar y -\\beta_1 \\bar x\\]\n\n\\(\\beta_1\\) 추정\n\n\\[\\begin{align}\\frac {d L}{d \\beta_1} &= \\sum x \\left(y-\\beta_0-\\beta_1x\\right) \\\\ \\\\ \\nonumber\n                                        & = \\left (\\sum xy -\\beta_1 x^2\\right )- n\\bar x\\left (\\bar y -\\beta_1\\bar x \\right) \\\\ \\\\ \\nonumber\n                                         &= \\left (\\sum xy -\\bar x \\bar y\\right ) -\\beta_1 \\left (\\sum x^2 - (\\bar x)^2\\right )\\nonumber\\end{align}\\]\n\\[\\begin{align}\n\\therefore \\hat {\\beta_1} &=  \\frac{\\sum xy -\\bar x \\bar y}{\\sum x^2 - (\\bar x)^2}  \\nonumber \\\\ \\\\\n&= \\frac {S_{xy}}{S_{xx}}  \\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "post/BDA/2022-03-08-(1주차).html#벡터-미분-매트릭스-미분",
    "href": "post/BDA/2022-03-08-(1주차).html#벡터-미분-매트릭스-미분",
    "title": "1. 단순선형회귀",
    "section": "벡터 미분 / 매트릭스 미분",
    "text": "벡터 미분 / 매트릭스 미분\n\\[L=loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\]\n\n벡터 미분\n\\[\\begin{align}  x^{\\top}y &= \\begin{bmatrix}x_1\\dots x_n\\end{bmatrix}\\begin{bmatrix} y_1   \\\\ \\dots \\\\ y_n\\end{bmatrix}\\nonumber  \\\\  \\\\\n      &=  x_1y_1 + x_2y_2+\\dots x_ny_n\\nonumber  \\end{align}\\]\n위를 미분하면\n\\[\\begin{align} \\frac {d x^{\\top}y}{d x} &= \\, \\begin{bmatrix} \\frac {d}{d x_1}  \\\\\n                                                                                       \\dots \\\\ \\frac {d}{d x_n}\n                                                                                        \\end{bmatrix} ( x_1y_1 + x_2y_2+\\dots x_ny_n) \\nonumber \\\\ \\\\  \n                                                         &=  \\, \\begin{bmatrix} y_1   \\\\ \\dots \\\\ y_n\\end{bmatrix}=y                         \\nonumber  \\end{align}\\]\n\n\n벡터 미분의 다른풀이\n\n(1)\n\\[ \\frac {d x^{\\top}y}{d x} = \\left (\\frac {d x^{\\top}}{d x}\\right) y=y\\]\n\\[\\begin{align}\\left( \\frac {d x^{\\top}}{d x} \\right)  = \\begin{bmatrix} \\frac {d}{d x_1} \\\\  \\dots \\\\ \\frac {d}{d x_{n}} \\end {bmatrix} \\begin{bmatrix} x_1 \\dots x_n \\end{bmatrix} =\\begin{bmatrix}\\frac {d x_1}{d x_1} & \\dots & \\frac {d x_n}{d x_1} \\\\\n                                                                         \\dots & \\dots  &\\dots  \\\\\n                                                                        \\frac {d x_1}{d x_n} & \\dots & \\frac {d x_n}{d x_n}     \\end{bmatrix} = \\mathbf{I} \\nonumber \\end{align}\\]\n\\[\\therefore \\quad \\frac {d x^{\\top}y}{d x} = \\left (\\frac {d x^{\\top}}{d x}\\right) y = \\mathbf{I} y = y\\]\n\n\n(2)\n\\[ \\frac {d y^{\\top}x}{d x} = \\left (\\frac {d y^{\\top}x}{d x}\\right) =y \\]\n$ y^{} x $는 \\(1 \\times 1\\) 차원이므로 인간이면 이해할 수 있을 듯?\n\n\n(3)\n\\[\\frac {d}{d \\boldsymbol{\\beta}} \\left ( \\mathbf{y^{\\top}X\\boldsymbol \\beta}\\right ) = \\mathbf{X^{\\top}y}\\]\n\\(\\mathbf{y^{\\top}X\\boldsymbol \\beta}\\) 는 \\(1 \\times 1\\) 인 스칼라 이므로\n\\[\\mathbf{y^{\\top}X\\boldsymbol \\beta} = \\left (\\mathbf{y^{\\top}X\\boldsymbol \\beta}\\right)^{\\top} = \\boldsymbol \\beta^{\\top}\\mathbf{X^{\\top} y}\\]\n따라서\n\\[\\begin{align} \\frac {d}{d \\boldsymbol{\\beta}} \\left ( \\mathbf{y^{\\top}X\\boldsymbol \\beta}\\right ) &= \\frac {d}{d \\boldsymbol{\\beta}} \\left (\\boldsymbol \\beta^{\\top}\\mathbf{X^{\\top} y}\\right )\\nonumber \\\\ \\\\\n                                                  &= \\left ( \\frac{d}{d \\boldsymbol{\\beta}}\\boldsymbol{\\beta}^{\\top}\\right)\\mathbf{X^{\\top y}} \\nonumber\\\\ \\\\\n                                                    &= \\mathbf{I\\,X^{\\top}y} \\nonumber \\\\ \\\\ &= \\mathbf{X^{\\top}y}    \\nonumber      \\end{align}\\]\n\n\n(4)\n\\[ \\frac {d }{d \\mathbf {y}} (\\mathbf{y^{\\top}y}) = 2\\mathbf{y}\\]\n\\[ d\\, \\mathbf{y} = \\left [\\frac {d}{y_1}, \\,\\frac {d}{y_2}\\dots\\dots \\frac {d}{y_n}\\right ]\\]\n\\[\\mathbf{y^{\\top}y} =  \\sum{y_i}^2\\]\n\\[\\therefore  \\quad \\frac {d }{d \\mathbf {y}} (\\mathbf{y^{\\top}y}) = 2\\mathbf{y}\\]\n\n\n참고할 틀린풀이\n\n아래와 같은 풀이는 1번의 벡터 미분의 다른 풀이처럼 풀면 안된다.\n\n\\[ \\frac {d }{d \\mathbf {y}} (\\mathbf{y^{\\top}y}) = \\mathbf{y}\\]\n\\[ \\frac {d\\,\\mathbf{y^{\\top}y}}{d \\,\\mathbf{y}} = \\left (\\frac {d\\, \\mathbf{y^{\\top}y}}{d\\, \\mathbf y}\\right) \\mathbf y= \\mathbf{I} y \\neq y\\]\nbecause 스칼라 경우를 생각해보자\n(틀린풀이 )\\(\\quad \\frac {d }{d\\,y} y^2 = \\left (\\frac {d}{d\\,y } yy \\right) = y\\) ?\n(올바른 풀이) \\(\\quad \\frac {d }{d\\,y} y^2 = \\left (\\frac {d}{d\\,y } y_1 \\right)+ \\left (\\frac {d}{d\\,y } y_2 \\right) = 2y\\)\n스칼라를 예제로 들었는데 벡터에서 이런 느낌이라고 생각하자 이게 표준적으로 사용되는 설명은 아니지만 이해적? 으로는 간편한 듯\n다른풀이 (3)번의 경우도 원래는 안되는데 값이 스칼라 이므로 틀린풀이 처럼 안되는 경우이나 1 x 1 행렬이므로 가능한 것이다.\n다시 벡터로 돌아오면\n(올바른 풀이) \\(\\quad \\left ( \\frac {d}{d\\,\\mathbf{y}} (\\mathbf{y^{\\top}y})\\right ) = A + B\\)\n\\(A = f(\\mathbf y)\\) , \\(B = g(\\mathbf y)\\) 라고 생각하자\n$ A =( ) = = $\n\\(B\\) 의 경우도 위와 동일하므로\n\\[\\frac {d\\,(A+B)}{d\\,\\mathbf{y}} = \\{f(\\mathbf {y})\\}^{\\prime} +\\{g(\\mathbf {y})\\}^{\\prime} = 2\\mathbf{y}\\]\n\n\n(5)\n\\[\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta} = 2\\mathbf{X^{\\top}X}\\boldsymbol \\beta\\]\n4번의 원리를 이용하면 이지이지"
  },
  {
    "objectID": "post/BDA/2022-03-08-(1주차).html#loss를-미분",
    "href": "post/BDA/2022-03-08-(1주차).html#loss를-미분",
    "title": "1. 단순선형회귀",
    "section": "loss를 미분",
    "text": "loss를 미분\n\\[L=loss=({\\bf y}-{\\bf X}{\\boldsymbol \\beta})^\\top({\\bf y}-{\\bf X}{\\boldsymbol \\beta})\\]\n\\[L = {\\bf y}^\\top {\\bf y} - {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta}\\]\n\\(L\\) 을 미분하면\n\\[\\begin{align} \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} L &= \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf y} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\bf y}^\\top {\\bf X}{\\boldsymbol\\beta} - \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf y} + \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} {\\boldsymbol\\beta}^\\top {\\bf X}^\\top {\\bf X} {\\boldsymbol\\beta} \\nonumber \\\\ \\\\\n            &=  0 - \\mathbf{X^{\\top}y} - \\mathbf{X^{\\top}y} + 2\\mathbf{X^{\\top}X\\boldsymbol \\beta}\\nonumber \\end{align}\\]\n따라서 아래와 같은식이 성립한다.\n\\[  \\mathbf{X^{\\top}y}= \\mathbf{X^{\\top} X} \\boldsymbol \\beta \\]\n\\[\\hat {\\boldsymbol \\beta} = \\mathbf{\\left (X^{\\top}X\\right)^{-1}Xy}  \\]"
  }
]